### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/001_Udemy - Become an AWS Certified Data Engineer part1 p01 1. Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以大多数人应该都听过这句话
现在数据分析师和大数据
我说啊 这个行业谈论大数据多久了
大数据本身
感觉像是永远在说对吧
你可能看过《黑客帝国》或者参加过关于大数据的会议
事实上你可能被问过你的组织是否有大数据问题
这显然是个简化
真正的问题是你的业务需要哪种数据分析方案
换句话说
应该如何存储
处理 从采集到展示全流程分析数据
云计算的核心就是快速处理大规模数据
配置简单
最高安全性和低成本
AWS认证数据工程师
助理级考试
验证实施数据管道的能力
根据最佳实践监控、排查并优化成本和性能问题
嗨 我叫萨德
我将担任本课程讲师
我在培训行业从业超过二十年
作为资深IT工程师
企业架构师、DevOps工程师
数据工程师，持有跨平台行业认证
我将传授实用知识和概念
助你通过AWS认证数据工程师考试
让我介绍一下考试内容
AWS认证数据工程师助理级考试验证你的任务执行能力
例如数据采集与转换
调度数据管道
应用编程概念
选择最优数据存储
设计数据模型
编目数据模式
管理数据生命周期
实现数据工程化
维护监控管道
等等
这个领域涵盖内容很多
我将逐一讲解这些领域
助你掌握考试所需技能
所以 例如课程大纲和主要领域
让我带你们了解这些
考试包含四大领域
首先是数据采集与转换
这大约占三 考试本身的四百分比
我将讲解数据摄取模式
例如频率数据历史
流数据摄取
批量数据摄取
例如调度摄取
以及如何处理数据的状态化和无状态事务
对的 从流源读取数据
例如亚马逊
Kinesis Dynamo
Db流 Aws
数据库迁移服务和Aws
SDMS和Glue
Athena 等等 将在本领域涵盖的众多服务领域
这是第一个领域
涵盖约34%的考试内容
领域二为数据存储管理
涵盖约26%的内容
对的 在本领域我将讲解存储平台
特性
针对特定性能需求的存储服务与配置
对的 我们的存储格式如
例如CSV文件
TFT文件
如何将数据存储与迁移需求对齐
如何确定特定操作模式的合适方案
其他涵盖内容包括如何管理日志
例如亚马逊Redshift
RDS 等等更多
领域三是数据操作与支持
约占22%的内容
在本领域我将讲解编排数据管道
例如Amazon MWa Step Functions
故障排查
管理工作流
通过代码调用SDK和处理数据的其他Aws服务
例如Amazon EMR
Amazon Redshift Aws
Glue 依此类推
第四域是数据安全与治理
目前该领域内容占比约十八%
我将讲解VPC安全
网络概念
呃 托管与非托管服务的区别
对的 身份验证
加密安全
等等 所有这些都在最后一个领域涵盖
即第四域
没错 客户管理政策
还有更多，我将逐一实践讲解每个领域
注重步骤分解
传授实操技能
助你通过考试
同时通过这些实操技能积累宝贵经验
我为初学者至中级学员设计了此课程
已有几年AWS平台工作经验
没错，如果你从事云计算或想成为AWS认证数据工程师
此课程适合你
如果你已是认证持有者
通过本课程你将获得更多见解
还在等什么
如果你想成为AWS认证数据工程师
立即点击报名 我们课堂见
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/002_Udemy - Become an AWS Certified Data Engineer part1 p02 2. Course Structure and Agenda.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
让我们谈谈课程结构，当然还有详细的议程
因为我将主要聚焦于
并且专注于考试主题
这样你不仅能通过考试
当然还能获得概念性和实践性的知识并掌握实用技能
所以 但在深入课程领域之前
我们先明确课程期望
让我先谈谈未涵盖的内容
好的现在
所有与考试数据工程部分相关的核心服务
以及AWS工具和服务都会覆盖到
我将讲解基础知识
这些概念
以及实践操作
所有属于该考试及领域范围内的服务
但考试中可能不会涵盖许多其他服务
尽管它们可能相关
但由于我的重点在考试主题并传授实用技能
我会尽量专注于课程领域
这些属于本课程大纲的部分
练习考试题目
是的 我会在课程结尾提供这些
你可以前往练习考试章节
查看模拟真实考试题目的不同题型
至少是这样
好的 这让你了解如何
你知道如何练习 不仅练习考试题目
在直播考试中也能轻松应对
然后是AWS工具和服务的基础知识
是的 你应具备AWS生态系统的知识
并能使用基础工具和服务
好的 如果我是新手
对 如果你是第一次
你知道加入这个课程
参加这个课程
且对云计算或可能有挑战
但我已经添加了多个课程
帮助你快速入门
如果你技术熟练
或对计算、存储有一定基础
例如或网络
那就太好了
你可以跳进去，不仅跳进去
还能跟上考试中所有领域的内容
好的 现在我想讲课程要求
让我们深入探讨考试的核心领域
AWS认证数据工程师助理DEA CO1考试指南没错
这主要验证候选人实施数据管道的能力
我们所说的数 据管道是什么意思
简单来说就是数据从采集
一直到可视化的过程
整个数据生命周期
以及你需要的所有工具和服务
这样才能高效监控
管理 创建数据管道
不仅如此还能排查和优化性能问题
好的 说到这里
让我们跳转到推荐的通用
技术知识 现在好了
当然你需要至少一两年的经验
如果你没有
我添加了几个讲座
你就能快速补足差距
如果你是新手 完全没问题
你应该能完成整个课程
因为课程内容相当详尽
好的 所以
但无论如何你应该具备基本的Git命令知识
例如 网络存储的基础概念
计算一些ETL流程
如果你有相关知识 否则课程中也会详细讲解这些内容
AWS的建议是
当然你应该掌握
你知道的 例如 如何使用AWS服务或了解AWS工具和服务
并掌握运行SQL查询所需的基础知识
基本的查询技能
如何创建
更新 分析数据
对吧
现在考试内容
让我们简要了解
这其实很简单
多项选择题 多选题
没问题 它们非常基础
大约有
我们可以假设 你需要大约七百二十分才能通过
评分标准是从百分制到千分制
最低通过分数正如我说的是七百二十
所以其实并不难
而且是测试版
所以价格减半
你应该能获得一些好处
没错就这样
让我们直接进入核心内容
本次考试涵盖的四个领域
领域一是数据采集与转换
约占
百分之三至四
领域二是数据存储管理
占比二十六
这里可以看到超过百分之五十
没错 或接近百分之六十的内容在领域一和二
领域三是数据操作与支持
占比百分之二十二足以通过考试
没错 不过
我们继续前进
现在检查领域四
数据安全与治理
占十八百分比的考试内容
当你完成整个课程后
掌握概念并实践操作
你将对这些领域有扎实的理解
每个领域都有逻辑连贯的课程讲解
因此 例如
从数据采集与转换开始
即领域一
理解吞吐量和延迟特性
一直到数据采集模式
流数据与批量数据
有状态与无状态数据事务
没错 我将在领域一深入讲解这些内容
当然你也会获得实用技能
比如在dynamodb或了解这些服务
如果你是新手 这是你学习的机会
例如 亚马逊Glue
亚马逊Redshift EMR DMS
Lambda Airflow等等
所以我将涵盖所有内容并给予扎实的理解
当然还有实操演示和实验
这样你就能快速跟上进度
当然这会提供该指南的深入分析
我将在资源部分提供这份指南
你也可以下载这份资料
还有一些编程概念
例如持续集成
持续交付
你可以使用SDK或CLI
直接运行一些基础命令
当然还有课程讲解
你可以进行实践操作
好的 领域二为数据存储管理
这里需要理解
例如 S3的相关知识
S3 深入覆盖AWS内容
S3 包括存储平台及其他存储方面
我将涵盖这些及不同格式
如何对齐数据存储
如何确定适合特定数据访问模式的存储方案
我将在领域二讲解此内容
当然还有不同任务
例如 数据生命周期管理也会涉及
数据建模与模式演进设计也在课程中涵盖
好的 领域三为数据操作与支持
接下来
我们将学习如何通过工具和服务实现自动化处理
如何通过脚本进行服务故障排查
例如 何时使用EMR
什么是Redshift
如何判断 数据仓库的概念
AWS的Glue服务
例如 或Athena快速查询
我将在数据支持部分涵盖所有内容
好的
接下来继续 当然它会提供这些细节
就像我说的我已经涵盖
我已尽力覆盖所有这些知识领域
你看到的不同领域
以便你能获得扎实的理解
同时也要实践
最后我们有数据安全与治理
即如何使用
例如 VPCs
我在创建策略
托管服务与非托管认证方法的区别
更新这些DPC组
凭证的管理和轮换
密码管理
在这里你可以学习如何保护数据
好的 所以这些是四大领域
当然这涵盖了考试的全部方面
现在在附录中
直接查看相关范围内容
AWS服务及功能
没错 这里有一个我提到的完整列表
基本上我已经提供了所有关于Athena的细节
EMR Glue Lake Formation
Kinesis
快速补充 让你跟上进度
请仔细阅读
如果有问题请告诉我
基本上这涵盖了课程中的整个考试内容
好的 有了这些
如果有问题请告诉我
如果你是新手
没问题
因为你可以作为初学者学习
正如我说的 我已经涵盖了很多
我提供了大量额外课程
如果你没有基础知识
你就能快速跟上
但理想情况下
当然你需要具备AWS生态系统的经验
例如 如果您需要一名认证的云从业者，这正是理想之选
这是开启AWS认证数据工程师之旅的绝佳方式，没错 让我 如果您有问题，我们接下来进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/003_Udemy - Become an AWS Certified Data Engineer part1 p03 4. Day-to-Day Activities of an AWS Data Engineer.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


AWS认证数据工程师是专门利用亚马逊云服务的专业人士
AWS
例如 进行设计
实施和管理数据解决方案
因此主要职责围绕数据相关任务展开
并利用AWS服务驱动数据洞察
管理 处理
高效安全地分析海量数据
接下来我将介绍关键任务
AWS认证数据工程师日常会执行哪些工作
首先是设计数据解决方案
使用AWS服务构建数据架构
选择最有效的
基于需求如可扩展性
性能和成本效益
数据采集与存储是另一项职责
嗯 建立数据采集
从多源收集并存储至AWS
然后使用Amazon S3等服务
AWS Glue
Amazon Redshift等服务确保数据完整性和可靠性
第三是数据处理与转换
开发数据管道和工作流
将原始数据处理转换为分析可用格式
正确使用AWS Glue等服务
AWS Lambda和Amazon EMR
第四是数据分析与可视化
使用Athena等工具
正确Amazon QuickSight或Redshift Spectrum进行数据分析
生成洞察并为公司利益相关者创建可视化报告
接下来是数据安全与合规
如何实施安全措施保护数据机密性
在AWS服务中确保数据完整性和可用性
以符合相关法规
接下来是性能优化与监控
优化数据工作流
监控系统性能
识别瓶颈并进行故障排除
确保数据处理顺畅
协作与沟通也是重要领域
向非技术人员传达技术概念
并指导数据相关决策
现在 持续学习更新最新AWS工具和服务
是AWS数据工程师的关键职责
以便优化现有数据解决方案
因此本质上 负责有效利用AWS服务进行数据管理的AWS认证数据工程师
处理 分析
并从海量数据中驱动可操作的洞察
同时确保数据安全
合规性和最优性能
与此同时现在
他们在推动数据驱动决策中发挥关键作用 使企业能够通过AWS平台的力量做出有效决策
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/004_Udemy - Become an AWS Certified Data Engineer part1 p04 6. The Value of AWS Certified Data Engineer.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
让我们谈谈AWS认证数据工程师的价值
换句话说
这个认证能为你带来什么
总之说来
AWS认证数据工程师实施数据管道
没错，不仅负责管理数据管道
还要确保所有数据准确无误
性能问题不再存在
数据具有高完整性
此外，AWS认证数据工程师还会监控
排查并优化成本
因为数据会产生大量成本
没错，企业拥有海量数据
为了管理整个数据生命周期
会产生巨大成本
其中一个核心职责是
当然 优化成本
例如
如果你的数据管理不当
无论是结构化
还是非结构化
都可能导致高昂的数据库费用
好的 例如
如果你使用亚马逊 Dynamodb或RDS
如果解决方案架构不合理
成本可能大幅增加
另一个好处
这是我最喜欢的
据杰弗森弗兰克报告
获得AWS认证可带来薪资提升
平均增加27%
现在 这可能适用于任何认证
不限于数据工程师
但平均而言这是认证带来的收益
好的 你应该具备
换句话说 有哪些先决条件
对吧 如果你有2-3年数据工程经验
非常好 如果没有
没关系 太
因为我设计的课程内容较为全面
好的 涵盖所有内容
所以如果你是有经验的人
请跳转到课程中的相关章节
这样你才能立即受益
如果你是新手或初学者
没问题 因为你可以通过所有课程并掌握实用技能
你应该具备至少一到两年的实际操作经验
使用aws生态系统中的各项服务
或者如果你有
比如aws云从业者认证就再好不过了
因为你具备相关知识
掌握所有aws服务的基础知识
并且实际使用过部分核心服务
当然还需要掌握基本的git命令
网络概念 子网
当然还包括计算知识
好的 现在来看考试内容，我之前已详细讲解过
但需知考试评分范围是100到1000分
及格分数线是720分
需要达到约72%的正确率
你将有约170分钟时间回答85道题
好的，题目为多选或选择最佳答案
好的 共有四个领域，我已深入讲解过
主要领域是数据采集与转换
占比约34%
数据存储管理占26%
涉及数据目录系统和数据生命周期等
数据操作与支持
通过使用cw服务实现自动化数据处理
不仅限于quick side
还包括athena、glue等服务
以及课程中将涵盖的其他服务
当然第四领域是数据安全与治理
应用认证机制
使用aws iam、安全组
策略及授权机制，确保数据加密与脱敏
好的 这些四个领域我将深入讲解
从第一领域开始
数据采集与转换
好的，很好 总结来说
DevOps工程师的价值在于
当然高薪待遇
我们知道中位数薪资约14万美元
现在
这个数字可能会上涨到
最高可达一百七十五
或者我有学生实际年收入超过两百万
没错，考试的有效期是在通过后三年内
没错 所以如果你现在参加考试
当然可以享受折扣
没错 价格减半
这还不算太差
标准考试将于第一季度举行
或者可能按月份计算
我希望这能帮助你理解AWS认证数据工程师的价值
如果有任何问题请告诉我 我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/005_Udemy - Become an AWS Certified Data Engineer part1 p05 7. What is Data Analytics and Data Analysis Solutions.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
那么让我们谈谈什么是数据分析和解决方案
在本节课中 我将讨论数据与分析解决方案并探讨挑战
在处理大型数据集时
必须快速生成有意义的见解
我们还将介绍五大挑战
没错五大挑战
正如我们所说
然后我还会讲解如何规划数据分析方案
数据分析对企业至关重要
无论大小企业
数据分析流程结合形成分析解决方案
帮助企业在何时推出新产品做出决策
何时提供折扣
例如何时开拓新市场
如果没有数据分析提供的数据
当然还有纯粹的运气
对吧 因此企业开始实施数据分析方案
现在挑战出现了
这些挑战源于数据本身的特性及使用场景所需的分析需求
在过去
这些挑战被称为大数据挑战
但在当今云环境
这些挑战同样适用于小数据集
没错快速数据集
例如 如今我们已成为数字社会
我们创建和收集的数据量显著增长
增长速度还在加快
更不用说我们已建立数据收集系统
这些系统能高效存储所有数据
但请想一下
如果每天收到邮件几个月
对了而不是打开
你只是把邮件放在厨房台面
它就这么堆在那里
逐渐堆积成山
这就是数据的情况
现在你会错过重要内容
从这些数据中遗漏关键信息
现在想想大型企业
现在 组织可能花费数百万美元用于数据存储
就像永远未被分析的桌面上的邮件一样
有时仅仅因为分析成本过高
现在需要大量资源
让我们谈谈数据来源
所以你可能已经知道
你可以从各种来源获取数据
人类生成的数据是由人类将信息输入系统或应用程序产生的
这是目前最大且最广为人知的数据形式，包括电子邮件
文档 电子表格
图像 数据库中的数据
还有更多，计算机生成的数据是由应用程序自身创建，无需直接人工指令
这是人类输入数据与系统自主收集信息结合的结果
例如 我可以在线填写表单并输入我的姓名
职业和地址
从人类生成的数据
应用程序随后可以搜索职业并学习我职业的平均收入
基于邮政编码
现在应用程序可以关联职位空缺甚至直接给我发邮件
所以所有这些都是获取数据
平均收入和职位空缺属于计算机生成数据
现在还有另一种与人类无关的计算机生成数据
这些数据由应用程序、行为系统和网络设备在系统内生成
这类数据通常以半结构化日志文件形式存在
现在也可能以二进制形式存储在日志文件中
这些结构有时难以分析
但通过关联分析能提供巨大价值
这帮助企业更快做出更明智的决策
问题不在于数据本身
而在于未能有效利用数据
这可能导致错失机会
成本上升
降低生产效率
然后 你知道的
削弱竞争力
现在 制定有效的数据分析策略
可帮助降低成本并提升运营效率
数据分析解决方案涵盖数据采集、存储、处理和可视化
现在大数据如何融入数据分析解决方案
大数据解决方案是数据分析解决方案的一部分
大数据术语主要描述处理海量数据的挑战
以高速度
快速产生洞察
但并非所有组织同时做到这三点
至少在我的经验中
没错 并非所有数据分析解决方案都需要大规模处理
这是两者的关键区别
好的现在
企业争夺数字化优势
认识到需要了解客户并利用这些信息更好地服务客户
现在这些企业需要大小不一的数据分析解决方案
实施的流程现在完全相同
接下来几节课我将讨论的内容就是这个
组织通过数据分析解决方案从数据中提取价值
现在 什么是数据分析解决方案
它能帮助您轻松管理整个数据管理周期
从收集、存储到处理、分析和可视化
以报告或仪表盘的形式呈现
因此大数据的一些优势
当然大数据是通过多种方式生成的
没错 关键问题是如何整合所有数据
如何创造价值
如何产生竞争优势并应对挑战
现在
许多数据分析解决方案可归纳为五大核心渠道
数据量、处理速度
你知道的 数据真实性
数据量、数据价值
好的 并非所有组织都面临这些挑战
但许多确实存在快速摄入大量数据的困难
另一些则在处理海量数据并生成新预测洞察时遇到困难
还有些用户需要在简单数据集上实时进行详细分析
现在数据分析解决方案的组成部分
数据分析解决方案包含多个组件
各组件执行的分析可能需要不同的服务和方法
希望这有所帮助
请告诉我 如果您有任何问题 让我们进入下一节课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/006_Udemy - Become an AWS Certified Data Engineer part1 p06 8. How to Create S3 Bucket with Amazon Q and New AWS Management Console.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
让我们谈谈新的亚马逊管理控制台
他们不断更新和调整
现在就在这里
如果您创建edwas免费账户并登录到itis管理控制台
您将被带到这一页
这就是控制台主页
现在AWS已经添加了许多小部件
例如 最近访问过
有应用成本与使用情况
您可以查看所有账单和账户动态详情
关于您的AWS账户健康状况
它将导向AWS
当然 然后构建解决方案探索
还有很多其他选项您可以向下滚动查看所有这些
您现在也可以添加更多小部件
如果您需要访问服务
例如 如果您需要查看亚马逊S3存储桶就在左上角
您会看到这些服务选项
所以如果您点击服务
您将获得AWS所有可用服务的完整列表
生态系统 在这里底部是存储服务本身
所以点击存储后
您将看到另一个列表
您可以选择需要访问的存储类型
例如 S3、EBS、备份等
所以如果我要查看S3存储桶
如果我需要创建S3存储桶
我只需点击S3
这将带我到该仪表盘
现在我可以导航并管理我的存储桶
好的 那我们回到之前继续讨论其他服务和控制台
管理控制台本身
这就是如何访问服务
现在列表是
嗯 在下拉列表中可以看到
然后从应用中选择所需服务
您可以直接从应用小部件创建新应用
当然选择您的区域
例如我的示例是us-east-1
右侧您会注意到
这里有亚马逊新推出的云服务
这就是亚马逊Q
这是AI驱动的聊天助手对吧
这就是你可以实际使用AWS力量和生成式AI系统的入口
你可以询问关于服务结束的任何问题
或者如何创建你想要的东西
你只需在这里的搜索栏输入任何内容即可
例如
如果我想创建新应用
对 使用WordPress的网络应用
所以我可以说给我步骤
创建基于WordPress的网络应用
对 所以我按下回车键
接下来会发生什么
亚马逊Q 目前仍处于预览阶段
这是一个全新服务
没错，他们刚推出非常强大的功能
它会列出所有步骤
例如对吧 以下是使用AWS创建WordPress网络应用的基本步骤
首先启动EC two实例
将实例连接到Apache web服务器对吧
然后运行命令
伪yum 安装web服务器的命令
下载最新版WordPress
为数据创建数据库对吧
然后更新WordPress配置文件
测试安装并完成WordPress配置
而这里的好处是
还会提供这些资源对吧
你可以查看附加文档
然后通过这种方式操作
所以你可以回答任何问题
这只是我给出的例子
然后 当然如果我关闭这个
你可以查看成本和使用情况
可以直接跳转到AWS成本管理控制台这里
或者检查你的AWS SL
例如 或者构建解决方案
例如
构建一个网页应用
部署无服务器微服务对吧
虚拟服务器启动虚拟机并命名
并且 当然他们的公告和区块
他们简化了管理控制台
非常强大 这里还有一个功能是在搜索栏
你可以在这里实际搜索s three
例如，如果我搜索s three会给出所有服务列表
这就是直接导航到s three存储桶的快捷方式
他们也可以查看其他功能
然后你还可以查看文档
知识文章
然后资源是新功能
例如 如果我点击资源
会显示资源搜索介绍
我可以进入资源探索器
然后可以展示跨区域
整个生态系统
我实际上可以搜索
这就是aws资源探索器
你也可以查看这个
所以让我们演示一下
让我给你们实际操作
你看这强大的功能和掌控力
bis管理委员会的强大功能
以及部分服务
我将使用最新服务
即亚马逊Q创建s three存储桶
例如
我们打开这里的AI助手
现在可以问我任何关于aws的问题
在我实际操作前
你会注意到另一个选项
底部左角的云壳
或者从顶部导航栏打开云壳
云壳打开后
你可以直接开始创建或输入命令
从而在aws平台直接运行命令
如果需要创建s three存储桶
只需在此处输入命令
它会通过命令行界面或CLI执行
需要注意一点
这是一个简单通知
云壳将从亚马逊Linux 2迁移到亚马逊Linux
2023年 从12月4日开始
2023年 提供便捷通知
如果需要查看这个
好的就这样
这就是云壳
我已经打开了
这里是我的光标
但我不知道这个命令
所以 例如 如果你不清楚如何用CLI创建存储桶的命令
我要去问亚马逊Q
所以导航到亚马逊Q
只需简单提问
给我创建名为clickdesk的S3存储桶的命令
我将创建一个S3存储桶
并将存储桶命名为
点击询问吧
这里抱歉有个拼写错误
这里应该是t h e 好了
给我创建名为layask的S3存储桶的命令
如果我搜索的话
它会继续搜索
让我关闭这里的PowerShell
你可以看到屏幕这里完美
它给了我创建名为laydesk的S3存储桶的命令
使用AWS CLI
就在这里 aws s3 api
create-bucket
bucket参数指定要创建的存储桶名称
所以我可以复制命令
我可以复制代码
但此命令的关键点也被解释了
这是一种极好的学习方式
另外 当你实际操作并使用AWS工具和服务时
所以bucket参数
例如参数指定了要创建的存储桶名称
即clears 在此例中你可以随意命名
作为作业请继续
只需按照这些步骤创建一些存储桶
区域无需显式指定
例如命令本身是aws s3 api
只需说创建存储桶
无需默认使用区域
存储桶将禁用公共访问权限，后续可修改权限
存储桶创建后
你可以设置存储桶策略并分配角色和组等
好的 复制命令后
打开此处的终端并粘贴
好的按回车键
它将执行
并将创建S3存储桶
好吧 如何判断桶是否已创建
例如或不是
那我们去S3服务看看
我要进入服务页面
当然 然后导航到存储S3
这会带我到S3控制台
在这里你可以看到已创建的桶
这有多简单
这绝对强大
现在你已经在管理控制台中使用CLI了
就在你手边
你不需要 你知道的
创建或从机器下载CLI
或者如果你可以 如果你想的话
但你可以直接使用Deus命令创建桶
这是学习CLI命令的好方法
当然亚马逊Q完美
因为它现在赋予你
你的AI生成助手来执行命令
这就是我的桶
所以我点击播放台
例如 这就是我的桶
现在我可以设置属性
权限等
好的 希望这对你有帮助
我想涵盖 并简要演示管理控制台本身
亚马逊队列
然后如何使用亚马逊队列并创建桶作为示例
如果有问题请告诉我 我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/007_Udemy - Become an AWS Certified Data Engineer part1 p07 9. Challenges of Data Analytics.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


因此，如果您需要全面的数据分析或基础分析解决方案
不妨先问问自己
您是否在应对数据量增长时遇到困难
或是数据产生的速度
或是数据源的多样性
例如 数据的准确性
您是否从数据中获得了价值
这些正是您讨论的问题
我将逐一讲解这些挑战
没错 所以是准确性、价值、数据量
没错 现在让我们逐一分析这些要素
当我提到数据量
指解决方案必须处理的数据规模
解决方案需高效处理并跨服务器分配负载
以应对下一阶段
接下来是速度
速度指数据进入并流经流程的速率
许多企业处理大量实时流数据
因此解决方案必须快速采集
处理这些数据
第三个是多样性
数据采集
从多种不同来源获取多种类型数据存在诸多挑战
聪明的企业会构建能处理结构化
半结构化和完全非结构化数据的解决方案
我稍后会详细讲解两种数据类型
但现阶段我们先聚焦于此
第四个是准确性
指数据的可信度
您是否听过这句话
我的字如我人
旨在建立信任
让您确信对方值得信赖
这就是数据可信度
必须了解数据的问题所在
好的 接下来是数据链的完整性
可确信数据未被篡改
数据收集很简单
确保其准确性和一致性
这才是挑战
这才是难点，这就是准确性
第五个是价值
即最终目标
没错 整个努力的核心是从数据中获取价值
包括创建您熟悉的报表和仪表盘
制定关键商业决策
还包括突出需要改进业务的领域
并使查找和沟通业务运营关键信息更加便捷
所以就是这样
这就是数据的五大特征
现在我已经讨论了这些指标
你可能需要数据分析解决方案
让我们先了解需要准备什么
你需要知道数据来源何处
大部分分析数据来自现有本地数据库
没错以及文件存储
实时数据正变得越来越流行
随着使用公开数据来丰富解决方案处理的其他数据源
接下来你需要了解数据处理选项
处理术语包括收集
清洗 转换并加载数据到分析数据仓库
这本身就需要大量工作
此过程可以手动处理或借助应用自动化
最后你需要了解从数据中应获得什么
没错 收集和处理所有数据的结果应具备可操作性
洞察 现在应为你创造价值
这些洞察通常以报告和仪表盘形式呈现
由于数据量不断增加
速度 数据量
多样性与价值
某些数据管理挑战无法用传统数据库和处理方案解决
现在数据分析解决方案就派上用场了
运行数据分析解决方案本质上整合多种分析方法来存储
处理 并可视化数据
规划数据分析解决方案需明确自身需求
了解数据来源
数据分析解决方案处理的大部分数据来自现有本地数据库和文件存储
这些数据通常需要预处理
解决方案需求将相应减少
当然 实时数据
例如 也越来越受欢迎没错现在这些数据源结构更松散
可能需要专用软件采集数据并使用特定处理应用正确生成
实时分析
而这正是这类数据集的核心
接下来是公开数据集
例如 人口普查数据
健康数据 人口数据
以及许多其他数据集
大型数据集没错
因此他们正在收集客户数据
例如 现在这些数据可能需要转换
以便仅包含企业所需内容
因此需要了解处理数据的选项
目前有许多不同的数据处理解决方案
顺便说一句，没有一种方案适合所有情况
没错 必须仔细评估业务需求
并与能提供所需结果的服务相匹配
在未来的课程中
我将介绍AWS的服务
它为每个组件提供的服务
没错 你需要从数据中学习
必须准备好从数据中学习并优化
以便团队能从数据中学习
因此发现趋势、建立关联并提高效率至关重要
盈利的业务
没错 基于你的数据
现在是时候让数据发挥作用了
而这正是这一切的核心
因此我介绍了数据分析和解决方案的概念
我还提到了五个V要素
来自大型数据集的不同挑战
你可以通过解决方案生成可操作的见解和组织价值
希望这有所帮助，如有疑问请告诉我 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/008_Udemy - Become an AWS Certified Data Engineer part1 p08 1. Throughput VS Latency.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
超级兴奋
我将讨论吞吐量和延迟的概念及其区别
好的 那么什么是吞吐量呢
简而言之，吞吐量类似于
处理或占用带宽，而延迟衡量的是这个过程的时长
吞吐量类似于
可以将其视为
容量 好的
带宽管道本身的容量，允许数据被推送进来
而延迟是时长或在该带宽内所需的时间
但我们现在先聚焦吞吐量
它指的是数据传输速率
这就是吞吐量
简言之，它决定了数据被摄入的速度
正如我所说，就像一个管道
对吧 你从一端推送数据
然后需要测量数据从a点到b点所需的时间
数据摄入速度有多快
处理 然后存储，这就是吞吐量
这对需要
例如 需要持续摄入大量数据的场景
现在 另一方面，延迟
正如我所说，就是时长
或从开始到完成的时间延迟
好的 当你发起一个操作
然后其效果的发生
这就是延迟
这可以被测量
对吧 如果延迟低
这对实时应用至关重要
高延迟会影响应用的响应速度
然后 当然
因此
可能导致关键数据摄入后的延迟或洞察
现在对延迟的理解很清晰了
当我们讨论AWS服务时
关于数据摄入
现在你可以使用像亚马逊
Kinesis Data Firehose这样的服务
数据摄取有许多应用场景
也涵盖了Kinesis流或数据火 hose
吞吐量确保高效的数据处理
而低延迟实现实时洞察
关于同构数据摄取模式
好的 现在
这些模式旨在以相同格式或存储引擎将数据迁移到目标位置
与源端保持一致
因此 例如
关系型数据和文件数据的摄取
基本保持相同格式
至于大型对象或Blob的摄取
同样适用 如果属于同构数据
则保持不变
另一方面
如果是异构数据
好的 该数据必须进行转换
要么通过ETL流程
例如 在写入目标存储系统时
如果你将其存储到数据仓库或Amazon Redshift
则可以转换后以关系型数据形式存储
可以使用不同引擎
好的，数据摄取模式
当然 完全取决于项目需求
或数据需要转换的内容
以及是否涉及历史数据
这就是异构数据摄取模式
希望这有所帮助 我只是想涵盖吞吐量和延迟的概念
一个基本概念
可能会有相关问题
或理解这两个概念和术语
以及它们如何与数据相关
如有疑问请告诉我 我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/009_Udemy - Become an AWS Certified Data Engineer part1 p09 2. Replayability, Stateful Vs Stateless Transactions.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎各位参加本次讲座
我将要讲解可重复性
有状态的状态事务
好的 另一个重要概念
当我们谈论数据摄入时
所以在深入探讨可重复性之前，我们先直接进入主题
让我们谈谈有状态和无状态应用
它们主要指如何管理和存储信息或状态数据
现在有状态应用会保留过去交互的状态或内存
并且数据本身及状态信息由应用存储并记住
通常在服务器或数据库中
这些有状态应用还通过它们不仅记住
还能维护之前的客户端交互记忆
从而记住上下文和过往操作
例如
数据库会话、网页应用和追踪用户进度的在线游戏
另一方面 无状态应用不会保留会话数据或过往交易记忆
这正好相反
每个请求独立处理，不依赖先前请求
无状态应用也不会在服务器存储客户端状态信息
而是要求客户端每次请求携带所有必要信息
例如
它们常见于RESTful API
客户端每次请求必须包含服务器所需的所有必要信息
以便正确理解和处理请求
希望这能帮助大家理解有状态与无状态应用的区别
但让我快速说明
关键区别就在这里
有状态应用保留过往交互的状态
而无状态应用不保留此类信息
好的 这就是第一个关键区别
第二个关键区别是有状态应用在服务器或数据库中存储数据
而无状态应用需每次请求提供所有必要信息
好的 接下来我们进入下一个主题
现在讲可重复性
在数据摄入的语境下
可重复性指系统或流程能够重新处理或重新
摄入之前已处理过的数据
它涉及重新运行数据管道
工作流或摄入流程的能力
以确保数据完整性
正确性 或解决初始摄入时可能出现的错误
因此数据摄入中的可重复性非常重要
原因有几项
首先是错误处理
好的 因此，在数据处理过程中，数据摄入流程可能会遇到错误或故障
因此，能够回放数据可以重新处理字段或错误数据
好的 第二个重要原因在于数据一致性
因为在某些场景下
尤其是在分布式系统中
可能需要重新处理数据以保持一致性
没错 第三个是上游变化
当数据上游
数据源或模式发生变化时
可能需要重新调整数据
第四是测试与开发现在
例如 在开发或测试环境中
回放数据的能力对于调试至关重要
测试新功能
或模拟场景而不影响实时数据
现在要实现数据摄入的可回放性
可以采用多种策略
例如 幂等操作
这些策略本质上是设计能够支持幂等性的数据摄入流程，确保
回放相同数据不会导致重复记录
或意外副作用
没错
可以采用的策略之一是日志与审计
即维护详细的日志记录
还可以使用一种称为检查点的策略
在数据摄入管道的各个阶段设置检查点或标记
这听起来合理
这有助于跟踪进度
并确定需要回放数据的起始点
另一个可用的策略是回填机制
这指具备从特定时间或事件回填或重新处理数据的机制
可以高效解决数据问题而无需重新处理整个数据集
这很有道理
因此，可回放性我们讨论了其重要性
并且 当然 多种策略
确保数据摄入的可回放性对维护数据完整性至关重要
可靠性和一致性
好的 现在谈谈挑战
我之前简要提到了一些挑战
首先是幂等性
没错 因为需要确保回放相同数据不会产生重复记录
对 我之前已经讨论过这个问题
现在 顺便说一下，这可能具有挑战性
尤其是在处理分布式系统或复杂数据转换时
没错 所以存在顺序依赖
例如 或者数据依赖
接下来是数据量与性能问题
重新摄入大量数据会消耗系统资源并影响性能
同时你还需要
比如及时性与延迟问题
所以在某些情况下，数据摄入的及时性同样至关重要
然后还有数据一致性与依赖
例如 可能存在顺序数据依赖
就像我之前提到的 确保不同数据源之间的一致性
尤其是在重新处理数据时可能复杂
不同数据集或系统间的依赖可能带来一致性维护挑战
同时还需要错误处理与恢复
这可能意味着识别初始数据摄入过程中出错的位置和原因
所有这些都是可重放性的挑战
另一个挑战可能是资源管理
没错 可能需要管理存储等资源
重新摄入过程中的计算或网络带宽资源管理确实有难度
尤其是在处理大规模数据时
好的 另一个重要挑战是测试与验证
验证重新处理数据的正确性
并确保摄入流程中的变更
不会引入新错误
需要全面测试
然后是验证策略
这也是可重放性的挑战
当然还有成本问题
因为成本考量同样是一个挑战
没错 因为摄入大量数据会产生额外成本
因为你需要重新处理所有数据
尤其是在云环境中按分钟或秒计费
没错，尽管是按需付费服务
但仍需承担相关成本
因此优化成本
同时确保有效可重放性确实具有挑战
解决我刚才提到的这些挑战
需要系统设计的谨慎规划
或者采用合适的技术和框架
而且 当然 实施强大的错误处理和监控
然后持续优化不仅一次就正确
而是持续优化流程以提高效率和可靠性
让我们谈谈有状态事务
然后我将稍后讨论无状态事务
有状态和无状态事务指的是系统如何管理交互或操作中的上下文或状态信息
在交互或操作期间
所以先聚焦有状态事务
在有状态事务中
系统在多个事务或请求之间维护交互的上下文或状态
这里系统会记录过往的交易
存储有关正在进行的会话或对话的信息
顺便说一句有状态事务
通常涉及服务器保留客户端会话的信息
这允许请求之间保持连续性
现在无状态
另一方面
不维护事务间的任何状态或上下文
每个事务完全独立且自包含，不依赖过往交互或会话数据
在无状态事务中
客户端的每个请求必须包含服务器处理所需的所有必要信息
最后让我们总结
我们讨论了记住过往交易的有状态系统
无状态系统则独立创建每个事务而不保留任何上下文
当然重要的是我们还谈到了可重放性和应对挑战的策略
正确保证
这基本确保数据完整性
恢复和灵活管理数据管道的能力
希望这有所帮助
如果有任何问题请告诉我
再浏览一次幻灯片
我也为大家提供了下载这些幻灯片
而且 当然查看一下这些 至此让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/010_Udemy - Become an AWS Certified Data Engineer part1 p10 3. ETL Process.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
让我们谈谈ETL流程：提取
转换和加载
现在源数据在上千个位置杂乱无章
你知道的
很少以相同的组织方式开发
所以要理清这个混乱
不转换源数据就像在人群中清晰听到一个声音
你或许能捕捉到部分对话内容
但会很快失去上下文和有效性
现在让我们谈谈如何将数据导入分析解决方案
首先使用ETL操作
什么是ETL呢
ETL是提取
转换和加载是收集原始数据源的过程
然后将数据转换为统一格式
新数据被加载到最终位置以供分析使用
对的，现在还要进行检查
在现代云环境中
我们通常称之为ELT
提取 加载和转换
顺便说一句
现在步骤顺序不同了
但结果是一样的，既然你知道源数据很混乱
数据已分布在上千个位置，对吧，且组织方式各异
若不实际转换源数据
就非常困难
或者数据可能也会损坏
因为有些数据有效，有些则无效
所以 必须转换数据
因为分析解决方案需要转换过程
然后迁移到新数据仓库
这就是ETL的核心
ETL操作确保数据准确
精确和深度
才能正确分析数据
并通过报告和仪表盘生成完整数据叙事
ETL提供机会整合不同来源的数据
帮助发现单独分析无法获得的见解
如果你只能逐一分析数据源
ETL是将源数据转换为专用数据集
用于生成高级分析结果
这些专用数据集可大幅降低分析时间和成本
现在你了解ETL的核心及其目标
首先讨论ETL的各个阶段，首先是提取
这可能是三个中最重要的一环
对生产系统影响最大
若执行不当
它可能是许多数据完整性问题的根源
因此在这个阶段你的目标是找到并识别相关数据的合适来源
然后将它们整合到ETL系统中
现在这些来源包括关系型和NoSQL数据库
数据湖 例如
基于文件的数据 甚至是平面文件
现在规划对成功结果至关重要
现在你必须知道源数据的位置以及如何访问数据本身
并且必须仔细规划何时执行提取
由于复制过程可能对源系统产生影响
必须规划初始数据提取以及后续每次提取
数据工程师还需规划ETL操作期间的数据存储位置
现在这被称为暂存区域
根据源数据类型
这可能是一个其他数据库
例如亚马逊RDS
例如 或数据存储如亚马逊S3
你所使用的ETL服务可能已包含数据存储方法
如亚马逊EMR的情况
最后必须规划提取的重复频率
对 提取通常每天发生多次
每小时 例如
或每十分钟一次
甚至流式更新已成为常态
提取的可靠性在规划中至关重要
ETL的下一阶段是转换过程本身
数据可以通过多种方式转换以满足不同目的
这一阶段极其多样化
ETL开发者
例如 创建一系列要在ETL中运行的规则或函数
应用程序数据从数据源中提取
并按相应规则进行处理
许多转换可针对同一数据集处理，转换可以基础
如清理数据、更新格式或进行数据替换
例如
将空值替换为零
或将'女性'替换为字母F
转换也可以更复杂
如应用业务规则计算新值
高级转换还包括过滤记录
复杂连接操作
聚合行
拆分列
数据验证转换还可包括合并两个数据集
将两个数据源的属性整合到一张表中
例如 可以从公开数据源提取人口普查局数据以丰富客户数据
员工数据
然后通过这些联合转换处理营销数据
转换操作也用于数据清洗
比如 你的数据源是地址表对吧
州字段允许使用缩写ca或全称加利福尼亚
现在 当数据被分析时
这些值会被视为不同位置并导致分析问题
一旦 你可以执行数据替换转换
将值统一为单一格式
数据清洗不一定是单次操作
顺便说一句 可能需要多次转换才能达到目标
数据清洗的另一个方面是反规范化
回想关系型数据库
应用规范化以确保数据格式和结构一致
这属于数据库模式的一部分
但在准备转换为分析系统时
这种形式往往失去实用性
记住分析系统需要数据组织
以便简化查询
并快速生成
例如生成聚合结果
在合并源表时
需特别注意如何转换值
例如 要自问问题
比如是否保留ID值还是用描述替换
是否合并所有列
还是只合并部分列
在聚合值时
新表是否需跟踪数据源及原始值
你可能需要整天处理转换阶段
但你应该明白核心概念
最后一个重要点
转换阶段也是调整数据存储类型的机会
换句话说
可以将NoSQL数据转换为关系型数据
反之亦然
非常重要
两种存储类型各有优势
转换阶段是进行这种变更的机会
ETL操作的最后一步是将数据加载到目标系统
转换阶段的规划步骤
决定了最终数据存储的形式（可能是数据库）
一个数据仓库
或者一个数据湖
当操作完成后
该位置的数据已准备好分析
所以让我们把这一切联系到准确性上来
在ETL操作过程中
你有多次机会影响数据的准确性，无论是负面还是正面
随着数据采集要求和转换流程的变化
必须保持高准确性
这意味着需要定期评估转换过程
然后根据需要进行调整
现在
看来现在讨论如何通过服务创建
并运行ETL操作是时候了
AWS提供两种ETL服务
Amazon EMR和AWS Glue
我之前课程中已经介绍过这两个服务
现在可以参考回去
必须谨慎选择适合ETL操作的服务
哪种服务最适合你的ETL操作
这可能是一个不错的考试题目
AWS的优点在于
然后你可以实施两种解决方案
正确，Amazon EMR和AWS Glue并行使用
使用Amazon EMR处理更复杂的任务负载
使用AWS Glue处理
需要更多灵活性和流动性的管道任务
AWS服务在ETL流程中
它们是如何运作的呢
覆盖ETL流程的每个阶段
从数据源存储到报告
AWS全面支持
当然 数据转换
例如 正确
比较Amazon EMR和AWS Glue
我们来谈谈如何选择使用
当你执行ETL的数据转换组件时
AWS有两种选择，正如我之前提到的
是EMR还是Glue
现在你可以选择任意一种
这两个服务提供类似的结果
但需要不同的知识和时间投入
Amazon EMR是创建数据管道的更主动方法
该服务提供强大的数据采集和处理平台
当你使用EMR时
需要团队具备扎实的技术知识
优势是可以创建更定制化的管道以满足业务需求
此外
您的基础设施成本可能比在glue上运行相同工作负载更低
所以glue是无服务器架构对吧
这是一个托管的ETL工具，提供更流畅的使用体验
比Amazon EMR更容易使用
现在这项服务非常适合简单的ETL任务
但您将无法获得与Amazon EMR相同的灵活性
您也可以使用AWS Glue
例如 通过使用AWS Glue数据目录作为元数据存储来管理最终转换数据
现在这个目录可以替代
你知道的 现有的高阶元数据存储
在选择这些工具时做出决策
要以最终目标为导向
您是否需要一个需要低维护开销的持续数据管道
是否需要大规模并行数据处理
您的数据解决方案需要多少定制化
要理解这一点需要大量工作
如何在整个数据分析生命周期中保持数据尽可能干净和一致
现在您应该了解并理解这些内容
在数据库层面实施数据完整性控制的过程
并在转换过程中维持这种完整性
您应该做好准备
规划您的首个ETL操作
基于我们刚才讨论的信息
本课程的目的是确保您理解相关考量
以及解决方案和适用场景
从考试角度考虑
当然在您公司的项目中
哪种ETL服务最适合生成分析数据集
希望这能帮助您理解
当遇到考试题目时
他们会给出一个场景
根据场景需要理解相关要求
然后选择正确答案
因为您需要知道何时使用AWS Glue
例如 或在ETL流程中何时使用ETL
或何时使用Amazon EMR
希望这对您有帮助
如果有任何疑问请告诉我 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/011_Udemy - Become an AWS Certified Data Engineer part1 p11 4. AWS Step Functions Use Cases Architectures.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回到本节课
我将讲解AWS Step Functions及其应用场景
这是一个非常重要的话题
因为这能让您更高效有效地处理数据
那么我们直接开始吧
AWS Step Functions使您能够实现业务流程
作为由多个步骤组成的流程
工作流中的每个步骤可以调用Lambda函数或容器
执行某些业务逻辑更新
例如
或更新数据库
比如DynamoDB
或发布消息到队列
当该步骤或整个工作流执行完成
对的 AWS Step Functions有两种工作流选项
首先是标准模式，另一种是极速模式
当您的业务流程预计单次执行超过五分钟
时
应选择标准模式
例如长时间运行的工作流
比如ETL编排管道
或工作流中的任意步骤
需等待人工响应才能继续
另一方面 极速工作流适用于执行时间少于五分钟的场景
且需要高吞吐量执行时
例如 每秒十万次调用
您可以单独使用标准或极速模式
或结合使用
让较长的标准工作流触发多个并行执行的短流程
对的 因此我们可以通过标准或极速模式自动化AWS Step Functions
好的 这里有一个简单的银行系统示例
好的 在这个架构中
在验证客户姓名和地址后创建新银行账户
该工作流
例如
从这里开始，启动AWS Step Functions账户处理工作流
这是整个工作流
当工作流启动时会调用两个Lambda函数
第一个Lambda函数并行执行姓名验证步骤
另一个Lambda函数则检查客户地址
两个步骤完成后
工作流执行审批申请的Lambda函数，您可以定义重试机制
例如
通过捕获子句处理任务状态错误
无论哪种方式都可以这样做
你也可以使用预
现在可以在工作流中处理这些lambda函数抛出的系统错误或客户错误
由于你的工作流已经处理了错误
Lambda函数可以专注于业务逻辑并减少代码量
表达式工作流更适合这个示例
因为lambda函数执行的任务是协同的
例如 耗时不到五分钟且无外部依赖
这确实有道理
我们继续下一张幻灯片
现在我们可以看到
例如
在这个架构中
有时可能需要人类审核或批准/拒绝业务流程中的某一步
以便工作流可以继续到下一步
现在始终建议
或者在此处建议
例如 现在使用标准工作流
当工作流需要等待人工处理时
或外部系统可能超过五分钟响应的情况
可以看到已扩展
新账户 例如
包含通知审批的开户流程
在这里你会看到中间
这里是通知审批任务状态
并发送到亚马逊SQ
所以下一个状态
审核要求是选择状态并有两个可能路径
这些路径是批准（是）或批准（否）
因此发送短信或邮件通知审批任务
或继续到审批申请状态
通知审批任务
然后发送邮件给审批人，这现在合理
一旦邮件发送给审批人
在收到回复前等待以进入下一个选择节点
根据审批人决定进行下一步
账户申请通过或拒绝由lambda函数处理
这里正是如此，lambda函数调用自身并处理
或拒绝
并且 当然这结束流程
这是另一个示例
好的 继续下一个示例
所以这里
自定义事件总线上的事件触发规则
并调用步骤函数工作流作为目标
这就是事件桥
这里有一个自定义事件Boss
现在会触发工作流本身
假设有一个需要处理过期客户订阅的客服应用
现在事件桥规则会监听订阅过期事件
并响应调用目标工作流
订阅过期工作流将禁用所有过期资源
保留订阅权限而不删除
并向客户发送邮件通知订阅过期
你会看到这些Lambda函数在这里
在工作流结束时
通过Lambda函数向事件总线发送新事件
表明订阅过期已处理完毕
对于这个示例
当然 随着业务增长，您可以使用Express工作流
并开始在事件总线添加更多事件
每秒可调用十万次工作流执行
每秒 Express工作流功能强大
接下来的架构
可以使用步骤函数创建自动安全事件响应工作流
或包含手动审批步骤
在此示例中，你会注意到包含手动审批步骤
步骤函数工作流在IAM策略创建时触发
当IAM策略创建后
工作流会将策略操作与自定义列表对比
受限操作列表
工作流暂时回滚该策略
通知管理员等待审批或拒绝
可以扩展此工作流自动修复
例如应用替代操作或限制特定ARN
好的 这在这个架构中很合理
好的 通过自动化响应AWS资源事件
可减少维护AWS云基础设施的操作开销
事件桥提供近乎实时的系统事件流
描述资源变更和通知的大多数情况
对于这个流
可创建规则将特定事件路由到AWS步骤函数
Lambda 或其它AWS服务进行处理
在此示例中
你会注意到基于事件源触发步骤函数工作流
来自AWS健康服务
主动监控流行代码仓库的IAM
访问密钥 例如
然后这些访问密钥会被公开暴露
好的
假设IAM访问密钥泄露到了GitHub
AWS健康服务会生成AWS账户中的凭证泄露事件
好的 这就是处理泄露流程的方式
已配置的Amazon EventBridge规则检测到此事件
并触发一个步骤函数工作流
借助Lambda函数的帮助
例如
工作流随后删除泄露的访问密钥
这里的第一项就是删除
正如你所看到的
然后借助Lambda函数触发步骤函数工作流
一旦删除成功
汇总泄露密钥的近期API活动
并将摘要消息发送到SNS主题通知订阅者
所以顺序是第一步、第二步和第三步
接下来将删除访问密钥
查找事件记录
然后通过Lambda函数通知管理员
很好
接下来可以使用Amazon S3存储
例如托管静态网站
在这个架构中你会注意到我们使用了步骤函数
还可以使用Amazon CloudFront全球分发内容
作为网站所有者
可能需要两个S3存储桶来上传网站内容
一个是源S3存储桶
另一个是目标S3存储桶
基本上一个用于预发布和测试
另一个用于生产环境
需要将生产存储桶与预发布存储桶的所有更改同步
无需每次更新网站时都从头创建新存储桶
在这个例子中
该
例如 这里看到的步骤函数工作流
就在这里 对的
这个步骤函数
在这个特定示例中是
在两个并行且独立的循环中执行任务
对的，这里
第一个循环将源存储桶中的所有对象复制到目标存储桶
但跳过目标存储桶中存在而源中不存在的对象
对的 一系列Lambda函数执行具体步骤
验证输入
获取源和目标存储桶的对象列表
然后批量复制或删除对象
好的 所以这里的lambda函数会删除过时的对象
而顶部的lambda函数会复制缺失的部分
好的，太棒了 接下来
在这个特定场景和架构中
我们有一个数据处理管道，实时处理来自多个来源的webhook数据
然后运行修改数据的lambda函数
在这一特定用例中
多个第三方应用的webhook数据被发送到亚马逊API网关
直接进入亚马逊Kinesis数据流
在这里 lambda函数
例如 从Kinesis流中提取数据并触发Express工作流
该工作流通过一系列步骤验证并规范化数据
最终 lambda函数更新NS主题
通过SQS队列触发下游lambda函数执行后续步骤
就在这里
这就是下游处理的位置
完成之后
并通过SNS/SQ混合队列进行后续步骤
此工作流每秒可处理高达十万次调用
以扩展数据处理能力
这又是另一个优秀用例
完美 我们现在继续前进
在这个用于刷新亚马逊Redshift的步骤函数ETL工作流中
当源S3存储桶中有新数据时
步骤函数状态机启动AWS批量作业
并监控作业状态或错误
然后从源S3获取ETL工作流SQL脚本
通过PL/SQL容器刷新目标亚马逊Redshift
就在这里
好的 现在
SQL文件包含数据转换每一步的SQL代码
现在 可以通过事件桥触发ETL工作流
或通过AWS CLI手动触发
或使用AWS SDK
甚至自定义自动化脚本
你可以使用其中任意一种方式
可以通过SNS发送警报，触发失败时的邮件通知
在工作流任一步骤
好的，行了
这个ETL工作流是使用标准工作流的示例
非常直接
好的，行了
我们继续下一个用例
现在你也可以并行运行包含多个任务的ETL流程
提取 加载与转换ETL操作
将原始数据转化为有用的数据集
将数据转化为可操作的洞察
这是一个非常实用的用例
你可以使用步骤函数并行运行多个ETL任务
你的源数据集可能在不同时间可用
每个ETL任务仅在其对应数据集就绪时触发
因此亚马逊Vent桥运行SQL脚本会触发步骤函数
然后流程开始执行
这些ETL任务可由不同AWS服务管理
例如Glue、EMR、Athena等
这里就是Glue
然后你可以运行Glue本身
或者也可以使用EMR或Athena
或者使用其他非AWS服务
在此示例中
在此架构中
例如 你有两个独立的AWS数据处理任务
Glue处理销售数据集和营销数据集
这里有两个数据集
销售和营销数据，一旦处理完成
第三个ETL任务将合并前序任务的输出
生成组合数据集
好的 这里就是处理销售数据的ETL任务
然后Glue运行处理营销数据的ETL任务
第三个Glue任务合并销售与营销数据
好的
步骤函数工作流等待S3中的数据就绪
而主工作流按计划启动
在S3存储桶配置了事件桥事件处理器
当销售或营销数据文件上传到存储桶时
状态机可触发ETL任务
即处理销售数据或营销数据
根据哪个数据集就绪
好的，现在这里非常重要
你可以用步骤函数处理数千万数据项
例如JSON数组
S3中的对象列表或CSV文件
然后可高并发并行处理数据
在此架构用例中
对 步骤函数使用分布式模式的映射状态
处理S3存储桶中的对象列表
步骤函数遍历对象列表
好的 因此他们通过对象列表进行迭代和批量处理
基本上在s3桶内部
然后启动数千个并行工作流并发处理项目
现在您可以使用计算服务如lambda
例如 帮助您用任何支持的语言编写代码
您还可以从超过两百种专为aws设计的服务中选择
包含在映射状态工作流中
当子工作流执行完成后
步骤函数可将结果导出到s3桶
使其可供审查或进一步处理
很好 在此示例中
步骤函数工作流按entbridge触发的计划运行
每日执行一次
工作流首先检查
例如 s3中可用的新数据
接下来执行etl任务转换数据
之后在数据上训练并部署机器学习模型
使用触发sagemaker任务的lambda函数
在工作流进入下一步前等待完成
最后 工作流触发lambda函数生成预测并保存至s3
这样设计很合理对吧
很好的用例
aws步骤函数
数据科学或sdk是开源库
它使您能够创建工作流
使用sagemaker和步骤函数处理并发布机器学习模型
sdk提供python api
覆盖机器学习管道的每个步骤
包括训练调优
转换模型 或配置端点
这些均为阶段
您可以直接在python中管理并执行这些工作流
或在jupyter笔记本中
在此示例中
展示了机器学习工作流的训练和转换步骤
并将模型文件输出到s3
一旦存入s3
状态模型使用s3中的模型文件创建sagemaker模型
转换步骤启动sagemaker转换任务
然后创建端点配置步骤定义sagemaker端点配置
很好 您还可以从pdf中提取数据
这也是图像处理的绝佳用例
所以在这个例子中
基本上结合了AWS Step Functions
Lambda和Amazon Text Track
用于扫描PDF发票提取其文本和数据
现在处理付款
Amazon Text Read 分析发票中的文本和数据
并通过S或SQS和Lambda触发步骤函数工作流
对于每个成功任务完成
因此工作流从Lambda函数开始
当然将成功发票分析结果保存到S3
这会触发另一个处理分析文档的Lambda函数
判断是否可以处理此发票的付款
这挺合理的 希望这些内容对你有帮助
我已经讲解了几种架构
多个用例和示例逐一分析
如果有任何问题请告诉我
我很乐意解答
因为这些有助于理解Step Functions的工作原理
以及其基本架构 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/012_Udemy - Become an AWS Certified Data Engineer part1 p12 6. AWS Step Functions Workshop.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回到本次讲座
我现在要讲的是AWS Step Functions工作坊
这基本上是一个大型工作坊对吧
这是一个实操型工作坊
所以大约需要五到六小时时间
但如果你完整参与这个AWS提供的工作坊，绝对值得
顺便说一句，你将非常精通AWS Step Functions
同时涵盖数据摄入和流程处理
架构与工作流设计
没错 这个设置的工作流
主要是学习AWS Step Functions的核心功能
通过一系列互动模块
对，这是一个新内容
这太棒了对吧 这将涵盖Amazon状态语言
例如 编排模式
状态定义 标准工作流
Express工作流 就像我之前提到的
你将使用工作流工作室的输入处理
SDK、CDK和AWS SAM部署
对 这还提供了托管和自助服务选项
正如我说的，你需要大约五到六小时专注完成
或者执行 通过左侧导航逐步进行
你会注意到它从介绍开始，提供详细内容可查阅
并包含我之前提到的示例用例
然后工作环境设置会指导你完成基础配置
当然包括Hello World环境搭建
该应用的工作流
然后你可以逐步完成每个步骤
你需要做的就是点击URL或访问该链接
将跳转到工作坊目录
现在你可以通过AWS访问这些工作坊
重要提示：请注意可能产生相关费用
没错 正如我之前在AWS中提到的
Step Functions费用相关
即使使用免费账户
但需注意 务必确保
你知道 删除或清理所有
或执行清理操作
正如他们所说 所以请务必这样做
所以它会一步步引导你
步骤函数的常见用法
呃 编排示例
工作流程
例如 函数状态正如我之前提到的
然后只需通过一系列交互模块导航即可继续
没错，这里会提供所有这些内容作为本次工作坊的一部分
好的 这是我想要重点强调的内容
当然作为课后作业或练习
你可以完成这个
无论是选择托管选项还是自服务选项
只需点击下一步
它会一步步引导你
关于在运行AWS函数工作坊时需要执行的操作
好的 这又是另一种实践学习的好方法
五到六小时 不过
值得投入 因为这就是你的目标
你想成为数据工程师
对吧 AWS认证数据工程师
你需要完成所有这些内容
这能让你在实际AWS平台上获得丰富的实践经验
当然完成所有这些内容后
如果在工作坊过程中有任何问题请告诉我
我自己也多次完成过这个
如果遇到困难
在操作过程中随时告诉我
过程非常清晰
另一个优势是你会通过一些链接
在操作过程中展示给你的
你知道的 可以深入或开始使用这些模块
好的 例如
在此处每个图像或工作步骤下方
会提供链接
还有白皮书
关于AWS的博客文章
对 这真是非常优秀的资源
我已经多次提到过这一点
如何使用这个
现在请继续
使用这个工作坊
如果有任何问题，请告诉我
我将留下这个工作坊实验室的链接
就在描述里
没错 所以就在资源里
或者你可以去下载资源部分
你也会在那里找到这个工作坊链接
所以有了这个 我们现在进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/013_Udemy - Become an AWS Certified Data Engineer part1 p13 7. AWS Athena.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，超级兴奋，继续推进这节课的内容
我将讲解几个核心概念
因为我们将会直接使用数据作为数据源
然后调用并使用这些亚马逊服务来转换我们的数据
为数据做准备
当然还要将数据转移到QuickSight
理解这些概念非常重要
这样你不会四处碰壁
同时理解这些术语和整个流程如何运作
我将从根本处解释
这样你更容易理解和跟随后续内容
并演示如何准备和处理数据
使用亚马逊提供的存储服务
在这节课中，我将介绍亚马逊Athena
这是一个无服务器查询服务
我还将介绍AWS Lake Formation
S3存储以及相关服务
几个重要概念
首先介绍AWS Athena
这是什么对吧
亚马逊Athena是一个交互式查询服务
查询服务这个词听起来熟悉吧
这意味着我现在可以查询我的数据
换句话说 我可以
排序过滤
在数据最终到达Amazon QuickSight之前提出问题
亚马逊Athena是交互式查询服务
轻松分析存储在S3桶中的数据
使用标准SQL
这是一个简单的SQL查询语言
可以执行如SELECT语句
更新等操作来查询现有数据集
Athena是无服务器的
这基本定义了无服务器
顺便说一句 如果你遇到过这个术语
可以理解为 你不需要本地服务器
完全使用云服务商提供的资源
在我们的情况下 AWS因此是无服务器架构
无需管理基础设施
仅按执行的查询付费
我相信是每TB
或运行五美元左右的查询才会收费
好的
易于使用
性能快速安全
Athena还与AWS Glue集成
好的 这就是你可以实际构建数据库架构的地方
你的表和其他结构现在
我稍后会演示并详细讲解
我们还可以将Amazon S3存储桶中的数据指向以定义架构
并开始使用标准SQL进行查询
这就是AWS Athena的核心功能
所以你要从实例进入
比如Amazon S3
你创建了不同的存储桶
这些存储桶中存储着不同的数据集
右存储桶本质上就是一个文件系统
在高层次或广泛背景下作为基本概念
只需将其想象为Windows资源管理器
你拥有所有文件和存储资源
Amazon Athena开始运作
你可以运行交互式查询
通过Athena使用
然后利用这些数据和查询结果
可以将它们导入Amazon QuickSight制作可视化图表
或进行分析
运行分析故事
执行计算
等等
这就是核心理念
你对这个概念应该很清楚了
接下来是AWS Glue
从S3到QuickSight
当使用Athena与AWS Glue数据目录时
可以使用AWS Glue创建数据库和表结构
供Athena查询使用
现在你了解AWS的作用了
在这里你实际上在创建数据库内的数据库
你正在创建表
当然还会使用Athena运行交互式查询会话
AWS Glue是一种完全托管的ETL服务，即提取
转换和加载数据
这是一个能分类数据的服务
清理并丰富数据并在不同数据存储间可靠传输
这就是你的
数据仓库
或执行数据转换跟踪的服务
提取数据
稍后我会演示如何使用实际向导
嗯 完成所有操作
这不仅仅是概念
我将进行实操演示
你将理解并亲眼看到
不仅仅是概念
但实际上这些概念的应用
AWS Glue 爬虫可自动从源数据中推断数据库和表结构
将相关元数据存储在蓝色数据目录中
当你在 Athena 中创建表时
只需选择使用 Glue 爬虫创建
因此工作流程非常直观
从 S3 开始
也就是你的存储桶
然后爬虫会与存储的数据集交互
因此会提取
转换 并将元数据存储在全局数据目录中
接下来进入 Amazon Athena 部分
执行交互式查询
你根据需求准备数据
最终到达 Amazon QuickSight
即 当然
在这里运行可视化分析
创建可视化图表
执行分析操作
等等
最终到达 AWS QuickSight
当然，QuickSight 的整体概述
是你可以使用任何数据源
拥有你的数据集
对这些数据集进行分析
当然还可以创建不同仪表盘
多个仪表盘并发布它们
分享这些仪表盘
组织内任何人
让他们查看你的仪表盘
你创建的故事
只是一个快照
这是所有仪表盘的快照
例如
假设你对某个数据集进行分析
正确 你创建的可视化内容之一是客户数量统计
将其保存为故事
接下来创建的仪表盘或可视化内容关于盈利能力
将其保存为故事
构建你的故事
可以将其视为
整体而言
或高层级的 PowerPoint 演示
因此当你 向他人演示时
或播放故事
会逐步展示从客户、盈利能力到销售等内容
所以这样 对你来说会更方便
或许市场部或销售部可以查看你创建的所有故事
基于你创建的数据集的分析
他们更容易管理
并相应采取必要行动
所以在本课中想简要介绍aws
A theta Aws glue
以及 当然还有快速侧边
以及工作流程中的概念
希望这对你有帮助
当然接下来要练习
多加练习
对吧 理解这些概念
了解这些内容
在结束本课前
让我快速展示这些服务的位置
我将切换到aws控制台
完美，进入控制台后
对 登录后
前往服务并看到所有服务
所以 例如我的存储在这里
我提到的s three
这是存储所有桶的云服务
如果我点击 S three
你会发现 会列出所有创建的存储桶
当然也可以创建更多桶
每个桶可存储任意内容
例如名为gb eighty eight的桶包含五千销售记录
Dot csv文件
好的
同样 返回主页
再次进入服务
现在展示athena服务
在分析部分
看到athena服务
直接点击
将跳转到查询编辑器
记住i是交互式查询构建器
左侧有数据库列表
然后可基于数据执行查询
然后当然你的表格会运行查询
你可以保存查询格式
查询等等 这就是你实际使用的地方
最终进入快速秒
所以你在这里准备数据
你也可以直接在QuickSet中准备数据
某些字段 但这里是更强大的区域
因为现在你可以真正执行并创建复杂查询
正如你希望在数据中实际进行的
我之所以展示这个
这里的重要性在于显而易见的问题
为什么对吧
为什么我们要使用它
我们可以在QuickSide完成
嗯 QuickSide
其实灵活性有限
好的准备你的数据
它只能处理字段大小或简单计算
但在Athena这里
假设你有数百万条记录
对了如果有1000万条记录
而你只想
你知道的
获取其中一部分记录
比如说
在1000万条记录中
你可能想根据特定条件查询100万条记录
这就是Athena非常有用的地方
然后将Athena的结果导入QuickSite
此时会显示效果
好的完美 我们再回到服务页面
当然还有AWS Glue
好的 这也属于分析部分
我将点击AWS Glue
这就是你可以操作的地方
进入AWS Glue仪表盘后
你可以查看所有创建的数据库表
这样一来
这是一个表格区域
可以存储供Athena使用的表
最终供QuickSite使用
这是我们之前课程中讨论的工作流程
当然它使用爬虫来提取
转换和加载ETL
好的 所以，与aws学校同义
想想etl 好的
你可以提取数据的地方
你可以转换数据
也可以加载数据
简要演示这些服务在aws中的位置
好的
我们看了三个
查看了athena
然后学校练习这些希望
这有帮助 如果你有任何问题
在讨论区留言
我很乐意回答 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/014_Udemy - Become an AWS Certified Data Engineer part1 p14 8. What is Kubernetes.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎来到关于Kubernetes入门课程
我将讲解Kubernetes到底是什么
它能做什么
以及如何解决应用扩展时的问题
所以在深入Kubernetes之前
我想再谈谈容器
因为它们非常重要
容器是解决软件在不同计算环境可靠运行的方案
当从一个环境迁移到另一个环境时
这很棒
这可能从开发者的笔记本电脑开始
到测试环境
从预发布环境到生产环境
甚至可能从物理机
数据中心内的
迁移到私有或公有云的虚拟机
我们之前讨论过容器对吧
但再回顾一下它们是什么以及解决的问题
它们究竟是如何解决这些问题的
容器包含一个完整的运行环境
例如
它会包含应用程序
及其所有依赖项
库文件
和其他二进制文件
以及运行所需的所有配置文件
通过容器化应用平台和依赖项打包
操作系统差异
发行版和底层基础设施被抽象化
好的 所以你只需创建一个容器
把所有东西装进容器
然后部署到平台
容器非常优秀
正如之前所说
但关于容器的几个要点
从技术角度讲 容器是内核的新模式
显然它们是新的且正在发展
容器正在改变我们交付软件的方式
不同于传统方法
容器仅运行在Linux内核中
仅包含控制组和内核命名空间
这是容器的局限性
没错
那么容器如何知道
例如 如果我的应用需要跨五个数据中心
简单回答是 它并不了解这方面
它所知道的只有盒子里的内容
好的 这一点需要理解并区分清楚
或者看看容器到底不能做什么
你可能有数百万个容器在那里运行
每个都有自己的空间
有自己的依赖项
应用代码和所有内容
但仅此而已
那么如何扩展你的高可用应用
我们需要kubernetes来实现
好的
假设你有一个正在运行的集群
集群内有所有这些容器
现在需要有效扩展你的应用
容器本身无法做到
因此我们需要一个名为kubernetes的工具
这是一个开源项目
接下来我们将介绍kubernetes到底是什么
但我想区分并让你理解容器的实际概念
以及kubernetes如何发挥作用
kubernetes是用于自动化部署
扩展和管理容器化应用的开源系统
好的 这一点很重要
它的功能 将构成应用的容器分组为逻辑单元以便于管理和发现
是开源容器编排系统
并提供广泛的行业支持
还支持多云环境、本地部署
虚拟机 裸金属等
更重要的是
只需理解并记住kubernetes是用于自动化
应用的部署
应用的扩展
和容器的管理
kubernetes使用的一些术语
我们将使用这些术语
事实上在本课程中会用到这些
容器再次是密封的应用程序包
没错 我之前演示了如何下载安装Docker
以及如何创建容器和镜像
然后Pod是紧密耦合容器的小组
将这些容器
创建Pod标签
附加到对象的元数据
选择器是对这些标签的查询
生成结果集
一个控制器
这是一个协调循环，驱动当前状态向期望状态靠拢
正如其名，它负责控制，服务是一组协同工作的Pod
Kubernetes中的新概念
或者新术语是Pod
所以我们只需创建这些Pod
这些只是多个容器的集合
一堆这样的容器
每个容器又包含自己的代码集
对吧 你可能有一个容器
或一个可以运行节点的容器
JavaScript应用
另一个容器可能包含Ruby on Rails等
另一个用Java
另一个运行WordPress等
将所有这些容器
组合成名为Pod的单元
然后当然要创建标签进行标识
然后可以使用选择器查询这些标签
找出所有这些标签
请记住这些容器可能有数百万个
对吧 谷歌每周创建20亿个容器
因此需要某种管理机制
对吧 Kubernetes的核心是管理容器
因此引入了Pod概念
标签选择器
控制器和服务
总结来说
我们讨论了容器
它们的定义 或容器的局限性
它们本质上是内核命名空间
无法独立执行任何操作
无法自我扩展
因此需要Kubernetes来处理
我们也讨论了Kubernetes如何扩展应用
以及Pod等新术语
服务标签
就Kubernetes而言
我们也提到了这些概念
接下来我将梳理这些术语
确保理解后再结束课程
我将在线演示如何访问Kubernetes
并展示其界面
让我切换回浏览器
进入Chrome浏览器后
只需搜索Kubernetes，首条链接
你会看到网址
这就是kubernetes
好的，我明白了
所以点击第一个网址
这会带你进入kubernetes主页
你可以阅读相关资料
它是什么 再说一遍，它有什么作用
它只是一个自动化的容器
部署 扩展和管理
好的 这是一个开源项目
你可以去GitHub查看代码
当然也可以向下滚动阅读
并更深入理解kubernetes到底是什么
例如 如果我在这里打开GitHub
你可以在这里查看GitHub
所以我点击它带你去GitHub
并实际展示代码
这里你可以查看
可以克隆或下载
kubernetes的压缩文件
或者使用Git或通过SVN在本课中获取
只是想演示kubernetes的实际概念
它是如何工作的
在扩展应用时如何解决问题
希望这对你有帮助 接下来我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/015_Udemy - Become an AWS Certified Data Engineer part1 p15 9. How to Create Kubernetes Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，继续本节课的内容
我将演示如何创建kubernetes集群
我们已经在谷歌云平台注册了账号
没错 我们已经登录成功
登录后
可以创建一个项目，我已经创建好了
项目名称是sidash五七五七
创建项目后
当然需要拥有或创建项目
放置在kubernetes集群中
在kubernetes集群内
我们将创建一个实例
这意味着要启动一些虚拟机
以便开始与gitlab通信
拉取仓库并使用nx访问网页
可以在网站上查看
整个框架基本合理
没错 因为我们以谷歌云平台为基础平台
没错 就像一栋房子
可以想象成一栋房子
容纳所有这些内容
在平台上连接gitlab
那里有开发者
遍布全球
可能全国各地都在协作和合并请求
一些拉取请求
定期进行代码修改
所有操作实时同步
使用谷歌云平台的kubernetes引擎或工具
在开发可扩展或企业级应用时
这一概念非常契合
除了这个宏观概述外
本节课重点讲解创建集群
在谷歌云平台内
立即开始操作
我先前往第一步
看看kubernetes引擎
进入kubernetes引擎后注意
目前没有集群
我们需要先创建集群
点击创建集群，操作简单
直接完成
给集群命名
可以任意命名
例如 我可以叫它click
询问五个集群
因为这是我正在使用的GitLab或其他地方的项目对吧
你可以用不同的名称 如果你喜欢的话
这就是集群的名称
描述是可选的
你现在可以选择区域
我们位于美国 中部区A或者你可以选择东部区B
东部区C等等，这里有不同的区域可用于创建集群
N
然后是集群版本
你想要选择的机器类型
是一颗CPU还是多颗CPU
你可以有8颗或16颗CPU
96颗CPU这会增加费用
所以请记住这是按分钟计费
对的 这只是我们自己的使用
但在现实世界中
你可能会遇到需要96颗CPU的企业级应用
这非常强大 不管怎样
我们现在先保持为1
然后选择实际的机器类型
即1颗CPU 它会显示对应的内存配置
节点镜像提供更好的安全性和性能
但可能有一些限制影响部分用户
我们将使用Ubuntu
对的 如果你受这些限制影响
我们可以使用Shell和命令行
这是更简便快捷的方式
容器优化
操作系统是32核CPU
将占用约7.5GB内存
你可以增加到3核如果需要
这取决于你喜欢的内核数量
集群实例非常标准
我们可以保留所有设置继续
在点击创建前注意
它会提示你将为两个节点构建
集群中的虚拟机实例
如果点击更多
会显示更多高级选项
你可以添加额外区域并调整Kubernetes配置
Alpha功能
客户端证书
自动扩缩
启动盘大小等
那我继续操作
滚动到底部并点击创建
系统将开始创建
集群
命名为click ask five
集群大小为两
配备两个CPU和7.5GB内存
大约需要几分钟
有时三到五分钟完美
当集群创建完成后
我们可以连接到该集群
点击连接并查看提供的选项
命令行访问
只需复制并在终端运行
或打开云控制台仪表盘
我们先运行云壳
这会为你打开终端
你可以 当然 继续顶部操作
注意顶部菜单的小图标
即终端
与谷歌云壳相同
将尝试连接到集群
这仅用于演示已连接集群
自动获取命令以便连接集群
现在按下回车键
将连接到集群
首先获取集群端点和授权数据
配置kubeconfig并生成click s five
准备就绪
在本课中 演示如何创建Kubernetes集群
请多加练习 我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/016_Udemy - Become an AWS Certified Data Engineer part1 p16 12. Understanding DevOps.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到这节关于什么是DevOps的课程
在这节课中，我们将尝试理解当我们说DevOps时具体指什么
当我们说DevOps时
其中一个定义是
它本质上是开发的复合词
因此得名Dev和运维Ops
这是开发与运维的结合
它本质上是人的联合
流程与技术
它允许您持续为客户提供价值
最终目标就是为客户提供价值
传统软件开发在这方面存在不足
这意味着使原本独立的开发、运维等角色
质量控制
安全团队能够协调合作，产出更优的
更可靠的产品
通过采用DevOps
这更多是一种文化而非仅是流程
加上某些实践和工具
团队能够更好地响应客户需求
增强对所构建应用的信心并更快实现业务目标
最终目标就是更快推向市场
更快实施变更
并让团队协作而非各自为战
DevOps提供了诸多优势
我已列出其中主要优势
现在 采纳这种文化
其实践和工具将变得高效
能更快构建更好产品提升客户满意度
首先是加速市场投放
通过实施持续集成或持续交付
可大幅缩短产品上市所需时间
在当今全球化世界，时间就是金钱
这是陈词滥调
其次 它允许您适应市场和竞争
正如之前所说
由于加速市场投放
您可以适应市场变化
或新兴的竞争压力
如今
市场在全球化下日益统一
竞争每日都在加剧
第三
它允许保持稳定与可靠性 因为能突破部门壁垒
所有团队能协同工作
各团队能以协调方式运作
因此您拥有安全性
你拥有运营团队
你拥有产品经理
你拥有客户服务团队
人们本质上也是协作中的客户
这使你能构建稳定可靠的产品
最后是提高恢复时间
这意味着你拥有DevOps管道或自动化管道
如果需要进行变更
或代码/软件中存在任何错误
可以迅速进行修正
非常迅速 相比传统软件周期
现在DevOps通过多个阶段影响应用生命周期
计划阶段 开发阶段
交付与运维阶段
每个阶段相互依赖且不设角色限制
因此在真正的DevOps文化中
每个角色都会参与各阶段
所以说 这不是封闭的团队分工
有一个属于开发的团队
还有一个属于交付的团队
如果你是DevOps团队成员
你将在每个周期中承担角色
首先我们有计划阶段
团队在此阶段
你知道的 他们定义
描述正在构建的应用的功能和能力
跟踪从细粒度到粗粒度的进度
从单一产品任务到跨多产品多应用的任务
这就是计划阶段
开发阶段包含所有实际编码内容
你知道的 应用的实际编码
编写 测试
代码审查 这就是软件开发实际发生的阶段
现在我们明确了目标
我们已开发出软件
我们已编写了代码
接下来是交付阶段
这是将应用部署到生产环境的过程
但关键在于稳定可靠的方式
交付阶段还包括部署和配置治理基础设施
构成这些环境的基础架构
你知道的 以前你需要手动配置服务器
你需要配置多台服务器
并在应用程序部署前完成整个基础设施的搭建
你知道需要配置一台虚拟机还是十台虚拟机
必须物理启动这些设备才能使应用程序正常运行
如今 你知道的
随着云计算、DevOps和无服务器架构的兴起
完成这些操作所需时间可从数周缩短至几分钟
在交付阶段之后
我们进入运维阶段
这包括维护和监控
在实际生产环境中排查应用程序问题
当你采用DevOps
团队需要确保
你知道可靠性
可用性以及零停机时间
因为在DevOps文化中
本质上应实现零停机
在运维阶段
团队持续寻求在
影响发生前 客户体验
所以你所做的就是
持续监控应用程序或软件以在客户反馈前定位问题
DevOps的总体目标是实现客户满意
这一流程使你能高效快速地达成目标
除了建立DevOps文化
你知道的 团队通过在整个生命周期中实施多项实践来落地DevOps
我已列出一些主要实践
在DevOps生命周期中首先
这也是你们可能经常听到的 持续集成与持续交付（CI/CD）
CI/CD代表持续集成和持续交付
现在 持续集成是软件开发中的一种实践
CI采用自动化测试
每次提交新代码时自动运行
主分支代码始终保持稳定
因此
持续交付是频繁将新版本自动部署到生产环境
通过自动化部署步骤
团队减少部署问题并支持更频繁更新
CI和CD并非同义
它们不是同一概念而是协同工作
当两者都实施时
最终流程称为CI/CD
涵盖从代码提交到生产部署的全自动化步骤
使团队专注于编码并消除人工操作的负担和潜在错误
你知道的 传统软件开发中通常需要的手动或重复性步骤
这使部署新代码的过程更快且风险更低
所以再次说明 持续集成本质上是自动将代码变更整合到项目中
而持续交付则是自动部署新应用版本
因此在DevOps生命周期中可以仅使用持续集成
或者仅使用持续交付
因此可以选择其中一种
或者同时采用两者以实现全自动化DevOps流程
版本控制本质上是管理不同代码版本的核心实践
追踪它们 它们的变更历史等等
这使得在开发运维生命周期中变得更容易
尤其是当你使用GitHub或Azure仓库时
这让版本控制更加便捷
它还支持敏捷软件开发，本质上是一种强调团队协作的方法
客户和用户反馈
以及高度适应性
这些都是DevOps文化的基石
你还拥有基础设施即代码
这本质上是一种部署基础设施的代码
因此得名基础设施即代码
所以你实际上在做的事情，就像我之前提到的
就是在部署DevOps流水线
你无需手动配置基础设施
无需联系采购部门
无需寻找供应商购买物理服务器
你可以直接部署虚拟机
在几分钟内于云端部署服务器和整个基础设施
现在这一切都可以手动完成
本质上 你知道的
在Azure云上逐个部署虚拟机
或者你可以将所有内容编码到一个JSON文件中
比如说 如果你想部署五十台虚拟机
你需要有x
防火墙等数量
所有内容都可以编码到JSON文件中
然后你可以直接使用该文件部署整个基础设施配置管理
提及管理
管理系统中的资源状态
包括服务器
您的虚拟机和数据库
最后是持续监控
全面实时掌握整个应用栈的性能和健康状况
以及底层基础设施
如果您不在无服务器环境中工作
如果您将环境部署到Azure
自行管理云资源并维护
显然你也需要进行持续监控
这些基本上是DevOps文化中的主要实践
希望你们对我们所指的内容有了清晰的理解 当我们说DevOps时
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/017_Udemy - Become an AWS Certified Data Engineer part1 p17 13. Continuous Integration Continuous Delivery.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎回到本节课
我们将学习持续集成和持续交付的所有内容
首先让我们了解什么是持续集成
所以 持续集成是指将所有开发人员的工作副本合并到共享代码主线
每天多次合并并用自动化构建验证每个集成
这到底意味着什么，又是如何运作的
存在一个主分支，所有开发者都在同一个项目上工作
针对需要编写的特定代码分块编写代码
所以他们都在同一个项目上工作
但有多个开发者在同一个项目上工作
来自不同地方的代码不断产生
持续集成将所有代码合并到主分支
然后进行构建、测试并部署
单元测试通常在构建过程中执行
单元测试是每天多次测试代码行
与持续集成结合以确保代码质量
持续集成作为最佳实践出现，因为软件开发者常独立工作
然后需要将更改与团队代码库整合
所以一个软件开发者在美国工作
另一个在其他国家
例如 英国
还有一个来自其他国家
我们公司有多个软件开发者
如果公司有二十到三十甚至五十名软件开发者
共同开发一个项目
不在同一办公室
持续集成非常重要，因为它会整合所有开发者的代码
现在来看持续集成的优势
首要优势是提供快速的代码质量反馈
开发者提交代码或部署小块代码后
单元测试立即运行并检查代码质量
并即时反馈结果
帮助开发者改进代码
他们的代码
还会触发每次代码变更的自动化测试
以及代码分析和技术债务管理
持续集成减少长时间
复杂且引入合并时的错误，因为小代码块被整合
在主分支中，质量测试人员可以测试代码
也增强了代码在生产前的信心
代码刚写完后
增强开发者信心
确保他们编写正确且符合项目要求的高质量代码
这些是持续集成的部分优势
接下来介绍持续交付
持续交付是一种软件工程方法，团队以短周期开发软件
确保软件可随时可靠发布
就像我们之前学习的
这些是由开发者编写的代码小块
因此这些短小的代码块被称为持续交付
而持续集成则结合这些短小的代码块或代码片段
并旨在构建
更快更频繁地测试和发布软件
同时最终减少交付时间
而且 在实际操作中
持续交付专注于自动化的部署流程
然而 在进入生产环境前可能需要一个或多个手动审批环节
因此持续交付中的每个流程都是自动化的
然而
项目管理者可以在持续交付流程中插入手动审批环节
为了提高已创建产品的质量
现在持续交付的优势有哪些
它在整个流程中实现自动化测试
没有任何手动操作
整个流程完全自动化
提供透明度
因此
通过自动化测试
错误率可降低且速度更快
还提供快速反馈循环
也使进入生产环境成为低压力活动
由于反馈循环快速，在生产阶段前开发者更有信心
同时运营团队也更确信产品质量，压力更小
持续交付在生产前大幅提升代码信心
就像持续集成一样
这两个优势对持续交付和持续集成相同
它们都在代码需交付前大幅提升信心
现在持续交付与持续部署有何区别
我知道这些术语非常相似且容易混淆
但只需掌握一个主要区别即可区分
在持续交付中，每个更改通过完全自动化交付到预发布环境
应用需手动点击按钮部署到生产环境
这意味着持续交付流程中存在自动化任务
但需手动点击按钮部署到生产环境
当项目准备就绪时
需要有人手动点击按钮
以部署到生产环境并发布给客户
然而在持续部署中
无需手动操作，所有通过自动化测试的更改自动部署
运营团队无需手动发布到运维团队
这就是持续交付与持续部署的区别
这是持续交付与持续部署的区别图示
如图所示
首先是单元测试且完全自动化
单元测试完成后
接着是平台测试
这些也是自动化的
接下来是部署到预发布环境
这也是自动化的
第四阶段是应用验收测试
这也是自动化的
但持续交付在这里有所不同
需要有人手动部署到生产环境
然而在持续部署中
生产环境部署也是自动化的
然后进行部署后测试，这些也是自动化的
因此持续交付与持续部署的主要区别只有一个
希望你已经掌握了这些对DevOps流程至关重要的术语 好了，让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/018_Udemy - Become an AWS Certified Data Engineer part1 p18 14. CICD Difference.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，这次重要讲座我超级兴奋
简要介绍五到六张幻灯片
我将重点讲解持续集成与持续交付的核心区别
同时也会强调持续部署的相关内容
但主要我们将讨论并尝试理解
并在我们的AWS环境中实际应用
首先让我们了解什么是持续集成和持续交付
持续集成与持续交付的基本区别
它们并不相同
好的 简单回答
看高CD
它们是两个不同概念
例如 持续集成
每次代码提交都需要运行所有测试并构建代码
因此全球各地的开发者都在协作
例如 他们使用IDE提交代码
通过分支推送代码
在此过程中每次代码提交都需要运行所有测试
对的 这就是持续集成
持续交付
另一方面
就是持续集成对吗
意味着每次代码提交运行所有测试
在构建代码时
并部署经过测试的更新版本到生产环境
好的 这就是关键区别
持续交付流程完全自动化
但你也可以手动批准变更
这样就清楚了
现在你理解了持续集成与持续交付的区别
持续交付的优势如下
完全自动化软件发布
这就是我们使用的
持续交付与AWS代码提交
AWS代码部署代码管道允许我们实现持续交付和集成
可以更快交付更新
能及时高效地为用户创造价值
让客户无需等待软件发布
客户不必等待软件发布
关键点在于提升开发者效率和生产力
通常开发者还需处理其他任务
没错但在我经验中看到开发者自行进行单元测试
在此场景下可大幅节省开发者时间
还能加快反馈周期
本质上持续交付的核心优势
这里有一个视觉示例
所以你可以实际看到作用范围
如何实现CI/CD集成
然后我将演示或查看AWS示例
在此图表中
注意代码流程
这些提交的代码
你 提交代码
然后是CI流水线
持续集成流水线包含构建
执行一些单元测试
行为测试 功能测试
没错 集成测试，所有这些都在CI流水线中进行
所以一个工具 例如我们常用的CI工具Jenkins
CD流水线涉及代码审查
预发布和生产环境
这就是持续交付流水线
它有时也关联持续部署
接下来我也会演示这一点
这只是CI/CD流水线的高层次视觉示例
完美，最后
但同样重要的AWS示例
高效有效地处理CI/CD
代码提交 整个流水线本身
所以你需要提交代码的应用
开发者正在创建这个应用
然后提交代码
进入源代码控制
AWS Code Commit开始发挥作用
所有更改已提交
下一步进入构建阶段
代码被构建并执行单元测试
Code Pipeline进入预发布环境
在预发布环境中使用云Formation
这些是用YAML或JSON格式编写的模板
你有弹性Beanstalk
现在还可以使用OpsWorks，AWS也支持Chef和Puppet
新控制台方面
AWS最新的服务之一也支持Puppet服务器
最后部署到生产环境
这就是AWS遵循的一般流程
现在 关键要点是查看持续集成发生的位置
持续交付发生的位置
然后当然还有持续部署
持续集成贯穿应用程序源代码控制和构建流程
好的 这就是持续集成的核心所在
还有jenkins 例如
正如我之前提到的，这是一个流行工具
它是一个流行的构建工具
我们使用jenkins为项目执行CI
然后持续交付继续通过整个流水线流程
你可以批准部署操作
换句话说 你正在使用持续交付
你可以手动验证开发人员提交到流水线的更改
而在持续部署中则是自动部署
换句话说 从提交到生产环境
全部自动化
这真的令人兴奋
在aws环境中工作时这些选项非常有用
开发者可以在不同分支工作
例如跟踪问题
你使用jira
或者也可以使用github跟踪器
因此这个流水线中还有多个其他组件
我只是想确保你理解持续交付的概念
持续部署
持续集成
以及它们如何适配aws场景的能力
因为这是我们后续项目部署时会用到的内容
当我们实际运行所有项目的部署流程时
无论是spring java应用还是php laravel应用
我们将完整经历这个流程
我也会在课程中演示如何部署应用并进行修改
创建分支 然后使用持续交付、集成及部署技术
作为devops工程师必须精通整个流水线
这样才能优化流程效率
这一点很重要因为开发者正在提交代码
你需要监控整个流水线状态
查看代码哪里出错
例如 哪里存在问题
同时还要关注源代码控制和构建阶段的qa测试人员
例如
在预发布环境执行所有测试 不仅如此
你可能还需要查看项目管理人员 他们是否有权限访问整个项目
你正在管理或监控的项目
所以DevOps工程师需要具备多方面的
你知道你需要掌握技能和技巧来管理整个流程
所以希望这对你有帮助
如果你有任何问题
请在讨论区留言
我很乐意为你解答 我们进入下一课吧
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/019_Udemy - Become an AWS Certified Data Engineer part1 p19 1. Cloud Migration Benefits.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到这节课
我正在查看迁移至云端的不同优势
当您迁移应用程序和数据及基础设施时
需要使用最符合您和组织需求的云服务
并确保整个过程尽可能顺利
为了充分利用迁移后的云服务
必须确保获得尽可能多的云优势
所以我做了的是
我尽可能列出了这些优势
这绝非完整列表
这些是我认为组织迁移至云可获得的顶级优势
首要优势是更快的部署时间
迁移至云端意味着您可以更快部署应用
服务和基础设施
许多服务允许您快速配置服务器和其他资源
只需几步
甚至几秒钟
正如我们在AWS中看到的，只需几个按钮
即可为您配置完整服务器
这显然是更简便的流程
相比购买服务器
安装操作系统
新建数据中心等操作
即使是中大型组织
采购服务器也需要数周时间
这显著加快了该流程
接下来是增强的安全功能
大多数云服务商尤其是AWS
会处理部分复杂的安全问题
例如阻止未授权流量
限制特定范围访问您的数据和应用所在机器
并确保自动应用系统安全更新
防止受到最新已知安全威胁
需要注意的是
您仍需制定安全策略
在配置服务器和虚拟机时
但这并不意味着无需安全措施
这意味着部分固有安全功能
例如物理安全
例如这些
操作系统安全
或AWS的选配托管服务
例如数据库服务内置安全功能
这些无需您担忧
例如硬件或软件防火墙
AWS已全部提供
但仍需您进行配置和管理
例如如果您自行运营
但如果您使用其托管服务
则全部由AWS管理
接下来是基础设施复杂性的处理
云系统倾向于剥离底层架构中复杂的基础设施
让你能够非常轻松地配置新机器
就像我几分钟前提到的
本质上在AWS中
当你点击几个按钮
实际上就配置好了新服务器
如果这还不够简化
我真心不知道还有什么
现在还有监控的便捷性或内置状态监控
AWS提供的许多服务都能提供监控功能
当应用程序或机器出现潜在问题时你会收到通知
或正在经历服务中断
现在 这节省了大量时间
无需亲自监控每个资源
例如 AWS拥有许多服务
许多管理工具可实现此功能
例如CloudWatch
当发生特定操作时会自动发送通知
例如 当某些服务器的CPU超过阈值
或内存使用率超过阈值
会发送通知让你及时获知
无需担心服务器宕机
或未收到服务中断通知
最棒的是
例如 如果服务器真的宕机
几秒内就能快速配置新服务器
现在还有自动备份和监控相关指标的日志记录
备份和日志服务非常重要
尤其是在当今时代
数据至关重要
这使灾难恢复变得简单且流程简化
备份可快速恢复系统运行
然后是集中化管理
或称为单一视图界面
优秀的云服务提供商
AWS让所有服务
看起来如同单一视图界面
意味着有一个统一仪表盘
可查看整个基础设施的所有内容
在AWS生态中即可实现
生态系统和第二
但并非最后且尤其重要的是
现在成本降低
在AWS配置服务器时
按需付费
你可以扩展规模
你可以根据需求和需求进行缩放
例如 假设你有一个电商平台
你知道黑色星期一即将到来
你预计会有
你知道 流量会出现显著激增
或者你正在推出新产品
预计流量会出现显著激增
或者你现在能获得的访客数量
如果你不在云端
这意味着你需要购买额外的服务器
以确保最佳满足需求
如果流量不会持续，这笔钱可能白白浪费
如果流量不可持续
购买和配置的额外服务器或硬件
将闲置直到下次流量激增
因此你浪费了很多钱
在搭建服务时消耗了大量资源
所有这些在AWS和云端都得到了简化
这一点 例如
你可以配置服务器扩容
当流量激增时
当流量激增结束后
服务器自动停用，仅按使用量付费
所以 例如 当流量激增发生时
你配置了五台新服务器
并为此支付费用
只要流量存在
当流量下降时
这五台服务器自动关闭
因此成本也随之降低
最后
尤其是保护地球
我想称之为或减少碳足迹 如果你是大型企业
你需要建立数据中心
或者数据中心会显著增加碳足迹
对于许多跨国企业和全球性组织
越来越多的机构开始关注此类行为
这些行为对环境有负面影响
他们有许多CSR计划来应对这些问题
采取减少碳足迹的行动
通过利用现有的数据中心和硬件
由AWS提供的资源
或通过合作伙伴降低
您的组织所使用的硬件数量以及产生的碳排放量
所以这只是部分优势中的几个
这绝非一份完整的清单
但这些都是我认为在迁移至云端时最为关键的几点
迁移到云端并非完美无缺
迁移至云端也存在一些弊端
因为并非适合所有人
这些都是我在迁移过程中遇到的弊端
在协助某些组织迁移至云端时
首先 数据敏感性问题
现在 许多组织拥有
或处理极其敏感的数据
可能涉及某些法规
或位于特定司法管辖区
这会阻止他们将数据上传至云端
例如在中东某些国家
无法将数据带出该国
如果AWS或微软在该国没有数据中心
则无法将数据托管在云端
必须在自有数据中心托管
这些就是具体情况
显然在迁移至云端时
在特定情况下可能不可行
第二个弊端
如果您的架构非常小型
例如所有人远程办公
实际上没有服务器环境
此时迁移到云端并无意义
因为没有需要迁移的内容
大家分散在工作组中
各自使用笔记本电脑 远程办公
在这种情况下
显然无法获得之前提到的任何优势
接下来还有延迟问题
现在 显然如果迁移到云端
将所有服务器移至云端
如果没有稳定的互联网或云连接
用户将面临延迟问题
相比本地服务器或同一网络中的服务器
取决于地理位置的连接类型
这可能成为弊端
最后提到的控制权丧失
因为物理服务器不再在您直接控制之下
因此无法亲自接触服务器
最后一点是控制权的丧失
因为这些物理服务器不在您的直接控制范围内
你不能取出硬盘并随意插入
因此你也会失去对硬件的控制感
这对组织来说可能是一个潜在缺点
最后是切换问题
这更多是变革管理或文化转变
但这确实是一个缺点
因为许多组织对此持犹豫态度
可能因为涉及的金额
培训或变革管理的工作量
但本质上如果情况如此
那你很可能在大型组织工作 此时优势远大于缺点
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/020_Udemy - Become an AWS Certified Data Engineer part1 p20 2. Cloud Migration Strategies.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你好
大家好，欢迎来到这节课，我们将探讨现有的不同云迁移策略
或者我们在云迁移时可以采用的策略
目前市面上存在许多不同的策略
接下来我将带大家了解
许多组织常用的几种常见策略
第一个是被称为重新托管
也称为迁移即使用（Lift and Shift）
因为无需代码
或不需要进行高级编码来迁移基础设施
你只需将现有系统迁移到云端
组织采用这一策略的主要原因是时效性
因为这是一种 非常快速的云迁移方式
每个应用或虚拟机几乎可以直接在云端部署
这种策略适用于多种场景
首要原因就是
需要快速完成迁移
如果时间紧迫
这是云迁移时首选的策略
或已架构为使用AWS基础设施的应用
某些应用需要代码适配才能在云端运行
但若应用已适配云服务
这将是最佳选择
或需要应用但无需功能变更的企业
这与前一种情况密切相关
或仅能通过AWS虚拟机满足的数据库需求
例如 若需部署新数据库或需硬件升级的新应用
显然重新托管是最优选择
因为可以快速完成迁移
无需代码修改的应用
即可在云端高效运行
接下来是重构策略
也称为重新打包
这一策略 需要调整应用设计
但不改动核心代码
应用可利用
例如 IaaS、PaaS等AWS云服务产品
例如 其托管数据库服务
为何选择此策略
当现有代码库和开发技能需要迁移且代码可移植性重要时
这是理想选择
快速现代化应用的有效方式
如果您正在使用老旧应用
这不仅能迁移到云端
还能同时现代化应用
通过利用DevOps和容器技术推动持续创新
例如Docker容器或Kubernetes容器在复杂度链上的应用
我们进行了重构
这本质上是修改或扩展应用程序代码以实现扩展
并针对云环境进行优化
现在需要将应用现代化为高可用性
高度可扩展
可独立部署的架构并使用AWS加速该过程
可以轻松扩展应用并管理您的应用
轻松操作 因此这相当于在应用架构层面降低复杂度
那么何时需要使用这种方法
并利用现有应用
已有投资
满足扩展性需求
引入仅在云端和AWS可用的新功能到组织或应用中
并通过采用创新DevOps实践提升敏捷性
比如代码提交
代码管道可以在应用或架构中实施
最后我们有重建策略
现在重建是指
从头开始构建应用
使用云原生技术
平台即服务提供完整的云端开发和部署环境
无需软件许可的高昂成本和复杂性
无需底层应用基础设施
甚至中间件和资源
通过这种云迁移策略
您管理自行开发的应用和服务
而AWS负责管理其余所有内容
这样您可以专注于应用开发资源
并无需
无需担心底层运行环境
因此何时使用此方法用于快速开发当现有应用拖慢进度时
并且显然需要将业务提升到新高度
构建新应用 使用云原生技术
构建创新应用以利用
并充分利用物联网、AI或区块链等技术
区块链可以在云端运行
但许多云服务
尤其是AWS区块链服务大大简化了操作
加速创新进程
同样采用创新DevOps实践
这些就是大多数企业采用的四大策略
无论是重新托管 重构
重新架构 还是重建
再次强调 从简单到最复杂的推进
这就是重建
这完全取决于你的组织
你的使用场景 你的业务
你想要实现的目标 如果你只是想迁移到云端
并且所有现有资源都能无缝迁移
无需任何修改
当然我们希望这种情况成立
但如果你的组织大量使用遗留应用
那么你可能需要考虑其他三种方案
甚至重建整个应用程序 或从零开始构建整个基础设施
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/021_Udemy - Become an AWS Certified Data Engineer part1 p21 3. Cloud Migration Process.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎回到
这就是第 我们接下来要做的就是
我们将了解涉及的流程
将您的基础设施迁移到云端
本质上这是一个四阶段迁移流程
旨在帮助您和您的组织以系统化方式推进迁移
你知道的 几个 几十个
几百个 甚至数千个应用程序迁移到云端
虽然每个阶段都有共通之处
是成功迁移的关键要素
但并非独立阶段
而是一个迭代过程
随着您逐步迁移更多应用
能够反复且可预测地优化流程和步骤
从而加速迁移进程
AWS 提供了一整套全面工具
并与多家第三方组织合作提供自动化和智能建议
基于机器学习技术
简化并加速四阶段迁移的每一步
迁移过程包含四个阶段：评估
准备与规划
实际迁移
以及持续运营与优化
现在让我们逐一深入了解
首先我们来看评估阶段
在迁移旅程的起点
您需要做的是
确定组织当前在云端运营的就绪水平
以及迁移可能带来的业务成果
因此对现有环境的初步了解至关重要
例如了解您的基础设施现状
这有助于制定迁移的商业方案
因为迁移并非低成本过程
实际成本可能因规模而大幅增加
因此需确保向管理层提交有力的商业论证
通过数据和实际资源使用情况
可以更精准预测总拥有成本
或运行这些工作负载的总成本
AWS 和第三方提供了大量工具
评估本地资源
并为在 AWS 上运行应用构建精准优化的成本预测
例如 TSU 逻辑
这是 AWS 拥有的另一家公司
可基于资源实际使用情况预测 AWS 总拥有成本
迁移中心
我们稍后会在本课程中详细讲解
生成正确的e
C 针对本地部署工作负载的两个实例或虚拟机推荐
因此可以帮助您通过云价值框架开发此商业案例
这是一种经过验证的方法，能生成有说服力的商业案例以说服管理层
还有一个云采用准备工具
帮助制定云采用和企业迁移计划
它基本上从多个视角评估您的准备情况
例如业务人员流程
平台运维与安全
完成调查后
您可以 你有很多
你知道的 将生成定制的评估图表和就绪度报告，您可以使用
并将其作为商业案例的一部分
现在它还有一个热力图和雷达图的实用功能
帮助评估组织的就绪度评分
这些都是评估组织是否准备就绪的三大核心工具
甚至判断云是否是正确选择
如果您是大型组织
假设您的总拥有成本达到
数万甚至数百万美元
可能最初的商业案例并不如预期强烈
完成评估后
如果已成功证明并准备好迁移
下一步是准备与规划
在此阶段
您需要解决评估阶段发现的组织准备缺口
解决评估阶段发现的就绪度问题
在此阶段您需分析环境
创建服务、基础设施和应用的关联映射
并确定迁移策略，如之前课程所述
您是重新托管还是重建
从而制定每个应用的详细迁移计划并设定优先级
完成之后您将
建立安全且架构良好的AWS环境和账户
这也被称为AWS着陆区
在准备与规划阶段
可创建包含初始迁移经验积累和优化商业案例的迁移计划
同时聚焦构建基础环境和运营就绪度
并在此期间提升技能
特别需要注意的一点
迁移成功的关键是收集应用资产数据并优化应用
使用常见的迁移策略
如重新托管、重新平台化或重建
需确保应用已准备好在云端运行
AWS在此阶段提供多种服务
例如 有应用发现服务
自动收集并呈现所有应用程序依赖关系和使用情况的详细信息
帮助您做出更明智的决策，选择适合组织的策略
他们还有风险管理和云迷宫等第三方合作伙伴
并添加德勤数据以辅助这些工具
或者在这个阶段现在
该平台自动规划和跟踪跨多个AWS及合作伙伴工具的应用迁移
因此它类似于迁移过程中的主控仪表盘
还有AWS架构转换工具
通过自动转换源数据库架构使异构数据库迁移可预测
并将大部分数据库代码对象转换为目标数据库兼容格式
所以假设你要进行迁移
你拥有Oracle数据库
并希望迁移到亚马逊Aurora
你需要架构转换工具确保架构转换
以适配Aurora数据库
类似我提到的着陆区
帮助您建立基于最佳实践的多账户AWS环境
基于最佳实践
在开始迁移首个应用之前
该应用 着陆区帮助您设置初始安全基线
最后我们有控制塔
帮助设置自动化着陆区
这是一个架构良好的多账户环境
所有操作不仅自动化且一键即可完成
使迁移过程更加简便
完成评估后
现在已做好就绪和规划
是时候实际迁移资源了
此阶段将从端口级别转向单个应用级别
因为每个应用都将被设计
迁移和验证
需要具备自动迁移能力
迁移一个或数千个来自不同源环境的应用
无论是物理 无论是虚拟到AWS
现在 这些应用通常涉及广泛使用的开源数据库
此外
可能需要一次性迁移大量数据到AWS
再次 迁移帮助您管理所有这些
许多应用的最佳方案是快速迁移至云端
然后在AWS中重构
云内迁移快速迁移多个源平台的大量机器再次
无论是物理或虚拟到AWS
无需担心兼容性
性能中断
或长时间切换窗口
对于无法在服务器上安装基于代理的迁移服务的情况
服务器迁移服务提供无代理服务，使过程更加便捷高效
用于将数千台本地工作负载迁移到AWS环境
现在基本上只需一次快照
如果您使用的是基于VMware Cloud Foundation的环境
VMware Cloud on AWS可快速迁移数百个应用
将虚拟化在vSphere上的应用迁移到AWS云仅需数天时间
最棒的是
它能保持与本地环境一致的操作
因此这是一个非常强大的软件
AWS为迁移过程开发的一套工具
迁移完成后显然工作并未结束
接下来需要持续运维和优化
确保迁移到AWS的应用和服务正常运行
并进行优化
在此阶段再次
高效运营
管理 在云中优化工作负载
理想情况下
您可以利用已有的基础能力
可使用AWS管理与政府服务实现端到端管理
为AWS和非AWS资源提供全生命周期管理
AWS托管服务可通过持续管理帮助加速迁移
成本 优化和运维基础设施
如果您缺乏迁移基础设施或应用上云的专业能力
也可通过AWS托管服务获取支持
借助其管理和治理服务帮助迁移并优化应用
因此这四个步骤构成了云迁移的主要流程
首先是评估 然后是准备与规划
接下来是实际迁移 最后是运维与优化
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/022_Udemy - Become an AWS Certified Data Engineer part1 p22 4. AWS Migration Hub.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到关于迁移枢纽的课程
现在我们已经了解或掌握了迁移枢纽的概念
呃 我们对云基础设施和迁移策略有了很好的理解
以及为什么应该或不应该迁移到AWS
迁移枢纽基本上是AWS主要使用的工具
帮助您以高效方式将资源迁移到AWS云
以非常流畅的方式
这是AWS提供的主要控制面板
允许您管理所有内容
整个迁移流程
需要注意的一点是您会看到
云室内 这是一个免费服务
免费迁移服务
我们将在下一实验室中查看
帮助您将基础设施迁移到AWS
本质上正如前一课提到的
由AWS拥有和运营的公司
这也是为什么使用云门在AWS内迁移非常顺畅
所以在主控制面板中可以看到
它会显示
您最常用的应用程序 您使用过的工具
迁移状态方面
服务器和应用程序的启动进度、完成和超时情况
因此它为您提供整个迁移生态系统的详细视图
并在这里右侧
您可以快速访问AWS提供的各种工具
我们之前简要讨论过的连接器
工具
程序 例如
如果您需要AWS的帮助 所以在这里
在左侧 是AWS提供的控制面板的各种选项
所以迁移至AWS生态或云端的主要步骤
总体而言
第一步显然是发现阶段 即我们进行发现
或您进行发现
了解环境中运行的所有基础设施和应用程序 完成发现后
接下来将所有基础设施迁移到AWS
并在完成后
还有一个评估选项
这是一个将在后续课程中详细讲解的优秀工具
根据当前硬件为您提供虚拟机或硬件的推荐配置
基于您的现有硬件
所以这是一个非常非常好的工具
尤其是当你在迁移过程中考虑重建策略时
这是一个非常值得使用的工具
在发现选项中，我们有多个选项在发现面板内
我们有服务器 应用程序
服务器中的数据收集器和工具
它能很好地概述迁移过程中拥有的服务器数量
你的服务状态如何
你包含的服务器有哪些
你未包含的服务器有哪些
正如大家可以看到的 出现了一个错误
因为我正在删除并恢复到原始状态
现在每个服务器都被分组到所谓的应用程序中
应用程序是很好的方式来
嗯 将各种服务器分组
因为大多数时候
假设你在做网页环境工作
你会有一到多个充当网页服务器的服务器
或充当数据库服务器的服务器
或充当文件服务器等
将这些服务器分组到应用程序中会更好
这样你可以逻辑地将基础设施迁移到云端
当然并非所有组织都适用这种情况
你可能只有一个服务器和一个应用程序
因为你只有一个处理所有任务的服务器
或者你有一个网页服务器
然后还有一个文件服务器等
每个应用程序中不需要有多台服务器
嗯 但它们会被分组到应用程序中
接下来是数据收集器
这些是运行在网络服务器上的代理程序
用于收集服务器数据
我们将了解如何使用数据收集器
收集基础设施中的硬件或应用程序信息
这里有三个主要工具由AWS提供
因此我们可以将基础设施导入AWS
例如
如果我们不想在基础设施上运行任何发现代理
我们只需硬编码要导入AWS的内容
在你知道的情况下 应该进行配置
我们可以导入CSV文件
所以我点击导入模板
这里是一个基本的导入模板
我们可以在此添加任意数量的服务器
对的 所以但就生物信息而言确实非常详细
IP地址
MAC地址 等等
所以嗯
我会将此包含在下载部分供你们查看
或者你们也可以再次或更好
你们也可以直接从AWS仪表盘下载
所以如果你想手动操作
你可以这样做 或者你可以使用发现连接器或发现代理
主要区别在于如果你运行的是VMware环境
则应使用发现连接器
因为它是一个无代理连接器，适用于vSphere和vCenter
如果你不使用VMware环境
则应使用发现代理
它基本上会安装在物理服务器或虚拟机上
并收集所有数据
我们将在后续实验中查看如何使用发现代理
此外你还可以点击这里
它提供了各选项差异的详细信息
它提供了成本的清晰拆分
这三者都是免费的
支持导入
或发现连接器/代理
这是一个非常实用的表格供你参考
并了解哪种更适合你的需求
你还可以查看合作伙伴方案
可以查看各种合作伙伴
AWS提供的这些合作伙伴帮助你迁移基础设施到AWS
这些都是AWS提供的不同发现工具
然后是迁移工具
它会引导你完成迁移流程
当前待处理的内容
已完成的内容 或任何未完成事项
这里会列出所有应用程序
你知道的 文件服务器
数据库服务器 等等
这些工具包括之前的三个工具
以及代理用于发现基础设施
在基础设施发现后实际迁移到AWS
有多种工具如服务器迁移服务
还有云内网
服务器迁移服务内置在AWS生态系统中
云内网是独立组织
但也是AWS公司
或还有第三方方案
这是一种运动或河流金属
你可以利用它来帮助你将基础设施迁移到云端
然后AWS还提供数据库迁移服务
专门帮助你将数据库迁移到云端
所以在完成发现阶段后
你可以使用这四个或五个服务或工具
这些由AWS或第三方提供的服务或工具
实际上用于迁移你的基础设施
基本上是复制服务器中的所有内容并迁移到云端 这就是迁移枢纽 这是一个非常强大且高效的工具，可帮助你管理整个迁移流程
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/023_Udemy - Become an AWS Certified Data Engineer part1 p23 5. Discovering your network infrastructure.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到本课，主题是通过迁移枢纽发现您的网络基础设施
在开始之前我们需要完成几项准备工作
接下来我们将开始发现网络基础设施
首先我们需要在Windows服务器上操作
需要修改防火墙以允许特定端口
端口号443需要放行
这是代理程序通信所用的端口
这就是我们需要完成的第一项任务
现在要确保防火墙已开放该端口
为了本教程的目的
我正在使用Windows Server 2019版本
在演示如何发现网络基础设施时
并将其迁移到AWS环境
如大家所见 我有一台运行Windows Server 2019的虚拟机
它 这是一个标准版评估版
但这并不影响操作
唯一区别是评估版不包含Hyper-V
尤其是当您在虚拟机中操作时
嗯 不过
首先我要做的是
我要在防火墙中创建新规则
将允许TCP端口
刚才提到的端口放行
嗯 确保代理程序正常运行
我将创建自定义规则并点击下一步
我要对所有程序生效，协议类型选择TCP
端口号 可以允许所有端口
但这样存在安全风险
不不 我将指定特定端口
具体是443端口
点击下一步，选择本地IP地址
此规则适用范围
为了演示方便我选择全部
但在生产环境和安全要求下
需确保仅允许特定IP地址
由于这是演示
我直接允许全部
确保连接可以正常通过
并为其命名
然后点击
完成，现在防火墙规则已创建
我们可以继续下一步
接下来我需要安装Visual C++运行库
以便网络代理程序能够运行
所以大家可以看到
我已经导航到下载中心
嗯在微软那边
我想下载的是vc redist x86
要确保是x86版本
而不是x64版本
呃 无论你使用什么架构
即使你在64位操作系统中
务必下载x86版本
因为网络发现代理专门运行在x86架构
所以必须下载x86而非x64版本
无论你使用什么架构
我现在要下载这个文件
接下来我要进入aws控制台
然后进入iam管理
因为我需要创建有凭证的用户账户
我将进入用户管理
我要添加新用户
目前还没有创建过
我将其命名为agent discovery用于演示
要确保他们有程序访问权限
仅为了演示
我还允许控制台访问权限
但仅程序访问即可
如果需要的话
让我调整虚拟机的显示设置
快点 这样大家能看到全屏
我将调整分辨率设为1600x1200
搞定
我返回控制台
现在你应该 现在大家能看到全屏
搞定现在能看到所有选项
大家可以看到我已经选择了程序访问
仅为了演示
呃 控制台访问权限
我将为其设置登录密码
并取消勾选这个选项
不需要他们创建用户
关于权限部分
要确保为用户附加现有策略
以便发现代理能与aws环境交互
接下来我们要点击
附加现有策略
而不是滚动查找
我直接搜索aws应用发现
我们继续 所以我们有两个选项
呃 我们将选择这两个选项
并确保用户被添加到这些策略中
继续点击下一步
如果你想添加标签
随时欢迎你
但我就直接在这里创建用户 好了
呃 现在我们有几个要做的事情
我们需要记录访问密钥和秘密访问密钥
它们将被发现代理使用
用于登录并与AWS环境交互
我将快速将这些复制到记事本
这样我方便参考
这就是访问ID
我也要复制秘密访问密钥
务必复制这个
一旦离开屏幕
你就无法再次看到秘密访问ID
你将不得不创建新用户账户
务必记录这些用于程序访问
现在我们已经完成了这两项准备
我们已下载Visual C++代理
并且已在IAM创建了用户账户
接下来我要前往迁移枢纽
我点击发现
因为我想要发现我的网络基础设施
我们可以点击黄色按钮
或者这里使用AWS提供的发现工具
为了再次进行网络发现
我们有三个选项
我将使用Windows代理
它会下载到我的下载区或桌面
取决于你保存的位置
这就是第三步
再说一次
呃 第一步是下载Visual C++
接下来是为代理创建IAM用户以进行发现
最后是下载发现代理启动运行
好了 现在这两个项目已下载到桌面
我将打开
接下来 我将打开命令提示符
然后安装AWS发现代理
使用此代码的MSI
再次 大家看到的密钥ID和密钥密钥
这里只是从我的便签本复制粘贴过来的
我已经保存了
哦 我忘记了一件事
显然我们需要先安装Visual C++ 分发文件
在继续安装代理之前
我马上双击运行它
安装过程非常快速
安装完成后
那时我们就可以运行
这时我们可以再次运行这个命令
来安装发现代理
安装发现代理的过程非常简单快捷
好了，安装完成后
我们继续点击完成
当安装完成后
我们将进入数据收集器
显然你需要收集基础设施或服务器的数据
我们点击代理选项卡
在这里可以看到代理已自动显示在我的仪表盘
你们可以看到代理ID
主机名
收集内容
健康状态等信息
接下来我们要启动数据收集
这里我们有不同选项
我们有一个数据收集选项
可以启用Amazon Athena数据分析
比如你拥有大量服务器和应用
我们可以这样做
如果想使用Amazon Athena的强大查询功能
我们将启动该代理
这样数据收集才能正常进行
如果回到服务器管理器
你们可以看到这两个服务已自动启动
AWS发现更新和发现代理已在服务器上启动
我的服务器也被自动填充
或者发现代理也出现在迁移中心
你们可以看到状态是否健康
一旦启动后
你们会发现服务器会开始出现在服务器部分
此时我只有一个服务器
然后是我的虚拟机
这就是为什么只显示一个
但如果你有十台、五十台或上百台
它们都会逐渐显示出来
随着代理开始运行
发现环境中的各种基础设施
这里会填充服务器的详细信息以验证
我们可以看到AWS仪表盘中的IP地址信息
那么我的服务器实际的IP地址是否确实匹配
也就是十点零点二点十五
这样我们可以确认确实是发现代理发现的同一台服务器
我们可以看到核心数
CPU信息 硬盘情况
现在有了非常详细的信息
如需更多详细信息
我们可以前往亚马逊
Athena和我们的仪表盘
在这里我们可以看到已经为我们创建了数据库
关于我们服务器的情况
我们可以运行不同查询来获取服务器数据
关于我们的基础设施
以获取我们拥有的更详细信息
以及如何迁移至AWS
这就是发现代理的核心功能
关于网络中已有的资源
以及如何将这些信息迁移到AWS 下一步是将这些信息或服务器迁移到AWS
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/024_Udemy - Become an AWS Certified Data Engineer part1 p24 6. Migrating your network infrastructure.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，现在我们已经完成了网络基础设施的发现工作
接下来我们将进行迁移到AWS的下一步操作
大家可以看到服务器已成功填充信息
发现代理已填充了服务器的规格信息
并显示了已安装和未安装的软件情况
在信息填充完成后
我们需要选择要处理的内容
在信息填充完成后
我们的目标是将其归类为应用程序
我们要将这台服务器归类为应用程序
接下来我们将为其命名
这样做是为了根据用途对不同服务器进行分组
了解它们的具体用途
比如财务系统 比如Web服务器
比如数据库服务器等
我现在将其归类为应用程序
或创建应用程序并包含这台服务器
当我进入应用程序管理界面时
可以看到服务器已自动添加
接下来我要做的是
点击该应用程序
可以看到已有一台服务器被添加到此应用
完成这一步后
我将点击工具选项
这里列出了所有迁移工具
我们之前讨论过的内容
AWS提供的迁移工具
我们可以使用数据库迁移服务
我们可以使用数据迁移工具
River Meadow 云门服务
或者直接使用服务器迁移服务
AWS的服务器迁移服务
稍后课程中我们将重点介绍
在本课程中
我们现在要查看Cloud Indoor
这也是AWS组织的一部分
这是一个免费提供的服务
如果您在测试或免费层级环境中
可以免费使用Cloud Indoor
正如大家看到的
我已经登录到Cloud Indoor控制台
接下来我要做的是
现在需要输入AWS访问密钥ID和秘密访问密钥ID
如果您之前创建过相关凭证
可以直接填写这些信息
但因为我似乎丢失了配置文件
我将快速创建新用户以访问AWS账户
以便输入Cloud Indoor
你们可以看到我之前创建的代理发现功能
但我把文件弄丢了
我觉得我删除了包含访问密钥ID和密钥的文件
所以我将快速创建另一个用户
命名为云内网
这样便于区分
赋予它们访问权限
给予权限 设置特定密码
搞定 现在我将输入访问ID和密钥并点击
在云内网仪表盘保存AWS凭证
这样系统会跳转到下一步设置
接下来是复制设置
迁移源显然是其他基础设施
如果你计划将云内网用于生产环境
还可以从其他区域迁移过来
在AWS内部
使用云内网也能实现
但我的目标是源为其他基础设施
也就是我的服务器
并迁移到美国西部俄勒冈州
完成之后
还有一些其他设置可以指定
关于复制服务器
磁盘类型
安全组等
这些都属于云内网专属配置
这也是AWS组织所有
在复制服务器部分
我可以指定 应配置哪种服务器作为复制服务器
基于我的内部服务器
这些复制服务器主要用于内部
用于数据复制
在AWS生产环境中部署前
还有其他安全组设置
如果使用直连网络
是否需要暂存区域标签
或带宽限制
具体取决于设置需求
以及环境内的复制类型
我保持所有设置默认
搞定 现在项目已配置完成
我只需点击演示
展示如何让云内网在服务器上运行并正常运作
我将从云内网仪表盘下载Windows代理
因为我使用的是Windows机器
你也可以为Linux系统操作
呃 如果你在运行Linux机器
一旦下载完成
我只需复制这个命令
他们已经为我们清晰列出
我将直接在命令行中运行这个命令
提示符 显然我需要进入同一文件夹
我下载了那个Cloud Indoor应用的文件夹
你们可以看到installer_wind.exe
在这个文件夹中我将运行这个命令
这需要几分钟时间，这个命令会
从云端下载并安装代理到你的服务器
并在仪表盘启动复制流程
你们可以看到已连接到控制台
正在检查磁盘空间
这是服务器的磁盘空间
我们正在尝试复制并迁移至AWS云
你们可以看到已识别并找到
五十吉字节硬盘
这就是我的 这是我创建虚拟机时指定的
所有磁盘复制已成功识别
完成识别后将下载Cloud Indoor代理
下载完成后
将在服务器上安装Cloud Indoor代理
就是这样 已下载并安装完毕
现在将其添加到云室内控制台的源机器
可以看到实例ID并成功完成
当我返回云室内界面时
就是这样 可以看到一台机器已添加到云室内控制台
并启动数据复制
这就是如此简单
或者如此直接
我不该说简单 其实非常直接
关于从基础设施迁移到AWS云的启动过程
审计日志提供更详细的执行信息
这就是使用Cloud Indoor的方法
在网络发现之后
我们如何迁移到AWS云
再次 正如我之前提到的
我们将继续查看
稍后课程中会介绍AWS服务器迁移服务 在本课程中
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/025_Udemy - Become an AWS Certified Data Engineer part1 p25 7. Amazon EC2 recommendations.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到关于AWS双实例推荐服务的课程
该服务由AWS提供
在我们的迁移中心仪表盘中
我们有一个选项为
C 双实例推荐
当我们开始使用这个功能时
我们可以指定几个AWS选项
推荐使用哪种
C 双实例
首先设置规格偏好
我们基本指定该服务器的使用类型
然后我们将部署到哪个区域
在共享还是专用定价模型中启动这些服务器
专用定价模型
嗯 同时也可以排除某些服务器类型
比如排除昂贵的加速计算实例
系统将不考虑这些
我们可以勾选此选项
当我们执行并导出推荐结果时
它会执行的操作是
生成一个Excel文件，对比已发现的现有服务器
使用网络发现工具
我们在之前的课程中使用过
并与行业最佳实践进行对比
为每个发现的服务器生成推荐文件
如大家所见
目前只有一行记录，因为仅发现一个服务器
我目前只运行一个服务器
它会为迁移中心发现的每个服务器生成推荐
这是一个非常详细的Excel文件，提供全面推荐
基本上涵盖所有内容 您的CPU
您的内存 哦
是否应使用预留或专用实例
您应配置的硬盘类型
硬盘容量大小
这是一个非常详细的Excel文件，再次提供
这些推荐基于AWS对您输入数据的分析
并基于已迁移的服务器
因为记得服务器已迁移
发现代理还会检查
您知道的 您的CPU利用率
您的内存利用率
您的网络带宽利用率，对比所有这些指标
这里有一个基本表格，可以带你
并详细描述每个字段
所以大家可以看到 这是一个相当详尽的列表
在细节方面
在提供的建议方面
我发现这对许多希望
你知道的
如果你的迁移策略不仅限于重新托管
如果你在重建
这是一个非常非常好的工具
因为你可以使用行业最佳实践
AWS积累的知识
呃 你知道的 凭借其广泛的数据中心和众多从小型企业到大型企业的客户
他们开发了一些非常优秀的指标来帮助你选择
哪种硬件最适合你的使用需求
这不是一刀切的
适用于所有情况 并非所有建议都完美
但如果你在重建网络基础设施
这是一个非常非常好的工具 能帮助你了解应在AWS中使用和配置的内容
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/026_Udemy - Become an AWS Certified Data Engineer part1 p26 8. Migration acceleration program.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎回来
在本节课中 我们将了解一个程序
AWS称之为迁移加速计划
那么它是什么
许多组织显然正在迁移到云端
出于各种原因
所以现在 虽然每个组织都有其独特的驱动力
为什么他们想要迁移到云端
每个组织都希望保持透明并降低风险和成本
在迁移到云端过程中
这一点在任何组织中都通用
无论驱动因素是什么
因此MAP或加速计划旨在帮助组织
主要是致力于迁移旅程的大企业
通过将现有工作负载迁移到AWS实现多种收益
现在MAP专门设计提供咨询
支持 培训和服务信用以降低迁移到云端的风险
建立强大的运营基础并抵消迁移的初始成本
他们在加速计划的实施方法上有独特之处
他们建立了在AWS上运行关键任务的基础
工作负载
您将构建可在各种项目中复用的能力
AWS拥有多种资源支持和维持迁移工作
他们在加速计划中的方法有独特方法论
主要包括迁移到AWS的三阶段方法
首先进行MRNA
即准备评估阶段
在此阶段他们会
确定迁移准备状态
并识别已有的强项
以及需要进一步发展的领域以实现规模化迁移
换句话说
他们实际上是在
沿多个维度评估您的云就绪状态
我们之前讨论过的内容
他们会检查着陆区
运营模式
安全与合规
迁移流程经验
内部技能
等等 并评估您当前状态与目标状态
接下来是准备与规划阶段
或MRP阶段
他们将
指派顾问团队帮助建立大规模迁移基础
并获得AWS专家经验
你知道他们拥有专有的方法论和流程
用于在迁移过程中实施最佳实践
核心目标是降低总拥有成本并最大化你的r i
这就是主要原因
本质上 许多企业使用此迁移加速计划的主要原因
是因为他们规模庞大
你知道他们拥有全球办公室
拥有海量数据
拥有极其复杂的基础设施
并希望迁移到云端
而且他们真的不知道从何开始
这就是加速计划的作用
AWS利用其专业知识和全球网络帮助你决策
确定最适合你的策略
如果迁移实际上
是你可行的选择
如果如此应采用何种策略
你知道你的服务器迁移
你的数据迁移
你的数据库迁移
从头到尾的迁移全包
整个过程完成
在就绪规划通过后
这才进行实际迁移
这里进行物理迁移操作
再次强调 根据企业规模可能需要一个月到两年时间
我见过一些企业
以稳定节奏迁移
这需要
你知道超过一年时间将资源从本地迁至云端
这个计划值得记住
如果你在为大型企业或咨询机构工作
且企业缺乏相关技术能力 可引入AWS协助迁移流程
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/027_Udemy - Become an AWS Certified Data Engineer part1 p27 9. AWS managed services.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎回来
在本节课中 我们将要做的就是
我们将了解AWS提供的托管服务
在迁移过程中
对于企业客户
我正逐步转向大规模采用云服务
有些人找到了人才，有些人则没有
那么AWS托管服务的作用
由我们代为运营AWS
提供安全合规的AWS落地区域
经过验证的运营模式
持续优化
日常基础设施管理
基本上由我们代为管理您的云基础设施
现在为什么要实施
利用他们的最佳实践和专业知识维护基础设施
他们帮助您降低运营负担和风险
尤其是如果企业内部缺乏相关 expertise 的话
它实现了自动化
许多常见操作
例如变更请求
监控补丁管理与安全
基本上涵盖了您本地云服务团队会执行的所有工作
AWS 为您完成这些
因此您实际上是在外包
不仅将基础设施迁至云端
还包括整个基础设施的云管理
显然有很多好处
嗯
我这里列出三个主要优势供您参考
首先
再次强调，安全性与合规性得到提升
因为它提供分步流程扩展云安全
身份认证 合规边界延伸至云端
并包含
您知道的 一些关键任务
例如 嗯 如果您使用Windows环境
Active Directory集成
或者您运营电商网站
您知道的 PCI DSS合规或GDPR合规
帮助您管理所有这些内容
如果您缺乏相关 expertise 的话
他们还能帮助加速云迁移
因为提供经过验证的企业级运营环境
这使您能够在几天内迁移生产工作负载
这可能在几个月内完成
你知道他们与许多不同合作伙伴合作
第三方帮助使迁移过程更加顺畅
最后您还消除了部分创新障碍
企业DevOps是
你知道的 本质上是现代开发最佳实践与现有IT流程框架的结合
例如ITL
现在它们提供速度与敏捷性同时保持治理
你知道它们维持安全性
它们维持合规控制
通过托管服务
这使企业DevOps成为可能
它 将所有内容打包为IaaS模型
你知道的 或基础设施即服务模型
构建为安全合规的平台让组织立即上云
现在运作原理是 其实是一个相对简单的过程
你知道的 在托管服务中
你拥有基础架构 包括迁移和运维
在基础架构部分
从预期运营目标反向实施虚拟私有云并构建完整基础设施
所有你需要自行完成的工作
在规划阶段
AWS为你处理就绪性工作
然后实际迁移
他们再次代表你执行
使用自身托管服务或工具
必要时结合第三方工具
例如 如果你有VMware vSphere
会使用特定工具或架构转换工具
如果你有Oracle数据库
迁移到Aurora
所有操作均由我们代为完成
然后是运维部分
持续维护和优化基础设施及应用
部分功能
嗯 更详细说明托管服务为您提供的功能
首先是
资源部署
使您能够快速
嗯 你知道的 并且可以轻松部署您的基础设施
大大简化了按需资源的配置
你知道的 云基础设施堆栈
虚拟机和应用程序
等等
它为您执行监控和事件管理
基于行业最佳实践配置日志和警报
因此您无需学习和配置补丁管理
以及业务连续性管理
它负责处理所有补丁管理事宜
在利用最佳实践进行备份方面
这些实践针对您的行业或组织
然后是可用性保障
嗯 你知道的 它在全球多个区域托管
每个区域都是独特的地理区域
托管服务的所有组件均部署
经过验证 并在区域内运行
然后是安全与访问管理
保护信息资产并确保基础设施安全
无论是防病毒软件
还是防恶意软件或入侵检测
或IPS 所有工作均由我们代为完成
合规性可能对某些组织至关重要
尤其是拥有电商网站的公司
或需要
嗯 遵守HIPAA或GDPR或PCI DSS
所有合规要求
所有复杂问题均由AWS管理
然后是变更管理
你知道的 提供安全高效的基础设施变更控制手段
确保合规性
确保应用程序连续性
保障业务流程
具备事件管理功能
最后是成本管理
你知道的 个人云服务交付经理会
本质上是您的专属账户经理
每月提供指标摘要
活动记录
成本明细 您可以从中获益
您可以灵活扩展
在哪里你可以缩减规模
所以本质上所有事情都由我们代为管理
你将拥有专属客户经理
他们为你处理所有事务
现在 当然这需要一定的成本
嗯 因为这专为企业组织设计，旨在将IT部门的工作转移至AWS
但许多组织可能从中受益
比如他们自身缺乏相关技术能力
在企业内部培养技术能力是一个漫长且复杂的过程
即使你成功引入了技术专家
你也无法确定他们是否真正具备专业水平
如果你将IT基础设施外包给AWS
你可以确信专家团队正在为你管理 并将你的基础设施迁移至云端
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/028_Udemy - Become an AWS Certified Data Engineer part1 p28 10. AWS Cloud data migration services.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到这节关于数据迁移的课程
数据显然是几乎所有组织的核心基石和构建模块
对于任何应用部署
您都需要数据
尤其是当从本地基础设施迁移到云端时
您显然需要同时迁移数据
在将数据迁移到云端时
必须明确迁移的目的
以及不同类型的数据迁移场景
还需考虑可用的网络资源
以及其他相关因素
AWS提供了多项相关服务
用于将本地数据中心或服务器的数据迁移到云端
我们有两种数据迁移方式或选项
首先是在线数据传输或混合云存储方案
然后是离线数据迁移到亚马逊
S3存储
对于在线数据传输和混合方案
使用这些方法
可以轻松创建与VPC的网络连接
将数据传输到AWS
或使用S3混合云存储与现有本地应用集成
这些服务可帮助您一次性迁移大量数据
同时协助整合现有
例如 备份或恢复流程
或持续数据流直接传输至云端
但显然 嗯
对于拥有海量数据的组织
例如拥有数拍字节数据
或数百TB的数据
即使是许多小型
中型组织
拥有拍字节数据也并非遥不可及
因为如今数据的重要性日益凸显
我意思是 手机已配备1TB存储空间
如果手机有1TB存储
想象组织服务器存储了多少数据
如果数据量庞大
离线迁移方案可能更适合您
AWS提供了多种离线迁移选项
用于传输TB级数据
甚至PB级数据
以下是对选项的简明概述
以及适合您的AWS服务
接下来我们将详细讲解这些服务
此表格基本展示了
您知道 你在尝试实现目标时拥有的选项
比如说 如果你正在尝试
你知道私下里
将数据中心通过网络链路直接连接到你的VPC和AWS
那么显然你需要直接连接
现在 你知道它覆盖了全球多个区域和众多可用区
因此在全球几乎任何地点，直接连接都不是不现实的选择
或者假设 如果你试图复制或同步文件系统到S3或EFS
那么你需要数据同步
嗯，或者连接现有的本地应用
所以很多组织知道他们决定不将应用迁移到云端
他们只想将数据迁移到云端
你知道他们想利用AWS的数据服务器或基础设施
但希望保留应用程序在本地
嗯 你有多种选择
你知道 除了存储网关
还有文件网关等选项
所以这个表格很清晰
你知道 从你想要实现的目标来看
以及AWS提供的在线工具选项
然后还有离线选项
嗯 AWS提供了三大主要方案
你知道有雪球设备
还有雪球巨兽
这取决于 你有多少数据需要传输
相信我
有些组织曾用雪球巨兽传输了数百PB的数据到云端
AWS开发雪球巨兽的原因
其实就是一辆装满数据的半挂车
这些都是离线传输数百PB数据的方案
即使使用AWS直接连接
技术上可行
但实际不可行
所以这些 嗯
你可用的两大主要选项
这取决于你的具体情况
你的组织目标是什么
你有多少数据以及正在搭建的基础设施类型
你知道 你想要实施的迁移流程和策略
将决定选择哪种方案
无论是在线还是离线
甚至在这两者之间
哪个选项 以及你将要使用的工具或技术 将数据迁移到AWS云
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/029_Udemy - Become an AWS Certified Data Engineer part1 p29 11. Online and offline data migration.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎回到我们数据迁移系列课程的继续讲解
在之前的课程中我们主要探讨了
你知道的 我们有两个主要选项
在将数据从本地系统迁移到云端时
在我们的迁移策略或流程中
现在让我们更详细地了解AWS提供的选项或工具
涵盖在线和离线数据迁移
首先在线迁移方面
有很多大型组织使用的主流方案
就是AWS直接连接服务
当组织希望选择此选项时
如果他们需要专用物理连接来加速
数据中心与AWS数据中心之间的网络传输
如果你的企业拥有大量数据
并希望实现本地服务器与AWS数据中心的直接连接
则会选择直接连接服务
它允许在两个网络间建立专用网络连接
并使用
你知道的
IEEE 802.1 VLAN
可划分为多个虚拟接口
因此你可以通过同一连接访问公共资源
如存储在S3中的对象使用公共IP
以及私有资源 如E C2实例
运行在VPC中的实例
可能使用AWS环境中的私有IP
其优势在于
它保持公共与私有环境的网络隔离
当你设置直接连接时
实际上是在本地网络与AWS生态之间建立连接
但对许多组织来说这成本较高
另一个工具是AWS数据同步
可轻松将本地存储数据传输到S3桶或EFS系统
并自动处理与数据传输相关的任务
这些任务可能拖慢迁移进程
或增加IT运维负担
包括运行自有实例
有时需要处理加密
脚本编写
数据优化等
因此它能以开源工具十倍速度传输数据 你可以通过直接连接或互联网链路使用数据同步
取决于你设置的方案
还有存储网关或合作伙伴网关选项
可根据已部署的方案选择
还有存储网关或合作伙伴网关等方案
因此，网关基本上部署在本地，连接您的环境与AWS云
这基本上是混合场景的理想解决方案
某些存储可能因性能或安全原因需要保留在本地
而部分数据可以卸载到S3存储桶中
假设您有大量的归档数据
许多组织可能会采取或希望采取的措施是
即 他们可以将归档数据卸载并利用
S3存储桶存储所有归档和备份数据
同时将实时数据保留在本地，以实现存储网关
它再次简化了本地或混合解决方案
因此您的现有应用程序连接到本地网关
它使用
你知道的 行业标准
块和磁带存储协议将数据存储在S3甚至亚马逊冰川
几点需要注意
嗯 你知道的 数据已压缩
安全性很高
它还配置了虚拟磁带库
因此是一个非常稳健的系统
许多组织使用它来将磁带备份或归档存储在云端
同时还有合作伙伴产品在存储网关方面提供相同功能
所以 AWS 与众多行业厂商合作开发了物理网关应用
这些应用帮助您连接传统备份与云端备份
然后还有 S3 传输加速功能
它基本上使公共互联网传输到S3存储桶的速度大幅提升
因此最大限度利用可用带宽
无论距离远近或网络波动
你知道的网络状况
无需特殊客户端或专有网络协议
所以你只需更改用于S3存储桶的终端节点
加速功能会自动应用
当你创建S3存储桶时
可以选择启用S3加速
还有一个名为Amazon Kinesis或数据流的工具
因此，这是将流数据加载到AWS的最简便方法
它可以捕获并自动将流数据加载到S3和Redshift
因此能够实现实时数据分析
使用您可能已经在使用的现有商业智能工具和仪表板
因此前端所需的变化非常微小
但通过使用AWS生态系统，后端流程更加高效
然后 AWS还拥有众多技术合作伙伴或技术供应商
使得将备份和归档轻松迁移到云端
最简便的数据迁移方式或许正是原因所在
你知道s三连接器
你可能希望将其嵌入现有备份软件中
例如
大型组织
他们可能没有内部专业知识
他们可以使用AWS技术合作伙伴来协助迁移过程
这些就是可用的在线工具
如果你选择在线或混合选项处理数据迁移
正如我之前课程中提到的
有时充足的带宽甚至网络都无法满足
你知道 大量数据传输的需求
因此AWS有三种选项
首先是AWS雪球
这是一种拍字节级数据传输解决方案
使用安全应用在AWS内外传输大量数据
所以基本上你
如果你想传输数据
AWS雪球会寄送一个坚固重型设备
嗯 到你的物理位置
然后将设备连接到你的网络
将数据物理传输到设备中
之后联系AWS
他们会 取走设备
将其带到数据中心
并将数据传输到服务器
你还有雪球边缘
雪球与雪球边缘的主要区别在于
雪球边缘具备内置计算能力
本质上是一个可携带的服务器
通俗来说
如果你想使用内置计算能力
则应使用雪球边缘
如果你只需要简单硬盘
那就是雪球的用途
最后还有雪牛车
实际上是一个半挂车
用于数据传输
用于传输艾字节级数据
嗯 在美国四十英尺集装箱内
使用雪球设备
解决大规模数据传输常见挑战
因为通过网络传输艾字节数据
这极其昂贵且
耗时漫长
因此使用雪牛车
它会物理送达您的所在地
您将数据转移到实际的半挂车中
然后传输至AWS数据中心并在此卸载数据
这些是数据迁移的主要传输选项
呃 或在迁移过程中
数据是任何组织的核心
您必须制定计划来迁移您的数据
从本地服务器迁移到云端
这只能在制定迁移策略后决定
您已制定计划
您已规划哪些内容保留在本地
哪些将迁移到AWS云
只有在完成此步骤后才能决定 无论是在线还是离线方式在迁移过程中使用
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/030_Udemy - Become an AWS Certified Data Engineer part1 p30 12. Server migration service.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎回来
在这节课中我们将介绍服务器迁移服务
在之前的模块中我们学习了将服务器迁移到AWS云
AWS还提供了一个名为服务器迁移服务的工具
它也能帮助您将服务器迁移到云端
它本质上是一个代理和配套服务
能让您更轻松地完成迁移
甚至可以将数千台本地工作负载迁移到AWS云
因此它允许您
你知道的 自动化调度并跟踪实时服务器卷的复制
并更轻松地协调大规模迁移至AWS
你知道的 这是一个非常易用的服务
这是一个非常强大的服务，拥有诸多优势
非常容易上手
您在
你知道的 创建为大规模迁移设计的自定义复制计划
使迁移过程更加敏捷且成本效益高
无需使用昂贵的硬件或软件来迁移服务器
很多组织尤其是IT人员会喜欢
因为它减少了停机时间，采用增量服务器复制
并显著降低服务器停机时间
这是一个由AWS提供的强大服务
当计划大规模迁移时这是理想方案
例如从VMware环境迁移到AWS
其中关键考量包括无代理工具
增量复制并在切换前测试应用
如果您计划搭建混合环境
如果您计划继续使用VMware
则AWS有云上的VMware选项
我们在前一模块已讨论过
但如果您计划脱离VMware
则服务器迁移服务（SMS）最适合您
最佳之处在于迁移期间该服务免费使用
您只需为迁移过程中使用的存储资源付费
例如EBS卷
快照和Amazon S3存储
那么它是如何运作的
SMS需要一个连接器
负责协调迁移流程
该连接器部署在vCenter中，在部署前
需确保环境满足SMS要求
包括正确的防火墙配置
嗯 IP地址等
您需要执行的操作
是部署SMS
连接器本质上是一个虚拟设备
你将部署该代理
轻量级连接器基本上运行在你的服务器上
它在你的主服务器上创建虚拟机
它会下载一个基本的ISO镜像，其核心操作是
或者服务的基本运作方式
在AWS环境中创建复制服务器
在AWS上复制你的实时服务器负载
创建一个快照
然后
从该快照在e实例上恢复
C 在AWS上创建两个实例
因此它实际上是在创建工作负载的实时复制并启动e实例
C 在AWS环境中使用该快照创建两个实例
因此整个过程几乎没有停机时间，你仍在实时环境中工作
并且服务器在复制过程中不会出现任何停机或网络中断
因此这是一个非常易用的工具
非常易用的连接器
同时使操作过程极其无缝
现在 最好的一点是
微软使用增量复制技术
因此切换时间将降至最低
具体取决于前一次复制运行的变更量
因此这是一个极其可靠的服务
例如 如果你打算迁移离开
如果你目前处于VMware环境
你打算离开VMware环境
这对你是完美的使用选项 在将实时工作负载迁移到AWS时
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/031_Udemy - Become an AWS Certified Data Engineer part1 p31 13. How database migration service works.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到这节课
我正在查看数据库迁移服务
AWS数据库迁移服务帮助您迁移
正如名称所示，可快速轻松地将数据库迁移到AWS
同时现在更加安全
源数据库在迁移期间保持完全运行
最大限度减少停机时间
本质上 这就是AWS全年的核心理念
迁移过程减少停机时间，使迁移无缝进行
这也是为什么它对组织迁移到AWS极具吸引力
现在数据库迁移服务有很多优势
你知道的 支持同构迁移如Oracle到Oracle
以及异构迁移如不同数据库平台之间
比如说 Oracle或SQL迁移到亚马逊Aurora
借助迁移服务
您可以持续复制数据并实现高可用性，整合数据库
本质上如果您想要进入PB级数据仓库
嗯 通过利用
亚马逊Redshift甚至A3
再说一遍 最棒的是您可以免费使用数据库迁移服务长达六个月
为您提供充足时间迁移数据库
它极其易于使用
只需在管理控制台点击几下即可开始迁移
我们稍后将查看
如何轻松通过管理控制台迁移数据库
一旦迁移启动
它会处理迁移过程中的所有复杂性
嗯 你知道的
尤其是如果你 如果你在迁移异构应用或异构数据库
它会使用AWS提供的架构转换工具将架构转换为兼容数据库
它最大限度减少停机时间并支持
你知道的 广泛使用的数据库如Oracle到Oracle
无论是同构还是异构的Oracle
SQL PostgreSQL
您可以使用亚马逊RDS
即关系型数据库服务
或E
C 运行数据库服务的两个实例
或反之亦然
因此在可用选项和所支持的内容上有很多选择
且成本极低，再次强调
该服务前六个月免费
因此你只需为存储付费
嗯
极其快速且易于设置，可靠性强
因为再次 这是AWS云的核心优势
可靠性与高可用性才是关键
现在我们可以看到DMS的实际运作组件
理解DMS底层组件能更高效迁移数据
并提供更好的故障排查洞察
所以一个AWS
DMS迁移主要包含三个组件
你有一个复制实例
你有源和目标端点
然后是复制任务
创建DMS迁移需配置必要的复制实例
在AWS区域设置端点和任务
复制实例的作用
从高层次来看 呃
复制实例是托管的E
C Two实例，支持一个或多个复制任务
屏幕上展示的图例是复制实例示例
运行多个关联的复制任务
单个实例可承载一个或多个任务
具体取决于迁移特性
复制服务器的容量
等等 提供多种复制实例供选择
呃 选择最适合的E
C Two配置
接下来是端点
DMS通过端点访问源或目标数据存储
具体连接信息因数据存储类型而异
但通常需为端点提供一些信息
如端点类型
例如 是源还是目标，数据库类型
服务器IP地址
需注意 若将本地迁移到AWS需
你需要公网IP
显然这对端点运行至关重要
端口号、SSL等
创建端点时通过控制台配置
它要求你测试端点连接
显然 嗯
并且必须在DMS启动任务前成功
单个端点可以被多个复制任务使用
例如 你可能在同一源数据库上托管了两个逻辑上不同的应用
需要分别迁移
因此需要创建两个任务
每个应用表对应一个任务
但本质上是同一个数据库
然后又是复制任务
在AC2实例中可以有多个复制任务
它们将一组数据从源端点传输到目标端点
本质上是从本地数据库迁移到AWS云
创建复制任务是开始迁移前最后一步
这里同样需要指定复制实例
端点和迁移类型
你要进行全量迁移
你要进行部分迁移
你要复制数据变更
仅 在任务选择上有多种选项
就像我之前提到的例子
如果多个应用使用同一数据库需要分别迁移
在表级进行独立迁移
也可以在具体任务中指定
因此现在概念非常完善
从图中可以看出复制任务执行两个基本功能
全量加载过程很简单
这个过程相当直接
数据以批量方式从源提取并直接加载到目标
可以指定要提取加载的表数量
在DMS中并行处理
目前
还可以使用DMS捕获源数据的持续变更
在数据迁移过程中
现在 AWS DMS的变更捕获过程
用于复制源端点的持续变更
通过数据库引擎的原生API收集日志变更
现在在变更
数据捕获或CDC过程
复制任务设计为从源到目标流式传输变更
使用内存缓冲区
正确
使用这些缓冲区暂存传输中的数据 现在如果内存缓冲区
假设耗尽
可能会出现问题
出于各种原因
任务将待更改内容写入磁盘的更改缓存
它还使用存储来保存任务日志，如上所述
因此本质上是一个非常简单或复杂的过程
这取决于您的迁移策略和迁移内容
所以如果您想要简单的复制任务
嗯 它将直接从源复制所有内容到目标
如果您想进行实时变更流传输
那么显然会复杂一些
但同样 所有操作均通过您的DMS控制台管理 使管理过程非常简便且无缝
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/032_Udemy - Become an AWS Certified Data Engineer part1 p32 14. Database migration use cases.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到关于数据库迁移服务用例的课程
现在我们对DMS是什么以及它如何运作有了较为清晰的理解
让我们看看数据库迁移的一些应用场景
首先是一个相对直接的
嗯 一个简单的用例，即同构数据库迁移
其中源数据库和目标引擎是相同的
它是Oracle Oracle或SQL
或者将 MySQL 连接到关系型数据库服务
或者将 MSQL 连接到关系型数据库服务
你知道这些引擎是相同的
因此由于模式结构
源和目标之间的数据类型及数据库代码兼容
这几乎是一步完成的过程
你只需创建连接源和目标的任务
然后通过一键操作启动迁移
DMS 会处理剩余工作
源数据库可以位于您的本地环境
在外面啊
Aws正在运行于e
C 两个实例 或者本质上是一个rds数据库
因此可以进行rds到rds的迁移
比如从区域到区域
所以这是一个同构的方案
非常简单
呃 迁移你的数据库
异构数据库迁移
呃 这里有点不同
呃 显然源端和目标引擎完全不同
比如从Oracle迁移到Aurora
在这种情况下
源端和目标端的模式结构、数据类型编码可能存在较大差异
这需要在数据迁移开始前进行架构和代码转换
因此使迁移过程成为两步流程
你知道的 第一步是
您将使用架构转换工具将源架构代码转换
以匹配目标数据库的结构
然后使用DMS将数据从源数据库迁移到目标
现在所有必要的数据类型转换将在迁移过程中由DMS自动完成
因此源数据库再次像之前一样可以位于本地
它可以位于e
C 两个实例 或者可以存放在我们的RDS数据库中
嗯 下一个场景是开发测试或开发与测试环境
现在 DMS还可用于在云内外迁移数据
用于开发目的
所以啊 为什么组织要这样做
你知道的 一个例子是部署开发测试或预发布系统到AWS
以利用云的扩展性和快速部署能力
比如这样
开发者和测试人员可以使用真实生产数据的副本
并将更新复制回本地生产环境
或者另一个例子是
如果开发系统在本地部署
并希望将AWS云生产数据库的副本迁移到这些系统
本地系统 一次性或持续进行
从而避免对现有DevOps流程的干扰
或者假设
如果本地硬件无法满足开发测试需求
可以使用该服务在AWS基础设施上完成所有开发测试
同时还能将多个源数据库合并到目标数据库
这适用于同构和异构迁移
可与AWS支持的所有数据库引擎配合使用
现在在这个场景中
源数据库可能位于本地或AWS
无论是AC2实例还是I
且可分布在不同位置
例如 一个源数据库靠近本地环境
另一个是E C 实例
还有一个是RDS
并将它们全部整合到
比如 你知道的
Aurora或RDS
或单个E C
实例对吧
这是整合多个数据库的绝佳方式
在制定迁移策略时
嗯 你知道的
之前 假设将部分本地部署迁移到云端
进行整合是个好主意
同时优化和简化您的业务流程
并且具备持续复制功能
呃 DMS拥有多种使用场景
比如灾难恢复
地理数据库分布
开发与测试环境
等等 根据您的组织需求
以及您的目标 它支持同构和异构环境的持续复制
因此有多种应用场景
这些都是DMS可以实现的功能
一款功能强大的软件
不仅支持迁移操作 还能满足数据库的持续使用需求
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/033_Udemy - Become an AWS Certified Data Engineer part1 p33 15. AWS Schema conversion tool.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎回来
在这节课中，我们将了解提供的模式转换工具
现在 这个工具是非常非常好的工具
因为它使异构数据库迁移
变得可预测且易于操作，通过自动转换源数据库模式
以及大部分数据库代码对象
包括视图
存储过程和函数，转换为目标数据库兼容的格式
所以在你的迁移过程中 比如说
如果你想离开Oracle
如果你正在将整个基础设施迁移到AWS
你可能想要使用Aurora数据库
我现在又在使用Oracle了
在这种情况下你会使用
或者你需要使用模式转换工具
无法自动转换的对象会被明确标记
以便手动转换以完成迁移
现在这个工具的优点
这个工具非常好的一点是
它可以分析应用源代码中的嵌入式SQL语句并作为转换项目的一部分进行转换
整个过程通过转换
比如说 将遗留的Oracle和SQL Server函数转换为等效的AWS服务
同时帮助你在数据库迁移时现代化应用程序
一旦转换完成
它帮助你将数据从各种数据仓库迁移到AWS
无论是Redshift还是Aurora
或者根据
你的迁移策略再次决定
这里是否是一个很好的转换工具
对吧 就源数据库和目标数据库而言
显然不需要你去死记硬背
它在AWS侧可用
因此如果
例如你正在考虑迁移
并设计你的策略时
假设你的应用程序正在使用
你知道的 较为老旧的应用程序或较为过时的应用程序，可能运行在多个数据库上
嗯 而你正在考虑迁移到AWS
显然你不仅需要升级基础设施
还要同时升级应用程序和底层架构
最佳方法就是使用模式转换工具
因为再次 它会为你更新所有内容以适配云环境
为你使用原生云应用
其中一些 呃
这个云
其中一些云
新的云应用
比如物联网或机器学习
或者人工智能 因此这是一个非常好的工具，值得使用和考虑
尤其是如果你有使用了很长时间的数据库
嗯 你可能需要研究这个工具，并看看这是否
你不仅能让你的应用或迁移变得轻松
还能同时现代化并优化你的运营流程
除了这个工具
他们还提供了一个工作负载评估框架
这能帮助你评估并规划向AWS的迁移
该框架利用转换工具收集信息，建模现有的Oracle或SQL工作负载
并提供将其转换为AWS数据库的指导
所以 它在这一方面为你做了大部分工作
通过分析你的表结构和代码对象
以及你的应用
代码和依赖项等
并分析所有相关特征，对整个数据库组合进行全范围分析
并帮助你分类迁移任务
因此这是一个非常 非常值得使用的优秀工具
比如说 如果你处于迁移策略的初期阶段
请下载此工具并运行在你的环境中测试
并查看 如果将数据库从本地迁移到AWS并转换 这将是对你最有效的方式，同时优化运营流程
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/034_Udemy - Become an AWS Certified Data Engineer part1 p34 16. Sample database migration to AWS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到关于数据库迁移服务的课程
正如你们看到的
我已经登录到我的AWS控制台
我已经成功进入DMS
这就是数据库迁移服务
这就是你们将看到的主要仪表盘
在开始之前
显然我需要创建几个数据库来进行迁移
我将前往RDS
这是AWS的关系型数据库服务
我现在要创建一个数据库作为源数据库
如果你们在生产环境中操作
你们将拥有一个源数据库
这将是你们本地的数据库
或者组织在AWS上使用的数据库
然后你们会有目标数据库
这是你们要迁移或迁移的目标数据库
但因为
你们知道的 我都没有
因为这只是演示用途
我将快速在RDS创建一个数据库
这是AWS的关系型数据库服务
所以我直接选择主选项
一些基本设置
我将采用简易创建方法
因为我希望保持所有
默认设置
在引擎类型方面
嗯 我们选择MySQL
我要确保选择免费套餐
请确保你们选择这个
如果你们自己练习
自行练习时
请确保选择免费套餐
让简易实例保持在t2.micro
这是免费套餐
我将保留其他所有设置
使用默认的管理员密码
搞定
如果你们想看的话
你们知道的 简易创建设置的一些默认选项
比如加密
VPC 选项组
子网 所有设置都使用默认值
这些都是数据库的默认选项
既然我们已经完成了简单的创建
如果你选择了其他选项
那么你可以继续修改所有这些选项
如果你想的话
我现在将创建该数据库 好了现在开始
正在创建数据库 这需要
根据时间或所在区域
此时数据库启动大约需要5到10分钟
创建完成后进入
进入备份模式
备份模式之后
这就是准备就绪状态
大家可以看见我创建了一个名为database one的数据库
可以看到当前使用的vpc
当我返回到数据库迁移服务
正如我之前所说 这是主控制面板
提供基础信息
这里有快速视频演示DMS的实际操作
大家可以再次查看使用场景
我们在之前的课程中已经讲解过这些用例和优势
接下来我们要做的是
继续创建复制实例
我将为复制实例命名
通常大家希望将其与其他部分
如果你要迁移多个数据库
描述部分根据需求填写
默认实例类别
默认选择t2.medium
但你可以选择t2.micro
请注意 有时t2.micro实例类会失败
大家可以查看按需实例的定价
如果还不了解的话
了解使用这些实例的费用
t2.micro在免费层级内
但t2.medium不在免费层级
请记住这一点
如果使用t2.micro
根据数据库大小
即使是测试用途 有时VPC会失败
当前显示的唯一VPC
因为我只有默认的VPC multi easy
如果需要冗余
建议创建在多可用区
多个可用区
可以设置公网访问
务必进行此设置
尤其是从本地迁移到AWS时
你需要确保这个资源是公开可用的
这样你可以从可用区外部环境连接到它
我确保你们只需保留它
没有偏好由AI自动选择
最适合的可用区
以及VPC安全组
你可以指定不同安全组或保持默认设置
我多创建了一个
还有默认的那个
默认情况下会保留默认设置
但如果你进行自定义迁移
可以修改这个设置
关于维护计划
你可以指定这些数据库的维护时间
因为复制实例是数据库复制的位置
并在数据迁移前保持暂存
到你的生产实例目标位置
所以我保留所有设置
默认选项 创建一次
我返回到界面 大家可以看到它仍在创建中
比如 我说过 这需要5到10分钟
有时数据库启动需要更长时间
好了 我们的复制实例已创建完成
可以看到当前状态为创建中
可以查看所有详细信息
点击刷新后
状态会更新为完成
整个过程大约需要10-15分钟
当我进入复制实例
可以看到所有相关信息
详细信息 状态显示引擎版本
正在使用的版本
嗯 云监控指标
之前你们看到的错误信息
它会立即尝试连接
需要几秒建立连接
因为仍在创建中
这就是为什么出现错误
连接成功后错误消失
可以看到创建复制实例时选择的所有信息
最后一个日志文件
因为目前为空 因为我们还未进行操作
所以我们要返回到仪表盘
这是主仪表盘
我们可以看到所有不同的复制任务
或它们的状态
哪些是活跃的 哪些已完成
哪些已成功完成等等
所以我们需要做的是 我们继续创建一个端点
显然会有两个端点
一个是源端 一个是目标端
对于源端 我们将选择刚刚创建的数据库
因为这是一个数据库
就是这个
数据库一 这是我要复制并迁移至环境的数据库
在AWS中
然后标识符
我将保持与源引擎的数据库名称一致
系统自动检测到是MySQL
服务器名称 它还会检测
端口是三三零六用户名
管理员 我输入之前设置的密码
在我创建这个数据库时
但这些是端点
特定设置 你可以设置详细
或为端点设置一些独特配置
如果你想使用KMS
即 你知道的 AWS提供的密钥管理服务
用于安全防护
你可以更改这些密钥或使用默认设置进行测试
现在这个端点
由于数据库正在创建中，测试会失败
因为它仍在创建中
创建已完成 但正在备份
因此尚未上线
嗯 我刚才有点提前创建了端点
现在正在备份中
这就是你们看到复制实例失败的原因
但无需担心
我们仍继续创建这个端点
数据库启动并运行后
我们就能看到效果
呃 成功创建了端点
我们这样做 我说数据库的端点现在已激活，针对数据库一
我想你们看到状态是活跃的
尽管测试再次失败
因为它正在备份
但一旦能连接并确认数据库实际活跃
它只是在备份
成功连接并测试了这个连接
现在我们已创建源和目标
现在显然因为我们创建了源
我们也需要目标
接下来我要回到我们的DS
创建另一个数据库作为目标
如你们所见 这里只列出一个数据库
因为我只创建了一个数据库
如果在生产环境中
当然可以连接本地环境
但此演示中没有本地环境
我将返回RDS创建另一个数据库
这将是我要迁移的目标数据库
或我想要迁移的目标
我会按照数据库一的步骤操作
再次选择快速创建
保持所有默认设置
现在创建数据库
搞定 现在DS一中有两个数据库
呃 数据库一作为源 数据库二作为
现在我们看到数据库一已可用
数据库二正在备份
确保第一个数据库完全可用
我们可以通过AWS环境内的端点连接它
或者因为它不可公网访问
因为我们使用默认设置
如果需要公网访问
创建RDS时需自定义设置
这里可以看到可用区
VPC 子网组
安全设置 这些都是该数据库的详细信息
我们在RDS中创建的MySQL数据库
好的 现在我返回数据库迁移服务
现在创建另一个端点
哪个是目标端点
正如你们看到的 现在数据库二已显示在RDS实例中
我将输入之前为管理员设置的密码
以便连接
我将测试该端点
它仍在备份中
我们将看看是否能成功运行
有时候会成功 有时候不会
所以 我快速运行测试
由于状态为测试中
很可能返回成功结果
正如你们在上一个看到的
立即响应了
因为正在备份 我没有立即连接
但这次能连接
很可能测试成功
如果要响应
应该一开始就响应
现在正在运行测试
好了 我们看到测试成功
能够成功连接至此
现在我们有了源和目标源
即要迁移的数据库
从数据库迁移
目标是想要迁移的数据库
现在进入数据库迁移任务
拥有两个端点后
创建任务并标识
快速命名为复制
实例是创建的源数据库并指向目标数据库
有一个复选框 创建时启动任务
可取消勾选稍后启动
如果需要的话 点击任务设置旁的信息
提供详细信息
关于该任务设置的作用
关于这个
目标表准备模式
可包含LOV列和复制
再次说明
如需更多信息
点击信息右侧会弹出说明
也可启用验证
还可启用云监控日志
如果你想进行详细日志记录并通过云监控进行编辑
我们可以选择引导式操作
呃 或者你知道的
通过Go接口
或者我们可以使用JSON编辑器
这完全取决于我们的偏好
对于一些高级设置，我们可以创建控制表并使用模式定义目标
我们可以设置历史超时分钟数
我们可以配置控制表设置，所以再次说明
如果你要迁移大型数据库的详细数据
你需要创建控制表以便全程跟踪所有内容
在继续创建之前
我们显然需要为此创建一些任务
我们将添加一个新的选择规则到模式中
我们将输入一个模式
并希望复制所有内容
因此保留所有百分比设置
这表示将包含所有内容
我们可以具体选择特定模式
我们想要迁移哪些表
如果我们不打算一次性迁移整个数据库
呃 但既然我们要迁移全部数据
就全部保留迁移设置
我们现在创建任务
完成测试迁移，大家可以看到数据正在生成
接下来它会执行
由于我们勾选了立即启动选项
会立即开始运行
大家可以看到状态已变为运行中
我们可以看到源端
目标端 类型为全量加载
针对整个数据库
进度 已用时间
表查询
持续记录数据库状态的详细日志
当你进入数据库或仪表盘时
可以看到当前活跃的百分比进度
如果出现延迟
如果发生失败
任务详情
实例信息
端点信息 提供整个任务的详细信息
或整个数据库迁移过程
现在进度显示为100%
可以看到所有表已加载
表格已排队
有多少张已晾干的桌子
哪些能在日志中查看
如果无法匹配表格
或者出现任何错误
复制部分内容会显示所有信息
所有详细信息均可在迁移标签、表格及表格统计中查看
在迁移任务中 可以看到详细的数据复制分解
已加载的内容 未加载的内容
表格是否已加载
我可以通过
你知道的 验证是否已修改
可查看表格是否被修改
是否已完成
或是否失败
此外 如果我们设置了云监控指标
一切也会在云监控中触发
如果出现故障
提供了详细的复制内容信息
未复制的内容现在
由于这是一个简单迁移
你知道的 缓冲区几乎为空
仅创建了一个表格
我操作得很快
任务中没有错误或数据波动
但如果你有大型数据库
显然会有更多场景信息
以及复制中的表格信息 以及使用的资源
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/035_Udemy - Become an AWS Certified Data Engineer part1 p35 17. Database migration best practices.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


嗨 大家欢迎回来
所以在本节课中 我们将要做的就是
我们将结束数据库迁移部分的内容
通过了解AWS推荐的最佳实践
每当您迁移数据库时
有很多不同的事项需要牢记
尤其是当迁移大型数据库甚至中型数据库时
因为这可能会变得相当复杂
首先我们需要牢记的是如何提升迁移性能
影响迁移性能的因素有很多
例如 资源可用性及源端情况
网络传输速度类型
复制服务器的容量
目标端处理变更的能力
等等
所以有一点需要说明的是，他们确实进行了迁移
他们举了一个例子，用约十二小时迁移约一万亿字节的数据
在一个单一任务中完成
显然这
可以在较短时间内完成大型数据库迁移
只要考虑到所有最佳实践
并具备相应的基础设施支持
理想条件包括源数据库运行在e
C 2实例和RDS，目标数据库也在RDS中
全部位于同一可用区
显然这样会更方便，因为都在AWS上
所有资源都在同一可用区
但在实际中
显然您是在将本地数据库迁移到AWS
因此对于大多数组织来说这些情况并非理想
但需要记住的是
确实可以在短时间内迁移大量数据
关于提升性能的几点注意事项
应并行加载多个表而非逐个加载
一次一个表
处理索引
触发器和参照完整性约束
需确保优化这些以提高性能
需禁用备份和事务日志
因为这会占用大量资源
应为单次迁移使用多个任务
如果您有不参与共同事务的表组
可能可以将迁移任务拆分为多个
事务一致性在任务内得以维持
因此需确保不同任务中的表不参与共同事务
显然需要深入了解数据库中表的操作方式
为了做到这一点
嗯 最后一点
你知道的 在提升性能时需要注意的一点是优化变更处理
因此默认情况下DMS以事务模式处理变更
这是为了保持数据完整性
如果你能够接受
你知道某些临时的事务完整性中断
可以改用批量优化应用选项
所以接下来我们 会以批量方式而非事务模式应用变更
这样能节省一些时间提升性能
这就是需要记住的第一点
最佳实践是采取这些措施来提升性能
下一个要点是选择复制实例的最优大小
这取决于你的使用场景的多个因素
你知道的 所以在全量加载任务期间
DMS会逐个加载表
默认同时加载八个表
这无需死记硬背
但需记住它是逐个表加载
并在全量加载期间捕获源端的持续变更
这些变更稍后可以应用到目标端
变更会被缓存在内存中
如果可用内存耗尽
变更将缓存到磁盘
当全量加载任务完成某个表时
DMS会立即应用所有缓存变更到目标表
在应用完所有待处理的缓存变更后
目标端处于事务一致状态
此时目标已与源端同步
关于你知道的 最后缓存的变更
随后开始目标与源端的持续复制
为了做到这一点 它从源端事务日志获取变更操作并应用到目标
你可以控制复制实例如何处理
变更处理 以及如何使用内存
但你知道的
就本课程而言
就所有目的而言
这种控制非常有限
选择复制实例时需注意的几点
你知道的 务必了解你的表大小
因为大表加载时间更长
所以嗯
你知道吗 这些表上的事务必须缓存直到表完全加载完成
你想知道事务的大小
你知道的 长时间运行的事务可能会改变
可能产生大量变更
迁移的总大小
你知道的任务数量
你有多少大型对象或LOB
因为它们加载时间更长
所以这些都需要记住
所以举个例子
如果你加载 如果你资源不足
假设两个 四小时
每小时产生两吉字节的事务
你可能需要确保至少有四
八吉字节的空间用于缓存事务
这些都是需要注意的小细节
确保你的复制实例足够强大
并能处理所有数据库任务
呃 下一个最佳实践是减少源端负载
DMS会在源数据库消耗部分资源
在全量加载期间
会对源表执行完整扫描
每个并行处理的表
如果你发现源数据库过载
可减少迁移任务或任务中的表数量
接下来要使用任务日志排查问题
在某些情况下 可能出现仅记录在任务日志中的警告或错误
务必使用任务日志
定期监控日志状态
下一个步骤是转换架构
如果你迁移不同数据库
或迁移异构数据库
例如将Oracle迁移到Amazon Aurora
需确保在DMS启动前完成架构转换
甚至在进入DMS仪表盘开始工作前
务必使用架构转换工具处理架构
若使用第三方工具转换架构以适配目标数据库
呃 迁移大型二进制对象或LOB
通常DMS分两阶段迁移LOB数据
在目标表创建新行并填充数据
但暂不包含关联的LOB值
随后更新目标表行的LOB数据
持续复制功能由DAS提供
你知道吗 比如我之前提到的持续数据复制
保持源数据库和目标数据库同步
因此它仅复制有限的数据量
定义语言或DDL
并且不会传播索引等元素
用户权限 等等
所以如果你计划使用持续复制
你应该在多可用区上启用多可用性
在创建复制实例时的便捷选项
这样做可获得复制实例的高可用性和故障转移支持
更改Oracle目标的用户模式
再次使用Oracle作为目标
DMS将数据迁移到目标端点用户的模式
例如 假设你要迁移一个模式
你知道的 名为custom的模式到Oracle目标端点
目标端点用户名为master
DMS将以master身份连接目标
并将对象填充到master模式中
来自你知道的
来自从cam的
你命名的模式名称
只需记住这一点并可覆盖
但请记住 这是Oracle的默认行为
关于Oracle目标，最后优化大型表迁移性能
如果你想在迁移期间提升性能
可以拆分为多个任务
要拆分迁移为多个任务
可以使用行过滤或分区键
例如 如果你有一个整数主键id从1到约八百万
可以创建八个任务使用行过滤迁移一百万条记录
每个任务
这实际上会将大表拆分为小块
可管理的任务
所以这些只是
需牢记的最佳实践
如果你想优化迁移流程 确保过程顺利高效
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/036_Udemy - Become an AWS Certified Data Engineer part1 p36 1. Data Volume.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
我将讨论大量数据带来的挑战
我还将谈到数据源类型
您可能需要摄入和存储的数据
当然最后在本课中
我将讨论数据存储的选项
好的 所以主要我们来谈谈亚马逊S3
这是一个简单存储服务
我将介绍基本概念
数据湖也包括在内 当然还有不同的存储方法
这就是本课的目标
如果您能够分析和处理数据
当然存在容量问题
没错 刚才我提到了五个V
在之前的课程中已经讲过
好的 现在数据量是首要挑战
如今许多企业正面临海量数据的困扰
事实上我们每天都在产生大量数据
实际上每个人都有平板电脑
手机笔记本电脑
你知道的 我们每天生成海量数据
如果你有可变设备
例如 记录你的健康活动
想想看 这就是数据
设备持续收集数据
然后发送到服务器处理
每个步骤都在进行
然后将结果返回给你
只有你能随时查看这些结果
这只是众多设备的一个例子
没错 每个人都在生成海量内容
物联网企业也从这些设备获取大量信息
不仅如此
没错 社交媒体改变了
企业向客户推广产品的方式
这也改变了
处理投诉和客户服务的方式
你能想象每分钟产生的数据量吗
仅通过社交媒体
让我举几个例子
我最近读到每分钟发送超过一亿封垃圾邮件
Netflix用户现在每分钟观看超过七万小时的视频
YouTube用户每分钟观看超过四百万个视频
当然还有国际数据公司
根据他们的研究没错
这家机构预测全球数据总量
没错 所有收集到的数据将从三三艾字节
将在未来几年增长到超过两百艾字节
顺便说一下这是泽字节单位
就在三年前
没错 我们还处于艾字节级别
而现在我们讨论的是泽字节级别
谁能想到这一切呢
不过嘿 这就是现实
那么数据量激增的原因是什么
没错
嗯 他们需要数据分析解决方案
企业处理海量数据
必须在分布式系统中高效运作
甚至跨越国家或全球范围
这些解决方案必须易于扩展
以应对数据流量的巨大峰值
让我为你简化一下
数据分析方案必须支持大规模
可扩展且持久的数据存储
还需能够从多源采集数据
这就是不同类型的数据
让我们讨论不同类型的数据
好的首先
来看关系型或结构化数据
这类数据通常存储在关系型数据库中
好的 就像数据库一样
数据高度遵循数据库内置的规则和约束
这种数据是事务型应用的核心
第二种是半结构化数据
这类数据常存储在非关系型
通常称为NoSQL数据库或XML/JSON文件
这类数据本身没有固定结构
且具有临时性
例如
在线游戏中的规则设置
例如 浏览器缓存或社交应用
会自动在一定时间后删除帖子
然后第三个是无结构数据对吧
这类数据通常以文件或对象的形式存在
没有统一结构，代表企业收集和生成的所有其他数据对吧
这类数据常被认为难以处理，因为它不需要传统格式
没错 需要标签和分类才能进行分析
这使得许多企业无法在数据分析中使用它
例如图像
电子邮件
文本文件
社交媒体 内容速率
短信和视频
因此企业产生的连接和生成的数据中只有10%是结构化数据
只有10%是结构化的对吧
所以这是你数据解决方案中最基础的部分对吧
这其实占比很小，但令人震惊的是这一点
好的，所有生成或收集的数据中10%是半结构化数据对吧
剩下的80%是结构化数据
所以 换句话说
企业数据中最大的部分也是最难分析的部分
企业们 在构建能够
准确有效提取有价值数据以提供有意义洞察的数据分析方案时
有时感觉数据像被冻结在文件服务器中难以理解
但这并不必要
所以这就是我们在讨论数据分析方案时需要重点关注的
我之前提到的挑战和数据类型
以及这些统计数据让你了解大部分是无结构数据
而这正是所有组织面临的挑战
希望有帮助，如有疑问请告诉我
如果你对此有任何问题 我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/037_Udemy - Become an AWS Certified Data Engineer part1 p37 2. Types of Data.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
我们来谈谈不同类型的数据吧
商业数据呈指数级增长
例如 企业已经存储了数十年的数据
这本身并不新鲜对吧
近年来变化的是分析特定类型数据的能力
现在数据源类型可分为三大类
好的 首先结构化数据是组织化的，以值的形式存储
以表格中的行和列进行分组对吧
半 结构化
通常以键值对形式存储在文件元素中
其他可能仅包含元数据
例如 所以在互联网文章中
拥有大量信息的内容基于结构化数据
新应用发布后能整理并提供对未开发资源的深刻见解
那么什么是非结构化数据
我们来讨论这个吧
你存储的每个文件
你拍摄的每张照片 例如
你发送的每封邮件都有这个特点
现代数据管理平台必须从多样化来源采集数据
速度和规模
数据需要整合到可管理的中央仓库
打破传统孤岛
现在 收集和分析所有业务数据的好处必须超过成本
现在 数据应该存储在哪里呢
AWS本身有S3存储桶
这是一个强大工具对吧
我们来谈谈这个
之前我提到过
并且我保证我们会讨论
呃 存储各种类型的数据对吧
我们从了解数据分析解决方案的组件开始
顺便说，存储是解决方案的重要部分
你必须收集数据并存储起来对吧
可能需要临时存储正在处理的数据
例如 然后要以可访问的方式存储结果
供需要查看的人使用
现在 任何优质方案的核心是良好的存储平台
AWS有一个名为亚马逊的存储服务
简单存储服务
简称S3
现在这项服务是一个对象存储
这意味着你可以在此存储几乎任何类型的离散对象
对象是亚马逊S3对文件的称呼
所以图片 文件
文档
应有尽有
它可扩展且能无限扩容
你需要 它具备高耐久性
意味着你的文件将
随时可用
它实现可扩展性
安全与性能
现在人们使用亚马逊S3
例如 用于网站
移动应用
数据分析 仅列举部分应用场景
当你开始使用S3时
你需要创建存储桶 好的
就像文件夹
但更大
更规范 且内置更多安全机制
好的 接下来
将数据存入这些存储桶
然后AWS提供了多种方法
无论数据量多大
甚至可单次传输艾字节级数据
现在你会发现多个
嗯区域 稍后会详细讲解
我将演示S3的工作原理
因此存储层构成了
整个数据分析解决方案
S3有多种实现方式
当然正如之前所说
我会
给你演示 实战演示如何使用S3
对 首先使用S3时
你可以 你知道你可以将数据存储方式分为三种
这就是存储与处理的解耦
现在你可以为原始数据设置独立的存储桶
临时处理结果和最终结果应分别存储在不同数据
没错 接下来是并行化
例如 在亚马逊S3中
你可以从任何进程并行访问这些存储位置而不影响其他进程
然后S3最终成为存储分析数据集的中心位置
同时为多个分析进程提供访问权限
这使解决方案避免了在存储系统与处理系统间移动数据的高昂成本
所以S3确实是一个优秀的解决方案
但为了满足你的存储需求
在接下来的数据分析方案中
我将为你演示 然后我们将讨论S3的其他概念
例如 如果你想充分利用亚马逊S3
有几个概念需要重点说明
首先S3将数据作为对象存储，如之前所述以存储桶形式
当然对象就像一个文件
描述文件的元数据以及将对象存储在S3中
只需上传文件即可
并将其存入存储桶本身
对的，你可以设置权限
这对所有对象都很重要
对的，存储桶是对象的逻辑容器，可以创建一个或多个
或任意数量
并控制访问权限
可以创建用户组
授予访问权限
还可以创建存储桶策略
对的，保留策略等
亚马逊有一个非常强大的工具
嗯
因此你可以通过亚马逊管理控制台访问S3
好的
核心概念是数据存储
好的 因此你可以使用
例如 创建存储桶名称
使用易记的名称
还可以使用其他区域
实际访问存储桶内的对象
好的
这就是S3存储的核心所在
好的 例如
对象键本身是唯一标识符
因为存储桶键和版本ID的组合可唯一标识每个对象
你可以把亚马逊
S3视为存储桶、键和版本与对象之间的基础数据映射
亚马逊S3中的每个对象都可以通过
端点 存储桶名称和键
然后可选地加上版本本身
在亚马逊S3上的数据分析解决方案是
它具有诸多优势
将S3作为数据分析解决方案的存储平台
存储与计算及数据处理的解耦是首要优势
这真的很棒
借助亚马逊S3 您可以以原生格式高效存储所有数据类型
然后启动任意数量的虚拟服务器
例如 然后将S3作为数据源
对 例如 您可以启动两个实例
并使用AWS分析工具处理数据
并且可以优化
C 两个实例也可提供正确的CPU
内存和带宽比例以实现最佳性能
解耦流程与存储带来显著优势
包括能用多种工具处理和分析相同数据
集中式数据架构
我们来谈谈这个
这意味着什么 对 S3使构建多租户环境变得简单
多个用户可将自有数据分析工具应用于同一数据集
这相比传统方案能降低成本并提升数据治理
传统方案需在多个平台复制多份数据
这可能需要额外步骤将数据加载到正确工具中
使用S3作为中央数据存储比传统方案更具优势
与无服务器AWS服务如Lambda集成
例如 您可以充分利用这一点将S3
与其他AWS服务结合查询和处理数据
亚马逊S3还与Lambda或无服务器计算集成
例如 无需配置或管理服务器即可运行代码
亚马逊Athena 例如
可直接使用SQL查询S3
无需将数据导入关系型数据库
对
因此这些功能仅按您实际处理的数据量收费
或您消耗的计算时间
按需付费
标准化应用程序
你知道的 API接口
例如 您也可以在S3存储桶中使用这些功能
没错 您可以充分利用这些资源
所以 呃
REST API 例如
本质上是用于与Amazon S3文件交互的编程接口
因此S3是RESTful API
例如 非常简单
易于使用
并得到大多数主流第三方软件厂商支持
没错，例如
Apache Hadoop及其他领先厂商
这也允许客户使用自己最熟悉的工具和知识
帮助他们对S3中的数据进行分析
希望这能帮到您
这能帮助您深入了解实际存储能力
在设计数据分析方案时
让我 如果您对此有任何疑问 我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/038_Udemy - Become an AWS Certified Data Engineer part1 p38 3. AWS S3 Storage Explained.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到这节课
我们来看看亚马逊的简单存储服务
或者S3
现在亚马逊 S3
提供了一个简单的网页服务接口供您使用
可以随时从网络的任何地方存储和检索任意量的数据
现在 S3是特意设计的
具备最小化功能集，专注于简洁性和可靠性
现在让我们看看S3中的几个概念
首先我们有一个叫做存储桶的概念
存储桶是S3中存储的对象的容器
每个对象都包含在一个存储桶中
你可以把存储桶想象成一个文件夹
如果你熟悉Windows
资源管理器
现在存储桶有几个用途
它们组织S3的命名空间
它们标识负责存储和数据传输费用的账户
它们在访问控制中起作用
并作为使用统计的单位
报告
现在你可以配置存储桶使其创建在特定区域
你也可以配置存储桶，每当有对象被添加时
亚马逊S3会生成唯一版本ID并分配给对象
这称为版本控制
我们稍后会详细讲解
关于存储桶需要注意的一点是它们拥有唯一命名空间
因此它们基本上可以通过DNS命名空间访问
所以你在AWS中创建的每个存储桶名称必须在AWS全局唯一
不仅在组织内部
当我带你们演示时
你们会看到具体实现
接下来是对象，这是S3中存储的基本实体
S3对象由对象数据和元数据组成
元数据是一组描述对象的名称值对
这些包括最后修改时间等默认元数据
以及内容类型等标准HTTP元数据
对象在存储桶中通过键名和版本ID唯一标识
第三个方面是键
这是存储桶内对象的唯一标识符
每个存储桶中的对象只有一个键
因为存储桶键和版本ID的组合唯一标识每个对象
S3可以视为存储桶键与版本之间的基础数据映射
和对象本身
现在 S3中的每个对象
可通过结合存储桶的网络服务端点唯一访问
名称键 可选版本
接下来我们有像我之前提到的区域
您可以选择存储创建的存储桶的地理区域
您可能选择一个区域以优化延迟
降低成本或满足监管要求
现在存储在区域中的对象永远不会离开该区域
除非您明确将其传输到另一个区域
最后 一致性模型
这一点非常重要
尤其是对于考试
现在 S3在所有区域为新对象的PUT操作提供读写一致性
在所有区域有一个例外
例外情况是如果您在创建对象前对键名发起HEAD或GET请求
在创建对象之前
S3为读写操作提供最终一致性
现在 S3对所有覆盖操作提供最终一致性
所有区域的PUT和删除操作
对单个键的更新是原子操作
例如 如果您向现有键执行PUT操作
后续读取可能返回旧数据或更新后数据
但永远不会返回损坏或部分数据
现在 S3通过跨多个服务器复制数据实现高可用性
在亚马逊数据中心内
如果PUT请求成功
您的数据已安全恢复
但更改信息需在亚马逊S3中复制
这可能需要一些时间
因此该表格基本描述了最终一致性读取和一致性读取的特性
所以对于最终一致性再次
正如名称所示
可能出现陈旧读取
您拥有最低读取延迟和最高读取吞吐量
而对于一致性读取
则是相反情况，无陈旧读取
但延迟和吞吐量略低于最终一致性
现在这里 您可以看到最终一致性与一致性读取请求的示例
当多个客户端同时写入同一项时
在左侧示例中
W1和W2在R1和R2开始前完成
其中W指写操作，R指读取返回
现在对于一致性读取
R1和R2均返回颜色等于Ruby（最终一致性读取）
R one 和 R two 可能返回颜色等于红色或颜色等于 Ruby 或无结果
取决于右侧示例中已过的时间量
W two 在 R one 开始前未完成
因此 R one 可能返回颜色等于 Ruby
或颜色等于 Garnett 用于一致性读取或最终一致性读取
同样取决于已过的时间量
最终一致性读取可能完全返回无结果
现在让我们看看 S3 中的一些重要功能
首先 我们有存储桶策略
基于多种条件提供对存储桶和对象的集中访问控制
包括 S3 操作
请求者
资源和请求方面
这些是 IP 地址
策略使用访问策略语言表达
并支持权限的集中管理
附加到存储桶的权限适用于该存储桶内的所有对象
个人和企业现在均可使用存储桶策略
当企业注册 S3 时
他们基本上创建一个账户
之后企业与账户等同
需对创建的亚马逊资源承担财务责任
他们还可授予存储桶策略权限并分配员工权限
基于多种条件
您还有身份与访问管理
例如 您可以使用 IAM 与 S3 控制访问类型
用户或用户组对 S3 存储桶特定部分的访问权限
我们还有版本控制
我们将在实验中详细讲解
若要执行生命周期策略需启用版本控制
版本控制可追踪对象的历史记录
可查看谁更新了它、何时更新
以及是否被删除
当通过版本控制删除 S3 对象时
对象从未真正从 S3 删除
我将在实验中演示这一点
此外 这些是最常见的可通过 S3 API 执行的操作
如创建存储桶
写入对象
或通过创建/覆盖存储桶内对象进行存储
读取对象
删除对象 和列出键
即列出存储桶中的所有键
在授予权限时
需确定谁获得权限
他们将获得哪些 S3 资源的权限
并允许在这些资源上执行任何特定操作
因此默认情况下
所有三个资源均为私有
只有资源所有者可以访问该资源
资源所有者指的是创建资源的AWS账户
所以 例如 您用于创建存储桶和对象的账户拥有这些资源
如果您在账户中创建IAM用户
您的AWS账户是父账户
如果用户上传对象
用户的父账户拥有该对象
存储桶所有者可以授予其他AWS账户跨账户权限
存储桶所有者对其他账户拥有的对象没有权限
现在 这非常重要
AWS建议不要使用AWS账户的根凭证
来发起请求
请确保创建IAM用户并授予其使用此类服务的权限
现在 大家看到的左上角第一个图表
基本展示了一个拥有资源的AWS账户
IAM用户
存储桶和对象
现在管理访问权限是指通过编写和访问策略授予他人执行资源操作的权限
例如 您可以授予某个AWS账户用户上传对象的权限
因此用户可以将对象上传到您的存储桶
用户和账户
或授予所有认证用户或拥有AWS凭证的用户
所以 例如
如果您将存储桶配置为网站
可能需要通过授予所有人获取对象权限使对象公开
现在访问策略描述了谁可以访问什么
您可以将访问策略与资源或用户关联
相应地 您还可以通过资源策略对S3策略进行分类
即大家看到的右侧图像
现在存储桶策略和访问控制列表是基于资源的
因为您将其附加到S3资源上
现在对于ACL
每个存储桶 对象都关联有ACL
ACL本质上是一个权限授予列表
标识受让人和授予的权限
您使用ACL向其他AWS账户授予基本读写权限
现在ACL使用S3特定的XML模式
桶和对象
Acs 使用相同的 XML 模式
现在针对您的桶
您可以添加桶策略以授予其他 AWS 账户
或者我是用户
对桶和其中对象的权限
任何对象权限仅适用于桶所有者创建的对象
桶策略补充并常替代基于 ACL 的访问策略
与 ACL 策略不同
桶策略使用 JSON 文件表达
最后我们有用户策略
这是您在左下角看到的图像
您可以使用 i am 管理对 S3 资源的访问
您可以在账户中创建用户、组和角色
并附加访问策略
授予对 AWS 资源的访问权限
包括 S3
现在 S3 基本上是一个简单键值存储，可存储任意数量的对象
现在您将这些对象存储在一个或多个桶中
对象包含以下信息
它有键
您为对象指定的名称
使用特定对象键来检索对象
然后是桶内的版本 ID
键和版本 ID 唯一标识对象
版本本质上是一个字符串
S3 在将对象添加到桶时生成
然后是对象标签
用于分类存储
每个标签是键值对，现在要管理对象
以便在其生命周期内高效存储
我们需要配置生命周期策略
生命周期配置是一组规则，定义 S3 应用的动作
针对一组对象的动作
现在生命周期策略中可定义两种动作
转换动作和过期动作
在转换中 定义对象的存储类转换
例如 您可能选择在创建对象 30 天后将其转换到标准存储类
或在创建后一年归档到 Glacier 存储类
或创建后 60 天
根据业务需求
过期动作定义对象何时过期
S3 会自动删除过期对象
跨域资源共享
或 CORS
定义 客户端
加载在不同域中的 Web 应用与资源交互的方式
现在支持核心功能
你可以使用s three构建丰富的客户端网页应用
并选择性允许跨域访问你的s three资源
最后 S three使你能够存储
检索 并删除对象
你可以获取整个对象或对象的一部分
如果你已启用存储桶版本控制
你可以检索对象的特定版本
此外还可以检索与对象关联的子资源并更新
在适用情况下
你现在可以复制现有对象
根据对象大小
某些上传和复制相关注意事项适用
例如 你可以一次性上传最大5GB的对象
可以复制最大5GB的对象
同样在一个原子操作中完成
此外还可以使用REST API操作对象 或使用任何AWS SDK库
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/039_Udemy - Become an AWS Certified Data Engineer part1 p39 4. How to Create Amazon S3 Bucket.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到本课关于亚马逊S3的内容
在将数据上传到亚马逊S3之前
我们需要做的第一件事就是创建存储桶
接下来我将导航到服务页面
我将找到存储选项
现在我要操作的是S3，这是S3的仪表盘
或者我们创建的任何存储桶都会显示在这里
由于没有存储桶 没有任何内容显示
首先我要点击创建存储桶
首先我们需要为这个存储桶提供名称
请注意存储桶名称必须唯一
在整个现有存储桶名称中唯一
在亚马逊S3中
在整个亚马逊S3范围内
不仅仅是您的组织
因为名称符合DNS规范
因此必须唯一
接下来是区域选择选项
我们可以选择存储桶所在的区域（可选设置）
如果我们已经设置过存储桶
且新存储桶需要相同配置
可以通过此选项快速设置
从现有存储桶复制设置
如果我们有现有存储桶
所有选项都会在这里显示
这便于常规存储桶创建
回到名称设置
还有一些注意事项
必须唯一
不能包含大写字母
必须以小写字母或数字开头
名称长度需在3到63个字符之间
最重要的是
存储桶名称创建后
无法更改
请谨慎选择名称
因为一旦创建
如果组织正在使用
发现名称需要修改时
需要创建新存储桶
因为名称不可更改
假设我将存储桶命名为test
亚马逊S3中很可能已有其他test存储桶
让我们看看会发生什么 如果我选择test
如你们所见提示名称已存在
现在 该名称在我的账户中不存在
但在亚马逊S3全局范围内
存储桶名称 test 不存在
所以我将给存储桶命名为 custom shaw s three
区域 我将继续使用弗吉尼亚北部
这是亚马逊AWS的主要区域之一
由于我没有任何存储桶
我将留空并点击下一步
现在我们可以选择不同的配置选项
第一个选项是版本控制
现在什么是版本控制
基本上会 它会跟踪同一存储桶内的所有对象
例如你创建了一个对象
然后你更新了它
而其他人也更新了它
然后你删除了它
如果启用了版本控制
它会记录该对象发生的所有操作
无论是更新 还是删除
如果你想保留对象的历史记录
启用版本控制始终是个好主意
日志功能提供对存储桶请求的详细记录
然后是标签
我们可以使用成本分配存储桶标签来标注计费
用于存储桶的使用情况
每个标签是键值对，代表一个标签
你想要分配给存储桶
如果大家还记得
这些标签也适用于e
C 两个实例和组
而我
还有对象级别的日志
这启用云追踪日志
我们稍后会在课程中简要讨论云追踪
但作为简介
云追踪基本上是一种方式
我们可以审计AWS账户内的访问和操作
然后是加密
它会自动加密存储在AWS中的任何对象
此外 一些高级设置
我们有一个对象锁定选项
如果你想现在锁定存储桶中的对象
请注意，为了实现这一点
你需要启用版本控制
因此我们将为存储桶启用版本控制
然后点击下一步
请记住，这不仅适用于你自己，也适用于云从业者
考试中默认每个存储桶都是私有的
所以我想要创建一个存储桶
如果你向该存储桶添加了对象
每个对象都将默认私有
这意味着只有组织内部人员可以访问
所以当你创建
比如创建静态网站并通过S3存储桶托管内容
所有内容都将默认私有
因此互联网用户无法访问此S3存储桶内容
我们暂时保持默认设置，点击下一步，最后
我们可以查看所有已选配置选项
可以继续修改
如果需要的话点击创建存储桶
现在我们成功了
嗯 我们的第一个存储桶已创建完成
可以看到访问权限为私有
并非公开访问 当前所在的区域是
以及存储桶创建的日期和时间
点击存储桶后
发现该存储桶内没有对象
可以查看存储桶属性
例如 如果我们未启用版本控制
可以在此启用版本控制
可以看到它已自动启用
服务器访问日志同样适用
对象级日志功能
以及默认加密设置
这正是我们在创建存储桶时看到的
此外还可以
这也提供了托管静态网站的便捷方式
如果我们计划托管静态网站
可以在此启用该功能，需要注意的一个选项是传输加速
考试中有时会涉及传输加速相关问题
可实现S3存储桶与文件之间的快速安全传输
考试题目可能会问：尝试向S3存储桶上传大量数据
S3存储桶 且时间紧迫
有哪些选项可以加速数据传输过程
正确选项应为传输加速
在权限标签页中
可以修改S3存储桶的设置
如果要将其设为公开而非私有
可以在此处进行设置，同样
始终牢记默认权限是私有
此外还有访问控制列表选项
通过策略选项授予其他AWS账户基础读写权限
或者甚至公开访问
比如说 如果我们想授予其他AWS账户访问此S3存储桶的权限
我们可以在这里完成
我们也可以编写存储桶策略
这是一个简单的JSON编辑器
我们可以创建JSON来授予人们访问此存储桶的权限
如果你不太熟悉JSON编写
AWS提供了一些文档说明
如果你想学习或使用策略生成器
通过这个策略生成器我们可以创建不同类型的策略
让我们说我们要创建一个S3存储桶策略
我们想要允许
特定人员访问存储桶并执行哪些操作
是否允许创建 删除
这提供了我们可以允许或拒绝的多种操作
一旦完成
它会生成JSON策略供我们复制粘贴
最后是课程配置
也称为跨域资源共享
假设你的S3存储桶位于另一个区域
而访问它的应用程序或用户位于另一个区域
这允许AWS内部不同域的访问
比如说 如果你想上传任何文档并且再次
这也是一个非常简单的过程
我们可以上传任何类型的文档，一旦上传
我们可以直接上传
或者点击下一步
我们可以为此对象定义特定权限
可以将此特定对象设为公开
为该对象授予特定账户的访问权限
在创建S3存储桶文件夹时非常实用
并希望允许特定人员访问某些文件夹
我们可以在这里完成
我们还可以为此对象设置属性
在上传到Amazon S3时涉及存储类别
是否选择标准存储
智能分层
IA
单区存储
或不常用存储Glacier及低冗余
加密同样适用
如果你想指定这些附加选项
需要在上传对象到S3时定义
我们已成功将第一个对象上传至S3存储桶
如果大家还记得，我之前启用了版本控制
在创建存储桶时
查看版本
我可以点击显示
它会显示该对象的所有不同版本
所以假设我要从S3存储桶中删除这个对象
该对象不再存在于S3存储桶中
但如果我进入版本管理
可以看到该对象仍存在于S3中
只是不在存储桶里显示
这是一个非常重要的实用工具
我们可以使用版本控制四来跟踪存储桶内的对象
S3存储桶中的对象
同时也要注意我们为S3存储桶设置生命周期策略时
生命周期策略用于归档未使用的数据
例如 如果我们想为某些文件夹设置策略
或30天内未被访问的对象
我们想将其转移到Glacier
假设我们要在30天后归档数据
必须启用版本控制以便归档生效
或使数据从S3存储桶转移到Glacier 必须启用版本控制
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/040_Udemy - Become an AWS Certified Data Engineer part1 p40 5. Create S3 Bucket and Cross Region Replication.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，超级兴奋继续前进
我将讲解S3存储桶的跨区域复制
所以我将用这个简单示例演示
这一点很重要，因为您所在的区域
比如现在我正在北弗吉尼亚区域工作
您会注意到下拉列表中
列出了其他多个区域，默认情况下
我已选择 或者我一直在使用美东北弗吉尼亚
AWS实例和存储桶与此区域关联
如果我有另一个区域
并且有实例 在不同区域有S3存储桶
例如
那么如何确保S3存储桶内的数据完全复制
因为记住S3存储桶是全球速率且近乎无限空间
S3存储桶内可容纳的内容
您可以存储对象
可以存储图片
存储数据 任何类型的信息在S3中
但在当前实例中我仅有
我正在使用美东弗吉尼亚区域
在本课程中
我将演示如何创建S3存储桶
如何配置版本控制
然后演示数据复制
从一个区域的S3存储桶复制到另一区域的S3存储桶
请注意这同样适用于不同账户
现在我使用claydee学习账户
您在不同区域有另一个账户
同样适用完美
让我们直接开始
我们先查看S3存储桶
如果不存在 则创建一个
您需要两样东西
需要源S3存储桶
即此区域北弗吉尼亚
然后切换区域创建目标S3存储桶
然后配置对象复制步骤完美
我将点击服务
您需要导航到存储部分
然后点击S3
这将带您到S3管理控制台页面
此时此区域没有存储桶
我将快速创建一个存储桶
点击加号并完成几步
只需命名和选择区域
配置选项
设置权限 然后在实际创建存储桶之前进行审核
所以这是存储桶名称
我将直接为其命名
我将其命名为
这是该区域内的存储桶名称
这里是美国东部北弗吉尼亚地区
我也可以从现有存储桶复制设置
但因为我没有现有存储桶
所以我将使用此名称创建新存储桶
点击下一步
这里是最关键的部分
这里是版本控制
请确保此选项已勾选
在显示 保持同一存储桶中对象的所有版本
需要注意的是
假设您已在存储桶中存有大量对象
之后再创建或启用版本控制
启用版本控制时
仅会影响设置后创建的对象
或配置这些选项后
我将点击版本控制
确保保持同一存储桶中对象的所有版本
我还可以记录访问存储桶的请求
稍后我会详细查看
在本课程中您将学习
但目前我仅演示版本控制
您还可以通过标签进行追踪
项目成本需要
我还可以选择对象级日志记录
好的 但目前我仅保留版本控制
点击下一步
还有其他选项
顺便说一下 如果我向下滚动
只需知道在高级设置中可以锁定对象
这将允许我永久锁定存储桶中的对象
或者我还可以使用管理功能和AWS CloudWatch
我可以创建警报或通知
以及其他可操作的指标
我将直接点击
下一步第三步是设置权限
创建存储桶后可授予特定用户权限
目前 这是该存储桶的公共访问设置
默认情况下请注意
这四个复选框均被选中
换句话说 管理此存储桶的公共访问控制列表
同时管理此存储桶的公共存储桶策略
如果我向下滚动
现在可以管理系统权限
不要授予Amazon S3日志交付组对此存储桶的访问权限
或者我可以选择其他两个选项中的一个
所以我保持这个设置
作为 不要授予Amazon S3日志所有权限
所以我将其设为默认
在权限区域保持原样
点击下一步然后在这里
我可以上下滚动查看所有设置
这就是我的存储桶名称
区域
然后版本控制是启用状态
好的 这就是我们在这一部分完成的所有操作
我将点击创建存储桶
这将创建该存储桶
我现在有一个位于一个区域的存储桶
此时是全局的
因为如果我点击全局选项
在此菜单中S3无需选择区域
我们配置的是跨区域复制
创建存储桶后
现在创建另一个存储桶
这次先更改区域再创建存储桶
让我们回到AWS管理控制台主页
我回到北弗吉尼亚区域
我向下滚动并更改区域
选择其中一个区域
我将选择
比如加拿大中部是另一个区域
如果我点击加拿大中部区域
将跳转到该管理控制台页面
现在我在中部区域工作
在中部区域
我将前往服务
进入存储服务
点击S3并创建另一个S3存储桶
点击创建存储桶
命名目标并注意此处的区域
因为这是实际创建的区域
该存储桶位于加拿大中部区域
尽管存储桶是全局的
我可以在任何区域创建
我现在选择了更长的路径
对 我只是想展示你可以
即使你更改了区域
但全局会为你显示三个
无论你在何处
如果你正在搜索其他区域
而你在想 等等这里
为什么我的区域没显示
所以 我只是想带你一步步演示
否则这是一个快捷方法
只需继续创建S3存储桶并在创建时选择区域
好的好的
我要确保这里选了加拿大
点击下一步，当然这个存储桶需要启用版本控制
基本上你只是重复之前的操作步骤
当你创建第一个存储桶时
点击下一步，再点一次然后创建
好的 这将为你创建目标存储桶
现在你有两个不同区域的存储桶
这很重要 现在你可以看到有两个区域和两个存储桶
然而 如果我点击全局选项或菜单
这里仍会显示S3无需选择区域
好的 我只是想慢慢带你走一遍
让你清楚了解S3和版本控制的工作原理
但当你有经验后
只需去S3创建多个不同区域的存储桶
好的 这样就很直接了当
当你创建了这两个存储桶（源和目标）
我先去访问源存储桶
位于美国东部北弗吉尼亚
目标存储桶在加拿大中部
我们先去访问源存储桶
向源存储桶中添加对象
现在打开这个存储桶
这个存储桶目前是空的
我将上传一些内容到它
接下来你需要做的是
当然上传一张图片或文档
或任何你想复制的文件
我将点击上传
再次弹出对话框
四个简单步骤
点击添加文件
打开你的文件资源管理器
我随便选一个
我想复制这个从一个区域到另一个区域的标志
所以我们只需选择这个标志
点击打开然后点击下一步
这是第二步
当然需要管理用户
或者在这个时间点
这个存储桶的所有者是shaw
也就是我自己
然后启用对象读写权限和对象权限读写
我也能访问其他AWS账户
我可以添加另一个账户
换句话说 该账户也将成为此存储桶的所有者或管理员之一
然后管理公共权限
不授予公众读取权限是推荐做法
我也可以选择其他选项
例如授予此对象的公共读取权限
这意味着如果我选择此选项
如果我点击这个 你会注意到出现一个小警告
这个对象已启用公共读取权限
全世界的人都能读取此对象
绝对不建议这样做
所以保持不授予公众读取权限吧
我将继续点击下一步设置权限
这是标准存储类别，适用于频繁访问的数据
并且需要至少三可用副本等
否则我也可以选择其他选项
所以我可以向下滚动
这里有许多选项
加密功能很实用
因为它增加了安全性
例如使用S3主密钥保护静态数据
或使用AWS KMS主密钥
KMS是基于加密的
并与云追踪配合记录所有密钥操作
这提供了额外的安全层
或者使用标准选项
S3主密钥同样可用
然后可以添加标签并填写元数据
我将继续使用标准权限下一步
只需设置属性
然后最终进行上传
这将启动上传过程
并为我上传源存储桶中的图像
这是美国东部北弗吉尼亚地区
现在一旦在存储桶中放入对象或多个对象（演示用途）
只需注意顶部的标签栏
有一个名为管理的选项卡
我将点击管理选项卡
还有一个名为复制的按钮
所以我现在点击复制
此时系统提示
我没有为此存储桶配置或创建任何跨区域复制规则
我将点击添加规则
这弹出一个对话框
这里有几个选项
我可以选择或选中整个存储桶
默认情况下已自动选中
并显示存储桶名称
或者我可以使用前缀或标签
因此选择前缀或标签取决于需求
例如 然后我需要输入或添加前缀或标签过滤器
因此我不必复制整个存储桶
我只能复制以特定前缀开头的对象
好的 也许我需要仅复制人力资源部门的文件
所以 所有与人力资源相关的文件
部门的文件将带有hr前缀
所以我输入hr
则S3存储桶内的所有对象都将被复制，完美
所以我们选择整个存储桶
当然，我可以复制使用AWS KMS加密的对象
我将点击下一步
系统要求我选择目标存储桶
正确，当然我们的目标存储桶是加拿大区域目标
因为我在列表中已创建了加拿大中央区域
因为我已经创建了该存储桶
如果使用其他账户的存储桶
我可以选择此选项并输入账户ID和存储桶名称
这样即可完成 我将选择本账户中的存储桶
并选择加拿大目标存储桶
选择目标存储桶后
提示Amazon S3无法检测目标存储桶是否启用了版本控制
仅作为警告 尽管我们已启用版本控制
但我们将自行验证确认
这是一个有用的提示
好的 因为需要两个存储桶都启用版本控制
才能开始复制流程
其他选项包括更改复制对象的存储类别
或更改对象所有权
目标存储桶顺序
如果选择此选项
这意味着所有权将转移至目标存储桶
由于我是两个存储桶的所有者
因此在此处变为灰色禁用
但如果有一个选项是在不同区域执行操作
例如或不同账户
那么你可以设置这些选项
所以只需选择目标存储桶或选择该存储桶
点击下一步 我将创建一个新角色
即使可以选择现有角色
但为了快速演示和同步操作
现场操作
AWS 提供创建新
我角色 我们继续点击
在此选择角色并直接选择
创建新角色
我将为这个输入名称
我要说 S3 角色右或复制状态已启用
点击下一步并确认所有选项
我将点击保存
这将添加复制规则并提示消息
恭喜 跨区域复制更新成功并显示详细信息
现在我有一个源
即弗吉尼亚 S3 源和目标存储桶
加拿大 S3 目标在两个不同区域
这里是权限设置
如果向下滚动
这里就是规则
S3 复制规则
整个存储桶
存储类范围
复制对象所有者
KMS 对象
状态名称
这让你了解如何轻松配置
设置不同对象的跨区域复制
注意尽管我们成功配置了跨区域复制
那源中的图片是否会被复制
思考一下答案是否定的
因为我们先上传了文档后才配置复制
记住课程开头部分需要先配置
在首次添加待复制对象前
如果我导航到我的 S3 存储桶
我将快速展示
然后前往加拿大目标
你会看到该存储桶为空
因为加拿大尚未接收复制的图片
因为图片是在配置复制前上传的
让我们返回亚马逊 S3
前往我们的源存储桶
现在如果我添加一张新图片应该会被复制
我们先快速看一下这个情况
我将点击
上传
添加文件 这次我将选择
比如所有Excel文件
这同样是一个Excel文件
我将点击打开
点击下一步
只需继续操作
我将快速导航
因为我们已经做过并上传了
当我在我在弗吉尼亚三区的源存储桶中上传
这是美国东部区域的源存储桶
并且现在正在上传并已完成上传
相反 我们再次导航到S3
点击加拿大三区目标
刷新并确认
太棒了
你已成功配置了跨区域复制
作为课后作业
尝试用两个不同账户操作
如果你能为你的工作空间创建另一个账户
希望这对你有帮助 如有疑问
在讨论区留言
我们将乐意解答 至此 我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/041_Udemy - Become an AWS Certified Data Engineer part1 p41 6. Introduction to Data Lakes Architecture.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
我们来谈谈数据湖吧
存储业务内容始终存在争议且常引发困扰
各类企业的业务
没错 内容应该按文件夹存储吗
对的 是否应通过前缀和后缀标识文件版本
正确 还是应按部门或专业划分内容
没错 这样的问题还有很多
问题根源在于许多企业开始实施文档或文件管理系统
出于良好意图
却缺乏预见性或基础设施维持初始数据组织
出于迫切需要组织日益增长的数据量
没错 数据湖由此诞生
数据湖是集中式存储库，允许存储结构化
半结构化和非结构化数据任意规模
现在让我们谈谈S3中数据的组织方式，再讲数据湖
对的 S3是对象容器
我之前也讨论过这一点
好的 可以有序存放内容
或者直接将数据倒入S3
无论数据如何到达
一旦进入S3
需要以有意义的方式组织
以便需要时能快速找到
这就是数据湖的作用
好的 数据湖是帮助管理多源多类型数据的架构概念
包括结构化和非结构化数据
通过统一工具集
让我们拆解理解
数据湖采用亚马逊
S3存储桶并按桶内数据分类组织
数据来源或类型无关紧要
可在亚马逊
S3数据湖中有效存储结构化和非结构化数据
现在 AWS提供全栈工具管理数据湖
不将每个存储桶视为独立对象
许多企业最终将数据分散到多个存储位置
正确称为信息孤岛
这些孤岛通常由不同团队管理维护
这可能存在问题
数据写入方式存在不一致
收集 聚合或过滤
当从这些不同数据孤岛中整合或组合处理时可能会出现问题
例如 一个团队可能使用地址字段同时存储街道号码和街道名称
而另一个团队现在可能将街道名称或街道号码分开存储
当这些数据集现在被整合时
地址存储方式出现不一致，这将使分析变得
非常困难对吧
但通过使用数据湖
可以打破数据孤岛，将数据整合到单一中央仓库
由单一团队管理，提供统一可信的数据源
由于数据可以以原始格式存储
无需在存储前进行转换、聚合或过滤
现在您可以将预处理留给处理系统的环节
而非存储系统的环节
换句话说
无需转换数据使其可用
保持数据原始形式
无论它如何到达
然而 当谈论x千兆字节的数据时
无法负担对数据进行所有可能的预处理
以呈现可用状态
现在让我们讨论单一数据源
当我们谈论数据的可信度时
需要数据的可信度
是否符合预期
例如 是否被篡改过
没错 能否验证数据链
现在 在创建单一数据源时
您正在创建一个数据集
没错 在这种情况下就是数据湖本身
可用于所有处理和分析
现在优势在于数据需保持一致且可靠
没错，值得信赖
综上所述
企业需要以多种方式便捷访问和分析数据
使用自选工具和框架
请记住之前课程提到的第二个原则
没错 数据在存储与处理间传输成本极高
现在 亚马逊三的数据湖为整个解决方案提供统一存储架构
满足所有需求并支持数据分析工具
无需额外工具
现在数据泄露承诺能够将所有业务数据存储在一个地方
直接存入单一仓库
您可以利用数据湖存储大量数据
而不是将数据持久化到数据仓库或仓库
现在像亚马逊S3构建的数据湖
例如 通常比大数据存储方案更经济且专业
这样您只需为使用、处理和分析数据时支付专业方案费用
而非长期存储
现在您的提取
转换 加载
你知道的 ETL或分析流程仍可访问这些数据进行分析
让我们谈谈一些优势吧
所以您明白企业为何要实施数据湖或构建此架构
AWS的第一个优势
数据湖的优势在于这是高效存储方案
您可以使用亚马逊S3存储近乎无限的数据
可实施领先的安保措施和合规性
AWS采用严格的安全措施
同时在S3桶中实现合规、隐私和保护机制
这使您能利用多种数据采集和摄入工具
将数据摄入数据湖
这些服务包括亚马逊Kinesis
例如 用于流数据
以及AWS雪球用于大量本地数据
帮助您高效简便地分类和管理数据
现在使用AWS Glue
例如 理解AWS数据湖内的数据
准备并可靠加载到数据存储
当Glue目录化您的数据后
即可立即搜索
可被查询 例如
并可供ETL流程使用
同时帮助您将数据转化为可使用的有意义洞察
例如 使用像Athena这样的分析服务
例如用于交互式分析等用例
使用Apache Spark进行数据处理
例如 或Apache Hadoop
数据仓库 实时分析
运营分析
仪表盘 可视化
你要什么就有什么
例如 亚马逊EMR和数据湖
没错，企业已经认识到数据湖的力量至关重要
数据湖中的数据
你可以实际分布数据源并使用框架
如亚马逊EMR支持的那些
例如 现在Apache
Hadoop 另一方面
Spark同样也受亚马逊EMR支持
这有助于企业以低成本有效实施数据处理方案
基于亚马逊S3数据湖
现在数据准备基本是考虑这些数据
科学家花60%时间在数据清洗上
这需要大量时间
数据准备是一项巨大工程
现在 数据清洗或转换确实没有捷径
为数据湖收集数据
但有服务能自动化这些耗时流程
如今搭建和管理数据湖
例如 可能涉及大量手动且复杂的任务
这项工作包括加载数据
监控数据流
为数据设置分区
然后 当然 加密现在你可能需要重组数据以复制
例如 匹配记录并审计数据随时间变化
现在数据湖形成使其更易调整
清理目录
转换并安全处理数据使其可用于分析
当然还有机器学习
数据湖形成提供中央控制台
可发现数据源
设置转换任务将数据迁移到S3数据湖
或去重匹配记录并编目数据
例如嗯
你知道的 这样分析工具才能访问
同时可实施安全策略
你可以审计
可通过AWS控制访问
分析和机器学习服务
数据湖形成会自动配置底层AWS服务以确保符合定义策略
现在如果你设置了转换任务
例如 正确，使用AWS服务
数据湖可以分析数据流
集中化其编排管理
然后允许监控所有任务的处理过程
所以我希望这能帮助你理解数据湖的概念
在设计数据分析方案时，构建高效的数据湖架构至关重要
如果有任何疑问请告诉我 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/042_Udemy - Become an AWS Certified Data Engineer part1 p42 7. AWS Data Lake In-Depth.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，这次重要讲座我超级兴奋
我将讲解亚马逊数据湖的核心基础概念
什么是亚马逊Athena
A flu s three and rite
首先 让我们看看实际
在亚马逊S3数据湖中的整体架构设置
这会带来线索
从左侧开始
虚线内的组件基本是数据
无论数据存储在网页应用
还是关系型数据库
比如SQL
MySQL
对于其他数据库
您的数据是否在本地存储
或者是否有实时流数据
一旦拥有数据
这些数据会发生什么
显然需要存储数据
这就是亚马逊S3的作用
S3提供存储桶
就像文件夹
可以创建子文件夹
对象
文件 图片等
所有数据都存储在亚马逊的存储桶中
称为亚马逊S3
这是历史性的
在亚马逊的存储基础上
我们可以使用Glue工具
这是无服务器计算
好的 AWS Glue的ETL意味着提取
转换和加载
这里的核心操作是
使用AWS Glue从存储桶提取数据
转换数据
然后加载数据
其工作原理
Glue的工作方式是
爬虫从S3存储桶获取数据
扫描数据集
然后填充Glue数据
目录
Glue数据
目录 进而作为中央元数据仓库
包含模式等信息
从蓝色目录中
然后我们可以朝着最终目标前进
也就是可视化我们的分析结果
或者使用亚马逊QuickSight创建可视化
但在它们的选项中
其中一个选项是亚马逊Athena
另一个是亚马逊EMR和Amazon Redshift Spectrum
一旦在Athena中编目
数据即可立即用于分析
在这三个选项中
亚马逊Athena
Amazon EMR和Redshift
接下来可以进入Amazon QuickSight
它允许你创建可视化
这是一个强大可靠的工具，可创建图表
查看你的数据
创建故事
发布仪表板等
Amazon Athena本质上是无服务器查询服务
从数据目录中
然后可以选择或查询数据以便开始分析
使用Amazon QuickSight
EMR只是一个Hadoop框架
类似于数据仓库
而Redshift Spectrum也是如此
你应该明白这个概念
我在亚马逊上构建了一个数据湖
但你可能不理解这些概念
在这门课程中，我将演示
几乎从原始数据开始
并将使用多个数据源
使用Glue转换数据
我也会演示这一点
尽管这是一门基础课程
好的 学习的最佳方式不仅是掌握概念
还要看到它们的实际应用
这就是本课程的目标
本课程不深入探讨Athena
没错 只需掌握足够基础的理解
以及数据从原始状态
从存储阶段开始
一直到Amazon QuickSight
在这门课程中，我将展示
如何运作
如何转换数据
然后我将演示使用Amazon Athena
我不会涉及Amazon EMR和Redshift
我只是要用胶水粘住雅典娜
然后转向快速侧边
这非常致命
所以想想这个
当作一个集中式仓库
允许你存储所有结构化和非结构化数据，任意规模
所以你可以存储数据
比如 无需预先对数据进行结构化
并运行不同类型分析
从仪表盘和可视化
到大数据处理
实时分析
甚至机器学习
以指导他们的决策
这只是数据仓库与数据湖的核心区别
你可以在图表中看到
好的 所以数据仓库在数据方面
在数据仓库中包含事务系统的关联数据
运营 数据库和业务应用
也称为业务线
而在数据湖中则是非关联和关联的物联网设备数据
网站
移动应用 社交媒体和企业应用
所以你看得很清楚
数据利用的强大之处
它描述模式摘要
价格与性能 数据
质量用户
分析
不同特性
与数据仓库对比
在框架内
接下来是aso
正如我之前简要提到的
亚马逊Athena是交互式查询服务
轻松分析亚马逊S3中的数据
使用标准SQL语句
事实上 演示实际操作如何使用
这是无服务器的
无需管理基础设施
仅按实际运行的查询付费
就像一万亿次查询
我们只需支付约五美元
或者类似这样的费用
使用起来非常简单
快速性能 安全并与您学校系统集成
其核心功能是定位s3存储桶中的数据
定义数据模式
然后使用中心化的查询语言开始查询
执行选择语句和更新操作
语句
草稿表
绿色表 优质数据库和交换
一种粘合剂
再次强调 是全托管的ETL管道
数据转换与加载服务，帮助客户轻松准备和加载分析数据
因此您可以创建一个
例如 只需在管理控制台提交几个请求即可启动理想任务
其工作原理是直接指向存储在AWS上的数据
比如s3存储桶
学校将使用爬虫发现您的数据
存储相关元数据
如表结构定义和模式在kerala中
当所有数据完成编目后
您的数据立即可用于搜索和ETL操作
还会生成执行数据转换和加载的代码
同样地 就像在lista中一样
它是蓝色的 也是无服务器架构
无需购买基础设施
无需设置或自动管理
提供 所需环境
任务
接下来是AWS S3
它只是一个对象存储服务
可从网站重新创建任意数据
移动应用 企业应用
物联网数据
传感器或设备
所有数据进入存储桶
这不仅仅是文件夹
实际上远不止于此
并设计为提供接近百分之百
九九点 九的持久性
存储来自各行业领军企业的百万级应用数据
S three 也允许
顺便说一下，您可以在数据上运行复杂的大型数据分析
无需将数据迁移到独立的分析系统中
所有内容都保留在 S3 中
因此您无需将数据从 S3 物理迁移至其他区域
然后运行分析
只需一次到位
即可立即使用
因此从这个数据湖
或从 S3 到 QuickSight
因此在使用 Athena 时
这里有一个小型工作流程与 Glue 集成
由于 Glue 与之无缝集成
我们使用 Glue 创建数据库和表结构查询
Glue 是完全托管的 ETL 工具，可分类您的数据
清理并丰富数据
然后可靠地在各种数据源间传输
爬虫可自动推断数据库
您也可以手动创建数据库
因此并非所有操作都是全自动
您可以
我们尝试手动创建数据库
而非使用 Glue
如果您使用爬虫
则会自动从源数据导入数据库和表结构
并将关联的元数据存储到目录中
当您在 Athena 中创建表时
例如 可以选择使用爬虫或手动创建
因此数据从 S3 流向 Glue 再到 Athena
然后 当然 连接到您的 Amazon
QuickSight
最终我们达到了目标
即 AWS QuickSight
当然同样
仅展示一个数据源与数据集的高层工作流程
结合分析过程 创建仪表盘和故事报告
当然可随时向团队成员展示
完美 总结来说
我们了解了数据湖的各个组件
如 Athena、Glue、S3
数据仓库与数据湖的区别
AWS 提供了强大的基础设施以达到
能够分析存储在 S3 中的数据
因此无需将数据从 S3 迁移到 Redshift
但数据已存放在特定位置
你只需要做一件事
并利用这些无服务器计算工具创建一些强大且稳健的可视化效果
图表和图形
故事叙述 仪表盘
图表信息等等
所以我希望这能帮助大家实践这些概念
我现在将进行演示
当然 所有这些实践环节都由你们自己操作
不仅能够看到
理解这些概念
还能掌握实际应用
因此通过这些 让我们进入下一个环节 让我们
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/043_Udemy - Become an AWS Certified Data Engineer part1 p43 8. AWS Glue - AWS Athena.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，超级兴奋，继续推进这节课的内容
我将介绍几个核心概念
因为我们将会直接使用数据作为数据源
然后调用并使用这些亚马逊服务来转换我们的数据
为数据做准备
当然，然后将数据转移到QuickSight
理解这些概念很重要
这样你就不会手忙脚乱
并尝试理解这些术语以及整个流程如何运作
我将从根本处解释
这样你更容易理解和跟随后续内容
我将演示如何准备和处理数据
使用亚马逊提供的存储服务
在这节课中，我将介绍亚马逊Athena
这是一个无服务器查询服务
我还将介绍AWS Lambda
S3存储以及相关服务
几个重要概念
首先介绍AWS Athena
这是什么对吧
亚马逊Athena是一个交互式查询服务
我的查询服务听起来很熟悉对吧
这意味着我现在可以查询我的数据
换句话说 我可以知道
排序过滤
在数据最终进入QuickSight之前从数据中提问
亚马逊Athena是交互式查询服务
轻松分析存储在S3桶中的数据
使用标准SQL
这是一种简单的SQL查询语言
可以执行如SELECT语句
UPDATE等操作来查询现有数据集
Athena是无服务器架构
这基本定义了无服务器
顺便说一句 如果你接触过这个术语
可以这样理解 你不需要本地服务器
完全使用云服务商提供的资源
以我们的情况为例 这就是无服务器的概念
无需管理基础设施
仅按实际运行的查询计费
据我所知是1TB
或在支付前执行的查询
比如五美元左右
好的
使用起来非常简单
性能快速且安全
雅典娜还集成了AWS Glue
好的 这就是你可以实际构建数据库模式的地方
你的表和其他结构现在
我稍后会演示并详细讲解这一点
我们还可以将Amazon S3存储桶中的数据指向以定义模式
并开始使用标准SQL进行查询
这就是AWS Athena的核心功能
所以从你的实例到
比如Amazon S3
你创建了不同的存储桶
这些存储桶中存储着不同的数据集
右存储桶本质上就是一个文件系统
在高层次或更广泛的概念中
只需将其视为Windows资源管理器
你的存储中有所有这些文件
从存储中
Amazon Athena开始运作
你可以运行交互式查询
通过Athena使用
然后利用这些数据和查询结果
可以将它们导入Amazon QuickSight制作可视化图表
或进行分析
运行分析故事
执行计算
等等好的
这就是核心理念
概念你应该很清楚了
接下来是AWS Glue这个术语
从S3到QuickSight
当使用Athena与AWS Glue数据目录时
可以使用AWS Glue创建数据库和表结构
以便在Athena中查询
现在你了解AWS的功能了
在这里你实际上是在创建数据库内的数据库
你正在创建表
当然还会使用Athena运行交互式查询会话
AWS Glue本质上是一个全托管的ETL服务，即提取
转换和加载数据
这是一个可以分类数据的服务
清理并丰富数据，并可靠地在不同数据存储间迁移
这就是你的
数据仓库
或者执行数据转换跟踪的服务
提取你的数据
稍后我会演示如何使用实际向导
嗯 完成所有这些操作
这不仅仅是概念
我将进行实际操作演示
所以你理解并亲眼看到它的实际运作
不仅仅是概念
而是这些概念的实际应用
AWS Glue爬虫可自动从源数据中推断数据库和表结构
将关联的元数据存储在蓝色数据目录中
当你在Athena中创建表时
只需选择使用AWS Glue爬虫创建
因此工作流程相当简洁
从S3开始
也就是你的存储桶
然后爬虫会与这些存储的数据集交互
所以它会提取
转换 并将元数据存储在全局数据目录中
接下来进入Amazon Athena部分
执行交互式查询
你根据需求准备数据
最终到达Amazon QuickSight
也就是 当然
运行可视化分析
创建可视化图表
执行分析
等等
最终到达AWS QuickSight
当然，再次快速回顾QuickSight的整体流程
你可以使用任何数据源
拥有你的数据集
对这些数据集进行分析
当然还可以创建不同仪表盘
多个仪表盘并发布
分享这些仪表盘
组织内任何人
让他们查看你的仪表盘
你创建了一个故事
只是一个快照
所有仪表盘的快照
例如
假设你对某个数据集进行分析
没错 你创建的可视化图表全部关于客户数量
将其保存为故事
接下来创建的仪表盘或图表全部关于盈利能力
同样保存为故事
构建你的故事
可以将其视为
整体
或高层级的PPT幻灯片
所以你实际上 在向他人演示时
或者你在玩这个故事
它会引导你从客户、盈利能力和销售等方面入手
这样一来 对你来说会更方便
市场部或销售部可以查看你创建的所有故事
基于你创建的数据集分析
他们更容易管理
并相应采取必要行动
所以在本课中想简要介绍AWS
A theta AWS Glue
以及 当然还有快速侧边
以及工作流中的概念
希望这对你有帮助
当然接下来是实践
多加练习
对吧 以理解这些概念
看看这些是什么
在我结束课程前
让我快速展示这些服务的位置
我将切换到AWS控制台
完美，进入控制台后
对 登录后
前往服务并看到所有相关服务
所以 例如我的存储在这里
我之前提到的S3
这是存储所有桶的云服务
如果我点击 S3
你会注意到 会显示我创建的所有存储桶列表
当然也可以创建更多桶
每个桶可存放任意内容
例如名为gb88的桶包含五千条销售记录
的.csv文件
好的
同样 返回主页
再次进入服务
现在要展示Athena
在分析部分
你会看到Athena服务
直接点击
这会带你到实际查询编辑器
记住i是交互式查询构建器
所以你的数据库在左侧
然后你可以基于数据执行查询
当然你的表格会运行查询
你可以保存查询格式
查询等等 这就是你实际使用的地方
最终进入快速秒
你在这里准备数据
你也可以直接在QuickSet中准备数据
某些字段 但这里是更强大的区域
因为现在你可以执行并创建复杂查询
根据你对数据的实际需求
我之所以展示这个
这里的重要性在于显而易见的问题
为什么对吧
为什么我们要使用它
我们可以在QuickSide完成
嗯 QuickSide
其实灵活性有限
准备好你的数据
它只能处理字段大小或简单计算
但在Athena中
假设你有数百万条记录
对如果有一千万条记录
而你只想
你知道的
获取其中一部分记录
比如说
在千万条记录中
根据特定条件查询一百万条记录
这就是Athena非常有用的地方
然后将Athena的结果导入QuickSite
此时会展示效果
完美 我们再回到服务
当然还有AWS Glue
好的 这也属于分析部分
我将点击AWS Glue
这就是你可以操作的地方
进入AWS Glue仪表盘后
可以查看所有数据库中的表
这样
这是表格区域
可以存储供Athena使用的表格
最终供QuickSite使用
这就是我们之前课程中提到的工作流程
当然需要使用爬虫来提取
转换与加载ETL
好的 所以和AWS学校同义
想想ETL 好的
你可以提取数据
你可以转换数据
也可以加载数据
简要演示这些服务在AWS中的位置
好的
我们查看了三个
查看了Athena
然后学校练习这些希望
这有帮助 如果有任何问题
请在讨论区留言
我很乐意回答 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/044_Udemy - Become an AWS Certified Data Engineer part1 p44 9. Data Storage Methods.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
随着数据量的增加，让我们谈谈数据存储方法
因此作为数据存储选项
传统存储方法如数据仓库
例如仍然非常流行且相关
但数据湖近年来更受欢迎
现在 这些新选项会让企业困惑
尤其是那些力求财务谨慎的
同时技术上相关
所以哪个更好
数据仓库还是数据湖呢
都不是，两者皆是当前答案
这些不同方案可协同使用以维护现有数据仓库
充分利用数据湖的优势
所以上一话题
我之前讲过亚马逊S3数据湖
架构和对象级存储
我之前已经讲过这个
这里 我将讨论结构化数据存储
现在 结构化数据存储通过数据库实现
比如RDS
亚马逊和数据库
我将聚焦一个特定数据库和数据仓库
现在最常见的解决方案
数据仓库作为多源分析数据的中央系统
例如
假设一家公司有多个不同数据库
因为各部门用它们追踪不同事物
现在这完全没问题
但难以全面了解各部门情况
数据仓库是多源结构化数据的中央存储库
现在这些数据会被转换
聚合并处理后再加载到数据仓库
数据仓库内的数据随后用于业务报告和分析
数据仓库是存储事务数据的数据库
以支持大型复杂查询
数据仓库几十年来一直是企业核心
从小型企业到数据集较小的企业
到拥有艾字节数据的大企业
可用数据仓库解析各类数据
在本地部署
数据仓库
需投入大量资源规划硬件成本并维护
许多小型企业无法承担
没错
然而 AWS提供数据仓储服务
此外，亚马逊红移非常重要
我也会给大家演示一下
所以你可以 呃
你知道的 几分钟内部署数据仓库
它专为存储从数百亿到千万亿级别的数据集而设计
没错，而且容量
现在我们拥有亚马逊S3来托管数据湖
以及亚马逊红移来托管数据仓库，完美
如果需要跨两个空间查询怎么办
嗯 有解决方案
现在介绍亚马逊红移光谱
这种光谱可将数据湖和数据仓库整合
仿佛是单一数据源
无需数据迁移 无需复杂查询逻辑
只需干净的全数据查询
现在可以对千万亿级数据运行单次查询
这相当惊人
好的 这确实是概念性转变
我们来谈谈云数据托管的优缺点
以及云中数据托管的优缺点
数据仓库的最大优势在于快速
集中化数据检索
现在 数据仓库提供整理好的数据集
可用于多种分析和记录需求
现在这允许你
使用及时的业务数据
有助于做出更好决策
现在 传统本地数据仓库确实存在显著缺点
主要在于成本
小企业可能没有时间和资源购买所需硬件软件
或配置规则约束以保持数据仓库内数据一致性
甚至执行ETL流程
还有持续维护和安全问题
这对小企业来说很困难
当然 传统数据仓库的可扩展性无法
应对业务负载的不可预测需求
现在 使用亚马逊红移
这种云数据仓库解决方案可解决这些问题
它是云原生且性能可达十倍
你知道的 这与本地数据仓库相当
现在我已经多次谈到过数据分析了
我需要确保你们充分建立数据与存储之间的联系
在传统的本地部署数据分析方案中
ETL操作
收集数据
例如 这些操作将数据转换为所需格式
然后按计划将其存入数据仓库
之后你将在关系型数据仓库上运行所需分析
目前这些分析过程仅限于数据仓库内的数据
如今在数据领域这已成为巨大限制
因此考虑到企业可获取的数据范围
例如
业务应用通常以csv格式存储数据
格式 xml或json文件
现在数据湖结合亚马逊
红移数据仓库
是补充数据仓库存储数据的优雅方案
现在你可以创建一个涵盖
结构化数据进入数据仓库
以及半结构化和非结构化数据存于数据湖
例如
所有数据都可见并可用于报告
仪表盘和洞察
这也是AWS生态系统的另一关键优势
是能够利用数据湖中的所有内容
从机器学习到实时分析准确预测未来结果
现在让我们回顾要点再继续
我已多次谈到在对象级别存储数据的建议
例如
推荐使用亚马逊S3存储海量数据
无论是结构化还是非结构化
现在数据湖是推荐方案
当存储大量结构化数据进行复杂分析时
建议使用亚马逊红移存储数据
当然最后推荐
这是生成分析数据集的ETL流程
在AWS生态系统中
有创建ETL操作的工具叫亚马逊EMR
亚马逊EMR是托管的Hadoop框架
回想我之前提到的快速处理海量数据的挑战
迅速生成洞察力
这就是亚马逊EMR的作用
Hadoop是一个长期存在的开源框架
在数据处理领域已十分成熟
稍等片刻
在继续讲解Hadoop之前
先明确Hadoop的定义
它不是数据库或现有数据系统的替代品
这不是单一的应用程序，而是
这是一个工具框架，帮助你同时存储和处理数据
所以现阶段
让我们专注于这个框架的存储部分
Hadoop 支持快速数据传输
这意味着可以加速复杂查询的处理时间
现在 无论你使用本地部署的 Hadoop 还是 Amazon EMR
你将使用相同的工具，有一个主要例外
Amazon EMR 使用其自有文件系统
这意味着你可以使用自己的 S3 数据湖作为数据存储
因此无需将数据复制到集群中
就像在本地部署的 Hadoop 中所做的那样
事实上 Amazon EMR 文件系统可以直接在 Amazon S3 数据湖中编目数据
同时也能兼容本地 Hadoop 文件系统
它结合了传统 Hadoop 系统的优势
与 Amazon EMR 的优点，为你提供最佳方案
数据分析的第一原则是分离存储与处理，Amazon EMR 正是如此
完美体现了这一原则
因此务必记住 AWS 支持多种数据存储方式
这为你提供了选择最适合解决方案的灵活性
以满足你的需求
我希望这对你很有帮助，非常重要
请仔细学习以理解不同数据存储领域
数据的摄入方式
以及数据在存储、仓库或 Amazon S3 中的存放方式
例如 对吧 我希望这对你有帮助
请告诉我 如果你对此有任何疑问 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/045_Udemy - Become an AWS Certified Data Engineer part1 p45 10. Data Warehouses.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
现在让我们看看数据仓库
没错 数据仓库是一个中央存储库
嗯 用于多个数据源的数据
现在这些数据已被转换
聚合 然后为业务报告和分析准备就绪
数据仓库是来自一个或多个数据源的信息中央存储库
现在数据从事务系统流入数据仓库
关系型数据库
没错和其他来源
现在这些数据源可能包括结构化
半结构化
和非结构化数据
现在 这些数据源在存储到数据仓库前会被转换为结构化数据
数据在数据仓库中按特定模式存储，这是一个关键概念
没错，模式定义了数据在表中的存储方式
列 和行
模式通过约束确保数据完整性
转换过程通常需要使源数据符合模式
在首次成功导入后
例如 将数据导入数据仓库
数据的导入和转换可以定期持续进行
现在商业分析师
没错 通过商业智能访问数据
没错BI工具
SQL客户端
例如 及其他分析应用
企业使用报告
仪表盘
和分析工具从数据中提取见解
监控业务绩效
并支持决策
现在这些报告
仪表盘和分析工具由数据仓库驱动
高效存储数据以最小化I/O
并以超快速度向数百上千用户交付查询结果
同时讨论数据仓库
没错 所以是的
工具 SQL客户端
以及数据仓库的其他子集
例如 没错
如今数据仓库规模巨大
分析这些庞大的数据存储可能充满挑战和困惑
现在 许多组织需要限制表仅保留与分析用户最相关的部分
目前
数据仓库的子集称为数据集市
现在数据集市仅关注单一主题或功能领域
现在数据仓库可能包含企业所有相关数据源
但数据集市可能仅存储单个部门的数据源
因为数据集市通常是数据仓库中已有数据的副本
它们通常 你知道的
现在快速且易于实施
亚马逊Redshift允许你这样做
让我们谈谈亚马逊Redshift的实际应用
因为亚马逊Redshift通过提供基于云的
可扩展安全的数据仓库环境克服了所有这些缺点
因此Redshift易于设置
部署 然后管理
并提供比其他数据仓库解决方案快10倍的性能
大规模数据存储
现在 我之前已经讨论了许多数据存储建议
当存储单个对象或文件时
强烈推荐
存储在S3
当存储海量数据时
无论是半结构化还是非结构化数据
在S3上构建数据湖会很理想
当存储海量结构化数据时
适合复杂分析
现在请记住将数据存储在亚马逊Redshift
从存储到处理，所有AWS处理服务均适用
我将在后续内容中讲解
在下一课 临时存储层，用于处理和分析期间的数据
现在这些数据最终会被转移到永久存储
在之前课程中我们已讨论过的其他解决方案之一
许多人认为处理海量数据
快速流动的数据
首先想到的是Hadoop
现在非常流行
AWS的Hadoop框架通过Amazon EMR和AWS Glue实现
这些服务通过Hadoop框架实现数据摄入
转换 分析并迁移结果至分析数据存储
所以Apache Hadoop采用分布式处理架构
其中任务被映射到集群的商用服务器进行处理
现在 分配到集群服务器的每个任务均可运行
或在任意服务器上重新运行
集群服务器频繁使用Hadoop分布式文件系统
HDFS存储本地数据进行处理
现在这些服务器计算结果会被汇总为单一输出集
现在指定一个节点作为主节点
负责任务分配并可自动处理故障
例如对吧
为什么要使用Apache Hadoop呢
Hadoop促进数据导航、发现和一次性数据分析
您可以应对意外情况
例如 通过快速分析大量数据形成响应
不同于传统数据库系统
Hadoop可处理结构化
半结构化
或非结构化数据
现在几乎涵盖所有现有数据格式
除了原生支持多种数据类型
如XML
CSV
文本 日志文件
对象等
SQL JSON
等等 或二进制
甚至可用Hadoop转换数据以更好地整合现有数据集
还可存储无模式或有模式数据并执行大规模ETL操作
由于Hadoop是开源的
多个生态系统项目可帮助分析
以及Hadoop处理或分析的多种数据类型
现在 这些项目为开发数据分析解决方案提供巨大灵活性
Hadoop的编程框架
例如 如Hive和Pig
可支持应用的任何数据分析场景
由于Hadoop的分布式架构
Hadoop集群可以低成本处理海量数据
添加更多数据和处理能力只需扩展集群服务器
在AWS上使用Hadoop托管服务非常理想
因为Amazon EMR是AWS提供的Hadoop框架服务
该服务可从几乎任何数据源以接近任意速度摄入数据
且Amazon EMR支持两种不同文件系统
HDFS，也就是Hadoop还是弹性MapReduce文件系统
现在文件系统是一套管理文件或数据存储的组织规则
HDFS是Hadoop用于快速处理海量数据的系统
处理系统需要一种方式将读写文件的负载分布到
呃 不同高性能服务器上
现在HDFS是分布式存储
允许文件在服务器集群中并行读写
这大幅缩短了每个操作的总时长
了解HDFS集群的内部运作也很有帮助
顺便说一句 现在HDFS集群主要由管理文件系统元数据的名称节点组成
当然还有其他节点存储实际数据
现在HDFS架构组件对吧
这样你就能更好地理解
现在 亚马逊EMR是如我之前提到的AWS服务
实现了Hadoop框架
现在亚马逊EMR
你知道它首先从一个或多个数据源摄入数据
然后使用HDFS存储这些数据
如果使用HDFS
文件系统以弹性块存储卷形式保存
EBS 现在这种存储卷基本上意味着它是
其临时性特点 当数据被复制到HDFS卷后
接下来进行数据转换和分析
结果随后发送到分析数据存储
如Amazon S3数据湖或Amazon Redshift数据仓库
因此EMR提供了HDFS的替代方案
EMR文件系统
基本上S3对吧
能确保HDFS数据的持久可信来源
这些数据存储在Amazon S3中，当实施EMR文件系统时
无需在转换前将数据复制到集群
这非常实用
对吧，这功能很强大
所以EMR可以基本目录化数据
对的，节省的时间能大幅提升集群性能
存储是任何数据分析方案的基础
必须了解你的选项
这样才能决定哪种方案最适合你的组织
我已经讲完了对象存储
以及创建数据湖整合多源数据的优势
例如 将多种数据形式整合到单一位置
我们也讨论了结构化
分析存储的优势与劣势，使用数据仓库
当然，我还详细阐述了如何创建ETL操作时需谨慎考虑的重要性
我也谈到了Apache Hadoop
它的功能能力 其优势所在
以及它如何与Amazon EMR集成
包括文件系统本身
希望这对你有帮助
请随时告诉我 如果你对此有任何疑问 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/046_Udemy - Become an AWS Certified Data Engineer part1 p46 11. Amazon RDS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好 欢迎来到关于亚马逊云科技关系型数据库服务的课程
也就是我们常说的RDS服务
RDS让在云端搭建
运维 扩展关系型数据库变得非常简单
提供高性价比且可弹性扩展的容量
同时自动化处理耗时的管理任务如硬件配置
数据库初始化 补丁更新
甚至备份
让您能专注于应用程序开发
因此可以为应用提供快速性能
高可用性
安全性和兼容性
从而为内部员工或客户提供更优质的服务
RDS提供多种数据库实例类型
针对优化内存需求
追求高性能场景
或 O
并提供六种熟悉的数据库引擎供选择
例如亚马逊Aurora
PostgreSQL
MySQL
MariaDB
Oracle数据库
以及微软SQL Server
亚马逊还提供数据库迁移服务
可轻松迁移或复制现有数据库到RDS
使您能非常便捷地开始使用RDS
以简单易用的方式
所以 接下来 让我们看看RDS的优势和特点
首先
管理极其简便
从项目构思到部署都非常便捷
可通过RDS管理控制台
或在演示中使用的命令行界面
几分钟内即可访问生产级关系型数据库功能
无需进行基础设施配置
也无需安装维护复杂的数据库软件
像大多数AWS服务一样
RDS具备极强扩展性并利用AWS全球基础设施
同时具备高可用性和持久性
因为运行在同一基础设施上
性能极速且支持高负载数据库应用
可选择两种基于SSD的存储方案
一种针对高性能OLTP应用优化，另一种适合成本效益通用场景
此外 当我们更仔细地观察亚马逊Aurora时
它以十分之一的成本提供了与商业数据库相当的性能
RDS还使您轻松控制对数据库的网络访问
RDS允许您在虚拟私有云中运行数据库实例
这使您能够隔离数据库实例
并通过行业标准加密IP连接现有IT基础设施
安全加密IP
秒
引擎类型提供静态数据和传输中数据的加密
最后 与其他选项相比成本极其低廉，费用非常低
仅按实际消耗的资源计费
此外 您可以享受AWS提供的按需定价
无需预先或长期承诺
甚至可以选择预留实例以获得更低的每小时费率
所以让我们继续 更详细地查看亚马逊提供的六种不同实例
PostgreSQL已真正成为首选方案
成为众多企业开发者和初创公司的首选开源关系型数据库
现在RDS使设置
操作和扩展云中的PostgreSQL部署变得简单
通过RDS 可在几分钟内部署可扩展、成本高效且可调整的硬件容量
自动管理复杂的行政任务，如软件安装和升级
存储管理、高可用性复制
以及读取吞吐量和灾难恢复备份
现在 RDS for PostgreSQL
为您提供熟悉的PostgreSQL数据库引擎功能
这意味着您今天使用的代码、应用程序和工具
可与亚马逊RDS兼容
只需管理控制台几下点击
即可部署PostgreSQL数据库
自动配置数据库参数以实现最佳性能
一旦部署完成
可立即扩展至16TB存储和4万IOPS
对于PostgreSQL
还允许通过单个数据库部署扩展读取能力
处理高负载数据库工作
MySQL是全球最流行的开源关系型数据库
而RDS使设置
操作 和扩展云中的MySQL部署变得简单
让您专注于应用程序开发
通过管理耗时的数据库行政任务（如PostgreSQL所示）
支持MySQL社区版
5.5版本
5.6
5.0 7
以及8.0
这意味着您今天使用的代码应用和工具都可以与RDS兼容
还附带亚马逊云服务提供的标准备份和恢复功能
同时支持高可用性和只读副本
这使得弹性横向扩展变得非常容易
突破单个数据库实例的容量限制
此外 RDS为您的数据库实例提供免费的Amazon CloudWatch监控指标
而RDS增强型监控可访问超过五十项CPU、内存、文件系统和磁盘I/O指标
O 最后的指标
正如在PostgreSQL中一样
通过使用Amazon VPC实现隔离与安全
或通过Amazon密钥管理服务
现在MariaDB并不像PostgreSQL或MySQL那样流行
但它实际上是由My Escl的原始开发者创建的
并且具备其他数据库引擎在RDS中提供的大部分优势
这些都包含在RDS中
我想重点强调的是高性能
因此您可以再次配置高达16TB的存储
每个数据库支持40000 IOPS
并选择配备最多32核CPU和240
4GB内存
因此部署MariaDB非常便捷
如果您在本地使用MariaDB并将其部署在AWS基础设施上
这不仅允许横向扩展
还能将部分管理任务交给亚马逊云服务
例如容错或备份
现在让我们登录管理控制台
并看看在 配置和部署这些数据库实例时的不同选项
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/047_Udemy - Become an AWS Certified Data Engineer part1 p47 12. Amazon DynamoDB.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好 欢迎来到这节课
在这节课中我们将了解亚马逊AWS提供的DynamoDB数据库
DynamoDB是一个完全托管的
NoSQL数据库服务，提供快速且可预测的性能并具备无缝扩展能力
DynamoDB可以让您卸载操作
和扩展分布式数据库的管理负担
因此您无需担心硬件配置
设置与配置、复制
软件补丁或集群扩展
同时提供静态数据加密
这消除了保护敏感数据时的可选负担和复杂性，现在使用DynamoDB
您可以创建可存储和检索任意量数据的数据库表
并处理任意级别的请求流量
您可以随时扩展或缩减表的吞吐量容量，无需停机或性能下降
并通过AWS管理控制台监控资源使用情况和性能指标
提供按需备份功能
还允许创建表的完整备份
用于长期保留和归档以满足监管合规需求
您也可以创建按需备份
为Amazon DynamoDB表启用时间点恢复
可保护Amazon DynamoDB表免受意外删除或操作
您可以随时将表恢复到任意时间点
仅作参考
默认存储可达35天
还允许自动删除表中过期项
帮助减少存储使用并降低存储无关数据的成本
现在介绍DynamoDB的核心概念
首先是表
项目和属性
表与其他数据库系统类似，DynamoDB存储
DynamoDB将数据存储在表中
即数据的集合
每个表包含零个或多个项目
由唯一标识与其他项目的属性组构成
最后是属性
每个项目由一个或多个属性组成
属性是基本数据单元
无需进一步拆分
我将创建一个表
除了表名
还需指定表的主键
主键唯一标识表中的每个项目
确保两个项目不会有相同键
支持分区键和分区排序键
然后是二级索引
可在表上创建一个或多个
允许使用替代键查询表数据
除了基于主键的查询
现在DynamoDB不再要求必须使用索引
但在查询数据时为应用程序提供了更多灵活性
最后还有DynamoDB流
这是一个可选功能，用于捕获DynamoDB表的数据修改事件
这些事件的数据会近乎实时地出现在流中
而这些事件发生的时间
DynamoDB还支持最终一致性或强一致性读取
最终一致性意味着在一致性方面会有轻微延迟
而强一致性则表示数据在毫秒级内完成复制
这张表很好地展示了关系型数据库管理系统
或者AWS支持的RDS
和亚马逊
DynamoDB请记住它是无模式的
并且使用键值存储
您还可以指定所需的吞吐量容量
作为完全托管资源，DynamoDB会处理其余工作
DynamoDB在后端也实现了完全同步
在您创建的区域内所有可用区之间的数据
表还与其他AWS服务无缝集成
例如弹性MapReduce（EMR）
并可轻松将数据迁移到弹性MapReduce的Hadoop集群
现在一些流行用例包括物联网
游戏和移动应用
再次 大家 可以清楚看到AWS提供的两种主要数据库的区别
现在让我们登录管理控制台，尝试创建数据库实例
并尝试通过我们的e C Two实例访问
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/048_Udemy - Become an AWS Certified Data Engineer part1 p48 13. AWS DynamoDB Hands-on Lab.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好 现在我们已经对亚马逊DynamoDB有了初步了解
它的速度和性能如何
接下来我将带大家完成一个小教程
我们将创建一些简单表格
进行数据扫描、查询扫描和数据查询
最后再进行清理
使用DynamoDB清理数据
再说一次 这是一个支持文档和键值存储模型的NoSQL数据库
非常灵活
可靠，而且最棒的是
可以根据需求自动扩展和收缩
它在物联网中应用广泛
呃 游戏、网页、移动应用
例如
每当亚马逊黑色星期五到来时
他们主要使用DynamoDB处理订单
数百万请求均由DynamoDB处理
正如大家所见
我已经登录AWS管理控制台
并进入亚马逊DynamoDB管理面板
首先要做的是
我将创建第一个表
在表名中给它一个简单名称
分区键用于跨分区分布数据以实现扩展
选择具有广泛值范围的属性很重要
并且访问模式分布均匀
由于我将使用名为音乐的表
使用艺术家作为分区键
这完全取决于你们的业务类型
以及数据库的主要用途
为了这个示例
因此每位艺术家可以写多首歌曲
这样我们可以启用排序键进行轻松排序
接下来点击
添加排序键
在此输入歌曲标题
这里可以看到键可以是字符串
二进制或数字
由于这些将是艺术家名称
这些将是字符串
保持字符串类型
DynamoDB还可根据需求自动扩展
根据请求量调整表的读写容量
通过IAM角色
名为DynamoDB自动扩展
将管理自动扩展过程
现在系统会为此创建角色
在该账户中首次启用自动扩展时，我们将执行
即 我们将指示DynamoDB创建该角色
通过清除默认设置
勾选此复选框，我们将继续清除
正如大家可以看到的
一旦移除默认设置
我们可以更改和调整的选项有很多
例如 我们可以添加二级索引
我们可以调整正确容量
所以 例如 预留容量符合免费层级资格
我们可以选择按需计费
您可以使用预留容量
例如 每单位读取容量
五张表的读写容量
系统会给出预估成本
如果启用自动扩展
您将设定目标利用率
正确容量配置
这些都是默认设置
接下来我们将
保持所有默认设置不变
这里将创建自动扩展服务链接角色
我们也可以使用现有角色
如果已有角色存在
在此输入角色名称
或如果没有则可以
允许DynamoDB为我们创建角色
静态数据加密
它还支持静态数据加密
我们可以选择默认选项
即使用AWS托管密钥的服务器端加密
或使用密钥管理服务
即使用客户主密钥
现在我们点击创建
需要几分钟
从几秒到几分钟完成部署
现在我们看到已创建音乐表
这些都是音乐表的默认设置
我们刚刚几秒前创建的
这就是创建表的简便性
现在我们将向表中添加数据
我们选择上方的项目标签
这是该表的所有功能标签
我们将创建一个艺术家字符串项
因为我知道全球将有学员学习此课程
因此我们将保持中立
在该条目创建之后
我们现在点击保存
我们再添加几个以便让内容更全面
这样就可以了 所以我基本上创建了四个不同条目
两个由一位艺术家创作，两个由其他艺术家创作
现在我们的表格已创建完成
我们的条目也已创建
让我们看看如何查询这个表格
这里有一个下拉列表
这里有几个选项
我们可以选择扫描或查询
我们现在进行查询操作
现在你可以通过控制台以多种方式查询表格
对于第一个查询，我们在艺术家框中
我们输入特定名称并开始搜索，搞定
大家可以看到这两个选项出现在底部
或者我们可以删除这个
我们可以尝试搜索另一个
我们可以升序排列
我们可以降序排列
我们有很多不同的查询选项
基于艺术家
基于标题再次
这是分区键
这是排序键
我们还可以添加属性过滤器
我们可以降序
我们可以添加属性
我们取消所有更改
在下拉列表中我们可以选择不同表格
但因为我们只有一个表格
它只列出一个表格
我们有很多不同的查询选项
再次说明
大多数时候应用会连接到这个Dynamo
数据库表 进行各种条目查询
由于这是NoSQL表格
NoSQL数据库
速度极快，再次强调
正如我说的 这是亚马逊在黑色星期五销售时使用的主数据库
这就是这个表格
这是他们处理所有商品的数据库
顶部的这些标签
带你浏览各种功能
例如 你可以创建备份并优先创建备份
恢复删除的备份
你可以自动化备份
或者如果你想的话
或者你可以恢复它
只需记住一件事
它会持续备份表的最后35天数据
如果你想更改这个设置
你需要联系AWS
他们可以帮你修改
但默认最多保留35天
这就是DynamoDB的基本概述
如果你未来继续练习
必须确保删除NoSQL表
否则会开始产生费用
所以当你关闭时
当你返回主控制台时
我们将点击我们的表
然后删除这个表
可选地 如果你在生产环境中
可以创建备份
比如你要删除一个表
要确保未来有备份
删除前先创建备份
但这里用于测试
我们直接进行
表已删除
现在我们回到
DynamoDB中没有表或数据
这就是开始使用DynamoDB的简易性
例如 你的组织需要具备
需要使用NoSQL数据库并希望
非常适合移动和网页应用
游戏和广告技术
甚至物联网
再次
确保根据需求弹性扩展
基于你的需求 这将是最佳选择
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/049_Udemy - Become an AWS Certified Data Engineer part1 p49 14. Amazon SageMaker.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好 欢迎来到这节关于深入探讨亚马逊SageMaker的课程
什么是亚马逊的SageMaker
SageMaker本质上是一个全托管的机器学习服务
现在数据科学家和开发者可以快速轻松地构建和训练机器学习模型
然后通过SageMaker直接部署到生产就绪的托管环境
它提供了一个集成的Jupyter
作者笔记本实例，方便访问数据源进行探索和分析
因此您无需管理任何服务器
它还提供常见的机器学习算法
这些算法针对分布式环境中超大规模数据进行了优化
还支持自带算法和框架
我们将在演示环节详细讲解
还提供灵活的分布式训练选项以适配您的工作流程
将模型部署到安全可扩展的环境
通过控制台一键启动
我们稍后也会详细演示
在课程中
需要说明的是，这是AWS提供的全托管服务
可实现从A到Z的ML模型部署
直接进入生产环境
让我们开始
让我们先了解SageMaker环境能实现的功能
首先能够收集和准备训练数据
帮助快速构建和管理高精度训练数据集
还提供对公有和私有标注服务的便捷访问
并提供预建工作流和界面处理常见标注任务
请注意该服务会产生额外费用
因为这是实际为您进行标注的人力服务，例如
如果您没有内部人员
还可以选择和优化您的机器学习算法
系统会自动配置和优化
例如 TensorFlow
Apache MXNet
PyTorch
Chainer Spark ML
以及更多常用ML框架均内置并针对规模优化
速度和准确性
市场还提供超过100个预训练模型和算法
当我们深入讲解机器学习时
将查看内置算法的具体内容
同时快速了解自定义算法配置
此外 可设置和管理训练环境
最棒的是
支持一键训练
只需单击即可开始训练模型
它能够处理所有底层基础设施，以扩展至
例如 处理 Petabyte 级数据集
您还可以调整和优化您的模型
自动模型调优利用机器学习快速调整模型以达到最高准确度
这一功能
让您跳过手动调整模型参数的繁琐试错过程
相反，在多次训练运行中
自动模型调优
通过发现数据中的有趣特征进行超参数优化
并学习这些特征如何相互作用以影响准确率
因此通过这样做
您实际上可以节省数天甚至数周
根据数据规模最大化训练模型的质量
最后
您能够在生产环境中部署和管理模型，再次
正如一切事物一样 模型训练完成后提供一键式部署到生产环境
并且能够处理实时或批量数据
模型运行在 SageMaker 的自动扩展集群上
由于这是一个跨多个可用区的托管服务
并提供高性能和高可用性
SageMaker 还内置了 A/B 测试功能
这有助于测试模型并尝试不同版本以获得最佳结果
因为在机器学习中
这是一个迭代过程
因此是试错过程，尝试一种方法
如果不起作用 则继续尝试另一种
整个过程通过 SageMaker 简化
因为它为您自动化了大量流程
因此您可以专注于分析数据和处理数据
而非纠结于哪种模型效果最佳
这些是 SageMaker 提供的一些基本功能
接下来我们将逐一深入探讨 在课程后续内容中
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/050_Udemy - Become an AWS Certified Data Engineer part1 p50 1. The AWS Machine Learning Service.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你好 大家好 欢迎来到由AWS提供的关于机器学习课程
AWS拥有广泛且深入的机器学习服务
能够满足各种规模企业的几乎所有需求
因此您可以选择多种不同的服务
例如预训练的计算机视觉AI服务
语言推荐功能
预测分析
此外还有亚马逊SageMaker，可用于构建
训练和部署机器学习模型
我们将在后续章节更深入探讨SageMaker
但为了让大家有个初步了解
SageMaker是亚马逊提供的主要机器学习服务之一
并且高度可定制，您可以自行构建
训练和部署专属的机器学习模型
基于您自己的算法
或使用AWS提供的内置算法
再次强调 正如我之前提到的
本课程中有一个专门章节讲解亚马逊SageMaker
我们将具体讲解如何构建机器学习模型
使用不同算法进行训练
然后将其部署到亚马逊云服务
让大家了解AWS提供的其他服务概览
这些AI服务
可以轻松集成到您的应用中以解决常见问题
例如需要个性化推荐
或旨在提升安全与防护
或希望增强客户互动并获取更多分析数据
以优化营销活动或改进运营效率
这些服务能让您轻松实现目标，且操作简便
其中一项服务名为推荐系统，为客户提供个性化体验
采用亚马逊官网使用的推荐技术
再次说明 相信大家对亚马逊官网的强大功能都很熟悉
以及它如何为您个性化几乎所有内容
您可以使用相同的个性化技术和服务
当访问其官网时，将其整合到自身组织中
无论是电商平台
还是自有企业官网
均可使用亚马逊
官网的技术并尝试在自身组织中小规模应用
还可基于机器学习技术构建精准预测模型
例如 预测下一季度或下一年的销售数据
或预测即将访问
您的网站或电商平台的用户数量
即将上线的 或正在建设的
你可以使用这些机器学习模型进行预测
甚至 例如
需要订购多少库存
这样可以最小化你们的库存
或者仓库中保留的库存
因此它们有多种应用场景可用于预测
还有图像和视频分析
你可以将其添加到应用中以分类资产
自动化 媒体 工作流程
提取含义 还有更多额外功能
你可以使用自然语言处理从非结构化文本中提取见解和关系
你还可以进行文档分析
例如
可以在几小时内从数百万份文档中提取文本和数据
大幅减少我们手动处理文档的工作量
在尝试分析不同文档时
无论我们目前处于哪个行业
还有其他多种用途
例如 还有语音 可以将文本转换为生动的语音
你可以使用聊天机器人
即称为对话代理
你可以使用翻译服务
还有加密服务
这可以轻松添加高质量的语音转文本功能
到你的应用和工作流程中
AWS提供的机器学习服务涵盖了多种不同的人工智能服务
所以 根据我们在人工智能框架中的目标
或在机器学习框架中
我们可以选择实施该框架内的各种服务
你们可以在应用中实现这些服务
在业务流程中
在这些人工智能服务之外
还提供了其他多项服务形成完整解决方案
例如 你可以为任何场景获取合适的计算资源
可以利用广泛强大的e
C 两种实例 从用于计算密集型深度学习的GPU
到FPGA
作为专用硬件加速
到用于运行推理的内存实例
因此这些e C 两个实例
这些基本上是你的虚拟机或虚拟机
提供一系列硬件选项供您选择
最棒的是您无需永久配置它们
您可以使用 例如
一种按需服务
它们仅在需要时进行配置
如果在非高峰时段
您可以缩减实例规模
在高峰时段
您可以扩展实例规模
这是一种非常高效的云端基础设施管理方式
同时还提供机器学习所需的分析与安全功能
要成功进行机器学习
您不仅需要相关能力
还需要适当的安全措施
需要合适的数据库
还需协同工作的分析服务
以确保机器学习模型高效运行
借助AWS的S3存储服务
通过AWS分析和安全服务
所有功能都整合在一个解决方案中
此外AI框架内还有一些实用学习工具
可通过AWS DeepRacer深入学习机器学习
即屏幕上看到的那辆赛车
这是一辆全自动驾驶的1/18比例赛车
专为帮助学习自动驾驶的强化学习设计
我们稍后会更详细讲解强化学习
这是机器学习领域最前沿的方向之一
尤其适用于自动驾驶领域
您可以体验
例如 真实世界赛车的感受
当将自定义强化学习模型部署到AWS DeepRacer时
还有DeepLens
即 嗯
即屏幕右下角的摄像头
这是全球首款深度学习摄像头
主要面向开发者设计
与SageMaker等AWS服务深度集成
可快速入门深度学习
通常设置时
可在十分钟内完成
最棒的是完全集成AWS生态
可通过AWS控制台实时查看结果
这主要是为了让大家初步了解
AWS在机器学习及相关能力方面的提供
所以它基本上是一个一站式平台，可以完成所有事情
这与人工智能或机器学习相关
对我来说
最棒的部分在于
所有功能都在一个控制台的一个区域中
因此你无需前往多个不同平台或程序进行机器学习 所有操作都可以在此AWS框架内完成
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/051_Udemy - Become an AWS Certified Data Engineer part1 p51 2. First steps in building a ML Model.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到这节课
我正在探讨如何构建机器学习应用
构建应用或机器学习应用是一个迭代过程，包含一系列步骤
因此构建机器学习应用
通常需要遵循五个不同步骤
例如 第一步是明确问题
然后收集并标注数据
分析数据、特征处理和数据分割
那么我所说的明确问题具体指什么
机器学习的第一步是确定要预测的内容
这被称为标签或目标答案
想象这样一个场景，你想生产产品
但每个产品的生产决策取决于其潜在销量
在这种情况下，你需要预测每个产品的购买次数
即预测销售数量
可以通过多种方式用机器学习定义这个问题
选择如何定义问题取决于具体用例或业务需求
例如 你想预测每个产品的购买次数吗
在这种情况下，目标是数值型
并解决回归问题
或者你想预测哪些产品销量超过十次
此时目标是二元值
并解决二分类问题
避免过度复杂化问题非常重要
要构建最简单的解决方案以满足需求
同时也要避免丢失信息
尤其是历史数据中的信息
例如
将实际历史销量转换为超过十次或更少的二元值会丢失重要信息
必须投入时间确定
哪种目标最符合预测需求
这能避免构建无法回答问题的模型
这就是明确问题
的过程 这将为确保所建模型
及预测内容真正满足需求和业务目标奠定基础
如果处理错误
后续流程都会崩溃
因此必须投入足够时间和资源确保
我们做对了
完成问题定义后需处理数据
机器学习问题当然始于数据
最好是有大量已知目标答案的数据
这里指的主要是历史数据
例如我提到的案例
需要获取所有历史销售数据
数据部分
已知目标答案的数据称为标注数据
而在监督式机器学习中
算法通过标签数据自我学习
我们提供的示例
每个观测样本
你的数据中的每个示例必须包含两个要素
目标和特征，目标是你想要预测的答案
例如 销售数量是特征，即示例的属性
可用于识别模式
以预测目标
因此 例如在邮件分类中
目标是一个标签，表示邮件是否为垃圾邮件
垃圾邮件的变量包括发件人
邮件正文中的文本
主题行中的文本
邮件发送时间
等等 因此 所有这些变量将帮助机器学习模型判断邮件是否为垃圾邮件
如今数据往往不会以带标签的形式直接可用
因此收集和准备变量
以及目标通常是解决机器学习问题最关键的步骤
示例数据应能代表模型实际使用的数据
即模型预测时所处理的数据
例如 如果你想预测邮件是否为垃圾邮件
必须收集正例
即已知的 你
应同时收集垃圾邮件和非垃圾邮件
这样机器学习可以区分什么是垃圾邮件
并完成数据标注后
可能需要将其转换为
算法或软件可接受的格式
例如 亚马逊机器学习需要将数据转换为CSV格式
每个示例对应CSV文件的一行
这特指亚马逊机器学习
若使用其他软件
需确保数据符合该程序的要求
或该软件
在将标签数据输入算法前
检查数据以发现问题
并深入了解所用数据
模型的预测能力仅取决于输入数据的质量
因此必须分析数据并注意以下几点
了解变量取值及数据中主导变量
可由领域专家运行这些摘要
针对你要解决的问题
所以先问问自己 或者询问相关专家
数据是否符合预期
是否显示出数据收集问题等
因此必须请专家审核
确保数据逻辑合理
同时了解各变量与目标类别的相关性也很有帮助
因为高相关性意味着变量与目标类别存在关联
通常应包含高相关性的变量
因为它们预测能力更强
需排除低相关性的变量
现在我们有了标注数据
已分析确认其正确性
并确保相关性恰当
这时可能需要进一步转换变量使其更易理解
这被称为特征处理
例如
假设有一个记录事件发生日期和时间的变量
这个日期时间不会再重复出现
因此无法用于预测目标
然而
若将该变量转换为表示一天中的小时数
一周中的星期几
当前月份
这些变量可能有助于判断事件是否在特定时段发生
特定星期几
或特定月份
此类特征处理可生成更具泛化性的数据点
能显著提升预测模型效果
当然 并非总能预先知道哪些特征有预测影响
因此应尽可能多包含潜在特征
且可能与目标标签相关
并让模型训练过程
筛选出最强相关特征
你的任务应尽可能多包含特征
让机器学习发挥作用
让它从所有数据中学习
因为正如我之前提到的
要确保数据量尽可能多
尽可能多地输入机器学习模型
但这不意味着输入无结构
或未分析、无关的数据
我们必须确保这些步骤完成
但在完成所有步骤后
必须确保 并确保尽可能多的数据输入机器学习模型
以便模型能学习并做出正确预测
最后一步是分割数据
机器学习的核心目标是超越训练数据实例进行泛化
我们希望评估模型以估计其模式的质量
数据和模型的泛化能力尚未经过训练
然而 因为未来实例的目标值未知
我们无法真正检查对未来实例的预测准确性
因为我们没有水晶球
我们需要使用已知答案的数据
作为未来数据的代理
用相同的训练数据评估模型并没什么用
因为它奖励能记住训练数据的模型而非泛化能力
因此常用策略是将所有可用标签数据
拆分为训练集和评估子集
通常以70-80%的比例用于训练
20-30%用于评估
机器学习系统使用训练数据训练模型
以发现规律
并用评估数据检验训练模型的预测质量
系统通过对比预测结果
与评估数据集的真实值（即地面实况）
使用多种指标进行评估
通常选择评估子集表现最佳的模型
对未来实例进行预测
现在 亚马逊机器学习会将通过控制台发送的训练数据
默认拆分为70%训练集和30%评估集
现在还可选择随机选取70%源数据用于训练
而非使用前70%并用剩余随机子集作为评估
并可通过亚马逊机器学习提供的API
指定自定义拆分比例及外部分割的训练评估数据
如果您已在外部完成
并希望导入这些数据
亚马逊机器学习支持此功能
如果您不希望使用该自动化系统
因此我们需要确保遵循以下五个主要步骤
以构建能够准确预测未来销售的机器学习模型
或预测未来销售
或有效识别示例中的垃圾邮件
如果我们不严格遵循这些步骤
可能导致错误预测问题
如果在生产环境中应用
可能对您和组织造成严重后果
这些是需要遵循的简单步骤
但必须详细执行这些步骤 以确保机器学习模型更可靠运行
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/052_Udemy - Become an AWS Certified Data Engineer part1 p52 3. Understanding AWS Datasources.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好 欢迎来到这堂关于深入了解数据源的课程
现在我们将介绍在亚马逊机器学习中将要使用的数据源
数据源对象包含输入数据的元数据
亚马逊机器学习会读取您的输入数据
计算其属性的描述性统计
并存储这些统计信息
这被称为数据源对象的模式和其他信息
在您使用
或创建数据源之后
您可以使用机器学习控制台提供的数据洞察
我们将通过这些来探索输入数据的统计特性
并使用数据源训练机器学习模型
输入数据就像
我提到的用于创建数据源的数据
您必须将输入数据保存为AWS的CSV格式
不同程序和软件有不同的要求
但对于亚马逊机器学习
必须是CSV文件
CSV文件中的每一列代表一个观测属性
例如 在大家看到的图表中
这是四个观测的CSV文件快照
每个观测占一行
每个观测包含八个属性，用逗号分隔
这些属性代表每个观测体的个体信息
例如 以客户ID开头
接着是职位ID
教育程度
是否基础教育 是否高中学历
住房情况
贷款活动
持续时间
以及是否响应活动，用0或1分类
0表示不响应 1表示
是
亚马逊机器学习要求每个属性有名称
您可以通过以下方式指定属性名称
在CSV文件的第一行包含
即CSV文件的标题行
或在单独的模式文件中指定
该文件应位于
与输入数据相同的S3存储桶中
包含输入数据的CSV文件需满足特定要求 例如
必须为纯文本
由观测组成，每行仅一个观测
它们需要用逗号分隔
如果属性值包含逗号
整个属性值必须用双引号包裹
每个观测值必须以飞行线结束符终止
属性值不能包含换行符
即使值被双引号包裹
每个观测值必须具有相同数量的属性
每个观测值长度不得超过一百千字节
这些是CSV文件必须满足的要求
你将作为数据源使用的文件
正如我之前提到的 你可以将输入以单个文件或多个文件形式提供给机器学习
这些文件还需满足其他条件
例如 所有文件必须具有相同的数据模式
且需位于同一S3存储桶及路径
例如 你们看到我给出的三个路径
如果输入数据文件名为
呃 输入一
二和三
而你的S3存储桶是
例如 示例存储桶
路径应如屏幕所示
最后，创建CSV文件时
每个观测值如前所述需以特殊结束符终止
此字符不可见
但会自动添加在每个观测值末尾
当你按下 回车或换行键
特殊结束符因操作系统而异
例如 Linux使用换行符（/n表示）
Windows使用回车和换行符（/r/n表示）
保存文件时
需确保指定正确的文件格式
根据使用的操作系统
特别是使用Mac OS和微软
在Mac OS上使用Excel创建CSV文件
必须确保保存为Windows格式
以正确适配亚马逊机器学习
因此
确保数据格式正确非常重要 在将数据输入机器学习模型前
否则若数据格式有误
尤其是处理大量数据时
后期修正会非常麻烦
务必确保数据格式正确
因此我们需要确保明确了解需求
然后开始遵循这些要求
在准备数据供机器学习模型摄入时
机器学习的核心目标之一是能够对未来数据实例做出准确预测
在使用机器学习模型进行预测前
我们需要评估模型的预测性能
以确认其实际运行正常
并用未见过的数据评估机器学习模型预测质量
我们可以预留或称为
将数据集的一部分划分为已知答案的数据
并将其作为未来数据的代理
从而评估机器学习模型的实际预测能力
在亚马逊机器学习中
我们有三种数据分割选项
可以预先分割数据
即 将数据分为两个输入位置
在上传到S3存储桶前创建两个独立数据源
可以使用顺序分割
或 即指示亚马逊机器学习按顺序分割数据
在创建训练和评估数据源时
或随机分割
系统会 随机进行分割
默认情况下按您的需求处理
亚马逊机器学习默认按7:3比例分割数据
即70%用于训练，30%用于评估
现在 简单分割训练和评估数据的方法
是选择数据的非重叠子集
同时保持记录顺序
现在 这种方法适用于评估特定日期的数据模型
或特定时间范围内的数据
例如 假设您有过去五个月的客户互动数据
并希望用历史数据预测下月客户互动
现在 使用时间范围起始部分进行训练
并用时间范围末尾数据进行评估
可能更准确反映模型质量
相比从完整数据范围随机抽样
图中展示的两种情况说明
何时应使用顺序分割策略
何时应使用随机分割策略
创建数据源时
可选择顺序分割数据源
正如我之前提到的
亚马逊会将前70%用于训练
剩余30%的数据将用于评估
因此我们必须清楚自己正在处理哪些数据
我们需要明确想要做出哪些预测
这将帮助我们定义
如何正确进行顺序分割或随机分割
在到达这一步之前我们必须充分理解数据
这将帮助我们正确分割数据
因为如果分割错误
机器学习模型
无论后端多么优秀
都会预测出错误答案
因为我们导入的数据
以及分割方式存在错误
最后是数据架构
我们有两种数据架构选项
我之前简要提到过
在此之前 可以让AWS自动推断每个属性的数据类型
在输入数据文件中
并自动生成架构
或者提供一个点架构文件
在上传数据到Amazon S3存储桶时
必须确保正确上传至同一存储桶
两种方式任选其一，取决于业务需求
组织的机器学习能力水平将决定是否提供自定义架构文件
或让AWS自动推断
对于大多数中小型企业
最好由AWS自动推断数据类型
如果你们在数据科学领域经验丰富且数据结构规范
可以自行提供架构文件并请注意
架构文件
是描述数据整体逻辑结构的骨架
它实际上是在告诉机器学习模型
数据是什么以及如何组织
你也可以选择让AWS处理
这是一种自动化方法
简单便捷的方式
或者如果需要更高级的机器学习功能
则可上传自定义架构文件
但请务必上传
或确保与原始数据源存储在同一S3存储桶
希望你们对数据结构有了清晰理解
以及AWS机器学习如何利用这些结构
确保数据结构正确的重要性
从CSV文件的角度
并遵循相关要求
AWS所规定的
关于CSV文件的标准
应存放的位置 以及其他细节
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/053_Udemy - Become an AWS Certified Data Engineer part1 p53 4. ML Training Models.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好 欢迎来到本课，我们将探讨AWS提供的不同训练模型
机器学习支持三种不同类型模型
你有二元分类
多类分类和回归
二元分类模型预测二元结果
两个可能类别中的一个
要训练这些模型
使用行业标准学习算法，即逻辑回归
例如，判断这封邮件是否为垃圾邮件
这再次是肯定 或客户是否会购买此产品
这个产品
是书籍还是农场动物
这条评论是客户还是机器人写的
同样在或多选中
可生成多个类别的预测
预测两个以上结果中的一个
AWS使用多项逻辑回归算法
例如 这个产品是书籍
电影或服装
这部电影
是浪漫喜剧
纪录片 惊悚或恐怖片
哪个产品类别最吸引这位客户
你会有一份产品列表
最后是回归模型
它基本上预测数值结果
AWS使用线性回归算法
例如，迪拜明天的温度会是多少
或这款产品能卖多少钱
这栋房子的价格是多少
等等
这些是亚马逊机器学习使用的三大模型类型
要训练机器学习模型
需要指定几个要素
首先
需要指定输入训练数据源
包含待预测目标的数据属性名称
所需的数据转换指令
最后是训练参数以控制学习算法
在训练过程中
机器学习会自动选择适合的学习算法
基于训练数据源中指定的目标类型
这是一个高度自动化且简便的机器学习入门方式
通常机器学习算法
接受可控制训练过程和模型特性的参数
现在在AWS
这些被称为训练参数
你可以通过控制台设置这些参数
API 或者现在甚至可以通过命令行界面
如果你不设置任何参数
不用担心 因为AWS会使用已知效果良好的默认值
适用于广泛的机器学习任务
你可以指定以下训练参数中的值
你可以设置模型大小参数
最大迭代次数
打乱类型
正则化类型及强度
所有这些参数都有默认设置
默认设置足以应对大多数机器学习问题
但再次强调 根据你的具体业务场景
你可以选择并定义自己的参数值并根据数据进行微调
接下来我们将详细讨论其中几个
我不会全部讲解 我不会逐一解释所有参数
但我会挑选几个重点讲解
当我们说最大模型大小时
这以字节为单位
即模型的总大小
默认情况下AWS会创建100MB的模型
你可以通过指定不同大小来调整模型
通过指定不同尺寸
例如
如果无法找到足够模式填充模型大小
系统会自动为你创建小型模型
例如 如果你指定最大模型大小为100MB
但仅发现50MB的模式
最终模型将只有50MB
它不会扩展数据
只会压缩数据
因此 选择合适的模型
本质上是在预测质量与使用成本之间权衡
小型模型可能因需移除大量模式以适应最大尺寸限制
影响预测质量
但更小的模型成本更低
所以再次强调 需要权衡的最大迭代次数
为了获得最佳效果
可能需要多次遍历数据以发现模式
通常单次遍历
无法正确预测并发现所有模式
因此默认会进行10次遍历
但你可以通过将次数设置为最多一百次来更改默认值
所以 例如 如果你将迭代次数设为二十
但AWS发现十五次迭代后无法找到新模式
它就会停止 即使你设定了最大一百次
它会在停止发现新模式时自动终止
然后你还可以打乱训练数据，其基本作用是
混合数据的顺序
避免日志对某一类数据产生偏色
防止连续出现过多观测样本
例如
如果你在训练模型预测产品类型
你的训练数据包含
比如说 一部电影
一个玩具
一款游戏 产品类型
如果你在上传前按产品类型列排序数据
算法会按产品类型字母顺序处理数据
并看到所有数据
嗯 先处理电影数据
模型开始学习电影相关的模式
当模型遇到下一个选项时
即玩具类别 在此例中
所有混合更新都会使模型适应玩具类别
即使这些更新会破坏电影类别的模式
这种突然切换
从电影到玩具类型
可能导致模型无法准确预测产品类型
因此打乱数据顺序可以
通过随机打乱使模型更精准预测
通过随机打乱实现
最后 正则化类型和强度
现在 非常大或复杂的模型预测性能往往下降
当数据包含过多模式时
模式数量增加
模型更可能学习到数据噪声而非真实模式
当这种情况发生时 模型在训练数据上表现优异
但在新数据泛化上效果差
这正是业界常说的
过拟合训练数据
正则化通过惩罚极端权重值防止线性模型过拟合
我不会深入讨论正则化的细节
因为它确实超出了本课程的范围
但 只是想让大家熟悉AWS机器学习中使用的不同参数 在训练过程中
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/054_Udemy - Become an AWS Certified Data Engineer part1 p54 5. Importance of Feature Transformation.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到关于特征变换的课程
特征变换
以及它在机器学习过程中的重要性
现在让我们考虑
如果一个机器学习模型的任务是判断信用卡交易是否
呃 存在欺诈行为
基于您的应用背景
知识和数据分析
您可能会决定哪些数据字段或特征需要包含在输入数据中
例如
您可以包含交易金额
商户名称 地址和信用卡持有人的地址对学习过程很重要
另一方面
随机生成的交易ID目前不携带有效信息
确定要包含的字段后
对这些特征进行变换以辅助学习过程
为输入数据添加背景经验
使模型能够从中受益
例如
商户地址可以表示为字符串
例如 十六街二十五条三十二条
纽约
四五六七八
仅看地址本身表达力有限
只能用于学习与该地址相关的模式
将其拆分为组成部分会生成新特征
例如地址具体为十六
二十五 三十 第二大道城市
具体位于纽约州
具体为纽约市
邮编具体为四
五 六 七八
学习算法可以更好地分组交易并发现更广泛模式
或许某些商户邮编区域欺诈活动更多
因此我们必须正确学习并变换数据
现在有两种数据变换方法
在使用亚马逊机器学习创建模型前
可以直接在展示给AWS前变换输入数据
或使用亚马逊机器学习内置的数据变换
AWS机器学习食谱包含数据变换步骤
它们使用类似JSON的语法定义
但有超出常规JSON的限制
所以食谱有三个不同的部分
首先 我们有分组功能
可以将多个变量分组以便应用转换
因此 例如 你可以创建所有涉及网页自由文本部分的变量组
例如
标题或正文
然后可以一次性对这些部分进行转换
接下来是赋值功能，允许创建可重复使用的中间命名变量
最后是输出，定义哪些变量将用于学习过程
以及这些变量是否应用转换
现在 当你在亚马逊机器学习中创建新数据源时
会为该数据资源计算学习和统计指标
还会生成建议的食谱
可用于从此数据源创建新模型
建议的数据源基于数据及其中的目标属性
为创建和微调模型提供有用起点
AWS 使用多种不同类型的数据转换
例如 有n-gram功能
输入文本变量并生成滑动窗口对应的字符串
通过可配置的词生成输出
例如
考虑字符串
文本字符串 我非常喜欢读这本书
指定窗口大小为1的n-gram转换
会提取该字符串中的所有单个单词
例如每个单词单独出现
我非常喜欢读这本书
若指定窗口大小为2
会组合 将两个单词组合，三个及更多依次类推
我想详细讨论这些内容
这超出了当前范围
也过于深入转换细节
但需知道这些是AWS主要使用的转换类型
如果你使用AWS服务进行转换
如果你不自行或独立处理
现在还具备数据重组功能
可创建基于输入数据部分的数据源
仅使用它指向的输入数据的一部分
例如 使用向导创建模型并选择默认评估选项时
会自动保留30%用于评估，70%用于训练
此功能通过亚马逊机器学习的数据重组功能实现
现在你可以使用亚马逊机器学习API或命令行界面
如果你想更改其中的一些参数
并且它允许你进行
它允许你更改某些参数
例如 比如 百分比用于指示数据源数据的起始位置
或百分比和用于指示数据源数据的结束位置
补集参数告知AWS
需要使用不在百分比起始到结束范围内的数据
而补集参数很有用
如果你需要为训练和评估创建互补数据源
然后还有策略参数
该参数将数据源数据从默认的70-30比例拆分
就是这样 所以如果你想使用50-50的比例进行训练和50%用于评估
你可以使用策略字符串来更改这些选项
这提供了AWS提供的各种数据转换类型
以及AWS提供的数据转换类型
并确保我们的数据正确转换和处理的重要性
以确保机器学习模型能正确预测并有效学习 并确保模型能够正确训练
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/055_Udemy - Become an AWS Certified Data Engineer part1 p55 6. Evaluating Models.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好 欢迎来到关于评估AWS开发的机器学习模型的课程
所以你应该始终遵循最佳实践
你应该始终评估模型以确定
它是否能在新
或未来数据上做好目标预测
因为未来实例的目标值未知
你需要检查模型在已知目标答案的数据上的准确率
然后将此评估作为未来数据预测准确率的参考
不要正确评估模型
你需要从训练数据中保留一部分已标注目标的数据
并评估预测准确性
使用与训练相同的数据是没有用的
因为它只是奖励模型记住训练数据
而不是从中泛化
现在在亚马逊机器学习中
你可以通过创建评估来评估模型
要为模型创建评估
你需要一个要评估的模型
还需要未用于训练的标签数据
所以让我们看看如何评估不同类型模型
从现在开始
当你评估模型时
它会提供或亚马逊机器学习提供行业标准指标
并提供多个指标来审查模型预测准确性
评估结果包含多个指标
例如
包含预测准确率指标以报告整体成功可视化 帮助探索模型准确性
除了预测准确率指标
还可以查看设置分数阈值的影响
只需记住
仅适用于二分类并提示检查评估有效性的标准
现在
指标和可视化的选择 取决于你要评估的模型类型
因此需要仔细查看这些可视化
以决定模型是否满足业务需求和预期
实际输出的许多二分类日志结果称为预测分数
分数表示系统对观察属于正类的确定性
二分类模型输出0到1之间的分数
作为消费者
需决定观察应分类为1或0
通过选择分类阈值或称为截止值
将分数与阈值比较
高于阈值的分数预测为1
低于的预测为0
默认截止值为0.5
你可以根据业务需求调整此阈值
因此可以更新此截止值以匹配业务需求
你也可以在控制台中使用可视化图表
我们将在演示中详细讲解
以了解截断值的选择如何影响你的应用
现在它为二分类模型提供了行业标准的准确率指标
称为AUC或我们的曲线下面积
该指标衡量模型预测正例得分高于负例的能力
与负例相比现在
因为它独立于得分截断值
你可以通过AUC指标了解模型的预测准确性
无需选择阈值
通常返回0到1之间的小数
接近1的值表示模型高度准确
接近0.5的值表示模型与随机猜测无异
接近0的值较为罕见且通常表明数据存在问题
因此AUC接近0时
说明模型已学习正确模式
但用于做出与现实相反的预测
请记住二分类模型的AUC基线值为0.5
这是假设机器学习模型的基准值
随机预测1或0结果
现在要探索模型准确性可查看
我们将在控制台通过评估图表进行分析
高准确模型会为实际正例赋予高分
为实际负例赋予低分
完美模型会在x轴两端呈现两个直方图
显示实际正例获得高分而负例获得低分
但机器学习模型显然会出错
典型图表会在某些分数处显示直方图重叠
表现极差的模型无法区分正负类别
两类直方图大部分重叠
现在屏幕上展示的案例
右侧正确预测图表与错误预测图表
存在真正例（TP）
预测值为1
真正负例模型预测值为0
真实值为0
然后是错误预测
即假正例或假负例
现在可通过调整得分截断值
改变模型的错误处理方式
需注意当截断值向左移动时
会捕获更多真正例
但代价是增加假正例数量
向右移动则减少假正例
但会遗漏部分真正例
对于预测应用
你需要做出决策
必须做出决策
通过选择合适截断值确定可容忍的错误类型
正如课程开头所述
机器学习是一个迭代过程
因此很可能需要测试不同的阈值点
以确定哪些更适合您的业务场景
接下来是多分类问题
该类别的实际输出是一组预测分数
这些分数表示模型对观测数据属于各类别的置信度
与二分类不同
无需选择分数阈值进行预测
预测结果直接为类别
例如 预测分数最高的标签
多分类常用的指标与二分类相同
现在在AWS中
宏平均 F1分数用于评估多分类模型的预测准确性
F1分数本质上是综合考虑精确率和召回率的二分类指标
精确率和召回率
它是精确率和召回率的调和平均值
取值范围仍为0到1
数值越大表示预测准确性越高
大家可以看到F1分数的数学计算公式
现在 宏平均F1是对所有类别F1分数的无权重平均
在这种情况下
不考虑数据集中各类别的出现频率
数值越大表示预测准确性越好
需要注意AWS为多分类模型提供基准指标
例如 如果您在预测电影类型
训练数据集中最常见的类型是爱情片
例如
基准模型会始终预测类型为爱情片
需要将您的模型与基准进行对比验证
判断您的ML模型是否优于始终预测固定答案的模型
此时可以使用性能可视化工具
大家看到右侧的图表
它通过混淆矩阵直观展示多分类准确性
混淆矩阵以表格形式呈现
各类别正确和错误预测的数量或比例
通过对比预测类别与真实类别
所以 例如
回到电影分类场景
尝试将电影分类到特定类型
预测模型可能判断该类型为爱情片
但其真实类型可能是惊悚片
评估多分类模型准确性时
AWS会识别这些误分类并显示在混淆矩阵中
大家可以在示例中看到这一结果
因此它基本上显示了一系列功能
每个类别的正确和错误预测数量
各类别的F1分数
评估数据中真实类别的频率
以及评估数据中预测类别的频率
因此它提供了可视化展示
并且混淆矩阵可容纳最多十类
按评估数据中从最频繁到最不频繁的顺序列出
回归模型的输出是模型预测的数值结果
例如 如果你在预测
比如说 房价
模型的预测结果
可能是一个数值 比如三十万或三十五万
七千 四百五十五
等等
对于回归任务
AWS使用行业标准均方根误差（RMSE）
这是预测数值目标与实际结果之间的距离度量
RMSE值越小
模型的预测准确性越高
完美预测的模型值为零
现在同样 就像大多数情况一样
AWS为回归模型提供基线指标
这是假设回归模型的基线指标
该模型始终预测目标的平均值
例如
如果你在预测购房者年龄
训练数据集中观察值的平均年龄为35岁
基线模型始终预测35岁
同样 和其他指标一样
你也可以使用性能可视化
如右侧所示
它为评估数据生成残差直方图
呈钟形分布且中心在零点
表明模型随机出错
不会系统性过高或过低预测目标值
接下来是交叉验证
一种通过训练子集评估模型的技术
在可用输入数据的多个子集上训练多个模型
并在互补子集上进行评估
因此你
会使用交叉验证检测过拟合
或无法泛化模式
因此 大家看到的这张图
展示的是 训练子集的一个示例
以及互补的评估子集
每个四模型在训练过程中生成
在四折交叉验证期间
第一个模型使用前两
5%的数据进行评估
剩余部分用于训练
第二个模型使用两
5%用于评估
依此类推，每个模型使用互补的数据源进行训练和评估
评估数据源中包含
并仅限于非训练数据源的所有数据
因此在执行此验证时
将为数据源生成四个不同模型以训练模型
四个用于评估
每个模型进行四次评估
最后 AWS 提供见解帮助验证模型评估是否正确
若评估未满足验证标准
AWS 会通过显示提示
告知验证标准被违反
并依据大家看到的这五个指标
评估模型是否在保留数据上进行
例如 若训练和评估使用相同数据源
是否为预测模型使用了足够数据
例如 评估数据记录数少于训练数据的10%
相对于训练数据中的观测数量
例如 或数据模式是否匹配/不匹配
所有评估文件记录均用于模型性能评估
最后是目标变量的分布
希望大家通过本课对这一部分内容有清晰了解
AWS 如何评估不同模型
以及如何利用 AWS 提供的指标进行评估
判断我们为业务场景开发的模型是否正常运行
或是否需要调整
然后了解可用选项及如何使用
基于二分类或多分类
学习如何通过微调机器学习模型 帮助我们预测正确答案
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/056_Udemy - Become an AWS Certified Data Engineer part1 p56 7. Preparing data for Machine Learning.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到将知识付诸实践的第一步
在实际在AWS中实施机器学习的第一步显然是准备模型
我使用的数据集在
对于 为了本教程的数据集目的，我包含的数据集
下载时基本来自UCI
机器学习仓库
并已按照亚马逊ML规范格式化
现在你需要下载两个独立文件
分别是banking.csv和banking_batch.csv
banking.csv文件基本包含已购买产品的客户历史数据
类似于银行定期存款和banking_batch
gs_v文件我们将用于
预测潜在客户是否会响应营销活动
在屏幕上的banking.csv中
你会看到行和列的数据
表头包含每个可见列的属性名称
例如 列a有h
职业
婚姻状况
教育程度 违约
住房 贷款等现在
如果大家还记得属性
本质上是描述客户特定特征的唯一命名属性
例如 列t中的nr employed表示客户的就业状态
每一行代表单个客户的观测数据集合
现在我们希望机器学习模型回答的问题是
该客户是否会订阅我的新产品现在在该数据集中
这个问题的答案由y属性决定
这是最后一列，包含1或0
1表示是
0表示否
机器学习模型需要学习预测的属性
也称为目标属性
我们的目标属性是y
我们已获取基于订阅历史的数据
是或否
并提供给ML模型使用
以进行预测
例如 如果你下载了原始数据集
大多数情况下数据不会直接提供0或1
可能会标注为是/否
因此我们需要确保数据进行二分类格式化
将所有是改为1，否改为0
现在大家可以看到，银行批次CSB文件
不包含Y列或Y属性
现在我们已经创建了机器学习模型
我们将用该模型预测此文件中每条记录的Y值
因此我们需要继续操作，将这两个文件上传到亚马逊
S3存储桶
我将快速进入我的S3服务
我将创建一个专门用于本教程的新存储桶
希望这个名称可用
确实可用
我将保持所有设置为默认并创建此存储桶
存储桶创建完成后
我将继续上传之前下载的这两个CSV文件
文件已上传完毕，现在我的数据已准备就绪
我的数据已就绪并已更新
或将它上传到 指定用于本机器学习教程的S3存储桶
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/057_Udemy - Become an AWS Certified Data Engineer part1 p57 8. Creating a datasource and model in AWS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你好 大家好，欢迎回到本教程
我们现在将创建第一个训练数据源
之前我们已经上传了两个csv文件
将用于训练的文件
以及将用于新创建模型预测的文件
S3桶
完成上传后，我们下一步
我们要创建一个数据源
这是一个包含输入数据位置和输入数据重要元数据的对象
亚马逊机器学习使用数据源进行模型训练和评估等操作
在创建数据源之前
我们必须确保以下几点
我们拥有亚马逊S3桶的位置
再次确认数据模式
包括数据中属性的名称和每个属性的类型
例如 如果是数值型
如果是文本型 如果是分类或二进制类型
以及包含目标预测答案的属性名称
亚马逊机器学习需要学习预测
在我们的情况中是'为什么'
客户是否会订阅我们的产品
现在打开亚马逊机器学习控制台
我们将选择开始使用
然后我们
由于尚未部署任何内容
如果之前有部署过
可以直接进入仪表盘
但这是首次启动
我们将点击立即启动
这里可以指定数据位置
是否在S3桶中
或使用亚马逊Redshift
如果数据量巨大
则需要使用Redshift
由于我们数据量较小
可以使用S3桶并确保数据安全
这是之前下载的数据
你已下载的数据
由AWS提供
你也可以直接从这里下载
如果之前未从课程部分下载
在此指定S3位置
我将输入桶路径和csv文件
对于数据源
我将输入
只需输入1
以便区分我们的
我提供的两组数据，一组用于训练，另一组用于预测
现在请继续点击验证
这里会提示我亚马逊机器学习需要读取权限
针对S3存储位置
我们需要确保授予读取权限
一旦可以访问并读取
你会看到与大家看到的类似页面
并查看数据的属性信息
这是CSV格式
模式是自动生成
因为我没有提供模式源
对于它 文件数量和大小
让我继续点击下一步
这里我们将建立数据模式
而模式本质上是信息
机器学习需要解析输入数据以构建模型
包括属性名称
数据类型和特殊属性
大家还记得有两种提供模式的方式
要么自动处理，如这里所示
或提供模式文件
本次演示将使用的方式
允许AWS推断模式
这里可以看到系统自动生成的结果
我们可以查看机器学习推断的属性数据类型
确保属性分配正确的数据类型很重要
以帮助机器学习正确读取数据
并启用属性的正确特征处理
只有两种可能状态的属性
如是或否
应标记为二进制
数字或表示类别的字符串
应设为分类数值
数值型属性和需作为文本处理的字符串
应设为文本
大家可以逐一检查这些属性
但为了演示目的
只需告知机器学习已正确推断属性
或所有属性的正确模式
接下来我要处理最后一列
也就是Y列
这里我们指定了0/1或是/否
告知机器学习该用户是否订阅产品
或未订阅该产品
这就是我要指定的目标属性
大家还记得目标属性
是机器学习模型需要学习预测的属性
完成设置后
我将点击继续
所以这里会询问数据是否包含
如果数据包含标识符
这基本上是一个可选的
行标识符有助于理解预测行如何对应输入数据的观测行
所以如果你选择创建一个属性
行标识符
模型会将该列添加到预测输出中
行标识符仅用于参考目的
因此在训练模型时不会包含该标识符
但为了这个简单演示
我们将确保并点击null然后点击审核
这里提供编辑我们已设置选项的选项
如果你想更改任何内容
但假设我们一切正确
我将点击 继续现在我们已创建训练数据源
可以使用它创建机器学习模型
训练模型 然后评估结果现在要创建机器学习模型
由于Amazon Machine Learning自动使用我们刚创建的训练数据源
会直接跳转到模型设置页面
在这里我们可以为模型命名
根据分类需求
出于演示目的
我们已指定默认评估设置
可以指定自定义设置
如果要使用高级机器学习训练功能
例如更改AWS默认的70-30规则
即使用70%数据训练和30%用于预测
可通过自定义设置调整所有变量
我们保持默认设置
我们将再次给评估同名
分类为一类
当我点击审核时
这里再次提供修改选项的机会
然后我们将创建机器学习模型
现在机器学习模型处于队列中
在处理过程中 状态显示为待处理
当系统创建模型时
现在 状态将变为进行中
最终变为已完成
模型完全训练完成后即完成
这是创建数据源和机器学习模型的步骤 使用Amazon Machine Learning服务
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/058_Udemy - Become an AWS Certified Data Engineer part1 p58 9. Serverless ML Inference with AWS Lambda.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好 欢迎来到关于如何创建lambda函数的课程
以及心理学习参考
对于不了解什么是无服务器架构的朋友们
无服务器计算基本上允许您构建和运行应用程序
无需考虑服务器和其他网络基础设施
这能减轻开发者搭建服务器的负担
配置 网络
集群 更新
以及其他物理服务器管理方面的工作
无需处理这些任务的巨大优势
使您能够专注于当前任务而不牺牲访问或扩展能力
需要注意的几点事项
当然我们需要确保拥有aws账户
您也可以使用免费套餐
这里大部分内容都不会产生费用
首先我们需要创建sagemaker生命周期配置
接下来我们将登录控制台
导航到sagemaker
您可以在机器学习分类下找到
或使用搜索框
仪表板包含所有主要组件的链接
笔记本
训练 推理
等等
我们选择生命周期配置选项
生命周期配置是初始化jupyter环境的启动脚本
可在实例创建时或每次启动时运行
点击后进入脚本页面
点击创建笔记本
我们将粘贴以下代码
上述代码会
执行以下操作 当实例创建时
从github仓库下载代码和必要文件
组织文件夹结构并放置到会话文件夹
设置文件夹权限
安装seven zip
用于压缩lambda包至最小体积
完成之后
我们将创建笔记本实例
点击笔记本实例并创建实例
这将使用我们创建的生命周期配置启动jupyter笔记本
最后一步
我们提供常规信息如笔记本名称
保留实例
选择可用的最小规格
我们将使用s3存储桶
我将确保它拥有访问所有s3存储桶的权限
更重要的是
我们将选择刚刚指定的生命周期配置
我们现在将创建我们的笔记本实例
现在需要几分钟时间启动该实例
在此期间
我们将前往s3控制台创建我们的存储桶
在等待笔记本实例启动时
因此我们将创建一个存储桶
它将托管我们的所有数据
该存储桶用于存储训练数据
以及我们在此次工作坊中创建的模型
创建存储桶后
我们需要设置IAM角色并附加必要策略
附加策略
我们需要向新创建的SageMaker角色添加写入权限
为服务器less推理创建新角色
我们将使用两个策略
我们将使用Lambda完全访问和S3完全访问策略
这些权限在笔记本中是必需的
因为
由于我们将上传对象到S3并创建Lambda函数
因此我们将进入
IAM仪表盘
我将点击角色
我将输入'sagemaker'
作为要搜索的角色之一
在摘要页面点击附加策略
再次使用搜索框添加两个策略
Lambda完全访问和S3完全访问
接下来我们将创建服务器less推理
Lambda的执行角色
因此 由于这是针对Lambda的
我们将点击Lambda创建新角色
我们将为此S3存储桶赋予完全访问权限
以及SageMaker
现在我们已创建并完成这两个角色
确保Lambda函数有访问权限
以及SageMaker对S3存储桶的访问
反之亦然
完成之后 我们将返回SageMaker仪表盘
当看到笔记本实例就绪时
我们将打开Jupyter启动实例
笔记本实例
进入后 我们导航至服务器less AI工作坊
Lambda心理学习推理
你想要开启心理情感分析
发几条你们想做的事情的推文
请替换掉其中的存储桶名称
用你在s three创建的存储桶名称
当我们创建存储桶时
接下来我们将运行所有这些步骤
它将训练scikit learn模型
内置日志
逻辑回归
使用推文数据集
在代码的最后一行
基本上是上传训练模型和验证测试数据集
回到我们之前创建的s three存储桶
完成之后
我们将返回jupyter笔记本文件浏览器
从jupyter笔记本启动实例终端
现在要设置lambda并启用新模型的推理
我们将使用aws命令行界面
同样可以直接在jupyter实例中操作
CLI已预装在bash单元格
每个sagemaker实例都提供
在jupyter笔记本右侧
点击新建
底部点击终端
将在新标签页启动终端实例
接下来在笔记本实例中创建lambda层
通过执行几个命令
完成之后
我们将发布lambda
完成后 使用lambda函数创建新函数
那个pi文件
所有依赖已打包在之前发布的层中
在最后一步
我们需要做的是
需要完整的arn或亚马逊资源名称来运行此命令
我们可以通过返回iam控制台获取arn
进入角色并复制名称
现在更新lambda以使用刚发布的层
完成之后
我将返回控制台
导航到lambda仪表盘
包已上传并实例化
现在可以按需调用
使用lambda测试功能调用模型
现在 lambda位于控制台的计算部分
如果不熟悉的话
点击函数并选择情感分析
在右上角
你们看到测试选项了吗
我们现在就点击那个选项
会弹出一个新窗口
我们只需将其命名为测试事件
我们再次执行一次
我们将输入我们的桶名称
一旦完成
我们将点击测试按钮
几秒钟后
我们将看到执行结果成功
所以就是这样，相当简单
这就是使用scikit-learn创建模型的方法
构建了一个Lambda层
并发布它 然后通过Lambda函数调用该层并在控制台获得成功测试
现在你可以将此模型扩展使用
可以选择通过API网关调用函数并投入生产
因此这个模型可以投入生产
只需进行少许调整 总的来说就是这样构建无服务器AI环境
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/059_Udemy - Become an AWS Certified Data Engineer part1 p59 10. What is SageMaker Revisited.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好 欢迎来到本期关于深入探讨亚马逊SageMaker的课程
什么是亚马逊SageMaker
SageMaker本质上是一个全托管的机器学习服务
现在数据科学家和开发者可以快速轻松地构建和训练机器学习模型
然后通过SageMaker直接部署到生产就绪的托管环境
它提供了一个集成的Jupyter
用于数据源访问的作者笔记本实例，便于探索和分析
因此您无需管理任何服务器
它还提供常见的机器学习算法
这些算法针对分布式环境中超大数据的高效运行进行了优化
还支持自带算法和框架
我们将在演示环节详细讲解
还提供灵活的分布式训练选项以适配您的工作流程
将模型部署到安全可扩展的环境
通过控制台一键启动
这我们稍后也会在课程中演示
在本课程中
需要说明的是，这是一个由AWS提供的全托管服务
能够实现从A到Z的ML模型部署
直接进入生产环境
让我们开始
让我们先了解SageMaker环境能实现的功能
首先可以收集和准备训练数据
帮助快速构建和管理高精度训练数据集
还提供对公有和私有标注服务的便捷访问
并提供预建工作流和界面处理常见标注任务
请注意该服务会产生额外费用
因为需要实际人员为您进行标注，例如
如果您没有内部人员
还可以选择和优化您的ML算法
会自动配置和优化
例如 TensorFlow
Apache MXNet
PyTorch
Chainer Spark ML
以及更多常用ML框架均内置并针对规模优化
速度和准确性
市场还提供超过百种预训练模型和算法
当我们深入讲解机器学习时
将查看内置算法的具体内容
同时快速了解自定义算法配置
此外 可设置和管理训练环境
最棒的是支持一键训练
只需单击即可开始训练模型
系统会自动处理底层基础设施以实现扩展
例如 拍字节级别的数据集
你还可以调整和优化你的模型
自动模型调优利用机器学习快速调整模型以达到最高准确率
这一功能
让你跳过手动调整模型参数的繁琐试错过程
相反在多次训练运行中
自动模型调优
通过发现数据中的有趣特征进行超参数优化
并学习这些特征如何相互作用影响准确率
因此通过这样做
你实际上可以节省数天甚至数周
根据数据规模同时最大化训练模型的质量
最后
你能够将模型部署并管理到生产环境，同样
正如一切事物 模型训练完成后即可一键部署到生产环境
并且支持实时或批量数据处理
模型在sagemaker的自动扩展集群上运行
由于是跨多个可用区的托管服务
同时提供高性能和高可用性
Sagemaker还内置了A/B测试功能
这能帮助你测试模型并实验不同版本以获得最佳效果
因为在机器学习领域
这是一个迭代过程
因此是试错过程，尝试一种方法
如果不起作用 就继续下一个
整个过程通过Sagemaker得以简化
因为它为你自动化了大量流程
因此你可以专注于分析数据和处理数据
而非纠结于哪种模型效果最佳
这些是Sagemaker提供的基本功能
我们将在后续课程中逐一深入探讨 随着课程的推进
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/060_Udemy - Become an AWS Certified Data Engineer part1 p60 11. Setting up AWS to use SageMaker.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到本节课，我们将学习如何在AWS上进行设置
现在 好的一点是它确实提供免费层级访问
这为您提供对免费层级中最常用资源的基本访问权限
如果你们访问AWS官网
Com slash free
你们可以看到并浏览所有免费可用的服务
以及可以使用的额度
例如 在Robo Maker中
您可以在Ground Truth中使用25小时的服务
或者在Well Architected工具中
这能帮助您全面了解免费模式下的所有服务
以及每个服务在12个月周期内或按月可用的额度
或按月计算
请确保熟悉可用额度
在创建AWS账户时
我们只需填写基本信息开始
例如邮箱
创建密码
然后为AWS账户设置名称
如果是个人使用还是组织使用
将取决于您选择的账户名称类型
我们将继续点击继续
这里我们可以决定账户类型
是专业版还是个人版
我保持个人并选择专业版
这会关联到您的组织
我们继续填写基本信息
我们同意该协议
如果你们愿意
你们也可以阅读客户协议熟悉条款
完成这些后
我们可以创建账户并继续下一步
需要注意即使有免费账户
AWS需要此作为保障
如果使用超出免费层级的资源
将按月计费
但请放心不会扣费
只要在免费额度内
这就是我建议大家熟悉所有服务的原因
以及各服务的可用额度
完成所有信息后
系统会发送确认
会向您指定的手机号发送验证码
输入验证码后
系统会显示成功
我们可以点击继续
这里提供了三种不同的支持计划，我们可以选择性地采用
我们有基础计划 这是免费的
接下来是开发者计划
然后是企业计划
根据您将如何使用AWS账户
您可以选择其中任意一种
我们继续使用基础计划即可
完成这一步后
我们可以登录控制台
我们将输入注册账户时使用的邮箱地址
用于账户注册的邮箱
输入我们的密码
登录AWS控制台
这是主控制台
通过此控制台可以访问所有服务
接下来我们要做的是 首先导航到用户和组管理
即IAM身份与认证管理
这是管理所有用户、组和策略的主要仪表盘
您可以看到部分项目旁边有感叹号
这些是需要优先处理的主要事项
AWS建议您进行整改
当创建账户时需激活多因素认证
创建独立IAM用户而非使用主账户进行所有管理操作
通过组分配权限
并设置密码策略
始终建议创建用户和组
并将策略附加到用户或组
而非使用根用户
我们现在正是用根用户登录
我们将点击管理用户
创建第一个管理员用户
点击添加用户
为该用户命名为管理员
因为我将通过此用户登录
执行所有底层管理任务
访问类型有两个选项
程序化访问或管理控制台
继续使用管理控制台
程序化访问是
若需使用AWS命令行或SDK
在此可为该用户分配权限
是否将其添加到用户或组
可从现有用户复制权限
或直接附加现有策略
我们将用户添加到组
由于尚未有组
我们将创建一个新组
这里可以为该组设置所需权限类型
无论是三个权限
无论是rd rds
具体权限
所以我可以挑选用户将拥有的权限
我想赋予该用户管理权限
组名 我要给这个组命名
简单命名为管理员
我们将把这个用户添加到该组
可选地，我们可以为特定用户或组添加标签
例如，如果用户属于某个部门
但我们现在先留空
这里会给我们一个操作回顾
我将点击创建用户
就是这样
我们成功创建了第一个用户
你们可以在成功提示底部看到
它会给出一个 你已退出
这个URL可用于该用户登录AWS控制台
开头的这个数字其实就是客户编号
这是AWS分配给我的
在我创建账户时
所以我现在直接粘贴这个URL
我将其记录下来
接下来我要展示如何创建组
使用AWS命令行界面
首先我们需要
在通过命令行进行任何操作前
必须先进行配置
为此 需要程序访问密钥
返回管理控制台回到IAM
我们需要访问密钥
以便通过命令行访问AWS系统
我们返回IAM
我们将生成新的访问密钥
可用于命令行并配置AWS CLI
我将点击管理安全凭证
我要访问密钥
我要创建新访问密钥
以便通过命令行访问AWS控制台
完成之后
系统会生成新访问密钥
建议下载文件保存
点击显示访问密钥
屏幕会显示访问密钥
请注意 关闭对话框后
将无法查看访问密钥或密钥
因此建议在关闭前下载它
所以一旦我有了这个
我只需复制访问ID
我将访问并用密钥访问密钥做同样的操作
这里可以指定我们想要工作的区域
默认会使用eu west
输出格式是
指定是否要使用JSON作为输出格式
默认是JSON
所以即使你按回车
无需输入json即可完成
现在我完成了这个步骤
我可以创建该组
我将输入组名
它叫admins
正如大家可以看到的
当我输入该命令时
创建了名为admins的组
为其分配了组ID
组名称
现在让我们查看已创建的所有组
我应该有两个已创建的组
一个是通过管理控制台创建的
名为administrators
然后是名为admins的组
这是我刚通过命令行创建的
就是这样 完成这些后我们就有这两个组
我们有administrators组
然后是admins组
用户创建完成后
我将使用刚创建的管理员用户登录管理控制台
刚才创建的管理员用户
如果你还记得我赋予了它管理员权限
这允许它在AWS控制台执行所有操作
我将使用管理员身份登录
就是这样 已以管理员身份登录
大家可以看到右上角
名称已从custom shaw变为
账户名变为administrator
显示账户编号
完成这些后
我们创建第一个S3存储桶
我们将存储所有数据
现在创建第一个存储桶
用于存储机器学习数据和日志
存储桶名称
请注意存储桶名称需在AWS S3全局唯一
因为它符合DNS规范
因此它不仅需要在您的组织内唯一
在整个AWS生态系统中也需唯一
我将把这个存储桶命名为custom samaker
希望这个名称可用且未被占用
看起来这里可用
我们可以为存储桶配置特定选项
版本控制会保留对象的所有版本
也就是说 如果您从存储桶中删除对象
它仍会保留并提示已被标记为删除
而非永久删除
这对审计很有帮助
如果您想查看情况
谁访问了哪些文件
始终建议启用版本控制
此外还可以为存储桶添加标签
最常见的例子是
如果您有不同部门 可为特定部门标记存储桶
最后还可以通过CloudWatch监控
可监控此S3存储桶的指标
请注意启用这些功能可能产生额外成本
所以我先留空
点击下一步
这里可以指定默认公共访问权限
所有新建存储桶默认私有
意味着
组织外人员无法访问
您必须 默认情况下需为S3存储桶分配特定访问权限
例如您托管的网站将存放在S3存储桶中
或公开文档存放在S3存储桶中
必须取消勾选这些选项
确保存储在S3中的信息或文档
所有对象公开
但请注意默认情况下
所有内容均为私有
如需更多信息
将光标移到描述旁的蓝色眼睛图标
靠近描述的位置
我将详细解释这些选项的含义
最后这为您展示了所有选项的概览
在创建存储桶前
点击创建
完成创建
我们的第一个存储桶已创建成功
我们所做的基本是
首先创建了AWS免费账户
创建了管理员用户并赋予管理权限
我们还学习了如何通过管理控制台和命令行界面创建组
如果这是我们偏好的方式
然后我们最终创建了一个S3存储桶 我们将在这里存储所有的机器学习数据和文档
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/061_Udemy - Become an AWS Certified Data Engineer part1 p61 12. Machine Learning in SageMaker.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到这节课
我正在讲解sagemaker中机器学习的工作原理
为了理解sagemaker内置日志的工作机制
让我们先退一步思考机器学习的一般行为
假设我们要构建一个基于机器学习的分类器
以番茄为例
判断它是水果还是蔬菜
我们的机器学习分类器通常分为两个阶段
首先是训练阶段
输入大量数据
正确分类番茄为水果
用于训练模型
当模型训练完成后
使用该模型进行预测
对新实例进行分类
这些实例之前未见过
大量正确标注的数据输入机器学习模型
在训练阶段
输出分类结果
并将分类结果反馈作为模型输入
这种反馈机制用于调整模型参数
当机器学习分类器完全训练完成后
希望能在输入信息中发现
例如
番茄的特征
使分类器正确将其标记为水果而非蔬菜
如果你要构建这样的机器学习模型
通常需要编写代码
代码可能使用scikit-learn等Python库
学习 或TensorFlow、MXNet等深度学习框架
无需为模型实现编写大量自定义代码
Sagemaker通过提供开箱即用的解决方案简化流程
许多常见机器学习模型
因此 内置日志的核心理念是
开发者无需编写任何代码
来搭建模型
只需编写代码输入符合要求的训练数据
Sagemaker内置算法基于AWS的Docker容器运行
如果你不熟悉容器技术
容器本质上是代码及其依赖项的隔离单元
它们允许
将代码和依赖项打包并跨机器迁移
Sagemaker内置算法在多个区域提供容器服务
只需选择最接近你的区域
在演示中我们将完成这一步
请注意这些内置算法未预先训练
开发者需按模型输入规范格式化训练数据
并将它传递给模型进行训练
现在 Stage maker将以分布式方式在e平台上运行这些内置日志的训练
C 现在有两个实例
当训练完成后
模型参数将保存在s三个存储桶中
然后模型将被部署并托管在e平台上
C 现在有两个实例
请记住e c
用于机器学习的两个实例是付费服务，如果你有免费账户
你只能进行到啊这一步
训练模型
你将无法实际部署模型或端点
因为这些仅限付费账户
并且会产生费用，免费账户不具备此功能
毕竟机器学习是一项资源密集型任务
因此aws会收费
或者你需要付费账户
为了部署你的机器学习模型并创建端点
所以在演示中请注意
接下来我将 带你了解如何训练模型
然后我会展示如何在环境中部署它们
使用端点
但请记住
如果你只是用于测试或自学
请记住对于你的账户
你将无法部署它们
除非你配置e
C 这两个资源密集型实例
模型训练已完成
模型参数将再次
正如我之前所说 保存在s三个存储桶中
然后你可以设置
然后可以为这些模型设置HTTPS端点
以便提供预测服务
许多我参与过的组织中最常用的机器学习模型
选择的是线性学习器
既可用于分类也可用于回归问题
Stage maker还提供因子化机器用于分类和回归
另一种常见的无监督机器学习模型
如果你在机器学习领域可能听说过
或K均值聚类
用于逻辑分组数据
以及主成分分析
用于降维
我希望在这节课中 大家对SageMaker如何使用机器学习有了初步了解
而且它完全即开即用
你无需进行任何硬编码
为了设置你的算法
几乎所有功能都已经为你准备好了
唯一需要注意的是确保你拥有的数据
如果需要编写少量代码以确保数据格式正确
数据已清理完毕
你必须训练模型以便 能够部署并做出正确预测
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/062_Udemy - Become an AWS Certified Data Engineer part1 p62 13. Intro to Linear Learner.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到这节课
我现在将更详细地讲解线性学习器对数
因为它今天在行业中被广泛使用
我想 为了让你们更深入了解它的功能和作用
所以 线性学习器本质上是一种监督学习算法
可用于回归和分类问题
回归问题是指机器学习模型的输出预测是连续实数值
不会落入离散类别
而分类问题则是输出为多个离散值之一
二分类问题的输出可能是零或一
而SageMaker提供的线性学习器本质上是二分类器
接下来我们简要回顾回归问题
如果你还记得机器学习基础知识
即使你没记
如果你没有学习过机器学习
没关系 你不需要了解机器学习
但回归本质上是寻找连接因果关系的函数
例如 原因可能是距离市中心的距离
在任何你想买房的城市
而结果可能是该房屋每平方英尺价格的变化
随着远离市中心
在最简单的情况下，单一原因与单一结果
你可以在这两个维度上绘制关系
其中x轴为原因，y轴为结果
线性回归是找到这些点的最佳拟合线
最佳拟合线是均方误差最小的那条
SageMaker内置的线性学习器对数
也可用于分类问题
更具体地说
它是一个二分类器
预测输出为零或一
属于两个类别之一
在底层
线性学习器执行逻辑回归用于二分类
接下来我们看看逻辑回归的含义
主要有两种方法
如果你有特定项目的截止日期
你可以选择开始
比如说 截止前五分钟
这是我们很多人都熟悉的
这种情况下很可能无法按时完成
或者你可能过度准备和计划
比如说 作为极端情况
在截止前一年开始
现在这可能有点过度了
因为你如果一开始就做起，几乎不可能完成其他工作
这意味着两种方法都不太优化或合理
所以真正的答案在两者之间
显然 你可以想象逻辑回归
将截止时间作为x轴
并将达成截止时间的概率作为y轴
如果你在截止前五分钟开始工作
这个点会出现在左下角
正如你们看到的 在这种情况下达成截止时间的概率几乎为零
另一方面
如果你在截止前一年开始
现在你有100%的概率达成截止时间，因为提前很久开始
但可能无法完成其他工作
但当你计划接近截止时间的工作时
你可能想要找到那个高效点
因此有95%的概率最可能达成截止时间
并且不需要花整整一年时间
这就是11天的作用
如果你在图表上标出这些点
你会得到一个S形或逻辑曲线
这就是逻辑回归曲线
如果从逻辑回归曲线画一条垂线到x轴
可知在截止前11天开始有95%的概率达成
如果你在截止前11天开始
而这11天就在屏幕上显示
所以逻辑
逻辑回归能帮助你分析行动对概率的影响
这有助于建立二元分类器
考虑曲线末端的S形
概率为零
而在S形曲线另一端
有100%的概率达成截止时间
你开始得太早了
肯定能完成
但可能浪费大量时间
现在 中间就是你想要的完美点
这就是高效工作的关键
知道何时开始为项目截止时间努力
使用屏幕上显示的阈值
以及我展示的例子
我们可以将输出分类为一或另一
我们已达成截止时间
或我们已错过截止时间
这正是线性学习器提供的二元分类
所以这只是线性学习器的简要介绍
其中涉及更多计算细节
我不想深入数学部分
因为这超出了本课程的范围
但为了让大家了解线性回归的实际运作方式
以及它试图实现的目标
希望这能让你稍微明白一点
线性回归是如何工作的
以及它是如何分类的 或者它如何通过回归找到最佳值
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/063_Udemy - Become an AWS Certified Data Engineer part1 p63 14. Preparing data for Linear Learner algorithm.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好 那我们来做个
现在进行一个动手演示，展示如何使用线性学习器
这是SageMaker内置的对数函数
正如你们看到的 我已经打开了我的Jupyter实例
我将启动第一个笔记本在
Jupyter上AWS
如果你们还没完成如何启动笔记本实例的课程
强烈建议你们先完成实例启动步骤
我们现在开始
你们可以跟着操作
我们将使用一个处理过的数据集
你们可以直接使用课程提供的代码自动下载
并判断手写数字是否为3
这就是使用线性回归的目标，判断一个
如果手写数字
手写数字实际上是哪个数字
是否为3
模型输出结果为1
如果是3则为1，否则为0
如果你们还记得线性回归
本质上是0或1，是或否
等等 我们在前一课设置的Jupyter笔记本实例
如果还没完成 你们可以去做
我们将打开一个Python3笔记本
因为我通常使用的代码都是Python3
这是最广泛使用的
嗯 目前的代码
但如果你偏好其他语言
欢迎使用你自己的代码
接下来我们将设置标准导入语句
用于下载、获取和清理数据所需的库
最后探索数据
接下来是SageMaker
特定导入
首先是boto3
这是AWS的Python SDK
可访问其他AWS服务
如果你想程序化访问
S3存储桶或弹性计算云
E C 2实例
将使用boto3库
SageMaker 正如蚂蚁的名字所暗示的
这是一个用于机器学习的Python SDK
用于在AWS上训练和部署机器学习模型
SageMaker Amazon通用库是一个通用库
包含多个实用函数，帮助将数据转换为训练所需的格式
并非所有AWS资源
无论它们是e
C 两个s lambdas
端点通过Amazon资源名称或ARN唯一标识
现在从SageMaker库获取执行角色可访问ARN
其凭据用于访问SageMaker的ML API
除了SageMaker默认的序列化和反序列化功能
您可以自定义输入序列化和API返回结果的反序列化
我们传递的预测输入数据将被序列化为CSV
从API获取的输出结果将以JSON格式返回
使用JSON序列化器进行转换
我们还将从deep learning dot net下载数据集
大家可以查看提供的URL
下载的文件名为minced_pkl
Gz
下一步是解压下载的压缩包
使用pickle库将其反序列化为Python对象
当前文件中的minced数据已分为训练、验证和测试三个数据集
用于训练、验证和测试
每个数据集是包含两个字段的元组
第一个字段是minced图像列表
第二个字段是对应图像的类别标签列表
我们需要打印训练集的长度
现在我要打印训练集的长度
可以看到这是一个包含两个字段的元组
让我们查看第一个字段的形状
即minced图像列表
可以看到共有五万张图像
每张图像由784个像素组成
即28x28像素
如果大家还记得屏幕上的minced数据集
由手写数字图像组成
每个图像标准化为28x28尺寸
总计784个像素
这是一个灰度图像
每个像素值构成单通道图像
元组的第二个字段是标签列表
共有五万个标签，每个标签为0-9之间的数字
现在确认训练集的长度
训练集包含五万张图像
让我们采样图像
这些数字从0到9
对应每个手写图像表示的数字
我们的训练数据集包含五万张图像
我们的测试和验证数据集各有十张图像
所以这里有一个辅助函数
使用matplotlib查看单个手写数字
现在我们传入训练数据集索引30处的数字
屏幕上可以看到这个数字是3
我牢记这一点
我们将使用这张图像进行预测
现在准备数据以输入线性学习器
将数据集从列表转换为numpy数组
使用np
这是五万次
七百八十四
形状没有改变
线性学习器作为二分类器工作
现在我们要构建一个ML模型来识别
输入到模型的图像是否为3
设置标签
使得标签 设置标签
使得标签等于1
当输入图像确实是3时
对于所有输入图像
其中不是3的 标签将设为0
正如我之前提到的
现在我们有五万张图像的0或1标签
所有对应3的图像标签为1
其他所有图像标签为0
现在数据已准备就绪
让我们进入我们的
让我们进入 看看如何训练数据用于未来预测
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/064_Udemy - Become an AWS Certified Data Engineer part1 p64 15. Training data using Linear Learner.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好 欢迎来到我们之前演示中开始创建的线性学习模型
现在我们准备好设置训练数据
以内置对数期望的格式
称为线性学习
内置模型期望输入数据以字节形式序列化
因此我们设置字节
I o 缓冲区并使用samaker
一个常用工具函数
我们将其引用为c以将此numpy数组写出
正如它所做的 密集张量
将输入特征和标签写入字节缓冲区
并将指针移至缓冲区开头
现在我们需要访问我们的s3存储桶
我们将上传此输入训练数据
将数据上传到我在s3创建的存储桶
如果你尚未创建存储桶
建议你先创建存储桶并在此处引用
key变量保存存储桶内的数据对象引用
数据将以cordial-dash-pb格式存储
现在我们需要实例化一个指向s3存储桶的资源
我们将使用s3库进行此操作
并使用该资源实例化存储桶的引用
这里是我们将保存数据的对象资源引用
现在我们将继续计算
s3训练数据的完整URL路径
并包含存储桶
前缀和具体键
现在我们将使用对象引用以字节形式上传数据
完成此操作后
让我们将训练模型的输出结果输出到同一存储桶
为演示简化操作
但我们只需添加不同的输出路径
以便将输入和输出数据分开
在访问线性学习器内置算法之前
我们需要知道笔记本实例所在的当前区域
只需运行此命令即可查看当前区域
或笔记本实例实际所在的区域
亚马逊sagemaker提供的内置算法在多个容器中可用
以及大多数aws区域
现在这些区域提供线性学习器
这些是注册到亚马逊
ECR 或用于docker容器的弹性容器注册表
如果你之前未接触过容器
无需了解容器工作原理即可继续
但简要说明
容器本质上是包含代码及其依赖项的隔离环境
大家可能听说过最流行的容器是 dagger 容器
这些容器是便携式的
也就是说 你可以将容器从一台机器迁移到另一台机器
并且不依赖操作系统
因此它基本上是独立封装的
并在操作系统范围之外运行
所以只需参考即可轻松迁移
SageMaker 实际上使用的是 Docker 容器
内置日志系统
由于它是最流行且最广泛使用的
因此可以访问特定版本的线性学习器
线性学习器会持续更新
使用最新后缀即可访问最新版本
通常需要选择距离最近的容器
大多数地区都有本地托管的容器
这就是我们之前检查笔记本实例所在位置的原因
并选择最近或同一区域的容器
这能最小化数据传输延迟
也能大幅提升运行速度
这在生产环境中非常重要
因为这是一个资源密集型任务
有时数据量可能很大
因此必须尽可能减少延迟
接下来我们将实例化会话以访问 SageMaker
然后获取身份与访问管理执行角色
或 IAM 执行角色用于训练模型
如果你还记得创建第一个笔记本实例时
这就是我们为其分配的角色
无论是新建的
但如果你使用旧实例
将使用这个角色
现在我们将设置 SageMaker 估计器
这是一个高级 API
与 TensorFlow 中的估计器非常相似
如果你之前使用过 TensorFlow
高级 API 会抽象化
后台运行的整个训练和预测流程
它接受输入
包含内置日志的容器
估计器的其他输入参数包括执行角色
我们希望训练运行的实例数量
以及使用的实例类型
大家可以注意到我们使用的是基础 ml xlarge 机器
现在 这比我们设置 Jupyter 笔记本实例的机器更强大
正如 如果大家还记得我们使用的是基础配置
即中型实例
现在 这就是我之前提到的原因
如果你要部署它
你需要付费账户
因为免费账户无法免费支持这些大型计算实例
接下来我们指定s3存储桶URL
用于存储模型文件并传入sagemaker会话
这是我们之前实例化的会话
还可以通过调用设置超参数方法
为线性学习器指定超参数
这里有三个超参数
第一个是输入维度，即784
第二个超参数指定线性学习器的使用方式
是用于回归还是分类
然后传入二分类器
第三个参数是批量大小200
当估计器配置好内置日志和超参数后
并指定超参数
接下来调用linear_fit开始模型训练
传入训练数据路径
我们可以在sagemaker仪表盘查看进度
如果进入服务
然后选择sagemaker
通过仪表盘 查看已启动的任务
查看已完成的任务
全面了解当前状态
返回jupyter笔记本实例
任务完成后
耗时取决于数据大小和处理速度
还可通过s3存储桶确认
输出文件是否已生成
这些是启动训练的基本步骤
并实际开始
但完成训练过程
通过jupyter笔记本实例 再次确认
训练完成后 接下来进入模型部署阶段
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/065_Udemy - Become an AWS Certified Data Engineer part1 p65 16. Creating a hyperparameter tuning job.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到关于超参数调优的教程
所以 超参数本质上是在学习过程开始前设置的参数
这与通常通过训练获得的其他参数值形成对比
现在让我们看看AWS是如何实现这一点的
首先我们需要做的就是创建一个笔记本实例
我们在这里已经完成的操作是
我已经创建了一个笔记本实例
我将其命名为超参数
我们可以通过AWS的SageMaker仪表板完成此操作
接下来我将带大家完成以下步骤
在创建笔记本后
我们将上传代码进行超参数调优和训练
在AWS中进行调优和训练
如果笔记本已创建
我们将前往S3存储桶
确保我们有一个创建好的存储桶
用于存储所有数据
完成之后
我们将返回笔记本实例
打开其中的Jupyter Notebook
启动Jupyter Notebook后
我们需要创建新内核
我们将使用Python 3
在本教程中
但您可以使用其他版本
选择您熟悉的版本
我通常会使用Python进行演示
完成之后
我们继续 将笔记本从未命名改为超参数任务
为笔记本命名是个好主意
这样 可以区分不同笔记本
尤其是同时处理多个时
接下来第一步是导入库并获取S3客户端
用于调用超参数调优API
我们需要在Jupyter中输入以下代码
代码的作用是
导入SageMaker
导入Boto3
导入NumPy并简写为np
同时导入Pandas
并为其设置简写
在代码中使用时无需全名
可以直接使用简写
我将快速运行这段代码
接下来需要获取笔记本实例的执行角色
这相当于创建的IAM角色
在创建笔记本实例时生成
接下来我们要做的就是将这个角色传递给调优任务
这就是我们从samaker导入所做的简单操作
我们获取执行角色并将其传递下去
我们现在就运行这个任务
之后我们需要指定一个存储桶和数据输出位置
这就是我们刚才查看并创建的存储桶
如果你还没有创建的话
调优任务即将启动
记住这个存储桶的名称
应该以sagemaker开头
我们必须确保这样做
并且必须全局唯一
就像大多数情况一样
就像所有AWS的S3存储桶
它们需要DNS合规
因此名称必须唯一
我们需要确保创建一个名称
现在创建存储桶并以sagemaker开头
在这个示例中 我们将使用包含银行客户信息的训练数据集
其中包括客户的职位
婚姻状况以及银行直接营销活动中的联系方式
这些信息或数据来自UCI数据库
我已经将该数据包含在文档中
你们可以直接从那里下载
如果你不想从URL下载的话
也可以使用相同代码从那里下载
为了使用数据集进行超参数调优任务
我们将下载数据 转换后上传到S3存储桶
我们通过这段代码完成基本操作
我们正在下载或解压数据
运行后立即将其放回S3存储桶
你们可以看到数据正在下载
并得到了该数据集的输出
这就是我提到的少数客户银行信息
之后我们需要准备并上传数据
在创建调优任务之前
必须确保数据已准备并上传到S3存储桶
以便调优任务可以访问
这是一个可以同时完成两步的简单代码
准备并上传数据到S3存储桶
完成之后
下一步是配置并启动超参数调优任务
为了指定调优任务的设置
需要定义一个JSON对象
并将该对象作为调优任务的值传递
在JSON中
需要指定要调优的超参数范围
调参任务可消耗资源的限制
超参数调优任务的目标指标
同样地 目标指标是调优任务使用的评估标准
正如我在本节课开头提到的
超参数的核心在于我们预先指定一个指标
而不是在训练过程中使用
因此定义了eta、alpha的取值范围
最小子树权重和最大深度的对数范围
所以 超参数调优任务的目标指标旨在最大化
这是我们可以使用的简单代码
并在完成之后
调优任务再次启动的训练任务
还需定义传递给训练任务的JSON对象
在这个JSON对象中
指定训练容器镜像
训练和测试数据的输入配置
存储位置
算法参数值
超参数
使用的实例类型
我已经指定了部分静态值
例如 评估
数量 目标
下降率
以及您屏幕上看到的算法内部参数
接下来我们可以为超参数调优任务命名
然后通过调用创建超参数调优任务启动
请记住
如果您有免费套餐账户
这就是您可以操作的最远步骤
因为启动调优任务
需要约六台高性能
E C 2型实例
这些仅限付费账户使用
免费账户只能进行到这一步
但无法启动 也无法监控进度
因为需要付费账户才能实现
所以如果你没有这个
这就是你能达到的最远点了
但如果你有 在启动后你就能继续
你应该能在SageMaker仪表盘中监控它
超参数调优任务将会显示
另一种方法是通过仪表盘
你可以直接通过SageMaker仪表盘创建超参数调优任务
正如大家看到的
我们可以指定之前提到的相同设置
就像我们在Jupyter笔记本中做的那样
并在其中为超参数调优任务命名
我们可以指定超参数中使用的算法
是否使用内置的亚马逊算法
或者如果你想使用自定义算法
如果我们点击下一步
这里可以指定超参数的配置
我们想要指定的不同值
针对超参数附带的一些指标
根据我们的目标进行指定
以及我们如何使用现有数据
我们必须为这些值指定参数
我们同样在Jupyter笔记本中完成了相同操作
这只是另一种配置超参数调优任务的方式
因此有两种方法可以实现
无论是通过控制台还是笔记本实例
正如我们刚才看到的 所以如果你点击下一步
你应该能创建调优任务并启动它
但同样需要注意，就像Jupyter笔记本实例一样
只有付费账户才能启动超参数调优任务
因此免费账户无法执行此操作
因为机器学习任务资源消耗较大
因此需要付费
但完成后作为清理提示
为了避免被计费
请
完成后
请 完成后
进入笔记本实例并停止删除实例
再次感谢观看本教程
快速演示
如何指定超参数调优任务
再次说明这是使用超参数算法的情况
你可以使用相同的
这些步骤创建和配置超参数调优任务 适用于你可能拥有的任何数据
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/066_Udemy - Become an AWS Certified Data Engineer part1 p66 1. Batch Data Processing.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
让我们谈谈批量数据处理
因此你可能会认为批量处理通常被认为是一个缓慢的过程
顺便说一句 嗯
其实并非如此
现在批量处理必须快速高效地一次性处理大量数据
这带来了与流处理不同的挑战
现在 批量数据处理使企业能够深入分析已收集的数据
生成复杂分析
没错，这在流式分析中无法实现
现在批量处理是在一台或多台计算机上运行一系列程序或任务
无需人工干预
数据异步收集成批
然后在满足特定条件时将批次发送至处理系统
例如指定时间
处理任务的结果
随后发送至可随时查询的存储位置
让我们深入探讨批量处理
并讨论可用于执行它的服务
在之前的课程中
我谈到了批量处理的定义
更复杂的定义是批量处理是系列程序或任务的处理
在一台或多台计算机上无需人工干预
现在数据异步收集成批，正如我之前提到的
现在 每个批次在满足特定条件时发送至处理系统
例如
指定时间
处理任务的结果随后发送至存储位置以便查询
而批量处理系统的优点
在于可以利用更低成本的资源
借助现代架构
可以优化批量处理系统
根据处理批次的频率和规模
现在 这避免了计算资源
例如 或存储容量
通常与传统本地系统相关的问题
批量处理支持任务优先级和资源对齐
你知道的 基于业务目标
在传统本地系统中
单一集中式系统收集并处理数据
对于小规模和中等数据集
这完全合理
但在当今世界
数据批次可能包含数百万甚至数十亿条记录
现在 这种庞大的批次规模使得这些系统难以满足大多数企业的速度要求
我的意思是 想象一下试图把火腿三明治塞进花园水管
没错 这就是数据量过大时会发生的情况
你知道的 与系统处理能力相比
当需求收集系统和处理系统失衡时
结果就是高
延迟对吧 并可能导致数据丢失
现在我听说过某些处理任务需要长达八小时才能完成
例如 现在
亚马逊 EMR通过解耦收集系统与处理系统解决了这个问题
这通过实施两种常见框架之一实现
首先是Hadoop或Apache Spark
这两个框架都能处理高速数据
但方式不同
Hadoop在Amazon EMR上配置EC two实例集群
例如
作为单一分布式存储和处理解决方案
这提供了速度
容错能力
并允许独立扩展收集和处理实例
现在 Hadoop通过同时采集和处理大量数据补充现有数据系统
结构化或非结构化数据来自任意来源
这使任一系统都能提供分析结果
可交付至现有数据存储以供进一步使用
另一方面 Apache Spark是Hadoop的竞争对手框架
现在 区别在于Spark使用内存缓存和优化处理以提升性能
分析首先过滤数据
然后聚合数据现在
Apache Spark避免将数据写入存储
始终优先保持数据在内存中
现在 Hadoop和Spark均支持通用批处理
流式分析机器学习
你知道图数据库和自定义查询现在
我已介绍过Amazon EMR的部分工作原理
好的，现在让我们看看系统如何与其他AWS服务协同
首先从一个简单架构开始
所以这里的解决方案很简单
我们首先从数据源开始
例如 请记住数据源可以来自任何来源
现在这些来源的数据首先会被添加到亚马逊S3存储桶中
目前
这个存储桶将成为所有后续处理数据的仓库
让我们使用名为AWS Lambda的处理服务
现在使用Lambda
可以创建每四小时运行一次的程序
从亚马逊S3存储桶中获取新数据
然后发送到Amazon EMR进行处理
现在 该程序目前会生成批次
当分析和处理完成后
结果将发送到名为Amazon Redshift的服务
现在Redshift运行速度快
可扩展的数据仓库
使其简单且成本效益高
可轻松分析数据仓库和数据湖中的所有数据
现在Amazon EMR提供了大量控制权，可按需配置处理需求
完全符合您的规格
但如果需要更托管的解决方案，减少配置工作
现在这就是AWS Glue发挥作用的时候
AWS Glue是完全托管的ETL服务，可分类、清洗和丰富数据
并可靠地在各种数据存储间传输数据
现在AWS Glue简化并自动化了耗时的数据发现
转换映射和任务调度等复杂工作
换句话说 它简化了数据处理
我们刚才查看了Amazon EMR的简单架构示例
如果使用Glue会是什么样子
而是使用Glue
因此我们实现了与之前讨论相同的架构
现在您正在使用 用Glue替代Amazon EMR
其实非常简单
接下来我将深入讲解流处理过程
但使用Amazon和Apache Hadoop进行批量数据处理
需要大数据解决方案的组织处理如此高流量和高速数据
传统环境无法满足需求
Amazon EMR是托管服务
我已经介绍过的，支持Hadoop工作负载
除了运行Apache Hadoop框架
还可以运行其他流行分布式框架如Apache Spark
每个基础
Presto 或其他解决方案
还能与数据交互的优势
与其他AWS数据存储如Amazon S3和Amazon Timestream
Amazon E Bar笔记本
例如 提供从单次查询和探索性分析开始的无服务器开发与协作环境
您可以操作数据，然后使用丰富的图形工具生成数据图表
因此EMR笔记本非常强大
监控任务帮助您调试代码
现在让我们看看Apache
Hadoop以及它能为您带来的功能
Hadoop是一个可扩展的存储和批量数据处理系统
它使用商用服务器硬件并通过软件实现容错
Hadoop通过同时摄入和处理大量数据补充现有数据系统
无论是结构化还是非结构化数据，来自任意来源
这基本实现了单一系统无法提供的深度分析
这些结果可交付至任何现有企业系统供进一步使用，且独立于Hadoop本身
因此Hadoop是一个提供独立模块的平台
例如
Hadoop公共模块
所以Hadoop Common是一套支持其他Hadoop模块的Java工具和库
这些库帮助我们提取文件系统和文件处理组件
这些Java文件和脚本是启动Hadoop所必需的
Hadoop分布式文件系统
HDFS
顺便说一句，它非常强大
它拥有自己的文件系统
那么HDFS是什么
本质上是存储在社区节点的高输出环境文件系统
这种架构确保应用数据具有极高的聚合带宽访问
Hadoop YARN是核心
Hadoop YARN是什么
YARN本质上是一个资源管理框架，负责调度和处理任务
接下来是Hadoop MapReduce
Hadoop中的MapReduce作用
例如是一个基于YARN的系统，允许在集群上并行处理大型数据集
因此批量处理架构现在将以非常
高效的方式运行，例如可以通过不同方式实现
您可以使用AWS服务
例如您可以使用Amazon S3存储数据
首先 您可以使用Lambda进行中间文件级ETL
使用EMR进行聚合ETL
繁重处理 集中化转换与加载引擎
或者使用Amazon Redshift作为数据仓库
托管报告所需的数据
现在批量处理用例
让我举例说明
首先是日志分析
Amazon EMR可用于处理Web和移动应用生成的日志
该服务帮助客户将
结构化或半结构化数据转化为对应用或用户的有用洞察
现在在这样的用例中
你知道的 日志通常会被批量收集
没错，然后汇总并在一天结束时进行分析以获取有意义的规模
现在跨多个数据存储的统一数据视图也非常有帮助对吧
你可以将其用于Glue数据目录
无需移动数据即可快速发现和搜索多个AWS数据集
现在数据完成编目后
即可立即通过Amazon Athena进行搜索和查询
Amazon EMR或Amazon Redshift Spectrum
就预测分析而言
让我们看看这是什么
在EMR上的Apache Spark
作为可扩展的机器学习算法
或者你可以使用自己的库
如果你想的话
通过将数据集存储在内存中，Spark可为常见机器学习工作负载提供卓越性能
我们针对Amazon S3数据湖
例如
数据湖正成为存储和分析的流行方式
结构化和非结构化数据
如果你想构建自己的Amazon S3数据湖
对的 AWS Glue可使所有数据立即可用于分析
无需移动数据
希望这有所帮助
我已经深入讲解了批处理数据处理
如果有任何疑问请告诉我 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/067_Udemy - Become an AWS Certified Data Engineer part1 p67 2. Batch Processing Architecture.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好 欢迎来到本期关于
如何利用亚马逊AWS基础设施进行批量处理工作的课程
现在 当今有许多不同的批量处理应用可以利用这种基础设施
例如 理赔处理
或大规模数据转换
媒体转码和多部分数据处理任务
现在 在AWS上进行批量处理可实现按需配置多
部分任务处理架构
可用于即时或延迟部署异构
且可扩展的工人节点网格
能快速处理大量批量任务
最棒的是它们可以
现在并行处理
许多批量处理架构常与高变异性使用模式相关，存在显著使用峰值
例如 在金融领域
通常有月末处理
随后是长时间低利用率期
AWS的优势在于能帮助克服这种变异性
现在让我们看看如何构建解决这些问题的架构
这里展示了批量处理基础设施
在AWS中的基本架构
首先用户将与作业管理应用程序交互
该应用将部署在e
C Two实例
这是控制接收
调度 启动管理
并完成故障作业的核心组件
此外 它还将提供最终结果
所有处理完成后通过与首台e
C Two实例交互后 接下来原始作业数据将上传至
S three实例 这里有一个大型S3存储桶
用于存储作业的所有原始数据
现在 我们将
而非让整个大型批量任务流经基础设施
这会导致瓶颈
我们将通过使用简单队列服务（SQS）拆分任务
作业管理器将为用户向SQS输入队列插入单个任务
在用户代理下
接下来要发生的是工作节点
或者基本上是一个e主机
C 两个部署在自动扩展组中的实例
而自动扩展将处理峰值和非高峰实例
此外 可以使用竞价实例
如果批处理工作将在非高峰时段进行
比如说 如果是大数据处理且可在非高峰时段完成
可以使用竞价实例
C 两个实例以进一步节省成本
所以回到前面 这两个实例将位于自动扩展组中
该组本质上是一个容器
确保工作节点的健康和可扩展性
工作节点将从sq队列中获取任务部分
Sq自动执行批处理步骤列表中的单个任务
在处理完这些任务后
工作节点的中间结果存储回亚马逊S3桶中
然后在第六步
进度信息和统计数据存储在分析存储中
根据数据类型
可以是亚马逊SimpleDB或DynamoDB域
或关系型数据库
如果需要复杂关系
则使用RDS服务
如果是简单数据
可使用DynamoDB
最后还可以有链式流程
在第七步可以看到
完成的任务可插入Sqs队列以连接到第二阶段处理
这完全取决于批处理工作的类型
如果需要还可调整第二阶段
所以 该基础设施通过拆分大型批处理作业
将任务分解为由Sqs队列处理的小任务
同时结合工作节点和自动扩展组
以应对使用高峰
再次总结 在总结
构建高效批处理架构所需的服务包括e
C 两个实例再次
用户主要交互的将是哪个
然后是工作节点
然后是亚马逊RDS或DynamoDB
简单数据库
还可以有亚马逊S3存储桶
我们将为工作节点设置自动扩展组 然后最后使用SQS队列将大型批处理任务拆分为更小的任务
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/068_Udemy - Become an AWS Certified Data Engineer part1 p68 3. Velocity of Data Challenges.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
让我们讨论数据速度带来的挑战
没错 与数据速度相关的问题
因此各种处理方法以及如何使用AWS服务构建解决方案
我将重点讲解三个主要领域
首先我会讲解数据处理方法
我还会介绍批处理
然后当然还有流数据处理
首先当企业需要从收集的数据中快速获取洞察时
但现有系统无法满足需求
存在速度问题
速度问题的本质是
数据生成的速度正在不断加快
电子邮件 照片
推特 Facebook动态
日志文件和物联网设备都是快速生成的数据示例，必须被收集
处理
分析并高速存储
数据的收集与处理被统称为数据处理
因此从定义上
数据处理指通过收集和操作数据生成有意义的洞察或信息
数据收集分为两个部分
顺便说一下首先是数据收集
其次是数据处理
我之前已经讲过数据分析的第一部分
即上节课提到的容量问题
现在我要讲解第二部分
V即速度
数据速度指数据的采集与处理速度
如今我们生成数据的速度越来越快
想想你发送邮件、分享照片的频率
推文和Facebook动态
以及设备产生的日志文件
还有物联网设备的数据
比如
追踪器和智能门铃
等等这些数据正在快速生成并需要被收集
处理 分析并存储
多年来从事数据分析工作，我学到的一点是
顺便说一句 当企业需要从收集的数据中快速获取洞察时
现有系统根本无法满足需求
确实存在速度问题
毫无疑问数据生成速度正在持续加快
我之前还提到过数据消费量
现在让我们讨论作为消费者对数据传输速度的需求
我们要求即时发送短信
没错 但我们用邮件
现在我们期望邮件接收需要几分钟时间
这就是速度的挑战
现在 当我们考虑这个挑战时
必须认识到在数据收集与处理之间
大部分时间将花费在处理阶段
我觉得是时候明确基本定义了
没错 数据处理是数据收集的过程
并通过操作数据生成有意义的信息
数据处理方法
现在让我们详细讨论
这是任何数据系统的核心
没错
数据处理定义了收集数据的方法
并将其呈现给存储以进行分析
现在深入探讨数据处理
总的来说
有两种处理类型
一种是批处理，另一种是流处理
这些需要不同的架构并支持不同层次的分析
批处理意味着
你知道的 成批处理内容
没错 当数据量很大时
例如要处理
并在特定时间间隔处理
例如按计划执行
或达到一定数据量时
这种处理应用于服务器日志等数据集
财务报告和流数据汇总
第三种是流处理
即对数据流进行处理
换句话说 处理持续生成的实时数据
小规模数据集，以千字节为单位
当你需要实时反馈或持续洞察时
这种处理应用于物联网中心数据等
例如 电商交易和游戏玩家活动
或社交媒体信息
许多组织同时使用两种处理方式
现在流处理用于获取初步洞察和实时反馈
而批处理用于复杂分析的深度洞察
例如
信用卡交易是一个很好的例子
所以，你有没有在刷信用卡后几秒钟就收到过税务通知？
我们现在所拥有的
这是一个实时数据防欺诈警报
这是一个即时发生的数据流处理过程
现在另一个定期运行的流程会使用相同数据，由信用卡公司处理
你知道的 几天需要时间对吧
所以相同的数据
两个完全不同的业务场景
不是两种不同的速度
数据处理
以及相关的挑战
往往由数据收集和处理的速度决定
现在批量处理分为两种形式：计划式和周期性处理
安排批量处理是处理海量数据的过程
定期执行
每小时、每周或每日，每次处理的数据量大致相同
工作负载可预测
另一方面 周期性批量处理在随机时间按需执行
现在 这些任务通常处理
一定量的数据
数据收集完成后
这可能导致工作负载不可预测且难以规划
现在可能看起来这些处理方式速度较慢
但即使安排了周期性处理也不代表需要数小时
仍可能需要高速处理
例如
当飞机降落在机场时
航班数据
发动机上的计算机和物联网传感器必须被收集和处理
现在地勤人员仅有几分钟时间判断数据是否
并确定飞机能否再次起飞
现在流处理也有两种形式
实时处理和准实时处理
两种类型都涉及流数据
正如你所知 会被快速分批处理
现在处理速度的差异
实时处理在毫秒内完成
而准实时处理在几分钟内完成
处理始终针对存储位置进行
现在让我们讨论不同类型数据处理中的存储机制
现在在批量处理中
我们通常使用单一应用收集、处理并临时存储数据
批量处理的最后一步是将数据加载到分析型数据存储
所以批量处理的最终步骤是将数据导入分析型数据仓库
亚马逊 EMR或托管框架
使用类似Apache的工具
Spark和Hive进行复杂数据处理
现在处理流程包含数据分析和数据本身
现在属于批量分析
整个数据集可供这些分析查询使用
这使得能在数据上执行高度复杂的分析
现在批量处理常用于例如
需要深度洞察的场景
需要高级分析的情况
批量处理系统的延迟为分钟到小时级
取决于执行的分析复杂度
现在在流处理中
我们使用多种服务
一个服务用于持续数据流的采集
另一个服务用于处理和分析数据流
还有一个服务将数据加载到分析数据仓库
如果需要的话在AWS中
我们有多个相关服务
包括Amazon Kinesis Data Firehose和Amazon Kinesis Data Streams
用于采集数据流并加载到分析数据仓库
现在Amazon Kinesis Data Analytics例如
用于处理和分析数据流
顺便说一下Amazon Kinesis Data Analytics
允许你进行查询
处理时间窗口数据
对的 比如 过去两分钟内的分析仅限该时间窗口内的记录
由于这些限制
流数据的分析通常是简单聚合
现在分析结果会显示在仪表盘中
或存储在分析数据仓库
流处理的延迟为
你知道的 这种处理速度在秒到毫秒级
所以流处理非常快速
系统的处理能力很大程度上取决于需求
因此选择正确的
你知道的 解决方案对成功实施至关重要
明确批处理规模和速度需求有助于选择合适服务
非常适合考试题目
话说回来
现实中可能没那么简单
但保持谨慎是合理的
当我们把一切说得太简单时
现在实际情况更复杂
好的 稍后我们会讨论难点
但先看看两个现实场景和处理类型
我现在推荐每个现在
想象一家零售连锁企业正在分析其特许店的销售点数据
现在
这些门店遍布世界各地
每个地点会定期在一天中传输数据批次到中央数据中心
现在客户希望在凌晨一点十五分进行数据集分析
东部标准时间
例如 此刻在那个时间点
所有数据集必须快速处理
以便尽快生成报告并交付给全球各分店经理
因此数据收集后快速处理是常见挑战
在这种环境下最佳解决方案是批处理系统
想象一家广告公司利用社交媒体点击流数据识别产品趋势
每秒产生的数据量非常巨大
收集必须足够快以确保不遗漏任何数据
一旦收集完成
这些数据必须与采集速度同步处理
持续数据流对吧 公司可以据此抓住最新趋势
现在 快速收集后快速处理数据是另一常见挑战
当然
最佳解决方案是流处理系统
现在让我们讨论数据处理速度的特点
系统的数据处理能力
当然 高度依赖其需求要求
选择合适系统对成功实施至关重要
现在我来展示或介绍数据收集与处理的特性
首先在数据收集方面
好的 批处理速度非常可预测
数据以计划间隔和周期性大批量传输
另一方面 速度在此更难预测
计划事件丢失会增加系统压力需特别考虑
近实时处理对速度要求极高
这些系统需在数据初始收集后分钟内处理
这会给处理系统带来巨大压力
涉及实时分析系统
在这种情况下 实时处理系统对速度要求至为关键
例如 不能花费几分钟处理对吧
必须秒级处理以保持有效性
在数据处理方面
有批处理和周期性处理
数据收集后可在受控环境中处理
在近实时和实时场景中需预留足够资源
数据收集会立即产生即时处理的需求
这取决于处理的复杂程度
清洗 去噪
数据编目
这会显著减缓解决方案的速度
因此必须相应规划
数据速度的另一个关键特征是数据加速
指大量数据被摄入的速度
处理和分析
现在数据加速并非恒定
而是呈爆发式增长
以推特为例
顺便说一句
话题标签可能在几秒内变得极其流行并出现数百次
或降速至每小时一个标签
这就是数据加速的实际应用
因此系统必须能高效处理每秒百个标签的峰值
而低谷则是每小时一个标签
希望这对你有帮助
我们讨论了数据速度
这很重要 如果有任何疑问请告诉我 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/069_Udemy - Become an AWS Certified Data Engineer part1 p69 4. Stream Data Processing.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
让我们谈谈流数据处理
好的 这就是数据处理领域增长最快的领域之一
现在实时收集信息的设备数量正在快速增长
这推动了需要与数据生成速度相匹配的处理解决方案的需求
所以流数据处理
你知道的 使企业能够在数据收集后几秒内获得洞察
现在 企业再也无法忽视
或避免来自网络应用的大量实时数据
电子商务交易和游戏活动
例如 没错，还有社交媒体信息
所以让我们深入探讨流处理及其支持的服务
流处理是持续收集和处理数据流的过程
在之前课程中提到的批处理解决方案中
批处理规模和速度相对稳定
但使用流处理方案
传输的数据量和数据包大小并不总是稳定
所以 流数据处理
使企业能够立即从收集的数据中获取洞察
流数据解决方案擅长处理随时间价值递减的数据
比如传感器数据来自
假设你的割草机现在传感器每60秒发送一次信号
判断割草机是否停止运行
这样系统可以触发警报
如果出现异常现在正是关键时机
否则割草机
你知道的 价值会立即消失
让我列举流数据的优势
首先是控制权
在流数据解决方案中
默认解耦数据采集系统（生产者）
与处理系统（消费者）
流处理方案为输入数据提供持久缓冲区
现在数据可以被处理
您可以按需发送数据
根据需求
第二个优势 每个流生产者可以写入同一端点
允许将多个
你知道的 分散的数据合并为单一数据流进行处理
例如
在物联网方案中
百万设备可轻松将各自数据写入同一端点
第三个优势是能够保留并维护数据的顺序
在数据流中保持事件序列至关重要
例如
生产者按顺序发送数据一、二
三 四
没错 消费者会以相同顺序接收数据
一、二
三 或四
最后消费者本身现在
请记住这是一个处理输入流的服务
数据并行消费是提升处理速度的最大优势
并行消费允许多个消费者同时处理相同数据
在数据上构建并行应用并管理时间框架
那么如何在AWS中使用Amazon Kinesis处理流数据
没错 Kinesis使收集
处理和分析实时流数据变得简单
可及时获取洞察并快速响应新信息
Amazon Kinesis提供高效处理任意规模流数据的核心能力
同时可根据需求选择最适合的工具
适用于您的应用
之前我已经简要介绍过Kinesis
现在让我们深入了解Kinesis
它主要具备以下能力
您可以操作
这些能力可单独或组合使用
我已经提到过其中三个
Amazon Kinesis Data Firehose
Amazon Kinesis Data Streams和Amazon Kinesis Data Analytics
Kinesis的第四个能力是Amazon Kinesis Video Streams
该服务可轻松将连接设备的视频安全传输至AWS
用于分析
机器学习
例如 另一种处理方式
现在让我们看看基础架构
没错 在即将讨论的这个架构中
在这里 传感器数据由Kinesis Data Firehose以流形式收集
非常直接
该服务配置为将数据发送至Kinesis Data Analytics
进行数据处理
然后Kinesis Data Analytics筛选相关记录
再发送至下一个Kinesis Data Firehose流程
现在并行进行
第二个流程从原始Kinesis Firehose选择相关记录
然后将这些数据存入亚马逊S3存储桶
现在此流中的无关记录将被直接丢弃
现在相关数据已存入S3存储桶
您可以使用亚马逊Athena查询这些记录
现在 亚马逊Athena是一个交互式查询服务
便于分析S3中的数据
使用标准结构化查询语言SQL
现在Athena是无服务器架构
无需管理基础设施
您只需支付 当然
查询扫描的数据费用
然后可使用这些查询结果生成有价值的
使用亚马逊QuickSight为利益相关者制作仪表盘和报告
亚马逊QuickSight是快速
你知道的 基于云的企业智能服务，便于向组织内所有人传递洞察
但让我们稍作回顾
没错 在这个架构中我们讨论了
解决方案是丢弃无关记录
现在如果我想深入分析这些无关记录
查看是否存在模式或其他有用信息
可能在无关数据中存在我们需要的信息
这是一个考试题目
没错 我稍后会详细讲解
同时说明如何从无关数据中提取价值
Kinesis的另一个优势
Firehose持续向多个消费者发送数据流
这里展示另一个架构示例
数据通过Kinesis Firehose流
所有记录随后存入S3存储桶
请注意当前架构中没有过滤层
现在让我们稍作调整
假设 贵公司还使用存储在不同S3存储桶的设备设置
现在 使用AWS Glue
例如生成综合结果
来自Kinesis数据流和设备数据
Firehose流与当前设备数据
可查询这些结果
与亚马逊Athena兼容
然后使用亚马逊QuickSight可视化
正如之前架构所述
让我进一步推进
当前架构中的双数据路径
批处理数据流程位于顶层
数据流处理位于底层
全部源自同一源头
请记住 这里有三个不同的存储层右和存储层
现在 将两个进程的结果存入不同桶中
将允许亚马逊Athena和亚马逊QuickSight进行更好的分析
处理大数据流
我们如何处理这些
好的 顺便说一下，让我们看看众多原因
现在对于流数据解决方案在批量
处理系统处理始终是异步的对吧
采集系统和处理系统常被整合在一起现在使用流解决方案
采集系统生产者和处理系统消费者始终是分离的
流数据使用被称为数据生产者
例如 正确 每个生产者均可将数据写入同一端点
我之前提到过对吧
因此多个数据点可合并为单一数据流
另一个巨大优势是数据的客户端处理
并能并行消费数据
这允许多个用户同时处理相同数据
使用亚马逊Kinesis进行流数据处理
例如 这里使用多项服务
一项服务用于持续采集数据
一项处理并分析流数据
然后将数据加载到分析存储中
如需的话
亚马逊Kinesis满足所有需求
您可以独立使用Kinesis服务构建最优流解决方案
所以Kinesis Firehose
嗯 例如
好的
现在这是做什么的
这是捕获数据的最简便方式
你知道的 传输并加载数据流至AWS数据存储进行近实时分析
使用现有BI工具
现在Kinesis数据流也非常强大
它使您能使用流行流处理框架构建自定义实时应用
所以亚马逊Kinesis视频流
同样非常强大
其工作原理 是让连接设备安全地将视频流传输至AWS
用于分析
机器学习
ML 例如
或其他处理方式
现在亚马逊Kinesis数据分析
另一方面
其工作原理基本是
这也是使用SQL或Java实时处理数据流的最简便方式
无需学习新编程语言或处理框架
例如传感器设备正在收集流数据
例如在你的汽车上
好的 现在
该服务配置为将数据发送至处理
使用亚马逊Kinesis数据分析服务
现在该服务过滤无关记录
然后将数据发送至另一个Kinesis Firehose处理
随后将结果存入S3存储桶的业务层
使用Amazon Athena查询S3存储桶中的数据
当然 现在可创建用于生成洞察仪表盘或向利益相关者报告
现在结合处理架构可整合这两种架构
需记住当前流分析存在限制
由于数据包大小及数据传输速度
受限于
你知道的
当前存在简单数据限制
比如聚合限制
企业常结合批处理分析以获取更深层洞察
再生成数据仪表盘和报告
此处流数据通过Kinesis Firehose收集
收集后
然而 数据直接存入S3存储桶
因此无需
你知道的 在服务间切换
而是 所有数据直接存入S3
Glue主要用于转换为单一结果集
加载至业务层的第三个S3存储桶
使用Amazon Athena查询第三个S3存储桶数据
例如 现在可通过Amazon QuickSight轻松查询
可用于生成仪表盘
从Amazon Athena和首个S3存储桶
即原始流数据首次加载的位置
这使企业可在两个数据桶间进行对比分析
最后讨论流处理用例
对 可构建视频分析应用
你可以使用亚马逊Kinesis
例如安全地从配备摄像头的设备和家庭传输视频
办公室 工厂和公共场所到AWS
然后你可以使用这些视频流
例如用于视频播放
安全监控
人脸识别
机器学习和其他分析
现在将这些数据整合到你的应用中
可以实现多种客户增强功能和数据挖掘能力
从批量处理进化到实时分析
例如
使用亚马逊Kinesis
你可以对数据进行实时分析
正在 你知道的
传统上使用批量处理分析
在数据仓库或使用Hadoop框架
最常见的用例包括数据湖
数据科学和机器学习
然后你可以使用Kinesis Data Firehose持续将流数据加载到你的Amazon
S3数据湖
你还可以随着新数据出现更频繁地更新机器学习模型
确保数据准确性和可靠性
并分析IoT设备数据
例如 假设你已向各种
你知道的 警车
它们使用所有这些转向灯
并配备多种传感器
并持续传输数据
我们可以利用这些数据发送实时警报或执行其他程序化操作
当传感器超过特定运行阈值时
我们可以使用示例IoT分析代码
AWS有这些资源
你准备好从头开始
有可以直接使用的模板
总结来说 我们讨论了各种方法的可能
并理解数据收集如何影响数据速度
希望这有所帮助
如果有任何问题请告诉我 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/070_Udemy - Become an AWS Certified Data Engineer part1 p70 1. EC2 Basic Infrastructure.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到这节关于弹性计算基础的课程
C 两种基础设施
在这节课中我们将探讨
你知道的 弹性计算的一些基础知识
C 两种就是它们是什么
它们是如何运作的 基础设施的情况
等等
所以一个弹性计算 C Two在AWS中提供基本的可扩展计算能力
所以 换句话说 AWS提供了基本的虚拟机或虚拟服务器
它们被称为弹性云计算
因此名称中的e
C Two
现在弹性计算 C
Two提供多种功能
它们非常强大
非常可靠
首先最重要的是虚拟计算环境或实例
换句话说
虚拟机
你知道我们可以通过弹性计算做很多事情
C Two实例 我们可以以多种方式配置它们
我们将在课程后续内容中详细讲解
弹性计算
C Two基础设施的第一个基本概念是亚马逊机器镜像
或现在称为镜像，本质上是包含软件配置的模板
所以 例如操作系统和多个应用程序
对于AMI
嗯 你可以启动一个实例或启动EC two
这是亚马逊机器镜像的副本
在云端运行的虚拟服务器
所以你可以拥有大量
或者启动多个AMI实例
嗯 例如 就像屏幕上大家看到的镜像
现在 因此您的实例将持续运行，直到您停止或终止它们
或者你知道的 它们失败了
嗯 出于某种原因
例如如果它们失败
或者您停止 您可以从新的AMI启动全新实例
它们本质上只是托管在AWS上的机器映像
我们可以通过这些来启动我们的AC
现在您可以启动预配置的实例
或者您可以自定义实例
我们将详细探讨
稍后课程中会更详细讲解
启动时的配置是AMI的副本
现在您可以从单个AMI启动不同类型的实例
实例类型本质上决定了您要启动的计算机硬件
我们可以在硬件配置方面启动多种实例类型
另一个基础架构的基本概念
E C 两个实例涉及可用区和区域
E C Two服务托管在全球多个位置
这些位置由区域和可用区组成
如果您不熟悉区域和可用区
区域本质上是地理区域
在区域内部有可用区或数据中心
每个区域至少包含两个可用区
或物理数据中心以实现高可用性和冗余
并且你知道的
运营非常先进且可靠的数据中心
与 你知道的 当今最新的硬件和软件
现在每个区域完全独立
每个可用区相互隔离
但同一区域内的可用区通过低延迟链路连接
正如您在图中看到的
Ect 资源要么全局，要么绑定到区域或可用区
现在查看AWS环境中的资源时
您只会看到指定区域关联的资源
这是因为区域如前所述彼此隔离
当您启动实例时
必须选择同一区域的AMI
现在如果AMI位于其他区域
你可以将AMI复制到你正在使用的区域
这些都是在区域和可用区中需要牢记的要点
关于可用区
当你启动一个e
C 首先启动两个实例
你现在正在选择带有可用区的区域
你也可以选择要启动的可用区
你访问你的e
C 现在启动两个实例
如果你将实例分布在多个可用区中，假设其中一个出现故障
你可以设计应用程序，使另一个可用区的实例处理请求
因此实现冗余和高可用性
在可用区中还有许多其他操作可以执行
例如 使用弹性IP地址来屏蔽故障
用户或应用程序不会察觉主机已故障
等等，可用区通常由区域代码表示
后跟标识符
例如 你们看到u s dash east dash one a现在
为确保资源在区域内的可用区中分布
将可用区映射到每个AWS账户的名称
例如 对于us-east-1a
你可能不在另一个AWS账户的us-east-1a位置
你可以选择要操作的区域
或启动你的e
C 两个实例
在启动e时同时考虑可用区
C 两个实例
可以让AWS选择默认可用区
或选择自定义可用区
建议让AWS在区域内选择可用区
因为他们会选择最适合你组织的可用区
你知道的 根据各可用区的负载情况
因为这些信息我们无法获取
这就是推荐原因
让AWS选择实例的可用区
C 两个实例应启动到
但你可以选择特定可用区
显然启动实例需要存储
当你启动实例时
根设备卷包含用于启动实例的镜像或AMI
所有这些都是基于称为E C的存储
两个实例存储
这意味着从aah启动的实例的根设备
I所使用的实例存储卷是基于模板创建的
通常存储在亚马逊S3中
现在嗯
最近不那么近了
可能大约一年到两三年前引入了EBS
即弹性块存储
这意味着现在，从AM启动的实例的根设备
通常由EBS卷或EBS快照支持
你确实可以选择使用基于实例存储的AI
或基于EBS卷的AI
我们将探讨实例存储与EBS卷的区别
在存储部分
但你可以选择使用其中一种
但建议使用EBS
因为其操作性比实例存储更可靠
对于使用实例存储作为根设备的实例
自动 提供一个或多个实例卷
作为根设备卷使用
当启动实例时
用于启动设备的镜像
或启动实例的镜像会被复制到根卷
实例存储卷上的数据在实例运行期间保持
但实例终止时数据会被删除
需注意这一点 这就是建议使用EBS的原因
这是因为使用EBS时
实例停止或终止时数据不会被删除
但使用实例存储时
实例终止时数据将被清除
或实例故障时，如果关键数据存于实例
且使用基于实例存储的存储卷
若因某些原因
意外终止实例
甚至例如
硬件故障 所有数据将丢失
而EBS 另一方面
若使用EBS作为根设备
由EIS引用的EBS快照会被使用
还可选择使用其他EBS卷
根据实例类型
对于可停止并重启的EBS实例
不会影响附加卷中的数据
EBS支持多种实例和卷相关操作，例如
可以修改实例属性
更改其大小
更新内核
您可以将根卷附加到其他正在运行的实例进行调试
或者任何用途 这就是为什么建议使用EBS支持
呃 E C 两个实例因为它们更加稳定
而可配置性对于EBS支持的实例显著更强
E 两个实例
现在大家看到E
C 两个实例的基本流程 例如
当卷被创建时
所以当你创建一个卷
它会进入待处理状态
从待处理状态可以转为可用
现在 如果您正在创建 它将进入可用状态
在可用之后 您可以将卷附加到EC two实例
现在这些步骤
此流程适用于EBS支持的卷
现在
在可用阶段可以执行多项操作
我们可以删除该卷
如果您删除卷
会回到待处理状态
从待处理状态转为已删除
您可以选择附加卷
或者您可以分离卷
假设我已经将卷附加到EC two实例
但启动了一个新的EC two实例
我可以分离卷并附加到新实例
再次如前所述EBS卷的流程更加可靠
相比实例存储卷
现在 这里有一个快速对比
与使用EC two Windows实例相比
这张图表很好地展示了传统本地硬件的差异
与AWS提供的虚拟化硬件相比
如果您使用本地物理服务器
资源和容量受限于服务器物理能力
而EC two实例则具备弹性扩展能力
假设您最初启动了一个实例
嗯 你知道的 这可能会
嗯 你知道的 如果你在物理环境中，内存有8GB
如果你在本地升级
这是一项相当耗时的任务
而如果是e
C 只需几下点击即可创建两个实例
你可以实质上在实例中升级服务器硬件
这就是迁移到云平台的主要优势
或者通过两个实例实现弹性扩展
当然你只需为实际使用的基础设施付费
一旦终止实例
你就会停止 你知道费用也会停止
停止支付费用
而如果你购买了服务器
你会一直持有它直到
嗯 你知道的 生命周期结束时
或者将其淘汰
或者转售它等等
这是一个很好的图表，帮助区分传统Windows
PC Windows
部署在本地的服务器
与使用e
C Two实例 相比，后者在云端
这里还比较了启动过程
如果你有本地服务
与e C Two实例
在顶部左侧
大家可以看到 传统的服务器搭建过程
处于关闭状态
启动电源
进入冷启动
然后进入运行状态
接着你会进行软重启
进入休眠模式 或者睡眠模式
这是服务器在不同电源状态下的基本流程
现在将这与弹性实例的电源状态进行比较
C 两个实例 再次
所有这些过程都是专门为EBS支持的实例设计的
因此当你从AMI或镜像启动时
你知道它会从待机状态转为运行中
你可以重启 你可以关闭
你可以终止 这是针对实际服务器的特定操作
现在关于E
C 两个实例 尤其是EBS支持的实例
关键在于硬盘实际上与物理服务器分离
因此你可以取出硬盘
将其卸载并附加到另一台物理实例
假设你在一个Windows服务器上启动了AMI
你注意到应用程序使用的资源比最初预期的更多
假设你原本认为只需使用8GB内存
但实际需要16GB或32GB
你可以卸载包含应用程序的卷
启动配备32GB内存的新实例
或配备64GB内存的新实例
并将该卷重新附加到该实例
从而实质上升级硬件而不影响应用程序
你的配置
你的软件现在在本地服务器上操作
这可能需要几天时间停机
增加物理内存
启动系统 配置等步骤
而在AWS或E
C 两个实例
只需几分钟
即可升级硬件且不影响用户或应用
这就是E
C 两个实例 许多人在听到E
C Two或弹性云计算时会感到困惑
但通俗来讲
它们是AWS提供的虚拟服务器或实例
非常可靠
非常可扩展
非常灵活 在本课程中我们将深入了解
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/072_Udemy - Become an AWS Certified Data Engineer part1 p72 3. Instance lifecycle.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，今天这节课我们将学习
我们将要探讨的是EC two实例的生命周期
C 现在有两个实例
通过使用EC two服务管理您的实例
从实例启动到终止的整个过程
您基本上能获得最佳体验
托管在实例上的应用程序或网站
现在 大家看到的示意图基本展示了实例状态的转换过程
需要注意的是您不能停止
无法对EBS支持的实例进行停启操作
这是因为AWS最近推出了EBS支持的实例
即弹性块存储
相比实例存储更加可靠
通常用于EC two
C 经典实例
这是需要记住的要点
另一个需要注意的是EC two实例
C 实例 您可以将EBS卷卸载并重新挂载到EC two实例
实例上
现在让我们更详细地了解每个状态
这里展示了EC two实例的不同状态
C 实例
当您启动实例时
会进入待机状态
启动时指定的实例类型决定了计算机硬件配置
没错 使用启动时指定的AMI
即 亚马逊机器镜像
即您要启动的操作系统或软件
实例准备就绪后
进入运行状态
您可以连接运行中的实例并自由使用
进行配置并开始使用
实例进入运行状态后
每秒计费，最低一分钟后开始
只要保持实例运行
您将被收取费用（免费账户适用）
存在特定实例类型
在实验中我们将使用t2.micro
每月免费使用750小时
这为测试提供了充足时间
在AWS上 但对于生产服务
当你在组织中时
请记住一旦实例进入运行状态
你将按每秒计费，最低一分钟起算
C 现在有两个实例
如果实例状态检查失败或无法正常运行应用程序
或者你的实例根卷是EBS卷
你可以停止并启动实例
开始解决问题
类似于重启服务器的操作
当你停止实例时
它会进入停止中状态
然后变为已停止状态
停止后AWS不再收取实例使用费或数据传输费
但会继续收取已连接EBS卷的存储费用
因为停止实例时
EBS卷仍会保留
所以 当实例处于停止状态时
你可以修改实例的某些属性
你知道的 例如更改实例类型
这意味着增加内存
调整处理器
等等
当你重启实例时
它会进入待机状态
大多数情况下会被迁移到新主机
假设你知道关机时
更改实例类型
再次启动时
通常会在新主机上重新启动
显然你不会察觉差异
但这只是信息说明
或告知你大概会在新物理主机启动
然后实例会被终止
实例永久删除且无法重启
这显然不会重建
但需注意EBS卷会保留
嗯 例如若卸载卷，终止实例时卷仍会保留
若不卸载EBS卷
终止实例时
EBS卷也会被删除
现在有一张表格总结关键区别
重启、启动与停止、休眠及终止的区别
在主机、IP地址等方面的影响
对IP地址的影响
对实例存储卷的影响
对根设备卷的影响
哪个是EBS
然后是计费
当你重启主机计算机时
它仍保持在同一物理主机上
IP地址保持不变
弹性IP保持关联
IPv6地址也保持不变
即使在实例存储中
你的数据得以保留
然后是计费
实例计费小时数不会改变
显然随着你继续深入
在停止、启动、挂起和终止等方面
会有变化
需要注意的一点是再次强调
对于实例存储卷
停止和启动时数据会被清除
以及终止时
对于根设备卷
默认情况下会被删除
再次终止实例时
就像我提到的 除非在终止实例前分离卷
这是一个需要记住的重要信息
并记住不同状态下的变化情况
当你重启时 当你停止、启动、挂起或终止时
因为 显然这与传统本地服务器有很大不同
相比来自传统本地服务器
这需要一定的理解
并通过实践适应不同状态下的操作
对你的数据 对你的物理机器
等等，因为如果你没有云经验
或者如果你不处理
C 两个实例
如果你 你知道
始终使用本地服务 确实需要一些练习或适应这些操作
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/073_Udemy - Become an AWS Certified Data Engineer part1 p73 4. Instance Types.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好 欢迎来到这节课
我们将讨论AWS中可用的不同实例类型
所以e C Two提供
你知道的 提供了适合各种使用场景的优质实例类型
所以实例类型你应该明白我的意思
当我提到 实例类型
它们基本上是ecc的硬件
到实例 比如你知道的CPU
内存 存储
网络容量
等等
每个实例类型包含一个或多个实例规格
这允许你根据目标工作负载需求扩展资源
现在AWS中有不同类型的实例可供配置
首先是通用型，显然称为通用用途
这些实例提供计算
内存和网络资源的平衡
可用于多种多样工作负载
这些实例适合均衡使用资源
很好的例子是Web服务器或代码仓库
甚至在通用用途中
有不同类型的e
C Two实例 所以我嗯
在这些幻灯片之后 我将登录AWS控制台展示
我们能在AWS中配置和部署的不同
你知道一些
其中的一些
通用用途的优势
以及AWS提供的硬件
你知道他们使用 Nitro系统是专用硬件与轻量级虚拟化管理程序的结合
Nitro系统是下一代e
C Two实例的基础平台 它使实例运行更快并降低成本
他们所做的就是
重新构想虚拟化基础设施
你知道的 传统虚拟化管理程序保护物理硬件和BIOS
你知道它们虚拟化CPU存储
现在有了新的Nitro系统
他们能够分解这些功能
将其卸载到专用硬件和软件
同时降低成本
所以你看
Nitro系统比AWS旧系统快三倍
或市场上的通用系统
然后还有计算优化型
现在 这些适合计算密集型应用，受益于高性能处理器
所以啊 你知道的
属于这个系列的实例
此类别或家族适合
比如说 批量处理
工作负载 媒体转码
嗯 高性能Web服务器
或HPC
即高性能计算
科学建模
游戏等等
如果你想获得额外性能
应选择计算优化实例
你有GP
通用型
突发稳态 你知道的 这些是T3实例
适合微服务
低延迟
小型和中型数据库
还有存储优化型
如果你使用NoSQL
或内存数据库
或使用弹性缓存
这会是个好选择
还有扩展型ARM
非常适合Web服务器
容器化微服务
缓存集群
然后是机器学习型
当然
适用于 如果你要构建机器学习模型
进行训练 测试
做基础级AI
然后机器学习计算实例会非常适合你
你有高单线程和高内存两种选择
接下来是哪一个
所以高单线程实例
非常适合用于
如果你在做CAD设计工作
如果你在进行游戏
如果你在处理关系型数据库
比如使用微软SQL Server
这需要高性能支持
嗯 或者如果你有高内存需求
比如说 如果你在使用弹性缓存
进行内存转码或内存数据库操作
这最适合你的需求
我看到很多不同选项
我现在快速操作
登录 不是登录
让我进入AWS网站
在这里可以看到AWS提供的各种实例类型
大家可以看到 种类相当多
首先是通用型实例 在通用型中还有A1系列
T3系列
T2和M5系列
这个表格展示了T3系列内部的不同配置
A1系列的CPU配置
内存容量 存储和网络性能
这里简要说明了通用型的用途
计算优化型同样如此
在计算优化型中
有多个子类 还有内存优化型
适用于高强度内存应用
大型工作负载
海量数据集
加速计算
比如比特币挖矿需要加速计算
这类实例考虑了GPU图形处理单元
如果你需要存储优化
用于文件服务器或大型存储数据库
这里详细列出实例特性
各种实例特性都很丰富
它们还有突发性能实例
可以临时提升性能超越现有基础设施
这个网页再次展示
你们可以看到上面的网址
这是EC two实例类型
这显然不是你需要死记硬背的内容
除非你在准备
你知道的解决方案架构
架构师专业考试
那时你需要了解特定的e
C Two实例及其能力
但我总是用这个作为参考
我总是将这个网址收藏起来
每次我启动e
C Two实例 或者我在研究架构师或为组织设计架构
我总会来这里参考哪种实例最合适
根据 你知道组织的目标是什么
以及当前可用的资源
这是一个非常实用的参考网站值得记住
随着时间推移
你会逐渐记住哪些实例是内存优化型
哪些是计算优化型 等等
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/074_Udemy - Become an AWS Certified Data Engineer part1 p74 5. Launching your first instance.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到本节课，我们将学习如何启动我们的第一个实例
正如你们看到的 我已经导航到我的EC two控制台
这是EC two的核心仪表盘
两个实例 有几个地方可以启动新实例
我可以直接从这里启动新实例
或者如果我进入实例管理页面
也可以从这里启动实例
如果我点击下拉箭头
现在可以启动我的第一个实例
当我进入第一个界面时，看到的是AMI选项
我们将讨论亚马逊机器镜像（AMI）
这允许我们选择想要的操作系统
这些系统已预装在我们即将启动的机器上，我们有很多选择，比如
从Linux机器到红帽系统
再到微软系统
有些镜像已预装软件
例如深度学习
Ubuntu系统自带
这些系统预装了各种软件包
无需额外配置即可进行深度学习
等等 或者
带有预装SQL Server 16或17的Windows服务器
这取决于需求
能节省我们时间
因为这些是AWS预装的镜像
如果点击免费层选项
会过滤出仅免费层可用的镜像
其他镜像会产生费用
使用其他镜像时会被计费
因为部分软件不免费
如果我选择这个选项
列表将仅显示免费层可用的镜像
假设我要启动一个Windows
2019基础服务器
现在选择这个选项
这里是选择实例类型
正如你们看到的 这里有各种实例类型的列表
根据服务器用途选择
表格显示CPU信息
内存等参数
是否EBS存储或实例存储
是否支持EBS优化
网络性能如何
是否支持IPv6
选择实例后
会提供更多详细信息
你知道CPU的速度
什么类型的CPU
它是 内存等在免费层级中有两个可用选项
要么是t1.micro标准型
大多数时候我总是用t2.micro
它比t1.micro性能更强一些
但具体还要看你的使用需求
但如果我要选择
让我们选另一个
让我们看看选t2.large会怎样
配置会变化 这个有八个CPU
八个虚拟CPU
每个配备202.3GHz
3.2GHz主频
3.2GB内存
它告诉我CPU类型等信息，我们有多选项可选
再来看 这取决于服务器类型
以及你要在服务器上运行的应用程序
如果你在生产环境工作
所以我们在此配置中保持t2.micro
我们可以选择实例的各种信息
我们可以同时启动多个实例
可以直接加入自动扩展组
比如我要启动五个实例
因为它们属于自动扩展组
可以直接在这里操作
这里可以选择竞价实例
显示当前价格
适用于弗吉尼亚北部可用区
如果我要用竞价实例
这里可以设置竞价策略
设置有效时间从和至
如果需要在特定时间生效
或者持久化请求
假设我要执行
你知道的 夜间批量处理
确保每晚都有竞价实例
或当预算允许时
每当预算到位时
可以提交持久化请求
此请求保持有效
或设置特定时间生效
默认进入默认VPC网络
如果你有多个VPC
可以选择目标VPC
同样适用于子网选择
是否要自动分配它
默认分配一个公网IP
此功能已启用 因此默认情况下
每个实例都会分配公网IP
V4地址已分配
您可以默认禁用此功能
例如如果您想将其作为内部服务器
并将它保留在您的VPC内
那么将会是
这将是禁用此选项的原因
是否要将其加入放置组
这是一个服务器的逻辑分组
是否要创建容量预留
域加入目录
假设您有一个正在运行的活动目录
如果您想使用AWS单点登录，可以实现
您可以立即加入域
是否需要任何 我已附加了这些角色
E C 两个实例
我们稍后会详细讨论这些选项
更详细地解释它们的含义
以及为何需要
我已附加了这些角色
这是附加IAM角色到实例的位置
关机行为
这基本告知您希望服务器进入的状态
当执行关机操作时
是否要停止它
或是否要终止它
终止保护主要防止
正如名称所示防止意外终止
如果我勾选了此复选框
终止实例前需要额外步骤
是否要使用云监控
详细监控租用
这里可以选择专用主机或独立运行T2微实例
我无法使用专用
T2微实例的唯一选项是专用
您知道 在专用实例上运行
这是因为符合免费层级资格
因为它属于低配置硬件
这就是为何只有此选项
我也可启用图形加速
如果要为实例添加GPU
是否需要突发稳定性能
好的 但请注意，如果您使用GPU，将会产生额外费用
或者如果您需要突发性能
以及是否需要在服务器启动时运行脚本
假设这是一个Linux机器
并希望运行脚本下载和更新
您知道的Linux内核到最新版本
您可以在此服务器或这些服务器启动时运行脚本
这些是我们可以下一步选择的基本配置细节
这是存储 默认会有一个根卷
EBS后端根卷
我们可以选择不同类型
是否需要IOPS或磁性卷
IOPS显然可以调整
如果我们配置预置IOPS
则可以更改IOPS和通用SSD
我们没有这个选项
终止时删除
这意味着此EBS卷将在终止实例时被删除
当我终止此实例时
我可以取消勾选
如果我想保留此EBS卷
使其与实例独立存在
然后我还可以加密此EBS卷
如果需要的话
我可以再次添加标签用于个人参考
标签也会在生命周期管理中发挥作用
例如在快照操作时
换句话说
您的备份策略
因此需要在资源中添加标签
以便创建生命周期策略
可以准确备份正确的实例
在正确的时间点
我可以配置安全组
由于这是Windows机器
将开放RDP端口
然后我可以查看最终界面确认所有安装内容
然后可以继续启动
启动此机器需要选择密钥对
我可以新建或选择现有密钥对
如果已有则下载该密钥
但我们将选择现有密钥对启动实例
这就是在AWS中启动实例的基本流程
C Two在亚马逊 AWS
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/075_Udemy - Become an AWS Certified Data Engineer part1 p75 6. Working with launch templates.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好
欢迎来到这节课
在这节课中我们将学习如何从启动模板启动实例
C 从启动模板启动两个实例
现在你可以创建包含基本配置的启动模板
C 两个实例 而模板基本上允许你存储不同的参数
这样每次启动实例时无需重复指定
例如 你知道的
启动模板可以包含你的MIT ID
实例类型
任何通常用于启动实例的网络设置
当你使用e
C Two控制台 稍后会演示
或者命令行
你可以指定使用的启动模板
现在每个启动模板
可以创建一个或多个编号的启动模板版本
所以 假设最初你使用了特定硬件的实例类型
使用t2.micro
但几个月后你发现t2.micro硬件
已经不够用了
并希望将硬件升级为
例如 比如c1或m1
但你不想更改
启动模板中指定的其他配置
你可以创建新版本并指定不同的实例类型
同时保持其他配置参数不变
启动模板的一些限制
每个账户最多创建5000个启动模板
每个启动模板最多10000个版本，这可能很多
但对于全球拥有数千服务器的企业
这个数量可能不够
但必须确保启动实例请求包含所有必要参数
例如实例ID
使用的启动模板
实例类型 等等
因此需确保启动模板包含必要参数
接下来我将导航到我的e
C Two控制台
找到了，在e
C 左侧有两个仪表盘
我可以在实例下找到启动模板选项
所以我点击启动模板
这将带我到启动模板的主仪表盘
目前我没有创建任何模板
但如果你已经创建了
这里会显示你拥有的启动模板列表
我将创建一个新的启动模板
这里就是我之前提到的版本选项
我可以创建一个新的启动模板
或者创建一个新版本
如果我选择版本
我必须先选择一个模板
但目前我没有创建的模板
所以我将创建一个新的启动模板
如果是版本或特定描述
你也可以在这里指定
让我们看看是否有生产Web服务器
或财务服务器、文件服务器等
嗯 如果你想再次复制模板
我们也可以这样做，所以
假设我想复制一个已创建的模板
我已经创建并稍作修改
为了什么 我将说 另一个部门或办公室
我可以在这里指定
如果我创建了多个模板
我可以选择该模板
并将该模板的所有信息填充进来
然后我可以修改任何需要调整的信息
现在我可以指定启动模板的内容
AMI ID
我必须在这里指定
如果我知道AMI ID 可以直接填写
或者搜索AMI
我可以选择AWS Marketplace的AMI
或社区AMI
如果使用快速启动
它 允许我在AWS环境中选择基础镜像
例如Linux或Windows
所以我将选择Windows Server
2019基础版
我将再次选择这个AMI实例类型
我们需要确保
如果我们想要 我们可以包含一个实例类型
我们必须确保它与我们选择的AMI兼容
所以 只是为了确保我保持在免费套餐内
我将选择t2.micro实例
键盘 这将再次用于登录到e
C2实例
我已经创建了一个
但如果你没有的话也可以
你不一定需要在模板中包含它
但因为我已经创建了
我要包含密钥对
同时网络类型 我可以保留VPC或经典网络
经典模式会启动经典
E C2或VPC是E的新改进版本
C2 这些是基于块存储的
对于这些选项中的任何一项
如果你想获取更多信息
如果你 例如
忘记 你知道不同的实例类型
如果你将鼠标悬停在眼睛图标上
它会快速展示不同类型的概述
每个类型的具体含义
你也可以点击
了解更多 这里会打开AWS的文档
你可以查阅更多详细信息
关于VPC等相关内容
现在如果我选择VPC
我需要指定一个子网
如果选择经典模式
嗯
它不允许我选择子网
因为会启动E
C Two经典
所以我将继续使用VPC安全组
并关联一个或多个安全组到实例
因为我只有一个默认安全组
以及默认VPC
我只需关联默认安全组
你可以为E关联多个安全组
C 两个实例 请记住安全组内的所有规则都会被聚合
呃 在这个环境中
C 两个实例 如果你允许多个安全组
网络接口
我们可以为此设备指定最多两个网络接口
所以如果我添加一个网络接口
嗯 我可以指定不同的接口
嗯，同样最多两个为此设备
因此该设备基本上
你知道的 网络接口的设备编号
例如 以太网零
我们将使用的网络接口ID编号
或者对于AWS我可以留空
再次创建新网络接口描述
仅用于子网目的
创建新网络接口
对于主网络接口
这是实例启动的子网
例如 对于我们来说
我们选择了以太网零
实例将位于该网络接口所属的子网中
我们也可以自动分配IP地址
是否在该网络接口上启用
现在这只能为单个
新接口 我可以启用自动分配
主IP 为子网范围分配私有IP地址
我们可以留空
让AWS自动分配相同的次级IP地址
如果留空 AWS将根据子网掩码随机分配IP地址
安全组ID是VPC中的组ID
与我们的网络关联的组
终止时删除仅为特定设置
如果我们想要
例如 在实例终止时删除它或保留账户
我们可以在此指定并启用弹性适配器
表示该网络接口是否为弹性适配器
以及EFA或光纤适配器
本质上是一种可附加到E的网络设备
C 两个实例以加速高性能计算应用或HPC应用
因此我们可以实现与本地HPC集群相当的应用性能
因此具体取决于
我们这个e的使用场景是什么
C 两个实例 我们可以使用弹性网卡适配器以提升性能
我将保留所有默认设置，由AWS自动分配
基于我们的默认子网
这里我们可以配置存储卷
我们可以附加默认
我们的EBS卷
由于这是eb s打包的a c two实例
我们可以指定EBS卷的大小
类型是什么 是否选择gp2
是否需要预置IOPS或使用HDD
等等 如果我们进行
例如 预置IOPS
我们也可以指定所需的IOPS
我们将继续使用gp2
我们也可以设置终止时删除
是否希望实例终止时删除该卷
或者是否保留EBS卷
无论实例当前状态如何
显然如果要在实例或CBS卷上保留数据
需确保不选择终止时删除
是否需要加密，再者
使用您的CAM密钥加密卷
实例标签用于我们的用途
因此我可以添加一个标签
我可以为此命名
例如 我将其命名为c to win
我们可以为该实例添加标签
同时为卷添加相同标签
高级选项 我们提供一些高级配置选项
是否需要请求Spot实例
IAM实例配置文件
这是与实例关联的AWS配置文件
例如 如果这个e
C 两个实例需要登录数据库或应用
需附加IAM实例配置文件
以赋予访问数据库或应用的权限
无论是想要停止还是终止实例，当执行实例关机时
我们要不要终止它 还是仅仅关闭它
混合行为终止保护也是如此
但我们想启用或禁用这些差异
显然，如果频繁启用终止保护
终止前必须勾选额外选项
禁用此功能则可立即从仪表盘终止
监控是否要将这些详情加入云监控
嗯 请注意
如果您处于免费套餐
启用监控会产生额外费用
弹性
嗯图形 这再次是
GP 嗯单位
或图形处理器
但这些仅在特定实例类型中可用
这些类型会产生费用
弹性接口也是如此
T2和T3无限
即允许应用超出基准资源
按需使用
仅适用于T2
T3实例
即T2微实例
如果我有一个T2微实例
应用可能需要超出硬件资源
可启用此功能以临时扩容
此处可将其加入放置组
这是ECS实例的逻辑分组
C2实例
嗯 需在同一可用区
EBS优化
提供专用I/O容量，非所有实例类型支持
因此请注意所选实例类型
该功能可能不可用
选项可能不同
共享 专用
或专用主机
若指定内核
对于内存
可在此处设置内存参数
磁盘ID和内核ID
所有已有的许可证
让我们说 如果你正在安装SQL服务器
或者访问某个应用程序
你可以进行许可证配置和用户数据设置
这些都是在机器启动时需要运行的脚本
但这些都是所有选项
高级详细信息是可选的
所有其他详细信息在启动和创建启动模板时都是必需的
所以我现在创建启动模板
搞定 你们可以看到启动模板已成功创建
如果我点击关闭
它会带我回到启动模板仪表盘
在这里我可以看到这个启动模板易于使用
实际上创建时会生成模板ID
并且你创建了一个路由
因为我以用户身份登录
以及不同版本
所以现在我可以启动e
C 通过此启动两个实例
通过此启动模板
如果我选择此模板
我可以从该模板和其他模板启动实例
我可以创建实例集 自动扩展组
删除此模板版本
删除此模板等
或者甚至编辑此模板
如果我需要的话 一旦模板创建完成
从其中启动实例非常简单
只需在操作中选择从模板启动实例
现在我们还可以通过AWS CLI创建启动模板
即命令行界面
现在我已经登录到命令提示符
对于尚未安装AWS CLI的用户
可以直接从AWS官网下载
下载该MSI文件并安装
安装完成后打开命令提示符
然后登录AWS
命令是aws configure
执行后 进行登录
尝试登录 你需要AWS访问密钥ID和秘密密钥
为了获取这些信息
返回控制台
对的 现在前往我的控制台
当我进入控制台后
我会点击我的用户名
下拉箭头菜单 这里有一个安全凭证选项
点击安全凭证后
会弹出一个警告
如果你是以root用户登录
显然建议创建IAM用户
应始终通过IAM用户执行所有管理操作
而不是使用root账户
但为了演示目的
我正在使用root账户
我将继续前往安全凭证
这里可以获取密钥
点击访问密钥的下拉箭头
因为我还没有创建任何密钥
我将创建访问密钥
可以下载密钥文件备份
以防我忘记
我将展示访问密钥
这里是密钥ID
我将复制这个ID
粘贴到这里和秘密访问密钥
我将复制这个长访问密钥
请记住 一旦关闭此页面
将无法再次查看秘密访问密钥
这就是为什么我需要下载密钥文件
我确认已复制了这个
关于区域设置
我可以使用默认选项
即无区域 但我想指定区域
所以我需要
再次回到控制台
回到EC two实例页面
确保创建第二个启动模板
在之前创建的同一区域
第一个位于弗吉尼亚北部
美国东部输出格式
嗯 这取决于你的需求
我通常使用JSON格式
他来了 现在我们已通过命令行登录AWS控制台
接下来我们要做的是启动
通过CLI创建启动模板
这是命令行界面
所以使用aws EC two命令
现在我们要操作EC two的创建启动模板
这是创建启动模板的命令
这是长期运行亚马逊的模板名称
ECS支持大规模工作负载
这是模板版本的名称
我可以添加版本描述
这是开发和启动模板
这是数据文件 这是一个JSON文件
现在这个数据文件在这里
基本上列出了所有ECS实例需要指定的信息
把字体调大一点
这样就对了，这里我指定了网络接口
没错 所以镜像ID
这是相同的Windows Server 2019
这是我首次启动模板时使用的实例类型
是t2.micro资源类型
这是实例标签
我可以添加任意标签名称
看看服务器CPU选项
这就是我创建的ECS启动JSON基础内容
你可以根据需要细化这个JSON
所以指定完后我点击确认
搞定
现在我们的启动模板已创建
我总是喜欢展示错误情况
每当发生这种情况 正如你们看到的上方
提示无法连接到端点
Url acp s EC two us east
亚马逊aws.com
因为us east不是完整区域名称
正确名称是us-east
连字符一 这是我下方指定的内容
务必确保
如果不指定具体区域
将无法启动模板
因为模板必须在特定区域启动
ECS实例也必须在特定区域运行
尽管刚才连接正常
所以没有报错 当我指定缺少区域编号的名称时
仍能连接到AWS账户
提示无法启动ECS实例
仍能执行全局操作
例如用户管理或非区域特定的S3存储桶
但对于区域相关操作
必须指定正确区域
完成设置后 正如你们看到的，启动模板已成功
所以我回到我的e
C两个控制台，这里好了，现在我有新的e
C 两个实例也
呃 新的修改模板也创建了
这是我刚通过命令行创建的
我也可以运行另一个命令来验证我创建的启动模板
所以这里我可以执行描述启动模板版本和启动模板名称
这是运行亚马逊
E C 两个工作负载规模化
如果我运行这个命令
它会给我刚创建的启动模板的详细信息
你可以看到它提供了整个启动模板的完整分解
它包含的所有内容
在资源方面
子网等
现在我也能为第一个启动模板执行相同操作
之前那里好了，所以这里
它给出了EC two启动模板的全部信息
关于我创建启动模板时指定的所有具体参数
所以同样，我们在控制台中能做的所有操作
现在也能通过命令行界面快速完成
如果我想创建 比如为这个EC two启动创建第二个版本
并指定不同的实例类型或子网
如果我点击
创建启动模板
我要做的是创建新版本
并且 因为我现在有两个启动模板
我可以指定哪个启动模板
要创建新版本
所以 如果我再次指定EC two启动
它会显示源模板
呃 默认只有一个版本
所以这里我可以将版本描述设为版本二
我的想法是
比如对于这个
我想将实例类型从T2微型改为
比如T2纳型所有内容
我想继续保留相同
我将创建启动模板
好了，当我关闭时
你们可以看到这里不会显示任何内容
但如果我们点击版本
我们可以看到这里有此启动模板的两个版本可供使用
我们可以看到最新版本
默认版本是1 最新版本是2
这就是版本控制通常的工作方式
对于需要创建大量启动模板的大型企业组织来说通常更方便
但假设他们只想修改启动模板中的某个特定内容
而不是全部重新创建
他们可以创建版本
此外这也便于审计追踪 只需记录当前状态和位置即可
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/076_Udemy - Become an AWS Certified Data Engineer part1 p76 7. Launching an instance from an existing instance.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到本次关于使用现有实例参数启动实例的课程
所以e C EC two基本上提供了一个非常便捷的功能
在这里你可以启动类似的向导
这使你能够基于已有的当前实例进行实例启动
已经运行的实例
此选项基本上会自动填充e
C EC two实例启动向导中的部分配置细节
现在在实际操作前需要注意几点事项
以下配置将从选定实例复制到启动向导
所以AMI ID
实例类型
可用区
IP 地址放置组
AMI 关于关机的行为
终止操作 监控存储倾向
等等需要特别注意的几点
这些不会从选定实例复制的内容
基本上向导会应用
默认设置是网络接口数量
默认是一个网络接口
即以太网零
主接口
默认存储配置由AMI和实例类型决定
因为不同实例类型支持不同的存储介质
所以这两项不会被复制到
嗯 例如 我们指定的新实例启动
使用此向导时需注意这些事项
所以如何 如何使用这个向导
操作非常简单 如果返回管理控制台
我将进入我的e
C2控制台
这里了 所以实例大家可以看到
我有一个正在运行的实例
实例名称为Web服务器
这是一个Windows 服务器2000
位于美东一区d可用区
使用t2.micro实例类型
假设这是运行中的服务器
我只是想为新办公室复制服务器
为新部门使用
我希望保持配置相同
所以我选中了服务器
然后点击操作
这里有“启动类似实例”的选项
所以我点击“启动类似实例”
系统会直接跳转到EC two启动向导
无需再经历顶部显示的所有步骤
直接跳转到审核页面
我可以审核所有配置并点击启动
这样大大节省了启动新实例的时间和精力
所以我点击启动
系统会确保我已验证
密钥对 我只需确认已拥有密钥对
然后点击启动
搞定 如果查看实例列表
现在可以看到有两个实例
正在运行 一个处于待机状态
一个在同一可用区运行
相同的实例类型
名称标签也相同 这让我能快速轻松地启动多个实例
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/077_Udemy - Become an AWS Certified Data Engineer part1 p77 8. Instance states.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


嗨 大家好 欢迎来到这个教程，我们将探讨当e发生时会发生什么
C 处于不同状态的两个实例
我们已经学习了理论部分
嗯 你知道在e的不同状态下
C 当实例被配置时启动和停止的状态变化
冬眠等等
所以我真正想做的是展示给大家看
你们知道实际上在实际启动时是如何运作的
C 两个实例
大家可以看到我已经登录了我的AWS控制台
这是e C 两个主控制台
我现在有一个实例正在运行
正如你可以在这里看到的
这是一个t两微实例
这是一个Windows服务器
这是我启动的
这是一个基础启动配置
这个没有任何自定义设置
你知道它有公网IPv4地址
它还有私有IP地址
并且有公网DNS等信息
这就是标准的t两微Windows实例
二零一六年我启动的服务器已进入我的EC
两个控制台
正如大家可以看到的状态
目前处于运行状态
这基本上展示了实例经历的不同状态
作为复习
你知道当你停止一个实例时
这是我们首先要做的
会发生很多事情
你知道它会执行正常的关机操作
你知道吗 如果你有一个本地服务器并只是启动和关闭
这实际上就是停止实例时发生的情况
我们附加到此实例的EBS卷
正如大家可以看到，我切换到左侧的卷管理
大家可以看到有一个卷附加到此实例
大家可以看到这是一个30GB的卷
它连接到我的E
C 两个实例 它正在使用中
所以这会保持在
这会保持完整
数据存储在主机计算机的内存中
呃 或者存储在磁盘卷中的数据将会丢失
大多数情况下会迁移到新主机
一台新主机 一台新物理计算机
这个虚拟机基本上百分之九十
百分之九十九的时间
AWS会将其迁移到新主机
通常关机时
它 会通知AWS出现问题
有东西无法正常工作
嗯 在物理机或虚拟机内部
为了防止重启后出现新问题
他们会将虚拟机物理迁移至新主机
再次说明
我们不确定是否会发生
当这种情况发生时
但这通常在后台进行
将保留其IP
IPv4地址和任何IPv6地址
但会释放公共IPv4地址并分配新地址
你们可以看到当前公共IPv4地址是34.20.4.106
所以我现在要快速
我只需将其复制到记事本
以便观察
嗯
当重启时 公共IPv4地址会发生什么变化
将保留弹性IP地址等
所以我现在要进入仪表盘
确保在操作中选中cc2
再次在实例状态中
我有多个选项
停止、休眠等
再次选择停止、休眠
这是这个实例
未启用启动时
意味着 我没有选择休眠选项
在启动EC two时
我可以重启它
或者终止它
再次终止 彻底移除
从我的控制台移除它
我想做的只是简单停止它
再次发出警告
任何持久化存储
实例存储将被丢失
内存将被丢失等等
我将确认并点击停止
现在它正在更改状态
现在它并未停止，仍在停止中
现在我们正在物理关机
可以看到实例已停止
你们也可以看到公网IPv4地址已消失
但私有IP地址和私有DNS仍存在
可以看到实例状态已变为停止
为了重启它
我返回操作选项
选择实例状态
我可以再次终止它
如果我想彻底删除
或者再次启动它
只需再次确认
如果我要启动它
如果点击确认
它将继续
更改状态 当前状态为待处理
因为它从停止状态转为启动状态
现在实例状态已从停止转为运行
你们可以看到公网
IPv4地址是34.20.12.209.9
如果我再次确认
你们可以看到已分配新公网IPv4地址
这些细节需注意
在状态变更时
现在你们可以看到
仍无法连接
因为它在进行状态检查
但已处于运行状态
每次启动时
都会重启
进行初始配置确保正常运行
即可正常使用
我们也可以通过命令行操作
打开命令行界面
登录AWS
AWS CLI的优点是
登录后
因为我之前已登录过
会记住访问ID和密钥
只需按回车即可
它会记住该区域，因此也会记住我之前的每一个选择
所以并不是说 我需要每次都输入核心概念和密钥
每次我只需操作一次，下次只需点击回车即可
然后它会自动登录
现在如果我想重复相同操作
比如说 如果我想通过控制台启动或停止实例
我可以执行相同步骤
这就是对应的命令
如果我要停止AWS实例
s e c two stop instances instance ids
这就是实例ID
如果我进入控制台
大家可以看到我刚从这里获取了实例ID
所以我直接复制了
从控制台复制的实例ID并粘贴到这里
当我按下回车后，系统会执行
它会完成与控制台相同的操作
大家可以看到实例正在停止中
状态显示为停止中...
它还能回退到之前的状态
实例从运行状态进入停止状态
如果我返回控制台
只需点击刷新
哦，找到了
目前实例处于停止状态
正如我之前所说，所有控制台能做的操作
都可以通过命令行界面完成
我们最后的选项是
嗯 在状态中再次
终止它
将其从仪表盘中移除
接下来查看卷的情况
大家可以看到该卷
无论怎样 停止后仍保持完整
EBS卷不受影响
这也是他们一直建议
不要使用实例存储卷
应始终使用EBS支持的实例
确保数据持久化 无论实例处于何种状态
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/078_Udemy - Become an AWS Certified Data Engineer part1 p78 9. Connecting to a Windows Instance.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到关于使用EC two实例的课程
现在我们已经在AWS中启动了一台Windows Server 2019
作为EC two实例
我们该如何连接并开始使用它
正如大家看到的 我在AWS中启动了一台Win服务器
对于EC two实例
我使用的AMI是Windows Server 2019
如果我要继续连接
当我选择该服务器时 再次
这适用于任何想要连接的服务器
这里有一个立即连接选项
针对Windows服务器
此选项会显示
如果你使用Linux系统 会出现不同的选项
因为对于Linux机器
显然需要SSH连接
对于Windows机器
我们可以使用RDP连接
我们可以这样做
下载此RDP文件
AWS 使我们非常容易连接Windows
2000或Windows服务器
我将保存这个RDP文件
当我打开此文件时
系统会确认我要连接
我将继续连接
现在会提示我输入密码
如果回到控制台
我可以选择获取密码
这就是密钥的作用
这里需要使用你下载的密钥
或在启动时指定生成的密钥
在启动EC two实例时
即使是Windows实例
仍需要该密钥生成密码
我将查找我的密钥
找到后 我将其放入保管箱
系统会生成一个很长的哈希值
接下来我需要点击解密密码
这就是我需要输入的RDP密码
现在我已经成功登录EC two实例
我的Windows 2000服务器
AWS US东区的EC two实例
所在区域 位于弗吉尼亚北部
现在基本上是在后台进行所有设置
现在我可以继续配置我的服务器
按照我自己的方式和需求
仅作为参考，每个e
C 您启动的两个实例 将在后台屏幕上显示
所有主机名信息
ID 公网IP
私有IP 以及可用区
等等 仅作为参考
连接到Linux机器略有不同
如果您使用Windows电脑工作
或在Mac或Linux机器上工作
因为您还需要SSH登录Linux机器
如果您要在Linux服务器上工作
但如果您在Windows环境中工作
大多数企业组织都是这样
但再次强调 并非全部
但如果您在Windows环境中工作 这就是启动并连接到基于Windows实例的简易流程
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/079_Udemy - Become an AWS Certified Data Engineer part1 p79 10. Connecting to a Linux Instance.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


嗨 大家好 欢迎来到这节课
在这节课中我们将学习如何连接到我们的Linux机器
刚才你们已经看到了如何连接到Windows机器
我们可以使用远程桌面协议（RDP）连接
而对于Linux机器
我们需要通过SSH登录
这取决于你使用的操作系统
如果你在Windows机器上工作
或者如果你有Mac或Linux机器
或者你使用的笔记本电脑
C 两个实例
如果你有Mac
或者如果你有Linux 可以直接通过SSH登录
具体操作方法是
如果我选中这个机器E
C两个实例想要连接
然后点击连接
这里会显示连接该机器的详细信息
我有多个选项
可以再次使用独立SSH客户端
在Mac或Linux机器上
可以直接SSH登录
或者执行EC two个实例连接命令
可以直接连接到这个E
C 两个实例通过浏览器或浏览器中的Java SSH客户端
同样需要Java支持
此时会提示Chrome不再支持插件
这有点麻烦
前两种选项更常用
我认为最常用的是第一个选项
独立SSH客户端
我们将同时介绍这两种方法
标准SSH客户端
如果你在Mac或Linux系统
只需使用此命令SSH登录
如果你在Windows机器上
需要额外步骤
因为需要特定软件才能SSH连接
Windows默认不支持SSH
因此需要下载名为PuTTY的工具
下载PuTTY后
首先需要完成几项操作
打开PuTTY Gen
该程序会同时下载
当我打开PuTTY Gen时
我需要做的就是
我必须更改为此机器下载的密钥的扩展名
所以我要做的是加载并找到已下载的密钥对
请记住，如果这里看不到密钥请重新下载
确保在此处选择所有文件
然后你可以看到
然后你就能看到.pem扩展名
这是从AWS下载的密钥对
我将这个加载到PuTTY生成器中
密钥生成器 我需要做的是保存私钥
你可以可选添加密码以增强安全性
但我就留空不填
我将命名为同名
这里会保存为.ppk文件
PuTTY要求密钥对为.ppk格式
与.pem格式不同
这是从AWS下载的格式
当我保存时 我可以关闭此窗口
现在打开PuTTY程序
现在主机名或IP地址
只需复制加号后的所有内容
粘贴到这里
还有一些其他步骤需要完成
我需要进入数据并填写用户名
加号前的内容是用户名
所以EC two-user
最后在SSH设置中
在认证部分
这里需要加载ppk文件
我将浏览找到ppk文件
唯一选项是选择ppk并加载
每次登录此机器都需要重复此操作
为了简化流程
可以保存此配置
我可以保存这个设置
下次只需双击并点击打开
现在我已成功登录EC two机器
搞定 我现在已登录EC two机器
这里可以验证IP地址
例如172.31.92.201和此处相同
173.172.31.92.201
这就是登录或SSH到Linux机器的方法
在Windows电脑上
如果退出这里
现在 另一种连接方式是通过浏览器
再次使用EC two实例连接
用户名是e c two dash user，然后我点击连接
这会打开另一个浏览器，通过浏览器实质上ssh登录到机器
这种方式更加简单
但可能并非所有人都喜欢
有些人更喜欢ssh命令行
这就是他们这样操作的方式
但这也是一个选项
并连接到我的e
C Two实例 这比之前的方法更容易很多
如果你在Windows机器上使用putty方式
但这可能不是你的选择 尤其是如果你在mac或linux机器上
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/080_Udemy - Become an AWS Certified Data Engineer part1 p80 11. Configuring your instance.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你好 大家好，欢迎回来
在这节课中 我们将探讨配置窗口的不同方法
E C 两个实例
正如你们看到的 我已经登录了我的远程桌面会话
E C 我们之前启动的两个实例
这是Windows Server 2016系统
E C AWS上的两个实例
从右上角可以看到
显示主机名
实例ID
IP地址
这是一个T2微实例
区域等信息
接下来我们要探讨的内容
首先是EC two启动配置
EC two启动配置是一组Windows PowerShell脚本
这些脚本替代了之前称为EC two配置的工具
E C
两个实例
E C 两个实例
它执行多项任务
如果你了解基本操作的话
可以修改计算机名称
更改壁纸
向控制台发送实例信息
发送RDP证书
指纹 为管理员账户生成随机密码
DNS 后缀等
接下来我们要做的是
我将打开PowerShell
这个脚本的主要功能是
验证已安装的EC two启动版本
E C
两个实例
当前Windows Server 2016版本为1.1.1
三点二
现在e C 两个启动程序基本上安装在Windows 2000的程序数据区域
启动目录
如果我们导航到启动目录
在程序数据中我们会进入亚马逊
会进入e C盘Windows
我们点击启动
这基本上是e
C盘配置文件在设置中的位置
启动目录包含多个内容对吧
首先有一个名为脚本的文件夹
包含组成EC启动的所有部分脚本
如果我双击它
这些都是实际构成EC启动的脚本
模块包含构建AWS相关脚本的模块
E C盘
然后是配置文件
包含脚本配置文件
这样我们可以自定义
如果需要事件日志启动配置和驱动器映射
然后是is prep
包含显然如名称所示的设置预配置资源
包含CIS预配置GUI的应用程序
如果需要执行预配置GUI基础
可以使用这个 然后是日志文件，显然包含脚本生成的日志
如果有的话
所以这个 这就是EC启动的主要仓库及其所有内容，现在
实例初始化后
我们可以首次配置EC启动重新运行
并执行不同启动任务
可以在launch_config.json中指定设置
这里是launch_config.json
如果我们打开它 可以指定不同内容
可以设置计算机名称
可以设置壁纸
可以添加DNS后缀
可以扩展启动卷
可以设置管理员密码
比如 如果我更改
如果我想将计算机名称改为Caso
我只需修改这个变量
然后保存launch_config.json文件
现在我可以做的就是基本上在部分
我可以使用这个命令运行
我可以安排脚本作为Windows计划任务运行
脚本将在下次启动时运行一次
然后禁用这些任务再次运行
下次我重启机器时
我的电脑应该被重命名为cassim
我们现在也可以安排EC启动在每次启动时运行
而不仅仅是初始启动
要启用EC启动在每次启动时运行
这里我所说的是将按每次启动计划
意味着每次启动都会运行
这里确认每次即时启动都会运行此命令
或者我们也可以运行可执行文件
例如 如果我们想立即运行
我们可以发出立即运行的命令
而不是使用参数
例如 我们有一个schedule参数和每次启动参数
如果没有参数
现在应立即运行可执行文件
需要注意的一点
现在 当启用EC启动在每次启动时
以下更改将应用于EC启动配置
json配置文件
管理员密码类型将恢复为无操作
处理用户数据将恢复为默认
只需注意几个小细节
嗯 如果每次启动都运行此操作
还有一些其他事项
我想向您展示
我们还可以做的是 可以在事件日志配置中指定设置
json配置文件发送到Windows
事件日志到EC two
到EC two控制台日志
所以 现在我打开这个文件
我们可以配置以下设置
所以这里 基本上会发送所有系统日志
系统日志到E
C Two控制台
完成操作后
我将保存此文件
然后返回PowerShell
我基本上运行以下命令
因此系统会将脚本安排为计划任务
每次系统启动时
我们之前对脚本所做的操作相同
接下来我们要用同样的方法处理日志脚本，就是这样
所以 现在它也显示已安排计划
E C 两个配置服务也可以在每次启动后向E发送Windows已就绪消息
C 两个控制台发送消息
因此E C 两个启动程序基本上仅在初始启动后发送此消息
为了与C两个配置保持向后兼容
可以安排启动程序在每次启动后发送此消息
而不仅仅是初始启动
因此我们可以使用PowerShell
如果我们启动一个脚本
即发送Windows已就绪的.ps1并安排此任务
就像之前两个步骤一样
它将在每次系统
启动时运行此操作或向E发送消息
C 两个控制台 表明Windows已就绪
这些都是我们可以进行的一些配置
并使用EC启动命令进行配置
请注意修改这些文件是可选的
这不是必须的操作
但对于自定义设置，您可以使用CIS预配置工具完成 同样
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/081_Udemy - Become an AWS Certified Data Engineer part1 p81 12. Creating and sharing AMIs.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到关于查看亚马逊机器镜像的课程
这里有各种不同的镜像
在启动实例时非常有用
保持实例配置
对于日常
各项活动
亚马逊机器镜像非常实用
无论是共享镜像
无论是AWS提供的镜像
还是您自己上传的自定义镜像
现在让我们看看AWS提供的各种镜像类型
以及我们可以使用的那些
接下来我要做的是
我将前往EC two控制台
在左侧
我们有镜像选项
这是EC two实例的主控制台
用于亚马逊机器镜像
存在多种不同的亚马逊机器镜像
最常见的共享AMI，共享AMI本质上是一个AMI
由开发者创建并可供其他开发者或用户使用
开始使用EC two最简单的方式之一
C Two是使用包含所需组件的共享AMI并添加自定义内容
我们之前启动的所有实例
都在使用共享AMI
需要注意的是
尤其是使用共享AMI需自行承担风险
现在 AWS无法保证其他用户共享的AMI的完整性和安全性
E C Two用户现在
这并不适用于AWS为我们创建的AMI
那些是安全的 因为显然由AWS提供
但共享AMI是由其他用户上传到AMI控制台的
如果您使用这些
您是在自行承担风险
现在 如何找到共享AMI 如您所见
当前顶部没有AMI
这是用于筛选不同AMI的过滤器
如果我点击下拉箭头
我有三个选项
我拥有的镜像
我创建并上传或在AWS环境中创建的
还有其他人共享的公共镜像和私有镜像
由其他用户 E C 两个用户或通过其他账户
或者通过组织内的其他管理员
这又取决于组织的设置方式
如果存在不同的AWS账户
根据地理位置等在组织内划分
如果我点击公共镜像
这些都是包含
在AWS环境中并可供我们使用的镜像
我们还可以进一步通过
例如实例类型
或i ID
i名称
或甚至所有者
我们可以使用亚马逊镜像
如果所有者是亚马逊镜像
这些基本上由AWS开发
显然它们是
您可以放心它们是安全的
并且没有
引号 与之相关的风险
因为亚马逊现在是所有者
如果我移除这个
假设我将所有者设为AWS Marketplace
现在 这些可能是其他AWS用户创建的镜像
同样地 如果您使用这些
需自行承担风险
我们可以使用的镜像数量并进行筛选
假设我知道想要哪种镜像
我可以按名称搜索
按架构或创建日期
比如我要最新的镜像等
这是一种很好的方式来启动实例
通过减少配置步骤
我现在当然也可以在命令行中执行相同操作
在命令行界面中
我将打开命令提示符
已启动 我通过CLI登录AWS控制台
我将运行一个列出所有公共镜像的命令
此命令主要用于列出
AWS中提供的所有镜像
如果运行此命令
将列出所有可用镜像
或可供我用于启动实例的镜像
E C 现在有两个实例
我做一个更改 假设不是用户
全部 如果我将其改为用户自身
这将是属于我的镜像
或者如果我将其改为
假设所有者是亚马逊
则会列出所有列表
或所有由亚马逊拥有的AI
因此有多种方式可以列出我们的AI，再次
正如我所说，所有通过控制台能做的事
都可以通过命令行界面完成
在可能的情况下，我会同时提供两个示例
所以如果我返回AWS控制台
正如我说的，这些都是所有类似我所说的
这些都是我们可用的所有AI
所以如果我要去
如果我想选择其中一个并启动实例
我可以直接选择所需的AMI
然后启动实例
另一种方法是
如果我们进入实例
并启动一个实例
这些都是AI，再次这些是
所有由AWS提供的AI
如果我点击免费层
仅显示符合免费层资格的AI
无需付费 共有十九个AI
我可以从我的列表中选择任意一个这些列表包含
这些是您已上传或在AWS环境中创建的
属于我或共享给我的
并根据不同的过滤条件
如果您有多个AMI
这是AWS市场
市场允许您购买云上运行的软件
由第三方供应商提供
例如再次
例如看到Juniper
Barracuda等
这里有大量可使用的软件和配置选项
并从该市场购买
本质上基于
了解您的组织目标
或服务器的任务
或将要执行的任务 无论是CRM、电子商务应用栈
等等，您也可以从这里安装WordPress
如果要开展电子商务
网站还是Joomla
所以选择市场平台而非自己操作
你知道的 上传软件还是购买光盘
传统上你会这样做
当你购买软件时
他们会寄来一张光盘
将光盘放入服务器
现在你无法亲临服务器前
这是购买或配置软件的最佳地点
你希望在服务器上配置的软件
作为即将启动的
E C 社区中的两个实例
这些都是用户
亚马逊世界的其他用户创建的
如果我们滚动
稍微向上滚动
快速启动是亚马逊创建的
社区镜像由其他用户创建，所以一个
如果你使用这些
请记住
使用时自行承担风险
所以要确保
部署后使用时
先运行一些测试
确认没有预装凭证
没有后门等
嗯 但亚马逊通常会做基本检查
确保亚马逊机器镜像无恶意软件
上传到市场后我们可以进一步筛选
例如架构
操作系统等
再次说明
AWS环境中有很多使用选项
假设我要创建自己的镜像
如你们所见 我有一台已运行的服务器
假设这是我的e
C 两个实例 并安装了应用程序
进行了配置
按照我希望未来服务配置的方式
补丁更新
网络配置等
并创建镜像以便下次或未来使用
当我启动一个新实例时
例如 如果我要创建一个自动扩展组
它必须有一个启动新计算机的镜像
当它扩展或缩减时
所以我想要创建一个镜像
如果这是我要创建镜像的服务器
我只需选中它
在操作选项中有一个镜像选项
在这里我只需创建镜像
这里允许我给镜像命名
我将其命名为测试镜像
你可以为其添加描述
在这里 还有一个无需重启的选项
无需重启 它实际上创建了一个实时镜像
它不会关闭机器来创建镜像
它创建了镜像
当机器正在运行时
请注意这可能对某些文件系统产生影响
或某些正在运行的应用程序
由于它创建的是实时镜像
在对机器进行镜像时
建议先关闭机器 或确保文件系统无更改
请记住这一点
我将继续保持选择
因为我在这台机器上没有创建任何内容
或未安装任何软件
这就是即将创建镜像的默认卷
我可以选择在此镜像中添加卷
比如在镜像中
我想添加一个额外的卷
当这个镜像在新服务器上启动时
将会有两个硬盘或两个EBS卷而非一个
我可以继续修改此设置
此外我也可以选择升级镜像的硬盘
如果我发现未来服务器需要
我需要配置
IOPS 我可以同时升级硬盘
我将保持默认设置并创建镜像
现在镜像已创建完成
如果我进入AMI
并过滤我拥有的镜像
大家可以看到这个镜像
这个测试镜像正在创建中
它处于待处理状态 因为它正在对机器进行镜像
一旦完成
它将会 现在将变为可用状态
假设我想将此镜像对亚马逊其他用户开放
就像你们在市场中看到的
在社区镜像中
这些是其他用户与社区共享的镜像
供他们使用 所以假设我也想将此镜像对社区开放
在镜像过程中 在待处理期间
我可以展示给你们看
如果您点击操作 有一个修改镜像权限的选项
所以我修改镜像权限
我有多个选项
此镜像目前是私有的
因为只有我知道
所以假设在我的组织中，基于地理区域有多个AWS账户
假设我们的美国办公室有独立的AWS账户
我们的办公室有独立AWS账户，依此类推
因此我可以创建镜像并分享给其他AWS账户
从这个角度来看 如果我们的南办公室有另一个AWS账户
我只需分享此镜像并在此处输入该账户号
这样它也会在那里显示
这样我们也可以在管理上保持独立
保持管理分离
同时集中某些操作
因为显然仍属于同一组织
这在跨企业地理分散的组织中很常见
在管理上使用独立AWS账户
在预算等方面
这样仍可保持某些事项集中
然后其他事项可以分散管理
或者我可以直接设为公开
这意味着亚马逊世界的任何人都能查看和使用此AMI
这些是我可为镜像选择的三种主要权限
要么保持私有
共享特定AWS账户 或直接设为公开
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/082_Udemy - Become an AWS Certified Data Engineer part1 p82 13. Copying AMIs.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎回来
在这节课中，我们将讨论复制亚马逊机器镜像
或者在AWS区域内部或跨区域通过管理控制台进行操作
复制源AMI基本上会产生
正如名称所暗示的，一个完全相同但独立的目标AMI，拥有自己唯一的标识符
可以说这是一个副本
但显然其唯一标识符在
比如 基于EBS的AMI
每个支持的快照默认会
被复制到完全相同但独立的目标快照
复制过程本身无需费用
但需注意EBS卷的标准存储和数据传输费用
或当您跨区域传输这些镜像时
所以在复制时
有几个要点需要注意
首要的是权限问题
如果您使用IAM用户复制基于实例存储的AMI
该用户必须具备相应权限
正如大家在示例策略中看到的
此策略允许用户在指定区域和存储桶中复制AMI源
正如大家看到的
效果是允许执行S3存储桶的列表操作
这赋予了他们访问S3存储桶的权限
接下来是获取对象的权限
没错 源存储桶
然后最后一个效果是允许创建存储桶
获取存储桶并上传对象到存储桶
简而言之 换句话说
S3的读写权限
允许他们在该存储桶内进行读写操作
然后是ARN（亚马逊资源名称）
这些权限确保使用IAM用户时
再次强调，始终建议使用IAM用户
在使用AWS时
务必确保该用户拥有复制所需的特定权限
如果要复制到S3存储桶
需确保他们拥有这些存储桶的读写权限
跨地理区域复制AMI能带来多项优势
所以 例如
若需实现全球部署的一致性
当您的办公室遍布全球时
需确保所有部署
无论地理位置如何
都符合组织政策和流程
然后是扩展性
可更轻松地在中心设计和构建全球应用
然后是性能和高可用性
显然可以通过分布应用程序来提升性能
你知道的 同时将应用程序的关键组件部署在靠近用户的区域
然后确保它们的高可用性
通过跨多个区域设计和部署来提高可用性
大家看到的图表展示了源与I和两个副本之间的关系
以及E和C
在不同区域
C 每个区域启动的两个实例
当你启动E
C 两个实例时
显然会驻留在与AMI相同的区域
如果你修改源
AMI并希望更改在目标区域的实例中生效
需要将源AMI重新复制到目标区域
因为E C 两个实例不会跨区域
AMI也不会跨区域
这一点需要记住
显然如果你集中管理所有内容
你可能会认为
或者你可能认为如果修改
你知道的 你的原始
AMI 更改会传播到全球各地
所有已复制实例的位置
但这并非如此
表格底部还支持加密选项
大家看到的
你知道的 展示了各种AMI复制场景
例如
你有 你知道的 在第一个区域是加密到未加密
这是支持的 所有情况都支持除了加密到未加密
对的 不能从加密转为非加密
在复制方面
但你可以反向操作
默认情况下不指定参数时
如果不指定加密参数
AMI的底层快照会保留原始加密状态复制
复制由未加密快照支持的AMI
生成与目标快照完全一致的结果
这正是你猜对了，同样未加密
如果源数据是加密的
如果源数据已加密
那么目标也将被加密
所以我们需要记住这些场景
如果加密将在组织中发挥关键作用
你们看到的右上角场景
所以我回到购买未加密根快照
被复制到带有加密根快照的实例
所以复制镜像操作正确
所以将要复制的是 镜像调用包含两个加密参数
包括客户主密钥CMK
因此结果是 根快照的加密状态发生变化
目标AMI基于包含与源相同数据的根快照
但已加密
使用指定的密钥
也就是CMK对吧
请记住您还将产生
你知道的 两个快照的存储费用
以及从任一实例启动的计费
所以虽然复制有其优势
但也存在一些限制
没错 这些限制并不严重
但确实存在需要注意的限制 尤其是如果要集中管理所有资源
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/083_Udemy - Become an AWS Certified Data Engineer part1 p83 14. Working with AMIs.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎回来
所以在本次实验中 我们将要做的就是
我们将快速查看复制操作
大家可以看到我
我仍在使用之前创建的AMI
大家可以看到它现在可用
那如果我想在这里复制这个
如果我有这个
我是否在操作中被选中
我有多个选项
我可以对这个AMI做注销操作
我可以 或者我可以注销它
我可以注册新的 这个AMI我可以从此启动实例
我可以使用这个发起竞价请求
我可以修改权限或标签
或者修改启动卷设置
这就是复制中的EBS
一旦我执行这个操作
会弹出一个相对简单的对话框
我可以指定要复制的操作
所以目标区域
我可以指定要复制到的区域
或者复制到这里
我现在在弗吉尼亚北部
也就是美国东部
所以从这里 假设
如果我在孟买有办公室
或者我在新加坡有办公室
我可以选择将此复制到亚太新加坡区域
名称我可以保持不变
大多数情况下你希望保持名称不变
因为要确保服务器区分度
保持服务器之间的区别
如果你有多个描述
这里再次提到的加密设置
如果要加密目标EBS卷
如果我要进行加密
显然需要指定主密钥
选择默认密钥或特定主密钥
这里显示了加密的具体密钥信息
如果我不勾选此选项
这里会提示将被复制到新AMI
这些是新AMI的设置
如果我点击复制，操作已完成
如果我点击完成
这样我们就完成了 正如你们看到的，我已经切换到新加坡区域
在这里当前镜像状态为待处理
因为它正在复制中
正如你们看到的，我这里有
这里只能看到一个镜像
对的 因为镜像具有区域专属性
所以我只能看到该区域内的镜像
所以在新加坡 我看到一个
而在弗吉尼亚北部 我也看到了原始镜像
我将快速执行取消注册
我不想让镜像在AWS中漂浮
当我返回原始区域
也就是美国东部
我的原始AMI
我应该还能看到它，这样对了，所以原始AMI
它还在那里 这就是基本流程
AWS内AMI的复制过程
但确实
在功能上非常完善
帮助集中管理所有资源
并赋予您更大的控制权和组织政策流程的合规性 特别是当组织地理分布广泛时
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/084_Udemy - Become an AWS Certified Data Engineer part1 p84 15. Monitoring an EC2 Instance.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到关于如何监控我们的e课程
C 两个实例
监控是IT职责中非常重要的一部分
尤其是在可靠性和可用性方面
还有性能方面
你应该从AWS解决方案的所有部分收集监控数据
这样你可以更容易
你知道的 了解当前状况并更好地进行调试
如果出现故障等情况
但在开始监控e之前
C 两个实例 有几个需要注意的事项
尤其是在制定计划时
因为制定监控计划总是有益且推荐的
在实施e的监控时
C 两个实例
以下是开始监控和制定计划前需要自问的问题
监控的目标是什么
在监控e方面你希望实现什么
C 两个实例，你是否想查看高CPU利用率
是否只是观察e的整体健康状况
C 两个实例 监控的最终目标是什么
为什么要进行监控
是否真的需要监控e
C 两个实例
在你确定目标后
定义你的目标 嗯
下一步是 你要监控哪些资源
你可以监控的各种资源包括
甚至在e内部 C 两个实例
我们可以监控e c的多个变量
两个实例的各个变量
在定义目标后
这将帮助你确定实际要监控的资源
在确定资源后
然后你可以决定监控频率
因为我们可以定期监控
每天进行监控
每周为基础
每月为基础
同时也可以按分钟为基础
对吧 所以如果我们能够
如果我们实施详细的e监控
C 两个实例 然后我们可以监控我们的实例
它将每分钟提供我们的e利用率统计
在定义好所有资源后大概两个实例
那时我们才能选择使用哪些监控工具
以及可能用于监控的工具
E C 两个实例能否仅通过e监控
C 两个仪表盘 或者是否需要使用云云监控
是否需要设置云监控告警
是否需要使用云追踪
这可以在确定并定义要监控的资源后定义
以及我们实际需要监控的数据粒度
然后谁来执行这些监控任务
你知道是IT经理负责吗
是用户主动监控还是需要
也就是说 是否需要有人登录e控制台监控
C 两个控制台进行监控
还是希望通过异常通知
比如出现高CPU利用率时
或实例故障时
是否需要接收通知
因此需明确定义这些任务的责任
然后这与上一条紧密相关
通知机制
如果实例发生故障
应通知谁
你知道的 是否要通知IT负责人
是否要通知实际
负责故障恢复的人员
所以再次 这取决于组织的政策和流程
但在实施任何监控前必须回答这些问题
C 两个实例
甚至更普遍地
对IT系统的任何监控
您想要实施的资源
您需要确保这些基本问题已得到解答
或至少在开始实施e之前牢记这些问题
C 两个监控计划
那么我们应该从哪里开始呢
因此要建立基准线
呃 至少我们不应监控这些项目
对吧 CPU利用率
网络利用率
磁盘性能
磁盘读写情况
以及内存利用率
这些是我们任何e监控计划中都需要监控的基础内容
C 两个实例 因为这实际上反映了我们e的健康状况
C 两个实例
并让我们了解
你知道的 是否需要升级
或者是否为该服务器分配了过多硬件
能否降级硬件配置
这些指标是最基本的
这是您应监控的e内容
C 两个实例现在
频率取决于您的目标和整体监控计划
但在指标层面
这些都是您监控计划中至少应关注的最低要求
C 两个实例
现在它提供了多种工具来监控EC two个实例
因此我们有两种监控方式
自动化与手动
在自动化方面
您可以使用列表中看到的多种工具进行自动化监控
对于e C 两个实例并在出现异常时反馈
这里有系统和实例状态检查
再次说明 系统主要监控
任何硬件故障
大量网络连接问题
系统电源状态 物理主机的软件问题
系统状态检查通常是由AWS负责处理的
物理上在他们的数据中心
这不是我们能够控制的部分
我们能够做的就是
你知道我们可以重启实例
或者停止实例再重新启动
因为通常当你这样做时
它通常会在新的物理主机上启动
我说通常是因为我们不确定是否百分之百有效
但AWS确实说明通常会分配新主机
当停止并重新启动实例时
所以这是在我们的控制范围内
这是我们可以通过系统检查做到的
或者我们可以进行实例状态检查
这是监控单个实例的软件和网络配置
这些检查基本上会检测需要你介入修复的问题
比如如果出现
你知道的 系统检查失败
或者内存耗尽
或者存在不兼容的
呃 内核等问题
这些检查是自动化的
与CloudWatch告警、事件和日志配合
事件和日志
因此我们可以监控指定时间段内的单个指标
执行一个或多个操作
你知道的 根据指标值进行判断
例如 如果想在CPU使用率超过70%时收到通知
你知道的 持续五分钟或两分钟我们可以设置告警
我们可以发送事件
然后显然可以进行日志记录
存储 并通过CloudWatch和CloudTrail访问文件
甚至可以将日志保存到S3存储桶
因此我们可以进行一些审计
我们可以在月底进行分析查看
我们投入的AWS资源是否
你知道的 确实适合我们的组织
还有两个监控脚本
即Perl脚本用于监控内存、磁盘
和实例文件使用情况
然后还有最终的
微软系统中心操作管理器的管理包
再次说明 这与微软特定相关
但基本上链接e
C 两个实例 以及运行在其中的Windows操作系统
因此，管理包本质上是微软系统操作管理器的扩展
并在数据中心使用一台指定计算机
称为监视节点
AWS API用于远程发现和收集AWS资源信息
所以再次 这些是我们可以使用的自动化监控工具
用于对我们的AC两个实例进行自动化监控
然后我们还有手动监控
监控的另一个重要部分是e
C 两个实例的手动监控
并不是像某种
并不是你只想要做
嗯 在全面监控计划中包含自动化监控
你也需要进行手动监控对吧
这些是
你知道的 我们可用于手动监控的一些工具
手动显然意味着你坐在电脑前
或在手机或平板上打开
并实际查看e
C 两个实例的健康状况
你知道是否通过e
C 两个控制台
是否通过云监控控制台
你知道是否使用e
C 两个监控数据来排查问题和发现趋势
进行资源指标分析
并查看告警是否收到所有通知
以及整体而言
你知道的 快速查看
全面了解AWS环境中的所有情况
仅仅如此 你知道一切
嗯 你知道的 亲身体验
而非仅依赖告警或自动化监控
始终有益 你知道
每天或每周一次
继续前往e
C 两个仪表盘或您的AWS控制台整体
只需查看所有已配置的内容
所有功能正常运行
所有状态健康且如此类推
因此这里有一些需要牢记的事项
并确保我们在监控AC两个实例时实施
务必回答我们定义的目标问题
我们想要监控的内容 以及我们实际如何监控它们
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/085_Udemy - Become an AWS Certified Data Engineer part1 p85 16. EC2 Monitoring Lab.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好 欢迎回到这节课，我们将讨论关于e
C 两种监控
基本上我们可以对eu两个实例执行两种不同的状态检查
首先是系统状态检查
然后是实例状态检查
对于系统状态检查
主要用于检测实例底层问题，需要AWS介入修复
S的参与来修复
当系统状态检查失败时
你知道的 你可以选择等待
直到AWS修复其硬件或网络问题
对于使用EBS的实例
你可以自行启动和停止实例
因为这些实例
一旦启动后
通常会迁移到新主机
这通常能解决系统状态检查失败的问题
同样在
你知道的 网络连接中断
系统电源 硬盘或处理器物理故障
因此当AWS
S需要物理介入处理实例状态检查
关于 我们可以监控单个实例的软件和网络配置
对于e
C 通过发送地址解析
协议或ARP请求到网络接口
这些检查能检测需要你参与修复的问题
而非AWS的介入
例如
可能导致实例状态检查的问题
系统检查失败
网络配置错误
内存耗尽
文件系统损坏
病毒或恶意软件感染
这些被监控系统检测到
这就是我们能进行的两种主要状态检查
要查看状态检查
如你们所见 我已经登录AWS管理控制台并进入e c
Two仪表盘
在这里我的实例中有一个正在运行
这是t two micro Windows实例
所以在这里，当我选择实例后
我可以查看此实例的所有不同信息
所以这里有一个状态检查选项
基本上是状态检查列
它将列出
它会列出每个实例的操作状态
在这里我们可以看到状态检查是
可达性检查已通过
并且实例可达性检查已通过
所以两种类型的检查
系统和实例均已通过
所以它们都正常运行
我们也可以通过命令行查看状态
所以如果我登录AWS，就是这样
所以我正在登录AWS控制台
所以我想要做的是 我想运行aws
E c two命令并描述实例状态，就是这样
所以大家可以看到它已返回
一切正常运行因为 我只有一个实例
所以它正在运行 它会提供实例ID和状态
可达性已通过，系统状态也通过
所以实例和系统状态也都正常
所以我可以快速在命令行运行
而不是登录控制台
检查实例状态以确保两项测试正常运行
如果遇到实例问题，也可以提供反馈
状态未显示为受损的实例
或者如果你想发送更多关于问题的详细信息
你可能正在经历的故障实例问题
所以在AWS控制台中，这里
所以这里我们有
我们仍在系统状态检查底部
还有其他资源
所以我们可以提交反馈给AWS
并告知他们 如果此实例存在未显示的问题
如果一切正常
但我们仍存在问题
我们可以提交实例状态表单给AWS 这将促使他们实际检查该实例并确保其正常运行
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/086_Udemy - Become an AWS Certified Data Engineer part1 p86 17. Using CloudTrail to Audit Users.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到这节课
如果我们想了解云追踪以及它们与EC two实例的关系
所以基本上我们可以记录e
C Two和RDS的API调用
云追踪本质上是一个服务，记录用户、角色或服务在e
C
Two和RDS中执行的操作 所以它捕获所有e
C
Two和RDS的API调用作为事件 包括控制台和代码调用API的操作
所以如果我们创建一个追踪
我们可以启用持续将事件传输到S3存储桶
包括来自e
C two实例
包括它们的EBS卷
所以云追踪在账户创建时默认已启用
因此当e
C
Two和RDS发生活动时 活动会被记录在云追踪事件中 同时包含事件历史中的服务事件
所以大家可以看到
我已经导航到云追踪
如果我点击事件历史
可以看到发生的不同事件历史
所以我没有启用云追踪
我也未进行配置
这是我第一次使用此账户进入此仪表盘
大家可以看到它已自动开始填充所有追踪事件
所以追踪本质上是一个配置，用于传输事件
到S3存储桶供我们分析
云追踪生成的日志文件包含
关于特定事件状态的信息条目
大家可以看到 我们可以使用云追踪审计通过e
C two实例连接的用户
所以大家可以看到
我们可以用此审计通过e c two实例连接的SSH活动
所以大家可以看到，如果我筛选事件源
这里有不同的筛选选项
所以如果我筛选事件源
并在事件源中指定e
C two
我可以可选地指定时间范围 所以对于审计目的
越详细越好
粒度越高
效果越好 所以我可以指定我想要的审计范围
嗯 特定日期
特定时间
并告知我此事件源的完整追踪记录
包括谁登录了以及发生了什么
所以有一个描述标签事件
对的 但在美东地区
这是使用的访问密钥
这是请求ID
来源 IP地址
因此每个事件的详细信息都非常全面
C 两台实例
如果我点击查看事件
这会以JSON格式提供完整事件信息
所以再次说
账户ID 使用的访问密钥
时间及来源
发生了什么 事件内容即描述
标签是用户代理使用的事件
来源IP地址，这些信息对日志记录非常详细
现在如果我们配置
我们的账户将追踪事件存入S3存储桶
我们可以下载并审计这些日志进行搜索
并基本存入数据库
如果需要的话 嗯
开发应用程序执行审计
这对内部审计团队跟踪谁
谁在访问
确保我们的政策和程序被遵守
尤其是涉及敏感信息的服务器
在某些e C 两台实例
这是一个非常 非常好的工具用于跟踪
所以在追踪中
我可以创建追踪来监控我的e
C Two
我可以将其应用到所有区域或特定区域
我想再次应用追踪到
我的组织
嗯 是或否
对 因此该组织可以拥有多个AWS账户
是否需要特定内容
我需要读取权限 写入权限或两者都有
基本上这里将存储
我想要创建一个新的S3存储桶
搞定 日志功能已开启
因此所有日志都将保存在此S3存储桶中
如果我继续进入我的S3
搞定 我可以确认S3存储桶已创建
里面已经有文件夹了
AWS日志
我的账户编号和追踪摘要
以及CloudTrail
它将开始为每个特定区域填充数据
正如您所见细节越来越丰富
对 这是账户名称
CloudTrail
区域 年月日
所以是2019年10月9日
这些都是所有CloudTrail记录
让我了解发生了什么
提供如此详细的信息
将持续追踪 请注意这属于计费事件
如果您使用免费套餐
在免费账户中
请务必注意
但对于审计非常有用
可追踪谁访问了什么
甚至非常详细的服务访问信息
C 关于访问的服务实例
我将返回并快速删除此CloudTrail
要删除追踪
操作非常简单 我们进入CloudTrail
打开它 这里有一个删除选项
以便清理所有资源
只需确认删除
搞定已删除
再次进入事件历史
这是默认设置
事件历史免费填充
所以这不是我们需要付费的内容
我们也可以选择下载所有这些事件
如果你不想使用付费云追踪服务
这些数据会自动填充到S3存储桶中
你可以手动进入这里下载这些文件
假设 针对特定时间范围
你可以将这些数据导出为CSV或JSON文件
然后通过这种方式进行分析 这样一来 你就无需为云追踪服务付费
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/087_Udemy - Become an AWS Certified Data Engineer part1 p87 1. Data Variety Problem Source Data Storage.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
让我们谈谈数据多样性问题
没错，这在原始数据层面意味着什么
因此速度是规划数据分析方案时极其重要的因素
同样重要的是理解存储在不同系统中的多种数据
现在他们说数据类型和员工数量一样多
没错 所以
如果你曾被
需要分析的数据源数量压垮
你应该明白我在说什么
这意味着你面临多样性问题
这在寻求分析多样性时会带来挑战
在这段视频中，我将讲解如何存储多种类型的数据
以便为企业的分析需求做出明智决策
挑战在于数据分析方案中的多样性
我也会讨论这一点
同时我还会介绍各种数据源类型
以及提供数据的数据库
所以 本质上 让我们先谈谈结构化数据存储
接下来我也会讲解半结构化存储
当企业面临大量需分析的数据源时
面对海量需要分析的数据源
却找不到系统
执行分析任务
你就遇到了多样性问题
现在多样性问题确实存在
由于数据类型多种多样
就像不同的人一样 没错
这就是多样性对企业构成的挑战
在寻求分析多样性时
通常无法仅根据存储类型分类数据
因此每个数据分析方案
例如 都始于数据源
现在数据源可以是任何事物
文件服务器上的文件夹可能是一个数据库
可以是网页
甚至某个设备也能被视为数据源
这些数据源以特定方式存储数据
某些数据源使用模式和索引
以提升性能
现在 其他数据源采用更灵活的方式组织数据，称为无模式
使用模式的数据源仍通过索引提升性能
现在 当人们谈论数据多样性时
有时他们指的是数字
文本和日期
但这些实际上属于数据类型，真正讨论数据多样性时
需要讨论这些数据类型在数据源中的组织方式
因此我们可以讨论数据多样性而不必涉及数据源类型
没错 以及每种类型如何组织不同类型的数据
每个数据分析方案都始于数据源
它可以是任何东西
文件服务器上的一个文件夹
一个数据库
一个网页 甚至像我之前提到的可穿戴设备
每个数据源都可以以不同方式组织以满足不同需求
现在让我们谈谈三种数据源结构
半结构化和非结构化
结构化数据是以表格形式存储的数据
通常存放在关系型数据库管理系统（RDBMS）中
这种数据基于
你知道的 数据关系模型并标准化数据元素及其相互关系
这被称为模式（schema）
通过模式，RDBMS系统能够强制执行合规性
结构化数据被组织成表格
列和行
结构化数据存在于事务型和分析型数据库中
事务型数据库支持大量写入和更新操作
而分析型数据库侧重大量读取操作
结构化数据的缺点是
一旦初始开发完成，灵活性受限
所以 假设 你决定在现有数据库中记录客户年龄
必须重新配置模式以支持新数据
还需处理所有该字段无值的记录
这并非不可能
但可能非常耗时，接下来是第二种数据源类型
半结构化数据源将数据组织为元素和属性
当半结构化数据存放在非关系型
通常称为NoSQL数据库
它不对数据施加模式或特定规则约束
当半结构化数据存储在文件中时
被认为具有自描述结构
即文件本身会说明如何解析数据
现在 这种优势在于每个数据库
表或文件都可以有自己的结构或模式
与结构化数据不同
半结构化数据当前具有高度灵活性
每个元素可以包含不同的属性
例如 因此我们可以更快地扩展以满足业务变化的需求
这种灵活性的代价是分析能力的损失
现在分析半结构化数据会更困难
因为没有统一的强制模式
你还会失去连接两个表或文件的能力
这是结构化数据的关键功能
所以结构化数据易于分析对吧
但结构难以更改，半结构化数据易于生成且高度灵活
但分析起来可能更困难
所以最后的数据源就是非结构化数据
非结构化数据存储在类似半结构化数据的文件中
但这些数据不遵循预定义的数据模型
也没有预定义的组织方式
现在非结构化数据可以是文本
厚重的
PDF文件
CSV文件构成了这类数据的主要部分
所以文件可能根本不包含任何文本，比如图片和视频
例如
现在针对非结构化数据
讨论了对每个文件存储的元数据进行的分析
现在要进行有意义的
你需要对这些文件进行预处理才能分析数据
现在有几种关键方法可以实现
不过 对吧
你可以使用服务将文本添加到数据库
根据你定义的规则
或者对数据进行编目以便查询服务使用
现在 听起来可能需要非常激进的思维才能解决
如何在同一解决方案中使用非结构化数据和结构化数据
这就是本讲座的核心目的
帮助你理解数据源类型
所以你能
你知道的 准备好将这些数据源类型
整合到数据分析方案中
需要新系统和应用来挖掘数据本身的价值
而许多公司就止步于此了
但他们不应该停止
新服务和功能不断推出
而挖掘非结构化数据的成本
正在迅速降低
让我们从这三个数据源类型的流行度和使用情况入手
相信或不信 顺便说一句
有一家公司收集在线对话数据
正确的帖子 文章
问题
然后发布在线表单
他们专注于关于数据和数据库的讨论
毫不奇怪
顺便说一句对吧
他们发现75%的沟通内容涉及关系型数据库和结构化数据
因此NoSQL数据库和半结构化数据仅占约15%的沟通内容
所以这一切其实
关系型数据库和结构化数据是最常用的数据源类型
我们来谈谈数据源类型吧
结构化数据以表格形式存储在RDBMS系统中
例如
现在这些数据基于关系数据模型进行组织
它定义并标准化数据元素及其相互关系
现在数据以行的形式存储，每行代表一个实体实例
例如 一个客户
这个 这些行由于表结构已明确理解
现在这使得结构化数据易于查询
缺点正如我之前提到的缺乏灵活性
比如你想跟踪客户的年龄
例如
为此你需要重新配置整个表结构
在实际完成之前
这就是这类数据的问题
对吧
所以我提到了三种不同类型的数据
结构化数据很热门
立即可用于分析
半结构化数据比较温和
部分数据可以直接使用
而其他数据可能需要清洗或预处理
非结构化数据是最难处理的
你需要明确所需内容
字段已填满
各种不需要的信息
新工具正在涌现
你知道的 帮助企业整合和分析各类数据
在商业分析方面
结构化数据通常是首选
因为它格式规范
企业开始意识到必须将半结构化和非结构化数据纳入商业智能方案
因此在讨论这些数据类型后
大多数企业的数据以非结构化为主
因为这才是主体部分
因为这才是主要部分
所以数据库引擎
例如
我提到过一家公司会扫描互联网并讨论数据
并收集关于数据本身的信息
没错，我还提到其中60%的内容是关于结构化数据
但许多组织往往忽视非结构化数据
所以希望这能帮助你理解三种数据结构
如果有任何问题请告诉我 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/088_Udemy - Become an AWS Certified Data Engineer part1 p88 2. Structured Data Stores.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
在之前的课程中
我介绍了三大主要数据源的基本特征，现在在本节课中
我将专门讨论结构化数据
结构化数据被定义为存储在数据库或数据库管理系统中的数据
Dbms
数据库是存储在您电脑上的结构化数据集合
没错，可以通过多种方式访问
因此数据库管理系统为数据提供结构
并能在其整个生命周期中维护数据
同时能够管理与其他进程和系统的交互
不同的数据库管理系统以不同方式管理数据组织
以实现特定目标
例如复杂分析
快速关系导航或会话状态检索
现在请记住结构化数据以表格形式存储
通常位于数据库管理系统内
现在我将探讨这些管理系统最初为何被创建
平面文件数据非常常见
谁没有处理过工作表
例如 Excel工作表或电子表格
对吧 这些文件以列和行形式包含数据
现在 平面文件数据通常在生成时没有对列中可输入值的规则或限制
或应输入的值现在这会导致后续分析时出现大量问题
对于平面文件
不同文件之间缺乏一致性
现在 在预处理前几乎无法合并数据
也无法评估数据本身的完整性
让我们不要搞错了
尽管这种格式的数据存在一些问题
平面文件存储也有其优势
因此它仍将代表数据解决方案需要收集的大部分数据
当组织需要超越简单平面文件存储时
通常转向数据库存储数据现在有许多类型的数据库可供选择
当我们讨论数据库类型时
需要了解特定的数据组织方式能显著提升数据分析
关系型数据库有很多优势
它们允许快速收集
更新和查询数据
值得一提的是
其强大的合规性保障能力
这是事务系统中用于确保数据完整性的机制
现在我将在后续内容中详细讲解
也在后续课程中
但在此要公平起见
让我们谈谈关系数据库的局限性
存储其他结构化数据
需要仔细考虑需要存储的数据
以及关系型数据库是否是最佳选择
现在关系型数据库这个术语其实很广泛
在这一特定类别下
存在两种主要的信息系统或信息组织方法
在线事务处理（OLTP）
在线分析处理（OLAP）
为什么这里有两种不同系统
嗯 这一切都取决于
当前数据库支持资源的使用方式
当你与数据库交互时
实际上只有两种操作
存入数据或取出数据
向数据库添加数据称为写操作
从数据库查询数据称为读操作
现在这两个操作使用相同的资源
但方式不同
这使得难以针对其中一种进行优化
小型数据库通常能容忍读写操作同时进行
然而在大型数据库中
必须牺牲读操作性能以支持高效写操作
或牺牲写操作性能以支持高效读操作
这听起来并不理想
是吗
确实如此
但对于正在扩展的数据库企业这是现实
解决方案是建立一个优化写操作的数据库
OLTP 和另一个优化读操作的数据库
OLAP 实际应用中
数据以高频写入OLTP数据库
系统记录定期复制到OLAP系统
在许多解决方案中
复制到OLAP数据库的数据也会进行转换
以回答分析问题
或预聚合以实现快速查询
这些操作均通过ETL完成
抽取转换加载
我稍后会详细讲解
ETL相关内容
但这里平面文件数据通常存在于工作表或电子表格
如之前所述
没错
严格来说并非数据库
但仍满足所有要求
这种格式为理解关系型数据库选型考虑因素奠定基础
在选择DBMS时
正确 平面文件存储可能无法满足结构化数据存储需求
因此下一步逻辑步骤就是转向更 robust 的解决方案
没错 也就是关系型数据库
这就是规范化过程
帮助企业将平面文件数据转化为关系型数据库
规范化是一系列协同工作的规则，旨在减少冗余
提高可靠性
并改善数据存储的一致性
因此关系型数据库用于存储结构化数据
可以被收集、更新
然后轻松查询
关系型数据库还依赖一系列称为表的结构
这些表通过结构化查询语言（SQL）进行导航
即 SQL
因此关系型数据库的表
按人员分组数据
地点 事物
或与数据相关事件
这些分组称为实体
每个实体存储为一个表
一列称为字段
例如 用于描述实体的一个属性
一行称为表中的记录
代表实体的一个实例
可以想象成电子表格，每行对应一列的单元格
每个单元格可包含值
模式内的规则定义必填或可选属性
然后通过以下步骤建立关系
使用主键确保表中每行唯一
这正是唯一性的概念
主键值可用于在表间建立关系
顺便说一下
外键使用来自其他表的主键值
定义该表中的记录
这就是表间关系的基础
某些数据库引擎可强制执行此关系
确保主键值可用于外键
让我简要谈谈优势
数据易于存储
使用通用 SQL 语言进行编辑和检索
结构可快速扩展
而另一方面弱点
没错 在存储非结构化数据时遇到困难
或查询
你知道的 由于复杂需求导致查询缓慢
或模式会使横向扩展困难
然后信息系统的类型也很重要对吧
因此有两种主要方式被称为关系数据库中的数据组织系统
数据可以组织以专注于交易存储
或者分析交易的过程
事务处理数据库被称为在线事务处理
OLTP数据库收集的数据对吧
通常会被输入另一种专注于分析交易数据的数据库
因此在线分析处理或OLAP
数据库从OLTP系统收集数据对吧
目的是为了组织分析操作
OLTP数据库
例如
是操作型数据库和在线事务处理
好的TP 例如数据库
它们也常被称为操作型数据库对吧
逻辑组织数据到表中，重点在于数据录入速度
现在这些数据库以大量插入
更新和删除操作为特征对吧
所有关于数据和存储属性的决策都基于
确保快速数据录入
然后更新
OLTP系统的有效性通常通过每秒事务量衡量
OLAP 另一方面
对数据仓库通常使用
例如 在线分析处理对吧
它们被称为数据仓库
没错 逻辑组织数据到表中
重点在于查询的数据检索速度
现在 这些数据库以较少的写入操作和缺乏更新删除操作为特征
所以 所有数据组织和存储属性的决策基于查询类型
以及将要执行的数据分析
OLAP系统的有效性通常通过查询响应时间衡量
现在讨论结构化数据时还有一个非常重要的概念需要涵盖
那就是如何优化数据检索
数据库使用索引组织表中的记录
索引表能比未索引表更快返回查询结果
例如
假设你要查找所有1月订单
未索引的表需要逐行扫描
所有匹配行都会返回
扫描操作非常耗费资源
想象在五十万行数据上执行这个操作
需要很长时间
与索引表不同
主键字段现在用于组织表中的所有行
如果在订单日期字段上创建索引
现在所有订单将按订单日期进行组织
这意味着当执行查询时
系统可以自动或跳转到索引中的正确数据
跳过此日期前的所有行和此日期后的所有行
现在这将显著提升响应速度
这种索引方式称为基于行的索引
最适合需要定期返回完整数据行的系统
现在在OLTP系统中
这些是最常见的查询
这些查询为每个匹配行返回多列数据
这些过滤条件通常基于表中的键列
例如
返回客户的全部地址信息或特定订单的详细信息
或者现在OLTP和OLAP系统都能通过基于行的索引高效运行
但还有一种同样重要的索引形式需要介绍
它被称为列式索引
在OLAP系统的情况下
最常见的查询是聚合查询
现在这些查询通常非常复杂
因此需要处理大量行并聚合值进行缩减
例如
统计客户数量或汇总运输成本
在OLAP数据库中运行基于行的索引
可能导致大量不必要的数据被读入内存
因为必须加载每行的每个列
所以
OLAP数据库使用基于行索引的另一个常见问题是 可实现的压缩量受限于必须用于标识每行的唯一值
现在这就是列式索引发挥作用的地方
列式索引改变了索引构建方式
因此不再按行而是按列进行索引
对于OLAP系统，直接索引列的优势显而易见
查询只需加载所需列而非所有列
如基于行的索引所示
现在这几乎消除了之前提到的数据浪费
因此另一个关键优势在于数据压缩
现在这几乎消除了之前提到的数据浪费
所以数据压缩的另一个关键优势是什么
AWS对于OLTP和OLAP数据库
对于使用基于行索引的OLTP或OLAP数据库
亚马逊关系数据库服务也称为Amazon RDS
现在该服务使设置
操作 并在云中扩展关系数据库变得简单
该服务同时提供成本高效且可扩展的容量，自动化许多
消耗管理相关任务
例如硬件配置
数据库设置
你知道的 补丁和备份
当你实施亚马逊RDS时
你可以选择许多流行的数据库管理系统，例如亚马逊
Aurora Post
SQL MySQL
Maria DB
Oracle和SQL Server
对于使用列式索引的OLAP数据库
我们有亚马逊Redshift
这是一个快速
可扩展的数据仓库
使其能够以简单且经济的方式分析数据仓库中的所有数据
以及基于行和列的索引
因此数据库中的数据应进行索引
以便查询能快速找到所需数据生成结果
现在索引控制数据在磁盘上的物理存储方式
没错 根据表中的键值将记录物理分组为可预测的顺序
这在查询速度和效率中起着重要作用
在OLTP系统中
最常见的查询称为查找查询
这些查询需要为每个匹配记录返回多个数据列
这些过滤通常基于表中的键列
在这种系统中
例如 你可能查询获取OLAP中特定订单的详细信息
另一方面 最常见的查询是聚合查询
这些查询处理大量行数据
然后通过聚合一列或多列的值将其缩减为单一行
在这种系统中
你可能查询特定日期的总销售量
OLTP和OLAP系统均可使用两种索引方法
然而 选择最适合的方法有优势
根据项目或所需解决方案
关系型数据库的行和列
例如
关系型数据库的主要优势在于
使用SQL 例如
它是经过验证的技术，被广泛采用和理解
关系型数据库风险较低
尤其得益于合规资产和大量专家社区支持
现在对事务延迟有预期
例如 尤其是在适当规模的硬件上，关系型数据库在OLTP场景中表现卓越
对于相对较小的数据集
现在关系型数据库存在可扩展性问题
随着数据集的增长
保持性能的唯一方法
是增加运行应用程序的服务器硬件容量
另一个关键问题是关系型数据库的固定模式
现在很难对底层数据库架构进行无中断修改
这会影响新功能的开发时间
准备关系型数据进行数据处理只需
例如 在使用关系型数据库时
还需注意数据在数据分析方案中的使用方式
通常会对同一数据进行多种类型分析
OLTP数据可能需要转换为反规范化形式
并存入数据仓库或数据集市
重叠数据可能无需任何转换
甚至可能有机会将数据转换到暂存数据库
可作为数据源使用
供其他分析流程使用
我已经深入讨论了结构化数据和关系型数据库
希望这有所帮助，如有疑问
请随时提出
多复习几次本节课
因为我涵盖了非常 OLTP与OLAP及结构化数据库之间
非常重要概念，这对考试同样适用
希望这能帮到你
如果对此有任何疑问 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/089_Udemy - Become an AWS Certified Data Engineer part1 p89 3. Introduction to semi-structured and unstructured data stores.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
所以我之前讲过结构化数据
在上一节课暂停过
没错，还有重要概念
但在这节课里
我将讨论半结构化和非结构化数据存储
我还会讲解这些数据类型的存储方式
以及处理这些数据存储时需注意的事项
半结构化和非结构化数据通常存储在非关系型数据库系统中
有时也称为NoSQL数据库
这个术语可能引起一些混淆
有时候确实如此 但必须记住SQL是一种查询数据的方式
这意味着精确的结构
非关系型或NoSQL并不意味着数据无法被查询
使用SQL
但更好的理解是不仅限于SQL
现在深入探讨
明确术语很重要
因为容易产生混淆
尤其是考试时
首先介绍SQL
这是一种用于查询数据的语言
不幸的是 有时这个术语被关联到关系型数据库
这在讨论中可能造成混淆
例如
当使用SQL时
需仔细辨别说话者指的是
SQL作为查询语言还是数据库类型
半结构化和非结构化数据可存储在NoSQL数据库中
这对半结构化数据非常常见
这类数据存储也可保存在文件服务器或数据湖中
这两者常用于非结构化数据
所以在这里
就我们的目的而言 我将聚焦于数据存储在NoSQL数据库中
在继续之前
先明确NoSQL术语
NoSQL数据库存储无需关系的数据
更准确的说法可能是非SQL
NoSQL数据库诞生于
当 网站和在线游戏的数据快速摄入
已无法被关系型数据库有效处理
想想你开始下单的时刻
然后决定稍后处理
现在购物车中被放弃的数据需要处理
现在
在关系型数据库中
可能需要删除跨多个表的数十条记录
但使用NoSQL数据库
你的整个购物车只有一个记录
现在当你放弃购物车时
只需删除那一条记录
现在 这就是NoSQL数据库的超能力
你知道的 要专注于NoSQL数据库的关系是正确的
例如 第一位顾客将产品加入购物车
现在已存储在NoSQL数据库中
当顾客开始结账流程
然后可以将NoSQL数据库的数据传递到关系型数据库
用于永久存储
数据安全存储在关系型数据库中
顾客完成订单
然后可以删除NoSQL数据库中的记录
数据库类型之间的协作是一种方式
大型电商网站每分钟满足数千客户需求
我们现在掌握了基础
让我们深入探讨关系型数据库与NoSQL的区别
关系型数据库中的数据是组织化的
一个表中的值可用于进一步定义另一个表的数据
换句话说 数据表之间存在关联关系
例如
假设有一个存储产品订单的关系型数据库
可能有一个表定义当前可售的所有产品
想象另一个表定义产品的供应商
在关系型数据库中
可以运行SQL查询显示所有产品数据
及其关联的供应商
尽管数据存储在两个不同表中
这就是关系型数据库的超能力
NoSQL数据库将数据存储在单个表中
NoSQL名称表明无需使用SQL查询
这意味着可以对NoSQL数据库中的数据使用SQL查询
回到之前的例子
在NoSQL数据库中
每个产品的所有信息
包括关联的供应商
会存储在数据库表中的单个条目
这意味着存在冗余数据
但NoSQL数据库要求我们以新方式思考数据存储
也要记住我们并未取代关系型数据库
而是将关系型数据库不擅长的任务
转移到更适合的数据库中
因此许多人难以理解非结构化数据存储的概念
现在让我们讨论一些具体的SQL数据库类型
为什么不呢
因此NoSQL数据库旨在以支持快速收集和检索的方式存储半结构化和非结构化数据
现在NoSQL数据库有几个主要类别
例如
文档存储以文件形式存储半结构化数据 这些文件形式各异
例如JSON
Xml
对吧
你可以使用多种语言导航这些文件 包括Python和Node.js
因此半结构化文件包含作为一系列元素存储的数据
因此每个人实例中的每个元素或项目
地点
事物或事件 所以
例如 文档存储可能保存一组服务器的多个日志文件
现在
每个服务器略有不同且会生成特定配置的日志文件 现在无需每个服务器的日志文件包含相同属性
文档存储的最大优势在于其灵活性
因此创建文档存储时用户无需预先规划特定数据类型
并且易于扩展
这也是由于缺乏严格模式
现在需要了解这种灵活性的权衡
因为它可能成为关键问题
此外文档存储牺牲数据合规性以换取灵活性
请记住
数据合规性 是指如何在关系型数据库中保证数据质量
许多企业有严格的数据合规性要求
因此需特别注意这一点
所以请密切注意
文档存储的另一个关键缺点是无法跨文件查询
这意味着虽然可以从十台服务器收集日志
但无法运行跨所有十份文件比较事件的查询
除非进行聚合或其他转换
键值数据库是另一种NoSQL数据库类型
它们以键值对形式存储半结构化数据
数据逻辑上存储在单个表中
此表包含条目，每个条目包含一个或多个属性
每个条目还必须包含唯一标识符
即主键
需注意键值表与关系表的区别
正确，键值表是无模式的
这意味着无需预先定义每个属性
在实际使用前
你还可以创建嵌套属性
这些属性本身拥有子属性
例如 对对例如
你可以有一个包含街道地址的属性
城市 州和邮编对吧嵌套在里面
因此关系型数据库可能会将这些嵌套信息移到另一个表中
假设企业决定需要开始跟踪每个产品的评分
每种数据库类型需要如何调整
好的 现在让我们从关系型数据库开始
而这正是最难的部分添加新数据
你需要在表中添加一个新列
现在 列添加完成后
这一列会是空的对吧
这可能存在问题因为关系型数据库通常要求每列都有值
因此你需要创建流程为表中现有记录确定正确值
一旦流程完成
就可以开始添加新记录的评分
好的 对于NoSQL数据库下一步的猜测是什么
嗯 无需猜测
好的 你只需开始添加带有新属性的新项
这就是全部操作
可以回头处理现有记录
但如果不处理也没关系
而NoSQL数据库在AWS中还有第二大优势
关系型数据库
例如 亚马逊DynamoDB是用于NoSQL数据存储的服务
DynamoDB是一种键值和文档存储数据库，提供单位毫秒级性能
无论规模如何
它是一个完全托管的多区域多主数据库，内置安全功能
备份与恢复
然后 当然 为互联网级应用提供内存缓存
换句话说
DynamoDB融合了这两种NoSQL数据库系统的优点
没错
我已经介绍了两种NoSQL数据库
但还有一个想讨论的
那就是图数据库
它们应该是下一个大趋势对吧
图数据库真正关注的是连接与关系
如果你有一个应用
例如 主要专注于遍历关系以发现其中的模式
然后图数据库可能是一个很好的选择
因此图数据库是专为存储各种类型数据而设计的
无论是结构化
半结构化还是非结构化
图数据库的组织目的是导航关系
数据库中的数据使用与所选软件工具关联的特定语言进行查询
现在关于图数据库还有很多需要学习的内容
但基础知识在这里
它们允许简单
快速检索层级结构
并且适合实时数据挖掘
图数据库还能快速识别节点间的共同数据点
非常适合生成相关推荐并快速查询这些关系
但它们也有局限性
对 目前无法充分存储事务性数据
分析师必须学习新语言来查询数据
数据分析可能不如其他数据库类型高效
现在让我们深入探讨图数据库的内部运作
假设你想收集产品信息或社交推荐
在此图中可以看到比尔认识玛丽，玛丽认识约翰
除了个人信息外
数据库还能追踪购买历史
这里显示比尔和阿特都购买了该产品
现在你可以看到凯瑞也购买了该产品
但与图中的其他三人没有关联
还可以进一步追踪他们的兴趣
例如最喜爱的运动
借助这些信息，你可以看到可用于推荐的另一层关系
约翰 例如
可能对喜欢运动的其他客户购买的产品感兴趣
玛丽可能想了解好友客户的购买情况
还有很多 许多更多类型的关系
这个图比较简单 但它展示了潜在能力
AWS的图数据库服务名为Amazon Neptune
因此它是一个快速
完全托管的图数据库服务，便于构建和运行
处理高度关联数据集的应用
正如我之前提到的
像Amazon Neptune这样的图数据库
例如 专为存储和导航关系而设计
这些数据库在社交网络等场景下相比关系型数据库更具优势
正如之前所说
推荐引擎和欺诈检测
需要在数据间建立关系并快速查询这些关系
在涵盖大量内容后
比较不同数据库类型可能有所帮助
我们从数据库中的数据表示开始
关系型数据库存储相互关联的数据和表
当前表格包含由模式定义的列和行
NoSQL数据库
以文档集合的形式存储数据对吧
或者键值对形式现在在那个模型中
这在Neptune中表示为表格
这表示为一系列节点和关系所以接下来
现在你已经有了关系型数据库的数据设计
模式决定了设计对吧
它经过规范化和维度化以及数据仓库
NoSQL数据库去规范化并可使用内部文档结构
或者完全没有结构
NoSQL图数据库同样去规范化并基于实体关系
因此数据库始终优化以实现特定目标
所以关系型数据库的目标是优化
NoSQL数据库的存储侧重计算能力
而NoSQL图数据库优化关系查询对吧
在数据查询方面
关系型数据库使用结构化查询语言
SQL
NoSQL和图数据库都支持多种查询语言
并专注于对象查询
顺便说一下我们不能忘记可扩展性
随着数据库增长其扩展方式不同
关系型数据库擅长通过增加资源扩展
这称为垂直扩展对吧
NoSQL数据库擅长跨多台服务器分布
这称为水平扩展
NoSQL图数据库支持水平和垂直两种扩展
最后比较这些数据库类型的典型实现
关系型数据库常作为OLTP业务系统的骨干
和OLAP数据仓库
NoSQL和图数据库常用于OLTP Web和移动应用
所以非关系型数据库
例如
非关系型数据库用于存储半结构化和非结构化数据
以实现快速收集和检索
希望这些内容对你有帮助对吧
因为非常重要
我讲解了非结构化数据
半结构化数据对吧
然后关系型数据库类型
NoSQL接着深入图数据库
如果有问题请告诉我
多看几遍这节课
理解概念 做笔记 接下来进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/090_Udemy - Become an AWS Certified Data Engineer part1 p90 4. AWS RDS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好 欢迎来到关于亚马逊云服务关系型数据库服务的课程
也就是我们常说的RDS服务
RDS让在云端搭建
运维 扩展关系型数据库变得非常简单
提供高性价比且可弹性扩展的容量
同时自动化处理耗时的管理任务如硬件配置
数据库部署 补丁更新
甚至备份
让您专注于应用程序开发
从而为应用提供快速性能
高可用性
安全性和兼容性
以向内部员工或客户提供更优质的服务
RDS提供多种数据库实例类型
针对优化内存需求
追求高性能场景
或 O
并提供六种熟悉的数据库引擎供选择
例如亚马逊Aurora
PostgreSQL
MySQL
MariaDB
Oracle数据库
以及微软SQL Server
亚马逊还提供数据库迁移服务
可轻松迁移或复制现有数据库至RDS
使您能快速便捷地开始使用RDS
操作简单高效
所以 接下来 让我们看看RDS的优势和特点
首先
管理极其简便
从项目构思到部署全程高效
可通过RDS管理控制台
或在演示中使用的命令行界面
几分钟内即可访问生产级关系型数据库功能
无需处理基础设施配置
也无需安装维护复杂数据库软件
同时具备高度扩展性
如同多数AWS服务
RDS具备卓越扩展能力并利用AWS全球基础设施
同时具备高可用性和持久性
基于相同的底层架构
运行速度极快且支持高负载数据库应用
可选择两种基于SSD的存储方案
一种针对高性能OLTP应用优化，另一种则适用于成本效益高的通用用途
此外 当我们更仔细地了解亚马逊Aurora时
其性能可媲美商业数据库，成本仅为十分之一
RDS还使您轻松控制对数据库的网络访问
RDS允许您在虚拟私有云中运行数据库实例
这可使您的数据库实例实现隔离
并通过行业标准加密IP连接现有IT基础设施
加密IP
秒
引擎类型提供静态数据加密和传输中加密
最后 与其他选项相比成本极其低廉，只需支付实际消耗的资源费用
并且仅对实际使用的资源收费
此外 您可以享受AWS提供的按需定价优势
无需前期投入或长期承诺，甚至更低的每小时费率
如果您想使用预留实例定价
那么我们继续深入了解
亚马逊提供的六种不同实例的详细分析
PostgreSQL已真正成为首选方案
成为众多企业开发者和初创公司的首选开源关系型数据库
现在RDS使设置
操作和扩展云中的PostgreSQL部署变得简单
通过RDS 可在几分钟内部署可扩展、成本高效且可调整容量的部署
自动管理复杂的管理员任务，如软件安装和升级
存储管理、高可用性复制
以及读取吞吐量和灾难恢复备份
现在 RDS for PostgreSQL为您提供熟悉的
PostgreSQL数据库引擎功能
这意味着您今天使用的代码、应用程序和工具
可与现有数据库兼容，同样适用于亚马逊RDS
只需管理控制台几下点击
即可部署PostgreSQL数据库
自动配置数据库参数以实现最佳性能
部署完成后
可立即扩展至16TB存储和4万IOPS
对于PostgreSQL
还允许通过扩展单个数据库部署的容量来处理读取
高负载数据库工作
MySQL是全球最流行的开源关系型数据库
而RDS使设置
操作 和扩展云中的MySQL部署变得简单
通过管理耗时的数据库管理员任务，让您专注于应用开发
如同处理PostgreSQL时所做的那样
支持MySQL社区版
5.5
5.6
5.0 七
以及8.0
这意味着您今天使用的代码应用和工具都可以与RDS配合使用
它还提供亚马逊云服务的标准备份与恢复功能
以及高可用性和只读副本
这使得弹性扩展变得非常容易
突破单个数据库实例的容量限制
此外 RDS为您的数据库实例提供免费的Amazon CloudWatch监控指标
而RDS增强型监控可访问超过五十项CPU、内存、文件系统和磁盘I/O指标
O 最后
正如使用PostgreSQL一样
通过使用亚马逊VPC获得隔离与安全
或通过亚马逊密钥管理服务
现在 MariaDB的流行程度不及PostgreSQL或MySQL
但它实际上是由MySQL的原始开发者创建的
并且具备其他数据库引擎在RDS中提供的大多数相同优势
这些都包含在RDS中
我想要重点指出的主要优势是高性能
因此您可以再次配置高达16TB的存储
每个数据库支持40,000 IOPS
并选择配备最多32核CPU和240GB内存的实例
4GB内存
因此部署MariaDB非常简便
如果您在本地使用MariaDB并将其部署在AWS基础设施上
这不仅允许您进行横向扩展
还能将部分管理任务交给亚马逊云服务 例如容错和备份
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/091_Udemy - Become an AWS Certified Data Engineer part1 p91 1. Data Veracity.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
让我们谈谈数据真实性问题
因此做出良好决策的能力
例如
Datadog指出，不良数据每年造成超过三万亿美元的损失
现在 这是数据工程师和分析师必须注意的问题
现在 这一挑战被称为数据真实性
数据真实性是指数据的准确程度
精确且值得信赖
现在数据必须经过妥善清洗和整理
这样才能确保数据报告的准确性
现在 这是企业从数据中做出正确决策的唯一途径
但问题还有另一面
许多企业拥有大量积灰的数据和数据库
没错，存放在文件服务器中
这是错失获取价值的机会
现在 这与分析不良数据造成的损失不同
但对企业同样有害
现在 这正是我真正感兴趣的地方
所以回到
如果我要查看
比如说表格
或者 你知道的 不同的行和列对吧
有时很难查看数据
然后理解正确含义
事实是企业多年来早已认识到真实性问题的存在
现在他们甚至知道如何解决
但他们并未投入时间
资金 或资源去修复
他们可以开发内置保护机制的应用程序
对吧 构建执行深度清洗的分析系统
并定期聘请整理和评估数据的专业人才
所以接下来我要讨论
并提供许多与数据真实性相关的重要领域
当你拥有未受管理的数据
来自众多不同系统且无法以有意义的方式整理数据时
你知道这就是真实性问题
让我给出一些基本但重要的定义
首先是数据整理
即选择和组织的过程
整理
然后要管理收藏品中的物品
第二个术语是数据完整性，非常重要
指在整个数据生命周期中维护和确保数据的准确性和一致性
第三个是数据真实性
其基本含义是数据准确性的程度
精确且值得信赖
现在让我们讨论真实性的问题
现在我们知道数据会随时间发生变化
在从一个流程到另一个流程、从一个系统到另一个系统的过程中
数据的完整性可能面临负面影响
现在你必须确保保持高水平的确定性
你分析的数据必须可信
因此数据真实性取决于数据的完整性
那么当前数据完整性本身是什么意思呢
数据完整性就是要确保你的数据可信
这意味着它是否具备完整性
没错，整个数据链是否安全且未被破坏
因此要理解数据的完整生命周期
并知道如何有效保护它将大大增强
增强你数据的完整性
现在，高数据完整性意味着数据源可以可靠地用于规划
决策制定
然后是运营
这是企业使用的所有数据源的目标
然而并非所有数据都是以相同方式生成的
因此 数据完整性是一个广泛适用的概念，应用方式各不相同
在数据生命周期的每个阶段
现在这个生命周期包括创建
聚合
存储 访问共享与归档
所以在创建阶段
数据完整性意味着确保数据准确性
现在这需要对收集数据的系统有一定的信任
现在关系型数据库通过资产合规性来强制数据完整性
现在 确保数据准确无误
需要定期审核软件系统以确认两件事首先
它们生成有效数据或文件
并且这些更改不会对系统的完整性产生负面影响
无聚合是下一阶段
现在数据并不总是需要聚合
但在许多分析系统的关键阶段，指标必须明确定义
因此 此阶段的数据完整性确保用户获得预期价值
他们从提供的聚合结果中期望的内容
此阶段完整性受损通常源于聚合值命名不当
开发者在满足用户需求时规划不足
因此在存储阶段
数据完整性是指以安全形式维护数据并确保所有更改的准确性
某些数据具有高度波动性
意味着它会频繁发生变化
而其他数据则稳定且不会改变
同时只有授权用户和服务才能更新波动数据
现在进入访问阶段
此时数据对用户可见
数据以只读格式提供
现在 这意味着不允许对数据进行任何更改，数据完整性不可被篡改
此阶段的数据完整性是其他阶段完整性的证明
因为终端用户正在查看它
这是业务分析师开始创建用于填充数据的数据
用于报告和仪表板
现在这将进入共享阶段
报告和仪表板创建完成后将在企业间共享
在利益相关者之间
与访问阶段类似
此阶段数据仍为只读
因此数据完整性仅能被证明或存疑
或在此阶段数据已具备良好完整性
此时归档阶段将发挥作用
此阶段数据完整性关乎正确归档
现在您可以删除数据
但更常见的是将其聚合为其他形式并丢弃原始数据
例如
当个人
销售记录已十年历史
其数据可能被聚合为每日或月度总计
此时原始记录将被丢弃
当确保各阶段高数据完整性时
您将保障数据的可信度
换句话说 您拥有高真实性的系统
数据生命周期之外
还存在数据完整性的相关问题
组织内外均需关注
在内部数据源中
若发现外部数据源完整性问题可提出改进建议
例如人口普查局数据源
例如 需识别数据完整性
在发现精度或准确性问题时进行数据转换
现在
希望这些内容有帮助，我们讨论了数据生命周期
不同阶段及
更重要的是
数据完整性及其与真实性问题的关系
希望这些内容有帮助 请告诉我
如果对此有任何疑问 我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/092_Udemy - Become an AWS Certified Data Engineer part1 p92 2. Common Data Issues.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
今天我将在这堂课中讨论常见的数据问题
在需要被纳入数据分析方案的数据中
没错 数据可能存在某种形式的损坏
损坏的数据可能包含不完整的条目
违反业务规则的条目
或被随机输入到记录或文件中的值
建议进行数据清洗以去除这些损坏
数据清洗是检测并纠正数据损坏的过程
无论是记录表、数据库还是文件
数据清洗作为ETL操作的一部分应用
现在 每次从源端提取数据时
ETL操作会再次评估并清洗数据
既然我们谈到了数据清洗
下一个概念是如何确保数据完整性
在确保数据完整性方面
数据库模式是主要防线
数据库模式用于组织对象并强制完整性约束
例如关系
数据库模式还定义了数据库的属性
描述每个对象及其在数据库中的交互方式
逻辑模式常用于帮助分析师理解数据流
这些模式展示表、视图及它们之间的关系
当编写查询时这些信息并不直接有用
逻辑模式帮助分析师编写优质查询
信息模式也协助数据库向查询提供数据
这种模式是数据库中所有对象的元数据集合
元存储还包含表内数据的统计和分布
数据的索引和分区
数据库执行查询时会考虑这三个因素
因此管理
维护和导航这些模式可能耗时
同时优化硬件、软件性能和完整性也具挑战
这就是AWS的作用
如Amazon RDS等托管服务
Amazon Redshift和Amazon DynamoDB可为您处理
这些服务可强制执行模式并优化数据库性能
例如 Amazon RDS
数据库实例
没错 已预配置适合所选引擎和类别的参数
您可在几分钟内启动数据库实例并连接应用
数据清洗和强制数据完整性并非唯一需考虑因素
在评估数据真实性时
我稍后会进一步讨论
此外它们如何确保合规性
但让我们看看更多重要术语
首先首先
数据清洗是一个检测并纠正数据中错误的过程
现在 参照完整性是确保表关系约束
关系得以执行的过程
域完整性是确保输入字段的数据
与该字段定义的数据类型匹配
实体完整性是确保字段存储的值
符合该字段定义的约束条件
识别数据完整性问题非常重要
因为作为数据分析师
你可能需要执行数据完整性检查
在此过程中
你将寻找数据完整性问题的潜在来源
数据可能来自内部和外部来源
极不可能
你能够查看业务外部生成的数据
对 你无法控制
但在你的业务中
你可能有能力提出数据源改进建议
你将接触的数据源
如果无法更改数据源系统的摄入方式
通常由数据工程师或业务分析师
负责检查数据源完整性
并调整以
弥补数据源可能存在的完整性缺陷
因此数据清洗的最佳实践
首先是明确什么是干净的数据
在做任何事之前
就项目中干净数据的标准达成共识
现在 这可能指原始格式的数据
应用业务规则后
也可能指已规范化的数据
聚合后的数据 并应用值替换以规范所有条目
现在 这是两种完全不同的干净定义
接下来
需要明确错误来源
当你发现错误时
追溯到错误源头
这有助于预测哪些数据加载存在完整性问题
并提高ETL操作效率
不 什么是可接受的修改
例如 脱离上下文查看数据值可能导致严重问题
例如
假设你正在
你知道的 正在清理学生的成绩表对吧
在空列中填入零看似是简单的数据清洗操作
但需警惕这一改动的影响
学生成绩列中的零值会降低学生的平均分
而空值则不会对吧
同样道理
将无序或库存编号合并到月度报告中可能看似无关紧要
然而 这些数据可能落入库存管理员手中
他现在认为存在库存损耗问题
这些都是看似微小却可能造成巨大负面影响的例子
所以最后你必须知道原始值
嗯有 你知道的
重要值 例如
在某些系统中 一旦数据被转换，原始数据可能不再有价值
但在高度监管或波动性数据中
必须在目标系统中同时保留原始数据和转换数据
例如
在在线游戏系统中
记录每一步方向可能没有价值
你知道玩家在地图上的操作对吧
唯一重要的是玩家进入或离开关键区域的时刻
在另一个应用场景中
例如 像银行应用
每笔交易的详细信息对审计至关重要
即使客户只关心交易是否成功
我已简要提及
简而言之 讨论过数据库模式对吧
关系型数据库依赖数据库模式
来组织数据库内的内容
并强制执行引用完整性和域完整性
程序员在编写数据库交互软件时使用这些模式
因此数据模式是数据库使用的元数据集合
用于组织数据对象并强制完整性约束
模式定义了数据库的属性
描述每个对象的特征
例如 以及它与其他数据库对象的交互方式
顺便说一句，同一数据库中可以存在一个或多个模式
目前有两种类型的模式
逻辑和物理
逻辑模式关注数据库内数据应施加的约束
这包括表的组织结构
熔断 完整性检查表和视图可以相互关联
现在模式定义了每个关系的信息
以及如何执行约束
模式还可以提供域完整性
通过定义表中特定字段允许值的约束
提供域完整性
不过完整性检查有不同的形式
但 目标是确保对数据库的任何更改不会导致数据丢失
一致性
物理模式 另一方面
它们关注数据在磁盘或云存储中的实际存储
现在这些模式包含文件细节
正确索引
和分区表
集群 等等现在
数据工程师常使用物理模式来估算存储空间
所需及系统潜在增长
这些模式对灾难恢复和基础设施规划也很重要
还有信息模式
你是否想过数据库管理系统如何管理所有数据库
表和关系
嗯 答案就在信息模式中
信息模式是存储数据库内数据对象元数据的数据库
微软 SqlServer拥有所有信息
拥有主数据库中的模式，如Oracle使用
例如 数据字典表和元数据注册表
Apache也使用元存储
每个数据库管理系统可能有不同的名称
但数据结构基本相似
对吧 目的相同，记录数据库中所有对象的位置
最终信息
现在这些数据库存储信息
如表名和大小
表的索引
表中数据的约束
用户的安全设置及外部数据管理配置也可包含
在数据库权限允许下
可查询信息模式了解数据库对象
现在 当处理查询时
这些信息用于优化查询性能
信息模式还可用于数据库维护
好的，希望这对你有帮助
我之前讨论过需要留意的不同数据问题领域
作为数据工程师
没错，当然我还深入讲解了相关解决方案
如果有任何疑问请告诉我 我们进入下一课吧
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/093_Udemy - Become an AWS Certified Data Engineer part1.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回到本节课
我想谈谈一个非常重要的概念——数据库一致性
没错，当数据以文件形式存储时
一致性由开发这些文件的应用程序控制
现在 当数据存储在数据库中时
一致性由存储数据的数据库本身负责
现在我要讨论两种数据库实现一致性和可用性的方法
我所说的acid和base
没错 让我们看看如何实现高一致性
现在数据可以以多种方式存储
但目前我将专注于关系型和非关系型
通常称为nosql数据库
在之前的课程中
我已经讲过如何检查和强制数据完整性，现在
另一个保证准确性的关键因素是确保合规性
以确保数据库内数据的一致性和可用性
现在这里有几种不同的方法，首先
我想谈谈这两种方法
正如我提到的acid和base
现在这两种方法非常不同
在强制数据一致性和可用性的方式上
首先我们来看acid
代表原子性
一致性
隔离性与持久性
现在 acid是实现和维护关系型数据库
数据一致性和可用性的最常见方式
在一个像亚马逊RDS这样的数据库中
例如 一系列操作称为事务，现在可以连续执行数百万次事务
数据变更非常频繁
数据约束也同样常见
acid合规数据库的目标是返回所有数据的最新一致版本
并确保系统中所有数据始终符合所有规则和约束
这一过程会引入系统延迟
必须加以考虑
所有这些属性共同作用
例如 一致性
隔离性 和持久性
acid通过协作确保关系型数据库的一致性和可用性
许多法规要求企业实施acid合规数据库
现在你已初步了解acid在关系型数据库中的作用
接下来谈谈nosql数据库如何实现一致性
没错 许多nosql数据库选择不强制acid一致性
但为什么这是正确的呢，好吧
这归结于一个事实
酸性一致性最关注所有数据的绝对一致性
现在强制这种一致性需要时间
这对于许多无法承担的NoSQL数据库来说是一笔时间成本
如果它们要满足当前应用的需求
这就是BASE一致性的作用
现在 BASE是基本可用软状态的缩写
最终一致性对吧
这是在结构化或半
结构化数据库中维护一致性和完整性的方法
现在BASE一致性最关注数据本身的快速可用性
现在BASE常用于NoSQL数据库和分布式系统
以及非结构化数据存储以确保可用性
数据变更在修改时立即生效
但可能需要时间在所有实例间复制变更
最终所有实例将完全一致
在酸性和BASE一致性方法中
是由数据库实现一致性
而非访问数据的应用程序
强制一致性过程对用户隐藏
对吧，以提供干净的
可预测的数据体验
现在我将在下一讲中详细讲解ETL
但这里我们聚焦这两种方法
首先是ACID
让我们深入探讨 正如我说的原子性
一致性
隔离性和持久性
这是在结构化数据库中维护一致性和完整性的方法
现在ACID合规实际上意味着它是数据
完整性的坚实堡垒 顺便说一句 在亚马逊RDS这样的数据库中
一起处理的语句序列
称为事务
正如我之前提到的
现在 数百万事务正持续不断地执行
因此关系型数据库中的数据和约束非常活跃
ACID合规数据库的目标是返回所有数据的最新版本
并确保系统中输入的所有数据符合所有规则和约束
始终有效
正如我说的对吧
A代表原子性
好的 事务可以包含多个语句或指令
原子性确保所有语句要么全部成功要么全部失败
没有其他语句的支持，任何单一声明都无法成功
C代表一致性
现在 为了交易顺利完成
所有内部语句必须满足数据库中设定的所有约束条件
如果任何单一语句违反
这些检查 整个交易将回滚
数据库恢复到之前状态
现在 这也确保数据更新不会被发布
直到所有副本也完成更新
I代表隔离性
隔离性确保一个事务不会干扰其他并发事务 数据库确实是繁忙的地方
因为隔离性确保当多个事务请求相同数据时
数据不会被破坏
因此
现在隔离性意味着有规则管理数据变更
这有点像转闸
例如 如果两个事务要更新同一条记录
必须等待另一个完成或失败
D代表持久性
数据持久性确保所有更改真正生效
一旦交易完成
持久性确保交易结果永久有效
即使系统故障
这意味着所有生成新记录或更新现有记录的完成交易
都会写入存储而非仅驻留内存
许多法规要求企业使用合规数据库
但在非结构化数据存在
非关系型数据存在
分布式系统正在占据
企业数据消费的日益增长比例
另一种选择也是必要的
这就是BASE发挥作用
即基本可用
软状态 最终一致性BASE
这是维护结构化或半
结构化数据库一致性与完整性的方法
现在BASE合规意味着支持非关系型数据库的数据完整性
有时也称为NoSQL数据库
如非关系型数据库亚马逊
Dynamodb仍使用事务处理请求
现在这些数据库非常活跃
首要关注点是数据可用性而非一致性
为确保数据高可用
数据更改立即在修改实例生效
但变更可能需要时间在实例集群中复制
目标是让变更最终在整个车队中完全一致
让我们进一步拆解右侧内容并深入探讨什么是基础（base）
这里的b代表基本可用（basically available）
现在 这使得数据库的一个实例可以接收新记录或更新现有记录
并立即使该变更对当前实例生效
当这一变更在其他实例间复制时
其他实例最终会变得一致
在资产系统中
变更只有在所有实例一致后才会生效
这就是基础系统的核心所在
完全一致性让位于即时可用性
最终两种一致性模型都能实现完全一致性和可用性
区别在于哪者优先
base中的s代表软状态
在基础系统中
允许实例间存在部分一致性
因此 基础系统被视为处于软状态
也称为可变状态
相比之下
在资产系统中
数据库处于硬状态
正确 因为用户只能访问完全一致的数据
base中的e代表最终一致性
我之前提到过
最终一致性意味着变更立即在某个实例生效
并最终同步到所有副本
在此期间 数据始终以某种形式可用
正确 新旧数据都有
让我举例说明
亚马逊DynamoDB事务
例如 他们当时使用ACID模型
亚马逊推出了DynamoDB事务功能
此功能实现跨一个或多个表的资产一致性
在同一AWS账户和区域内
在需要协调插入的应用开发中可以使用事务
正确删除操作 或作为单个逻辑操作的一部分对多个项进行更新
希望这有所帮助
我已详细讲解了ACID和BASE概念
正确数据库一致性保障
如有疑问请告诉我 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/094_Udemy - Become an AWS Certified Data Engineer part1 p94 4. AWS ElasticCache.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你好 大家好 欢迎来到关于亚马逊云科技弹性缓存服务的课程
可能很多人对弹性缓存或其功能不太熟悉
弹性缓存本质上是米国提供的服务
非常容易部署
运行和扩展流行的开源兼容内存数据存储
类似于名称所示，用于缓存内存中的数据
它是一个完全托管的Redis和Memcached服务
允许您再次部署内存数据存储
您可以利用它构建数据密集型应用
或提升现有应用的性能
通过从高吞吐低延迟的内存数据存储中获取数据
因此它是热门选择用于
例如 游戏、广告科技或金融服务
甚至物联网应用
其优势显著
首要优势是极致性能
再次说明 作为内存数据存储和缓存服务
支持需要亚毫秒级响应的高要求应用
再次强调 通过端到端优化的堆栈运行在专用节点上
提供安全且极速的性能
完全托管
无需执行硬件配置等管理任务
软件补丁 设置
配置监控
等等 持续监控集群确保工作负载正常运行
让您专注于核心业务
更高价值的应用部署
最后是可扩展性
可横向扩展以应对组织或应用需求波动
或您的应用
支持内存扩展通过分片
副本提供读取扩展
多种选项
使用弹性缓存的优势众多
例如 如果 您有内存密集型应用或场景
这是一个非常
非常优秀的强大服务
由亚马逊云科技提供
工作原理 嗯
在环境中的交互方式
假设你有一个互联网规模的应用程序
你知道你在使用游戏
媒体 流媒体
社交媒体
任何需要快速数据访问的应用或服务
这就是弹性缓存发挥作用的地方
因为它提供内存数据存储作为数据库缓存
消息代理 并提供内存存储以实现亚毫秒级响应
例如所有数据都存储在数据库中
无论是Aurora
无论是SQL
响应时间会比弹性缓存慢得多
同样
一些用例是实时交易
你知道的 聊天
商业智能分析
因为如今用户需要快速访问或响应
你知道的 如果出现延迟
即使服务与竞争对手之间有1秒延迟
他们很可能转向竞争对手
必须充分利用新硬件
新技术允许响应时间更慢
尤其是每秒或每分钟数百万请求时
数据库处理
所有请求会显著变慢
相比使用内存数据存储如弹性缓存
弹性缓存支持的两大引擎是Redis和Memcached
现在 Redis本质上是一个提供亚毫秒级延迟的数据存储
支撑互联网规模 实时应用基于开源Redis并兼容其API
弹性缓存Redis也兼容Redis客户端
使用开放数据格式存储数据
自管理的Redis应用可无缝对接弹性缓存Redis
无需代码修改
因为它结合了速度
简单性和开源Redis的多功能性
以及亚马逊提供的可管理性、安全性和扩展性
另一个是Memcached
即Memcached
兼容的内存键值存储服务，可用作缓存或数据存储
提供高性能
易用性和Memcached的简洁性
完全托管
可扩展且安全
使其成为频繁访问数据的理想选择
必须再次存入内存
它在移动应用中非常受欢迎
用于游戏开发 适用于广告技术和电商领域
这取决于你的具体情况
使用的平台或类比方式将决定是否采用Redis
或者是否使用Memcached
以下几点差异能帮助你选择使用哪个
适用于你的组织或应用
大多数组织都使用Redis
但有时也会使用Memcached
许多组织也会重复使用Memcached
这取决于你的基础设施
你的部署方案 你的技术能力
但此表格能提供基本概览
关于各产品提供的功能对比
主要区别再次强调
大家看到的是Memcached采用多线程架构
而Redis则不是
这是Memcached相比Redis的主要优势
但再次说明
根据开发的应用或使用场景决定选择
Redis的一些典型应用场景
我想带你们了解几个
帮助你们理解其使用方式
首先是缓存功能非常合适
Redis同样适合用于缓存
实现高可用分布式安全内存缓存
可降低访问延迟
提升吞吐量并减轻数据库负载
能快速提供高频请求数据
实现亚毫秒级响应
便于在不增加后端数据库成本时扩展负载
若完全依赖后端数据库
必须扩展或横向扩展数据库
成本远高于使用弹性缓存
还有聊天消息场景
支持发布/订阅及模式匹配
支持高性能聊天室
实时评论流 以及服务器间通信
还可通过发布事件触发操作
可构建基于Redis和弹性缓存的聊天应用
以提升或降低延迟并增加吞吐量
因此
基本能提升用户体验 为访问聊天的用户提供更好体验
这非常实用
例如 嗯
你们看到的例子中
你知道的 无论是金融股票还是体育赛事
所以我可以说现在有一场大型体育比赛正在进行
用户们会更频繁地进行聊天
因为有数百万用户
数千万用户登录观看该事件
无论是奥运会 无论是
你知道超级碗
无论你是观看还是主持的活动
这将是一个非常
非常好的工具
为用户提供
毫秒级响应以快速处理memcached
Memcached的两大主要用途是缓存或会话存储
会话存储很容易通过Memcached实现
你可以直接使用Memcached提供的哈希表
并分布在多个节点上
扩展会话存储只需添加节点并更新客户端
利用新节点
这就是使用弹性缓存Memcached的两大主要场景
你知道终端用户
你有Web服务器
Memcached作为内存数据存储连接数据库
本质上这就是Memcached的用法
根据你的具体需求
如果你处于高性能环境
例如 金融市场的聊天、物联网或游戏
Redis更适合你的环境
如果你在Web服务器环境中减轻数据库负载 那么Memcache可能更适合你的环境
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/095_Udemy - Become an AWS Certified Data Engineer part1 p95 5. AWS AppStream.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到关于亚马逊上游服务的课程
我们将进行快速概览
并了解应用程序流提供的功能
这是AWS在服务层面提供的内容
这是一个非常棒的AWS特色功能
因为它允许您完全管理应用程序流服务
您可以在AppStream中集中管理桌面应用程序
同时安全地将它们分发到整个网络内的任何PC
无论是本地还是全球
您可以轻松扩展至全球任意数量的用户
最棒的是您无需获取
配置 任何硬件或基础设施来实现
因为您可以使用AWS
遍布全球的基础设施
我们将探讨这些内容及其优势
呃 接下来幻灯片会介绍
所以总体而言
这是一个非常优秀的服务
Windows也提供了类似功能
允许您在整个网络中分发应用程序
但如果您考虑向云端迁移
这是一个值得使用的优质服务
您可以借助AWS全球覆盖的基础设施
现在看看AppStream提供的优势
首先
再次将桌面应用程序分发到任何PC
用户随时可以访问所需应用程序
通过AppStream抽象化
将应用程序从AWS传输到任何电脑
包括支持Mac
PC 甚至Chromebook
接下来是无需基础设施或扩展的弹性
因为这是AWS提供的全托管服务
优势在于采用AWS按需计费模式
因此 您只需为实际使用付费
可轻松扩展应用程序流至全球任意用户数量
无需获取任何硬件或额外软件
接下来是安全的应用程序和数据
因为应用程序和数据不存储在用户电脑上
使其更加安全
应用程序以加密像素流传输，数据访问在网络安全内
因为运行在AWS基础设施上
因此您可以放心
并受益于AWS全球构建的数据中心和网络架构
同时提供流畅且响应迅速的用户体验
因为每个用户的应用程序都高度响应
因为它们运行在针对特定使用场景优化的虚拟机上
现在我已经看到很多组织仍在使用老旧硬件
但他们想使用一些新软件
与其升级五十或一百用户的硬件
他们不得不让应用程序在旧硬件上运行
这严重破坏了用户体验
最好的一点是应用程序并未运行在用户本地硬件上
你可以配置针对所用软件优化的虚拟机
这再次大幅提升用户体验
最后两个好处对IT管理者非常有利
尤其是可以有效管理应用程序
并能无缝集成到现有IT架构中
每位用户都能访问相同版本的应用
可简单管理不同用户可用版本
此外还能连接活动目录
网络 云存储
甚至文件共享
用户通过这种方式访问应用
使用现有凭证和安全策略管理权限
这就是最佳部分
最佳之处在于
无需额外配置或复杂设置
例如无需添加用户
无需创建权限
可无缝集成到活动目录架构中
所有这些均可迁移至App Stream
显然其运作方式非常简单
操作逻辑无需复杂技术
只是让工作更轻松
例如 第一步是
像在普通电脑上一样安装应用
不同之处在于在虚拟机上操作
再次连接现有身份系统
如果使用活动目录
则连接该目录
或使用上游提供的内置用户管理和持久化存储
由上游提供给PI的
所以 例如 如果不连接内部活动目录
若不需要该集成
上游提供了独立用户管理系统
完成这些步骤后
通过AWS仪表盘进行管理
管理应用程序
并向用户提供流畅体验
用户可安全访问应用
大多数情况下他们甚至察觉不到差异
或者他们不会感受到差异
如果应用程序未本地安装
如果他们通过时钟访问
总的来说这是一个非常非常好的服务值得使用
尤其是当你在分布式环境中部署大量应用时
如果你是全球分布或地理分布的
这是一件非常好的事情
这是一个很好的服务 用于部署和管理你的应用程序
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/096_Udemy - Become an AWS Certified Data Engineer part1 p96 6. AWS GuardDuty.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


大家好，欢迎来到关于亚马逊守护卫士的课程
在当今环境中
安全非常重要，安全至关重要
尤其是网络安全
守护卫士主要保护您的AWS账户和工作负载
通过智能威胁检测和持续监控
现在我们必须采用某种安全措施和监控措施
确保我们的环境免受恶意软件和病毒威胁
尤其是当你在工作时
嗯 在处理敏感数据的环境中
这变得更加重要
但无论您属于哪种组织
仍需具备防恶意软件和病毒保护
并配合持续监控
让我们看看守护卫士如何实现这一点
它本质上是一种威胁检测服务
持续监控恶意活动和未经授权的行为
以保护您的账户和工作负载
在云环境中
账户和网络活动的收集与聚合大大简化
但安全团队仍需耗费大量时间
持续分析日志和潜在威胁
因此使用守护卫士
您可以获得整个AWS云环境的智能高效持续检测方案
其最大优势在于使用机器学习
异常检测
并整合威胁情报识别和优先处理潜在威胁
它不仅仅是防病毒或防恶意软件服务
本质上是一个智能服务，监控全部工作负载和网络
分析数十亿事件跨多数据源
例如 云追踪日志
您的流量日志
您的DNS日志
只需在管理控制台点击几下
即可启用守护卫士无需额外软件或硬件部署维护
如同AWS的大多数服务
这是一个托管服务一键即可部署
再次说明其工作原理
其运作方式非常简单
它是一种威胁检测服务
您只需
启用守护卫士
在管理控制台中
进入守护卫士以监控您的AWS账户
没错无需额外硬件配置
持续分析您的网络和活动
进行分析的机制
您可以看到您的VPC流量日志
或者DNS日志或云追踪日志
因此，本质上所有来自数据库账户生成的日志均由防护服务监控
它将全面覆盖所有内容
这些日志包含所有访问者信息
何时进行访问
传入的数据是什么
传出的数据是什么
最棒的是它
能智能检测威胁
不会以简单方式处理这些日志
结合规则集检测可能发生的威胁
或必然发生的威胁
可以利用分析所有日志获得的知识
并指出检测到威胁或潜在威胁
所以你知道
就像大多数标准杀毒软件
它们会在威胁进入电脑后通知你
防护服务的最佳之处在于可以预防此类事件
因为通过机器学习
以及预防措施
可阻止恶意软件或入侵程序进入系统
因为它能检测日志中的异常
然后在造成损害前采取行动
或保护你的数据
大家已注意到其巨大优势
这是全面的威胁识别
通过自动化增强安全性
具备企业级集中管理
可跨多个账户进行管理
假设你的组织遍布全球
拥有多个配备防护服务的AWS账户
可在同一控制台集中管理多个账户
还可执行自动化修复操作
可通过AWS Lambda或函数实现
例如编程设定当检测到特定情况时
立即触发某些操作
无论是特定邮件活动
还是安全操作等
无需整个团队分析日志
分析防护服务提供的信息
大部分简单任务可通过其他服务自动化
如AWS Lambda
由AWS提供
这是一个非常强大的工具
我认为这是必不可少的工具
无论组织规模大小
每个组织都应部署 嗯 彻底在网络中实施
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/097_Udemy - Become an AWS Certified Data Engineer part1 p97 1. Value in Data Analysis Solution.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来
让我们谈谈价值与数据分析解决方案
当你需要大量数据来支持少数见解时
你可能正在错失数据的价值
现在让我们聚焦这个问题
没错，过去五到六节课里
我一直在讨论进入系统的数据量
数据的摄入与处理速度
以及需要摄入和分析的数据源多样性
我也提到了数据的真实性问题
没错，三大特性（3V）
到目前为止 数据的完整性和可信度
接下来我将讨论价值方面的挑战
在数据分析解决方案中
在做决策之前
必须从数据中提取价值
现在我将介绍所需的分析方法
你知道的 生成有意义的信息及AWS服务
执行分析
并生成报告和仪表盘
我也提到过数据收集
数据收集的速度
数据存储、处理与分析
所有这些都有一个共同点
它们不评估数据的实用性
存储无人查看的海量数据
分析或可视化是徒劳的
现在真正的努力应放在发现数据的实际价值上
然后学习如何提取这些价值
从企业收集的海量数据中
无意义的数据就是无意义的
没错，你不理解的语言文字同样无意义
只有赋予意义，数据或文字才能被轻松理解
现在数据分析可分为两类
信息分析与运营分析
信息分析是通过分析信息挖掘其内在价值
这是数据分析的广泛分类
涵盖从企业财务会计
到分析安全建筑的进出人数
第二种分析形式是运营分析
与信息分析类似
但专注于组织的数字运营
现在让我们讨论不同方法及如何分析数据提取价值
包括可使用的AWS服务
如何通过数据做出决策
没错，分析就是答案
接下来我将介绍两种分析形式
没错 我已经提到过信息权和操作权
主要的两个 现在信息分析是通过分析信息来发现其中蕴含的价值
当然这是一个广泛的分类，涵盖从企业财务会计
到分析安全建筑内的人员进出数量
第二种形式
这就是操作权，现在它与第一种非常相似
但它主要关注组织自身的数字层面
例如
软件应用的行为方式
或数据收集
网络设备与服务之间的连接
但让我们首先聚焦信息分析
因此分析解决方案首先需要确定需要了解的内容
问题是什么
解决方案应探索或定义的问题或现状
这将指导所需分析类型的确定
现在一旦确定这一点
您会选择程序以可视化结果或仪表盘形式展示
现在有五种分析类型
描述性 诊断性
预测性 建议性和认知型
这通常被称为数据挖掘
第一种 这是最古老且最常见的
该方法通过汇总或比较历史数据来回答问题
发生了什么 或正在发生什么
例如
上个月的销售额如何
或第四季度至今最畅销的猫粮是什么
这种分析提供见解
需要大量人为判断将这些见解转化为可操作信息
接下来的分析形式是诊断性
该方法通常将历史数据与描述性分析结果与其他数据集对比
回答了问题
为什么会发生
通过这些信息
您可以回答问题
例如为什么社交媒体曝光量上月下降
或客户投诉增加的可能原因是什么
正如描述性分析一样
这种分析提供洞察并需要人为判断转化为行动信息
接下来的分析形式是预测性
该方法基于过去发生的情况预测未来可能发生的事
例如
未来销售可能如何 销售情况可能如何
你知道吗 也许在未来十年
基于当前销售数据
你能预期的订阅总数是多少
根据去年的发展轨迹
这种分析方式提供了洞察
称为预测
这些预测还需要人工判断来评估其有效性
确保其现实可行
接下来是指导性分析
这才是现在真正的关键所在
这种方法通过分析历史数据和预测来回答问题
到底应该怎么做
就是这样 确实如此 而这种分析形式与之前的三种不同
需要应用规则和约束条件以生成智能建议
这种分析形式的最大优势在于现在可以自动化
实施指导性分析的应用可以提供建议或决策
并根据建议采取行动
机器学习 例如
现在这种方法变得容易实现
机器学习模型 例如
执行分析任务
现在这些模型通过训练过程变得更准确，你训练模型对吧
在训练过程中，应用会多次运行数据通过规则和约束
这提升了模型生成准确建议的能力
例如
亚马逊点 com推荐
例如 或aws推荐
基于购买历史
在制定预测分析时需要人工参与的唯一时间是
当你构建和训练模型本身时
一旦模型进入生产环境
即可独立运行无需人工干预
这种分析提供洞察和建议
最终的分析形式是认知分析
这种分析使用一种称为深度学习的人工智能，现在深度学习
与指导性分析结合，用于基于视觉、听觉甚至自然语言输入做出决策
听觉或甚至自然语言输入
深度学习通过结合现有数据模式模仿人类判断
并在每次分析中得出结论
结果反馈到知识数据库以指导未来决策
这形成了自学习反馈循环
现在想想亚马逊Alexa
例如 或者亚马逊Q这个刚推出的工具
系统从你那里获得的信息越多
效果就越好
就像预测性分析一样
人类参与的重点在于构建和训练模型
当模型投入生产环境后
可以几乎无需人工干预运行
但模型的表现取决于
你知道的 人类的输入是否正确
这种分析能提供洞察和决策
甚至可以建议并执行相应操作
现在 我刚才提到的这五种分析形式
都能利用其他形式的分析结果
正是这种分析组合的力量创造了最强大的分析解决方案
既然我已经讲过
这也是另一种分析形式
实际上它是信息分析中的一个子类，称为运营分析
目前
这种分析专门用于检索
分析并报告运营数据
数据包括系统和安全日志
复杂的IT基础设施事件
例如或流程
用户交易
甚至安全威胁
现在 运营分析使用数学算法和其他创新技术从
你知道的 管理监控技术收集的数据海洋中提取有价值信息
运营分析还结合了这五种分析形式
呃 信息分析
目前
运营分析以软件应用的形式提供
当然 这很合理
这些应用可以进行根本原因分析
帮助用户识别系统行为的原因
服务影响分析和问题分配
帮助组织将事件分配给正确团队调查
现在很多应用现在提供实时行为追踪
同时使用深度学习的认知分析
这些应用 你知道的
会从环境中其他应用和系统的使用模式中学习
这是通过实施认知分析实现的
现在信息分析是挖掘信息中的潜在价值
这实际上与数据分析非常相似
正确的商业决策往往在缺乏可靠信息支持的情况下进行
但希望 当然
每个决策都有坚实的事实依据
但不幸的是现实是决策往往基于假设
这些假设可能有现实基础
但它们仍然是假设对吧
什么 企业需要
是 一种快速高效的方法
从当前收集的所有数据中获取有价值的信息
这就是信息分析发挥的作用
没错
因此它是一系列流程，旨在为企业提供洞察
而运营分析就像我之前提到的
它们有 你知道的系统
或数千个应用程序或系统持续生成数据
所以运营分析只是帮你收集这些数据以供评估
这可能是日志、安全日志或相关威胁
或组织内部
收集所有IT操作数据
现在这些数据是系统日志
安全日志 复杂的IT基础设施事件
例如 或平台内的其他资产
或整个组织基础设施
对吧，例如
IT研究
或IT
分析就是从IT基础设施的全部运营中提取有价值信息
现在在AWS中
例如
亚马逊Elasticsearch服务用于实现分析
运营分析的优势
例如对吧 为什么需要这些
所以你需要这两个
它提供了从流数据中收集的运营数据获取洞察的途径
并且实时进行
当你了解更多信息
就能做出更多行动，通过将本地系统与云基础设施对齐
创建环境让运营洞察驱动决策并节省时间和精力
眼见为实对吧
可视化收集的数据结果
使用运营分析可获取企业系统和大数据源的洞察
因此创建交互式仪表盘
例如 允许用户立即找到所需信息 知识就是力量对吧
因此操作分析能产生强大的
基于事实的决策系统 使企业能够探索历史事件
然后开始使用高级分析预测未来趋势
潜在威胁和重大机遇
所以希望这有所帮助
我已详细讲解了信息与分析及操作分析
没错 这是数据分析解决方案中数据本身的 价值所在
如果有任何疑问请告诉我 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/098_Udemy - Become an AWS Certified Data Engineer part1 p98 2. Types of Analytics.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们深入探讨分析类型的分类
现在你已经了解了分析的两种分类方式
没错 是时候讨论如何实际分析数据本身了
这些分析类型可以同时应用于信息分析和运营分析
那么我们为什么要分析数据
希望到目前为止
所有课程和我们讨论的内容都帮助你构建数据分析方案
没错 核心在于从数据资产中提取高价值可操作的洞察
现在 理想情况下数据会通过自助式商业智能工具如亚马逊QuickSight提供给利益相关者
作为数据分析师或工程师，简单将数据堆砌成柱状图还不够
没错 你的职责是进行描述
向消费者报告
他们实际看到的内容 以及其重要性
并说明如何基于提供的信息推进工作
现在 你也需要明确所需分析类型
以满足每个分析请求的需求
所以预测分析
我们来谈谈这个
亚马逊ML机器学习和一系列服务
包括人工智能AI
例如 使开发者能够轻松将预测分析应用于数据
并向应用添加智能数据处理功能
亚马逊在机器学习领域有着悠久且丰富的传统
许多积累的技术已打包为用户服务
通过这项服务
机器学习栈包含三个关键层级
应用服务使开发者能将预构建的AI功能集成到应用中
无需关注支撑这些服务的ML模型
平台服务让任何开发者都能快速入门并深入ML领域
以及面向ML从业者的框架和接口
例如 在这里你注意到架构示例
使用亚马逊ML为应用用户提供实时预测
在此架构中
多个服务协同工作生成预测结果
亚马逊DynamoDB
例如 是应用数据的存储位置
现在AWS数据管道
负责数据流的调度和预处理以供Amazon SageMaker使用
你可以在Amazon SageMaker中训练机器学习模型
基于用户行为生成实时预测
接下来是认知分析
这就是最新的数据分析形式
它提供了提供高度专业推荐的绝佳机会
无需任何人工干预直接为企业服务
在ML模型的初始配置和训练之后
一些现实中的例子对吧
让我给你们一些实际应用场景和案例
金融软件提供准确实时的事实依据投资建议
是医疗软件
为客户提供可靠治疗方案和最新医疗选项的访问权限是另一个例子
或兽医软件帮助医生快速准确诊断
或类似梦幻足球联赛的软件管理球队
右分析服务和处理速度
基本上第一次将数据推入数据分析系统时
数据将从采集流入暂存存储位置
现在数据将从暂存位置进行处理
并可能被存入分析数据仓库
现在 从暂存位置处理数据可能需要多次
多次以生成多种分析结果
此时需要充分理解不同处理类型的特点
了解不同处理类型能提供什么结果
批量分析 例如
涉及查询大量数据
批量分析在大型数据集上实施
定期生成大量分析结果
基于亚马逊等系统
EMR是支持批量分析的平台示例
运行在Apache Hadoop上的高级框架
例如 可简化并显著加速
你知道的 加快你的批量分析速度
应使用的框架基本取决于你的工作负载类型
与Hadoop搭配最流行的框架有Hive
你知道的 Tah base对吧
Presto或Pig或Spark
这些框架均可安装在Amazon EMR集群
因此各种系统和平台可以相互堆叠或组合
例如
Hive查询引擎使用MapReduce任务
实现SQL查询的各个组件
大多数数据仓库都支持运行批量分析
交互式分析涉及在复杂数据集上高速运行复杂查询
现在 此类分析具有交互性
允许用户即时查询并查看结果
而批量分析通常在后台运行
以定期报告的形式提供分析结果
因此，对于交互式分析
亚马逊Athena可轻松在亚马逊S3中直接分析数据
使用标准SQL查询分析S3和亚马逊S3 Glacier中的数据
Athena是无服务器架构
无需设置或管理基础设施
即可开始查询数据
几秒内获取结果
仅按实际运行的查询付费
只需指向亚马逊S3中的数据
S3存储 定义数据模式
然后使用标准SQL查询，多数结果秒级返回
顺便说一句
亚马逊ES可让您搜索
探索 过滤
聚合 并在近实时中可视化数据
该服务提供易用API和实时分析能力
在可用性
可扩展性
以及生产级工作负载所需的安全性
亚马逊Redshift支持运行复杂
针对PB级结构化数据的分析查询
并包含Redshift Spectrum功能
可直接在S3上的EB级结构化或非结构化数据上运行SQL查询
无需不必要的数据迁移
而流分析需要持续摄入数据序列
并逐步更新指标
针对每个写入数据记录生成实时报告和汇总统计
此方法适用于实时监控与响应功能
数据处理需要两层架构
存储层和处理层
存储层需支持记录排序和强一致性以实现快速
低成本
并且 当然
这很有道理
处理大规模数据流的读写操作
例如
数据存储在存储层
处理层负责从存储层消费数据
没错 执行计算并通知存储层删除不再需要的数据
许多平台提供了构建流数据应用所需的基础设施
包括亚马逊Kinesis
例如 Apache Kafka、Apache Spark Streaming或Apache Storm
亚马逊Kinesis是AWS的流数据平台
当然没错 提供强大的服务，轻松加载和分析流数据
并允许您构建定制化的流数据应用以满足特定需求
现在Kinesis提供两项服务
Amazon Kinesis Data Firehose和Amazon Kinesis Data Streams
如果您的数据流需要格式转换
增强或过滤
您可以使用AWS Lambda函数进行预处理
可以在应用程序SQL代码运行前轻松完成
或在应用程序从数据流创建模式前完成
您可以在Amazon上安装自选的流数据平台
弹性计算云
或 EC 2和Amazon EMR
并构建自己的流存储和处理层
因此在Amazon EC
2和Amazon EMR上构建流数据解决方案 例如
您可以避免基础设施的摩擦 比如
基础设施 资源分配或通过多种流存储和处理框架获取访问权限
此层的选项包括Apache Kafka
您Apache Flume
流处理层的选项
例如包括Apache Spark Streaming和Apache Storm
数据分析方案与AWS服务相辅相成，例如
数据采集
可以批量或流式处理数据
帮助您的服务有Amazon EMR
所以 例如
如果您需要查看和存储数据
可以使用Redshift或Amazon Glue或Athena
所有服务可根据您的方案协同工作
没错 无论使用Kinesis还是Kinesis Streams或Firehose
这完全取决于您的项目需求
数据存储方式有两种：存储对象
或在数据库/数据仓库中存储记录
关于这些
可以使用Amazon S3
这是一个强大的服务，我已介绍过
您也在Amazon S3看到过演示
或使用数据湖架构
对的 可以构建完整的数据湖架构
或使用Amazon RDS关系型数据库或DynamoDB
没错 所以你也可以使用这个
所有这些服务都与你的数据库相关
没错，比如亚马逊Redshift或Redshift Spectrum
所有这些服务都是相辅相成的
正如我之前提到的
这就是其核心所在
数据处理和分析通常分为几个阶段
这是一个交互式过程
首先需要进行初始处理和数据清洗
然后数据将被存入分析型数据仓库，现在这可以是
你知道的 以对象或记录级别存在
数据可能需要重新处理
从分析型数据仓库服务中再次处理或分析数据
对于这些 例如 你可以使用亚马逊机器学习
这会创建模型
然后你可以运行或生成分析
或者使用亚马逊EMR，它能调用其他服务
对吧，或者Glue、Athena
所有这些服务正如我之前所说
所以你可以根据自身需求选择这些方案
这可能是一个好考题
因为他们会给出一个场景
然后让你识别哪个服务
最适合该特定场景
好的 务必仔细分析所有考题
正如我提供的这些例子
对的 所以要理解基于场景的分析
理解每个服务的功能
以及它们的应用场景
正确答案或解决方案
或正确方案
根据你的工作需求
好的 其他服务如Redshift、Spectrum、Glue或Tina
所有这些服务 你需要逐一理解每个服务
然后了解它们如何协作
希望这对你有帮助
我主要讲解了这些服务
以及它们与不同类型分析的关系 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/099_Udemy - Become an AWS Certified Data Engineer part1 p99 4. Creating Your First Visual With AWS QuickSight.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，超级兴奋
继续推进本节课内容
我将演示如何基于自有数据源创建首个可视化图表
所以接下来我们要做的第一步是
当然要导入包含示例销售数据的CSV文件
然后我们将查询首个可视化图表
然后我将演示如何轻松使用Quick Site
接着查看可视化图表的高级功能
首先当然需要登录AWS账户
登录完成后
只需导航至服务页面
在分析类别下点击Quick Site
这将带您进入Quick Site首页
当您在首页时会看到多个数据集
我将导入数据集
通过点击管理数据或新建分析
无论哪种方式都会导向同一页面
我将点击管理数据
这里显示您已有的数据集
我将点击新建数据集
然后当然选择上传文件
因为我将导入CSV文件
现在点击上传文件
先使用仅含100条记录的简化数据集
只需选择100条销售记录
我将提供这些示例文件供您练习
点击打开
这将导入上传文件至Quick Site并加载数据
弹出对话框
确认文件上传设置，可调整字段
例如可能需要排除某些记录或字段
可点击编辑设置准备数据
这将是后续内容
现在专注于基于现有数据创建首个可视化
点击下一步
显示数据源详情
显示100条销售记录
CSV格式 文件名称
将导入至Quick Site
这是后台运行的内存引擎
约1GB存储空间
背景中可见进度
右侧区域
确认销售记录后
CSV文件 点击可视化
将进入实际可视化页面
可开始创建自定义图表
现在默认导入完成
然后在右侧会显示相应的信息
百分之百成功导入
共完整导入一百卷以调味
非常接近 这个美元消息盒
让我先放大尺寸
只需从右下角拖动即可调整
亚马逊快速侧边栏提供的正是自动签名
在显示位置
选择一个或多个字段
让快速侧边栏选择最合适的图表
如果你从左侧字段列表中选择一个
这将为你推荐图表
但我们不会这样做 我们将手动创建
我们的第一个可视化
我们知道所有字段都列在这里
这些是CSV文件的字段
当然图标代表字段类型
所以数字字段会是
例如总成本是计算器字段
数字字段或文本字段
依此类推
首先选择可视化类型
你想要创建的图表类型
我将选择垂直堆叠条形图
一旦点击后
顶部这里
字段区域部分
这会生成x轴
数值
然后分组颜色
我可以将字段添加到任一框中
我当然可以 当然 从左侧拖动字段列表
左侧导航面板
并将其放在x轴上
图表就会显示出来
由于我们创建的是垂直堆叠条形图
需要至少一个x轴维度
好的
这就是图表类型
在返回之前先看另一个例子
如果选择垂直条形图注意
当我悬停在每个选项上时
会显示所需最小维度类型
水平条形图需要一个维度和y轴
例如 饼图需要一个分组颜色维度
树状图
一维和告别折线图
一维和x轴等等
所以你可以大致看一下
例如 散点图需要至少一个x轴度量
这意味着计算字段
正确的度量字段和一个y轴同样使用自定义聚合在一维和颜色
这就是如何得到散点图
因此你需要理解不同可视化类型
这样才能创建相应图表
由于我们的数据涉及销售信息率
这就是我们的销售数据
我们有国家 我们有销售的商品类型
我们有订单日期
订单 Id优先级
地区销售渠道
我们如何销售 是线上还是线下发货数据
总成本 利润收入等等
所以我回到垂直堆叠条形图
需要x轴一维
我也可以选择 当然简单的垂直条形图
那我们继续做一个简单垂直条形图
好的 我们先从简单可视化开始
然后当然我会演示其他类型
垂直条形图表示x轴一维
换句话说 我可以在这里选择x轴字段
这是水平轴对吧
比如我想选择销售渠道
当我尝试将销售渠道拖到工作区
注意它给了我一个蓝色区域
蓝色虚线圆圈
然后中心的圆圈
以及下方的绿色部分
注意字段井顶部这里
我可以拖动添加到x轴
我也可以直接添加
嗯 直接在图表上操作
好的 这里向下滚动一点
或缩小尺寸
这样更清楚完美现在当我拖动销售渠道
这意味着我们要么在线销售产品
要么线下销售产品
这显示我们同时拥有线上和线下渠道
我们线下销售五十种产品，线上同样销售五十种
因此一半在线对应线下
因为有120条记录中的12条
接下来注意到它显示
这个比例很合理
零到二十 四十
六十，依此类推
销售渠道
当我悬停在此处时
会弹出这个小图标
这里显示
左箭头指向
右箭头指向
如果我点击这个按钮
它将进行排序
好的 这就是排序功能
从A到Z
即升序排列
线下和线上对应
同样从Z到A会是线上和线下
好的 这是快速在视图内排序的方法
好的 完美，接下来
让我们在图表中添加另一个字段
我将选择
例如
订单优先级
我要拖动订单优先级
并按组或颜色分类
当我这样做时
它显示我实际使用的四种订单优先级类型
C、H、L或M，默认颜色编码
由QuickSite
当我完成这一步时
它告诉我订单优先级C有九笔线上销售
而H有十三笔
L有十五笔，M有十三笔，依此类推线下同理
我很容易更改颜色编码
例如 如果需要更改
比如绿色
只需点击这个垂直绿色条
然后选择颜色M
并将其改为红色
这将同时改变线上和线下的销售情况
同样地 当然我也可以更改其他颜色
或许在线优先级排序优先
我需要将其改为另一种颜色比如黄色
好的 所以你就能明白这个非常强大的功能
操作非常简便
现在我们有了优先级设置
当然还有我们的销售渠道
我也可以添加其他字段
例如 如果我需要
假设
总营收
好的
如果我在这里添加总营收指标
系统会按销售渠道和订单优先级汇总总营收
好的
现在我的营收数据在Y轴显示
没错 这显示这里是两千万
这里是四千万 六千万
没错，黄色柱状图代表最高优先级项目
在此处线下销售约为三千万一
七十七万两千
九千五百四十四美元
依此类推
这样你可以轻松拖拽创建符合需求的可视化图表
如果我要更改图表类型
操作相当简单
比如做一个饼图
只需点击饼图选项
系统会自动生成饼图
如果需要堆叠垂直柱状图
同样可以生成
同样的操作
比如水平柱状图
同样可以生成
如果不想要这些字段
只需点击字段旁的下拉箭头
移除该字段即可
同样地
如果不想要销售渠道
可以移除
就会回到初始状态
这样你可以根据需求自由创建可视化图表
比如我想添加单价
我想查看单价的指标
然后它会显示五千、一万这样的数值
所以这将在x轴上展示
基于你的自身需求
当然 你可以生成并尝试所需类型的可视化图表
只需让你初步感受创建自定义面部的便捷性
还有其他功能 当然还有其他强大的元素我将逐步介绍
比如计算字段
稍后课程中准备数据
但现阶段只需熟悉拖放字段
然后添加或创建你的可视化图表
在结束本课前我还想展示一个内容
例如 如果你想在x轴或值轴上显示两个数值
或分组颜色
比如说我喜欢选择国家
所以我拖动国家字段并放到分组颜色
现在它显示为水平排列
对的因为我选择了水平方向
让我们选择垂直条形图
这也展示了国家列表
基于线上线下销售渠道的销售数据
所以在这里我可以
当然可以展开
让图表更大对吧
同时注意到右侧有滚动条
对的我可以向下滚动查看不同国家
然后选择每个国家
简单查看有多少个国家
从上到下按字母顺序排列
这些图表同样适用
我可以点击任意一个
例如 如果需要修改标题
只需点击标题本身
然后更改文本
显示内容为部分企业按销售渠道和国家
我可以修改这部分和国家
这样你就能理解其强大之处
这样你就可以自由添加和调整图表细节
同样可以通过点击垂直条更改颜色
当然可以仅关注线下数据
可以排除线下选项
只需聚焦塞拉利昂
例如等等
让我们专注于这一点
现在将只显示塞拉利昂国家
并显示该国的线下销售额
你也会注意到当我这样做时
它创建了一个过滤器
因此过滤器图标出现
在左侧导航面板中
显示已应用的过滤器
现在我已经自动为图表应用了过滤器
所以我取消这个复选框
例如
将返回原始可视化效果
实际上创建可视化非常简单
然后尝试准确识别并聚焦所需内容
希望这对大家有帮助
我只是想覆盖创建可视化的基础
如果您有任何问题 请在讨论区留言
作为课后作业
练习使用不同数据集
如果您有一个小型Excel文件
完美使用它导入数据
上传文件
然后创建不同可视化图表 至此 让我们进入下一课
```

### /content/drive/MyDrive/bilibili/Udemy-BecomeanAWSCertifiedDataEngineerpart1/100_Udemy - Become an AWS Certified Data Engineer part1 p100 5. AWS UI Hub.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在本节课中运行快速站点仪表盘后欢迎回来
我将简要介绍界面本身及其外观
相比其他BI工具如Power BI等要简单许多
我发现AWS功能强大且非常可靠
在仪表盘中你将看到
当然首先是你的主页
或者快速站点的仪表盘本身
这是顶部的特定图标
你可以搜索分析数据集和仪表盘
如果你已知现有数据集或仪表盘的名称在右侧
在第一个菜单栏
这里是你的实例或账户的地理位置
这就是你的快速站点账户
快速站点的计数数字
你可以点击管理快速集账户跳转到其他页面
你可以实际查看
并管理存储桶、权限等设置
所以在演示前
让我先完成页面中心的其他区域
你会看到所有分析
所有仪表盘
目前这里将没有内容
因为我们尚未在此创建任何内容
当然还有一些教程视频
作为课后作业
你可以先浏览这些基础内容
以便熟悉第一套功能
嗯 创建第一个计划
或自定义可视化图表
等等 现在返回所有分析页面
这些是快速站点仪表盘的默认示例
例如网站与社交媒体销售流程分析
人员概况
然后 当然 业务审查分析
在快速站点图标下方
你会注意到新建分析选项
如果点击此按钮
将跳转到数据集页面
这些是之前看到的相同数据集
右侧显示快速站点自带的示例数据集
再次展示香料使用情况
记住香料只是内存引擎存储所有数据
因此可即时获取反馈
无需等待和提取数据
标准账户默认提供1GB存储空间
好的完美 所以回到主页这里
管理数据
按钮也会带你到同一页面
正确的数据集 所以操作相当直接
好的 无论哪种方式都可以创建新数据集
或者使用现有示例
如果我点击新建数据集
会跳转到查询界面
新数据集页面
在这里我可以选择多个数据源
正确的数据源
事实上 这就是我可以手动上传文件的地方
例如CSV文件
TSV文件
可能还有Excel文件
我可以连接到Salesforce
现有的S3 RDS
MySQL SQL Server等
这里有多种数据源
如果你需要使用
看看其他云平台
比如谷歌云平台
例如
或OpenShift
你会看到类似的可连接数据源
可以连接到Spark
马里奥 TB
GitHub
Twitter等
这赋予了你能力
顺便说一下还可以连接ServiceNow
好的完美
多种选项
如果你想创建自己的数据集
我将在后续演示中展示
如何连接这些大多数
以及它们的实际工作原理完美
让我们回到主页
这就是界面本身的全部内容
操作相当直接
我想展示的是账户部分
由QuickSite管理
我将点击此处管理QuickSite
这会带你到账户名称
这显示了现有用户
由于只有一个用户
这是管理员账户
当然你可以查看你的订阅信息
香料容量
例如 大约一吉字节
此时如果需要可以购买更多
但默认免费账户自带1吉字节
你可以点击账户设置
在这里可以修改权限
换句话说 如果需要连接另一个存储桶
例如你可以这样做
如果我点击编辑AWS权限
这将带我到一个页面
我可以在此指定额外权限并编辑现有AWS资源的快速访问
在之前的课程中
还记得我们选择了自己的存储桶对吧
而不是使用默认站点存储桶
所以在这里你可以 当然选择自己的存储桶
你可以创建自己的存储桶
如有需要
这让你了解实例中的操作
我们正在使用clear ask dash admin存储桶 正确
所以关闭这个
顺便说一下在关闭前
如果你有其他存储桶
可以点击S3存储桶
你可以选择不同的存储桶
或选择其他AWS账户的可访问存储桶
由你决定
所以关闭这个
返回我们的快速侧边仪表盘主页
这关于界面操作相当直观
只需多加练习
如有疑问
在讨论区提出 我们进入下一课
```