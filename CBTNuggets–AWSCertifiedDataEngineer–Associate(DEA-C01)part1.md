### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/001_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p01 1. Skill Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你好 大家好，我是斯科特·弗莱彻
这是AWS认证数据工程师关联课程
无论你是经验丰富的数据老兵
想学习这个新潮的AWS玩意
或者你是资深的AWS专家
想要增加你的工具箱中的数据工程技能
你在正确的地方
在这个课程中 我们将深入探索AWS和数据工程的世界
我们将讨论从存储解决方案到大数据分析
再到高级分析
我们还会加入一些机器学习
因为完整的互联网视频
怎么能没有滥用机器学习的地方呢
在这个第一技能中
我将给你一些课程信息和期望
然后为了圆满结束
我们将创建一个AWS账户
这将成为我们自己的数据工程实验室 让我们开始吧
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/002_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p02 2. About this Certification.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我们现在开始这个派对
有时候我跟您说话
我会像这样全屏
有时候我会像这样在角落变小
有时候我会像这样隐藏在阴影中
所以我在这里切换到画中画模式，让我们谈谈aws
认证数据工程师关联考试
关于这个特定的考试，我们首先应该知道的是，他们希望你具备一定的经验
这不是一个入门考试
也不是一个入门课程
所以他们希望你在数据方面有两到三年的经验
并且在aws上有一到两年的实际操作经验
我理解实际操作的程度可能不同
因为如果你每天做同样的工作两年
只使用aws的一个服务
这将与使用17个不同服务在aws上的经验大不相同
所以请将此视为参考
我真的想要的是这里
我建议如果你如果我使用这种句子
让我们使用我们的etl工具将csv数据转换为json格式
如果你能理解
那么你可能没问题
如果听起来像是完全不同的外语
那么你可能想后退并尝试一些更入门级的东西
如果我说结构化
非结构化或半结构化数据
如果你觉得这有道理
那么你可能走在正确的道路上
如果你完全听不懂
那么也许
你可能想回头重新考虑一些入门的东西
如果我做出这样的声明
如果你能理解这个并且知道这说的是什么
那么我想你不会有任何问题
如果另一方面
你看着这个说这到底是什么
这意思是
那么是的
这可能对你来说课程有点太多了
所以我现在要回到阴影中去
我们将转到下一张幻灯片，谈谈考试本身
现在考试本身是多项选择题或多项选择
多项选择题就是提出一个问题
然后你可能有五个不同的选项
你有一个小的复选框
你可以选择这五个不同的选项之一
多个响应略有不同
因为你可以实际选择多个响应
所以会有小复选框
他们会问你以下哪三个选项是正确的
你可以勾选
你知道 勾勾 勾那种东西
现在有50道评分问题和50道未评分问题
为什么他们会在考试中放未评分的问题
嗯，考试编写者一直在为考试库编写新问题
但是让他们写问题有点不公平
然后只需将它们放入测试数据库中
也许问题表述有误
可能它有点令人困惑
也许它有点有争议
他们通常会在问题真正被铸造到测试数据库之前对其进行贝塔测试，类似于这种情况。
所以你可能会看到一些奇怪的问题
或者它们可能覆盖某种刚刚在几周前发布的尖端服务
这可能就是那些问题现在的样子
我经常被问到一个问题，那就是嘿
上周刚推出的一项新服务
我需要担心它在考试中吗
答案是不
你不需要 因为aws说一个服务必须在至少六个月内广泛可用
才会在考试中以得分问题出现
所以不要担心从天而降的服务
它们在尝试构建问题时可能会以未得分问题出现
关于那项特定服务的数据库
但这不会得分
所以别担心 如果你答错了
现在 及格分数是七百二十分一千分
现在 什么是七百二十分
是七十二分的题目
不 完全不是
这有点像谜团
他们如何实际评分他们保留给自己
真正对账
考虑到没有两个考生会得到完全相同的考试
因为他们有一个庞大的问题库
问题有不同的难度级别
他们覆盖不同的东西
所以他们在后台使用这种缩放因子来尝试找出
然后对考生公平
现在我们对考试本身谈了一点，下一集我们会继续
我想解释一下我的学习哲学以及我们如何开始
让你达到感到舒适的程度 进入那里并参加这个数据工程副学士考试
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/003_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p03 3. Course Expectations.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我完全认识到，参加这门课程的每个人都会有不同的学习旅程
我们来自不同的背景
有些人可能对学习充满热爱
他们渴望走出去，学习一切
有些人可能参加这门课程
因为他们希望职业晋升
他们想要加薪或晋升
或者他们希望从其他职业转向数据工程职业
还有一些人可能感到好奇
或者可能持怀疑态度
他们听说过这个aws
以及aws在数据工程和数据管理方面的能力
他们想要了解更多
这就是我如何进入机器学习的
事实上
我是持怀疑态度的 我正在试图弄清楚这一切
它不是魔法
嗯，不是 它不是魔法
这就是我如何进入的 所以我能理解如果你持怀疑态度或者只是感到好奇
还有一些人被要求参加这门课程
我也同情这一点
也许你们的组织决定迁移到aws
或者将数据仓库迁移到aws
作为其中的一部分，我们必须确保
组织中的每个人都得到了足够的培训
以便你知道你在做什么
我认为这很值得尊敬
但我也理解有些人可能将这门课程视为一种负担
所以我想说的是，请耐心
这不是传统的课程
我没有传统的sage on the stage类型的教学风格
我更像一个guide on the side
所以希望您在课程中会看到这一点
我想向您介绍一些东西
这叫做bloom's taxonomy
这在我们试图确定我们如何试图攻击事情
或者我们如何学习新事物时是有用的
基础是记住
这是关于记忆
你只是发展出了回忆事实的能力
在上面
复杂性和 sophistication 的下一个层次 从学习角度来看是理解事物
你可以解释事情或概念
在上一个层次是应用
一旦我们理解了事情
我们需将这些应用到某些事物上的逻辑是合理的
在本课程中，我们将尝试专注于这里应用的步骤
在aws数据工程师蓝图中，他们有明确指出的东西不在范围之内
那就是分析数据
我们将操纵和移动的数据
基本上那就是下一步
那就是取数并据此做出商业决策
他们明确指出这不是
这个认证的目的或范围
我完全理解我们的意图
我想让你达到能够将这些概念应用到构建数据管道
或数据清洗过程
或类似的东西
这将帮助你构建你的数据工程过程在aws上流畅运行
所以为了达到这个目的，我们不会花太多时间在记忆
或理解方面
我会向你展示东西
然后我们会一起解决
通过一系列的挑战
一旦我们到达第一个挑战
我会告诉你它们是如何工作的
但想法是
它们让你选择自己的路
选择自己难度
希望你会选择最难的
因为我认为这是最有趣的，你会学到最多
但它是根据你的水平稍微调整的
所以如果我们看aws的认证家族
这里
我们有数据工程师关联 所以它是一个中级认证
入门级是云从业者
这些高级一点的是在你完成这些中级项目后
你可能要参加的东西
所以我们不会谈论超先进的东西
但如果对你来说是新的
那么它将是相当具有挑战性的
我确定
你报名参加了这个课程
你将获得认证
很好 认证意味着你知道这些东西吗
它是知识的证明吗
不是的
我知道很多很多聪明的人
但在考试环境中他们只是崩溃了
也许你也是
相反，我也遇到过很多证书一大堆的人
但在实际操作中
他们连在aws上如何脱困都不知道
所以为了确保我们意见一致
我不认为认证等同于知识的证明
完全不是
真的
我认为它们提供了一个学习的框架
它们设定了一个框架，你可以学习或朝着这个方向发展
而参加考试并不是
最终的结局 那只是学习过程中的一个检查点
总结一下
记忆与理解
应用与分析
我肯定想要我们专注于理解
应用与分析
认证将帮助我们保持专注
让我们直说吧
在aws中有很多内容
你可以深入研究
但考试大纲中明确指出
这是范围 这是出范围
这些框架将帮助我们保持专注
我也想传达一个事实
我相信这不仅仅是通过考试
当然你可以通过考试
有很多优秀的考生可以通过考试
但这是否意味着他们真正理解他们所做的事情
可能不
在很多情况下，在这个课程中
我们将讨论基于场景的活动
我将构建一个场景
你将需要解决这个场景
这就是我们如何避免
这种记忆的东西
并真正专注于
理解应用
在这个课程中，可能不会太多分析
但确实会应用和理解
所以 我们在这个课程中如何做好这些事情
显然，我们将有音频视觉组件
霍先生说
就是你现在正在学习课程的平台，还有一些其他功能
我会包括一些文本
有时是为了澄清
有时是为了强化
有时它就是一个完全的非 sequitur 来保持事情的有趣
有时我可能会引导你到特定的aws资源
比如一个我找到的特别有帮助的YouTube视频或博客文章
我们也在整个技能中设置了测验问题
这些问题中的一些设计得非常接近考试问题
目的是让你多练习
实际上 在每个技能结束时，我们都会有一个验证部分
这部分将由一些模仿或看起来像AWS的测验问题组成
考试问题
我将向你们展示这些问题
我会问你们尽力去回答这些问题
然后，视频结束时，我会有一个视频
来解析这些问题
不仅揭示答案，还会分享
我的策略，告诉我如何简化这些问题
并提高我们得到正确答案的机会
现在，关于这门课程，我认为
学习最好的方式之一就是卷起袖子
动手去做，去一些技能中犯错 我会在AWS上走一遍活动，你们可以跟着做，如果你们愿意的话
此外，我会有一些挑战，这些都是基于现实世界的场景
你们可以决定难度级别
我刚才在视频中提到过
等我们到第一个挑战时，我会详细解释
那么，还有其他什么方法可以加速你的学习吗
嗯
我发现一个有用的方法是
如果你和你的同事一起参加这门课程
也许你们一起参加，然后一起学习
也许你们做一个小组学习会 或者午餐学习会
你们一起看视频，然后互相挑战做挑战，互相交流
之后，我看到其他人制作闪卡
这更多是记忆的东西
但有时这对帮助你建立联系是有用的
是的 AWS的文档相当不错
虽然我一般不喜欢在需要真正理解某事时阅读文档
当我真想了解某事时，我会去AWS的文档
他们还有一些小信息，当我们看到这些时，我会指出
有时文档中有一些特别标注的信息
这些都是很好的 学习考试的好地方
因为AWS相信你真的需要了解这些事情
有时文档中会有些称为关键信息的东西
这些都是很好的
学习考试的好地方
因为AWS相信你真的需要了解这些事情
有时文档中会有些称为关键信息的东西
这些都是很好的
学习考试的好地方
因为AWS相信你真的需要了解这些事情
我会指出一些，当我们看到这些时
也会和这些人交谈
与那些有过经验的人交谈
那些在aws上有经验的数据工程师
和他们交谈，他们总是最愿意分享他们的经验
在大多数情况下，他们总是愿意分享他们的经验，他们是如何走到今天的
以及他们希望当初能做得不同的地方
所以，课程结束时
我希望你能熟悉常见的数据工程策略和概念
特别是它们与aws和aws平台之间的关系
你应该熟悉AWS的行话
至少在数据工程领域
你也应该了解一些最佳实践
以及AWS希望我们知道的一些潜在危险区域
以确保我们不会做坏事
当然，我希望你能够通过AWS
认证数据工程师关联考试
但我也希望你能够帮助别人开始自己的云之旅
因为我们在学习过程中的一个重要事情是
一旦我们学会了某事
这能帮助他人非常有用
教会他人
因为这不仅帮助了他们
也帮助我们 因为这帮助我们巩固信息
我相信你听说过这句话
看一个 做一个 教一个 这是一个非常有力的学习机制
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/004_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p04 4. Walkthrough Creating our AWS Account.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的，剩下的技能部分
我们将卷起袖子，动手做
我们将进入aws控制面板
但在我们进入aws控制面板之前
我们将不得不创建一个账户
我明白你可能正在与一个已经有账户供你使用的公司开始
不要用生产账户来做事情
在这个课程中
你可能会惹上麻烦
从安全角度来看这样做
我推荐的做法是创建一个只属于你自己的特殊账户
也许如果你们组织愿意为你创建一个账户
比如一个沙盒账户
在那里你可以几乎做任何事情
你需要对这个账户有管理员权限
以便进行我们在这门课程中要做的一些活动
所以我的建议就是从头创建一个账户
你将处于免费层
大多数情况下 对于我们将要使用的大多数服务来说
它们可能属于免费级别
或者至少使用那种东西的费用只有几美分
所以只管出去
只管去那里创建一个账户
然后你就不必担心
无意中将你公司的秘密暴露在互联网上
或者类似的事情 所以让我们开始吧
所以创建我们的账户
我们将前往aws amazon com
这是aws s的主网页
您的屏幕可能看起来不同
这里的广告和促销活动经常变化
如果您的屏幕看起来不同
那么请不要太担心
我们最感兴趣的是这里这个按钮，上面写着
创建aws账户
我将点击它
它将带我到一个注册页面
或者至少是注册页面的第一部分
如果我有一个现有的账户
我可以直接登录
但我打算创建一个新账户
所以这里是第一个屏幕
让我放大一点
我们被要求提供一个根用户电子邮件地址
这个根用户是我们aws账户中最有权限的用户
它将是我们账户中的第一个用户
并且将是最有权限的用户
它具有在账户内执行几乎所有操作的能力
使用根用户执行大量操作并不是最佳实践
因为根用户非常强大，容易出错
最佳实践建议我们应该使用另一个账户
稍后会详细介绍
但现在我们需要一个根用户
这可以是你的电子邮件地址
这可以是别名
这可以是转发到其他位置的电子邮件地址
它需要一个真实的可送达的电子邮件地址
因为你注册时
他们会向你发送信息和确认
以及每月的账单
发送到这个电子邮件地址
我将输入这个电子邮件地址
斯科特·普莱彻加上云实践在cbt nuggets com
我们被要求提供一个账户名称
这个账户名称可以是任何你想要的
但它必须在其他账户名称中必须是唯一的
所以可能需要一些试错来找到最好的账户名称
所以我要尝试
但是你的云练习
我们要验证这个电子邮件地址
这将向这里的地址发送一封电子邮件
并确保这是一个真实地址
所以我现在不得不去查看我的电子邮件
它要求我输入验证代码
所以我来做
我有我刚收到的验证代码
这是903
然后验证
因此它已经验证了我们的电子邮件地址确实是一个真实的电子邮件地址
现在我们被要求输入我们的root密码
因为root用户是我们账户中最强大的用户
我们应该使用一个相当复杂的root密码，不容易被猜测
因为如果它容易被猜测
某人可能会进入您的账户并做坏事
所以我将在这里输入一个密码，以满足这些条件
这里它需要有大写字母
有小写字母，数字和一些非字母数字字符
而且我必须在这里再次输入以确认
然后它说嘿
我们可以继续进行下一步
所以我们要点击那个
现在它问我们将如何
使用我们的账户
它是商业还是个人
我只在这里选择个人
它会要求
我们应该联系谁关于这个账户
这是我们的联系信息
所以我会把我的信息放在这里
然后我会填写我的电话号码、地址和邮政编码
以及所有其他信息
然后我会勾选这里的小方框，上面写着'我已阅读并同意服务条款'
我鼓励你
如果你对这些事情感到担忧
请阅读服务条款
这里有一些你不能做的事情
你确认你同意不能做某些事情
这些特定的事情是你可能不会做的
比如使用aws账户攻击其他账户或公司
或者发送病毒等疯狂行为
它还说你同意支付账单
所以我会填写所有必要信息
然后继续到下一个屏幕
我已经进入下一个屏幕，它正在询问我的账单信息
这里有一个注释
说明在免费阈值以下我们不会收取费用
但我们可能会暂时保留1美元作为待处理交易
只是为了验证这确实是一个有效的信用卡号
所以我会输入我的信用卡信息，然后继续到下一个屏幕
好的，我在输入完信用卡详细信息后
我进入了下一个屏幕
它正在询问我确认身份
我可以通过两种方式做
我可以让他们给我发短信或打电话，我会选择发短信
我会在这里输入我的手机号码
我必须完成这个小的验证码来证明我不是机器人
我猜的
然后我可以点击发送短信 让我现在做
好的
所以我已经输入了我的手机号码信息和验证码 我会检查我的手机
果然
我有一条短信，让我输入那个代码，好的，继续 它正在询问我们想要注册哪种支持计划
因为我们只是想学习
我们可以选择基本支持
基本上就是电子邮件支持
你将处于队列的底部
如果你想要更高级的支持
你可以选择开发者支持每月29美元
或者选择商业支持每月100美元
如果你是一家打算在aws上运行工作负载的公司
我强烈建议你至少选择商业支持级别
因为如果你在运行关键任务
我强烈建议你至少选择商业支持级别
因为如果你在运行关键任务
对您的业务至关重要的关键应用或应用程序
那么你肯定不想等待
直到有人最终回答您的支持问题
所以这将给您提供一些优先级响应时间，它还有其他好处
但现在我们将使用基本支持
我将完成注册
祝贺您
感谢您注册AWS
现在我们可以转到AWS管理控制台
我们可以点击这里的按钮
它将提示我们登录
我们有两个选项
我们有一个root用户
这是我们刚刚创建的用户
然后我们还有一个叫做iam用户的东西
我们将在后续的技能中讨论它
但如果我们现在想登录
我们可以在这里输入我们的root用户名
然后我可以点击下一步
它将提示我输入密码并点击登录 这将使我们能够进入我们的aws控制台
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/005_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p05 5. Walkthrough Applying Account Best Practices.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好吧，现在我们有了我们的aws账户，我们准备开始。
我们准备好了，对吧？
我们还有几件事情要做。
首先最重要的是，
我们需要为我们的根账户设置多因素认证。
我恰好使用ubikeys，
你也可以使用这个小设备，它给你一个数字，
然后你输入一次性密码，
或者你可以使用一个应用程序，
如果你有像bit warden或lastpass这样的东西，
或者类似的东西，
我们将为我们的根用户设置多因素认证，
因为根用户是我们账户中最重要的用户，
如果它被盗用，
我们就麻烦了，
所以，
我们将首先做这件事， 接下来，我们将为我们的日常使用创建一个iam用户，
使用根用户做所有事情是不好的做法，
因为它太强大了，
我们不需要它，
我们将为这个目的创建一个iam用户，
我们还将为这个iam用户设置多因素认证，
当然，我们应该总是使用多因素认证，
这从来不是谈判的选项，
好的，
所以我们要做的另一件事是，我们将启用云追踪，
我会解释云追踪是什么， 如果你已经知道，
那么你就知道了，
我们还将启用aws预算， 这将非常重要，
因为它将确保如果我们不小心留下了什么东西，
我们不会在月底收到这个巨大的账单，
所以这些都是我建议每个人在做新账户时做的事情，
让我们开始吧，
所以，
我现在想与你分享的一件事情是我们的根账户用户，
我们绝对想启用多因素认证，
你可能在你的个人账户中使用过，
比如你的银行或雇主， 你需要输入你的用户名和密码，
然后可能他们会要求你输入另一个代码，
一个应用给你提供的代码，
或者他们给你发送短信，
你需要输入那个代码，
这就是多因素认证，
所以我们绝对需要为我们的根用户添加这个，
这就是多因素认证，
所以我们绝对需要为我们的根用户添加这个
因为再次强调，root用户是我们整个账户中最有权限的用户
所以这里给了我们这个警告
我是怎么来到这里的
我去到这个小下拉菜单并点击了安全凭证
这带我到了这里
我想为这个分配一个MFA
现在我们有多种不同类型的MFA选项
我们可以使用验证应用
这有点像 我不知道
如lastpass bit warden duo
我认为also可以做到
你可以指定
你想要存储你的你的代码生成器
你的一次性密码代码生成器
在某种身份验证应用中
我们也有安全密钥
这是一个硬件密钥
Ubkey或其他FIDO安全密钥的例子
我对Ubkey很感兴趣
我尽可能多地使用Ubkey
尽可能多地地方
因为我只是喜欢它的易用性
它们非常安全，我们也有一个硬件令牌
现在 这有点老式
但你可能仍然有这些
你可以买这些小令牌，它们有一个小的LCD屏幕
你按一下按钮
它显示一个代码
你可以将这个代码输入到AWS登录过程中
当我们登录时
它将允许你通过
对于我们的目的 我将使用验证应用
只是因为我认为这可能是最常见的方式
你可能需要创建一个多因素认证
如果你没有应用来做这个
你可以很容易地下载一个
我认为微软有一个，谷歌
我知道有一个
基本上任何密码管理器
都会有这个能力
就像我说的，lastpass bit warden
那些东西 所以我要说
让我们看看bit warden
因为我使用的是密码管理器
我将选择验证应用
所以我将点击下一步
它为我提供了显示二维码的选项
如果我在手机设备上有这个应用程序
我可以轻松地扫描这里的二维码
如果我没有这个能力
我可以显示我的密钥
我将复制并粘贴这个密钥到我的密码管理器
然后它将要求我输入这两个连续的代码
两个MFA设备代码
这同步了我的设备
与我的设备生成的代码同步
与我的设备同步AWS保存的代码
它使用秘密代码
再加上时间功能
我将用我的Bit Warden移动账户扫描这个二维码
我将保存它
哦，好的 我已经保存了
我可以回去并重新获取它
它正在向我提供一次性密码
我将输入它
然后我将等待它生成一个新的
然后输入那个
假设你在安全方面
你在说 他怎么敢显示他的MFA代码，那不是很危险吗
一旦我录制了这个视频
我将删除这个特定的MFA代码并更改为其他内容
此外
可能到课程发布时
这个账户将不再存在
因为我经常更换账户
我将输入我的第二个一次性密码
添加MFA
好的 我可以注册多达八个MFA
如果你有机会，我真的建议你输入多个MFA
特别是如果你使用的是硬件MFA
它们可能会丢失 然后总是有一个备份
在这里，我的多因素认证已启用
下次我登录时
在我输入密码后
它将要求我输入这个一次性密码
这是必须再次做的事情
你知道，这不是必须的
但这是人们通常做的事情
那些这样做的人不会出现在前页新闻中
他们的AWS数据被盗或泄露
所以我们必须做最好的实践 我们必须遵循这些最佳实践
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/006_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p06 6. Walkthrough Applying Account Best Practices Part 2.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 所以我又回到我的直播账户
首先我们要做的事情是开启云追踪
所以我们可以在这里 搜索云追踪
它就在这里，因为我在使用控制塔和组织机构
这两种服务都超出了本课程的范围
但我是用那些服务创建了这个账户
他们已经为我创建了追踪
什么是云追踪
嗯 云轨迹 我差不多把它看作是
它是安全摄像头
它是天空中的眼睛，能看到一切
任何人与这个特定账户的每一次API交互
都会在云轨迹中记录
这非常有用
例如，如果你有某种安全事件
或者数据泄露
然后你可以使用这些云轨迹日志来尝试做一些取证
并尝试找出他们获取了多少数据
他们获取了什么之类的
所以我已经设置好了我的轨迹
如果你将账户设置为独立账户
你可能没有这些
所以我们只是来这里创建轨迹
我将其命名为某物
我们只将其命名为数据工程轨迹
它将询问我们是否想要创建一个新的S3存储桶
或者现有的s三桶现在
我只是把它放在这里来创建新桶
它将会以这个名字创建一个新桶
现在正常情况下
你可能只是让这个启用
这个日志文件
Ssc kms加密
这是用kms密钥管理服务来加密日志
当他们被写入那个桶时
你可能至少会认为最佳实践会说
是的 当然，加密所有静态数据
嗯 这个问题是每次注册一个API时
它会去S3中写入
并且它会调用KMS来加密这个数据
每次调用KMS都会花费一点点钱
但时间长了就会累积起来
所以我要取消这个选项
这只是一个沙盒账户
我对确保它加密不感兴趣
如果你在用真实账户
可能那些已经为你设置好这些事情的人会已经处理好了
你不必担心这个问题
但我现在要取消勾选这里
日志文件验证
我真的不关心那个
我真的不关心
S s 我会保持所有设置不变然后点击下一步
好的 现在系统在问我想要记录哪些类型的事件
我可以选择数据事件
我可以在这里找到数据事件
我可以选择具体的服务和要记录的特定数据事件
什么是数据事件呢
这发生在数据被写入、读取或修改的任何时候
如果我们在谈论lambda
例如或DynamoDB
那么这将记录所有事件
如果我们选择或者只能阅读事件
或者只写事件
那里有很多东西
所以我们现在不会那样做
我将要取消选择数据事件
我更感兴趣的是管理活动
那些是像登录日志中的事件
注销账户创建
那种事情
我将点击下一步
而且这将会给我一个小确认
我对所有这些东西都很满意，我的轨迹已经创建
这就是我所要做的一切
我已经创建了这个小轨迹
我们可以点击这个
它将带我们到那个s三个桶
在这里它已经创建了我们的云轨迹
它需要一些时间来那里填充一些数据
所以我们可以稍后再检查
作为事实 作为本课程的一部分，我们要做的事情之一
是使用云追踪来监控我们对数据服务的API访问
好的 所以现在启用了云追踪
接下来我想让你做的事情是出去设置一个预算
这样你就不会惊讶于超支
所以我们要去账单和成本管理
然后我们在这里滚动到预算
这就是我们的
这是我们的预算 我们现在要制定预算
我们有一些选项可以使用这个简化的模板，这里已经提供了
或者我们可以自定义预算
我将使用这个模板
现在我们可以选择零预算
这基本上意味着
如果我们花费超过一美分
它会提醒我们并说，嘿
你花费超过零美元
这非常有用 如果你想确保你坚持使用免费层
我总是发现这非常困难
尤其是我使用的一些服务
他们有时会花费金钱
但他们只花费几分钱
我将使用月度成本预算
我将保留这个名字不变
我只说50美元
如果我花费超过50美元
它将通知我
现在我需要输入我的收件人邮箱
这将是我的账户
这里我们完成，点击创建预算
就这样 这就是你所要做的
走开 你现在有了预算
现在 最后一件事我们要做是
我们要创建一个iam用户，我们将每天使用
因为我们不想用根用户做任何更多的事情
现在 我将前往
我是
身份和访问管理
在你的aws经验中
你应该有一点aws经验
iam是aws的核心服务之一，很难错过
所以在这个课程中，我们会看到iam
一点 所以我不会花太多时间详细解释它是什么
你应该已经知道
但我们会深入探讨一些规则和策略
所以我要创建一个用户
我将其命名为scott
我想要提供aws管理控制台的用户访问权限
是的 它将问我
我可以输入自定义密码
或者它可以自动生成密码
我将在这里输入自己的密码
然后它会询问
用户在下次登录时是否需要创建新密码
我将取消勾选
因为我就是我 我刚刚创建了我的密码
那就是我的密码 我将点击下一步
不要现在保存
我将在这里有这个选项将用户添加到组中
嗯 我认为这是一个很好的主意
我将创建一个组
我将创建一个具有管理员访问权限的组
我们可以在这里滚动
你可以看到我们有不同的管理员访问组
用于不同的服务
这是大师级管理员访问
在我们课程中要做的活动
您需要管理员访问权限
所以我将选择那个并点击创建组
哦，我得给它起个名字
管理员创建组，好的
现在我可以选择该组并说我想成为该组成员
我的用户名
该组成员
现在创建用户
当然 我想要做的是立即设置
为该账户启用多因素认证
所以我将基本上重复过程，启用多因素认证
它给了我一个小警告
这里 它说 来吧
我将转到安全凭据
在这里滚动到多因素认证
然后重复我已经做过的过程
我肯定你可以自己搞定
就这样
我们已经实现了我们的账户
我们已经用MFA保护了root用户
我们还设置了预算警报
我们还打开了云追踪
我们还创建了一个日常驾驶员用户在我们的个人用户中
从今以后我们将使用这个用户登录到我们的控制台
并且我们将忘记我们的root用户 因为我们不需要它
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/007_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p07 7. Validation Getting Started.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在技能结束时
我会有三道问题
它们将模拟考试问题
我做这个有两个原因
第一，只是为了尝试加强我们在技能中学到的知识，第二，参加aws考试
考试问题是一种艺术和科学
有一些技巧和窍门
可以帮助你真的增加正确答案的机会
即使你对问题的主题有点不确定
所以我会向你展示我是如何解决这些问题的
因为我们课程中的第一个技能
骨头上的肉不多
所以，话虽如此 所以，我们的问题有点轻松
但我仍然想走一遍
只是为了给你一个预期的想法
所以，让我们开始吧，第一个问题
以下哪些是最佳实践
在创建新的aws账户后
选择良好
我们有几种不同的方法
我们可以从不同的角度来处理这个问题 我们可以选择四个
或者因为选项只有12345个
我们可以找到明显不是答案的那个
剩下的就必须是答案
我们可以往下看
启用小丑轨迹 是的
我想那样做 为路由用户启用MFA
是的 创建预算
是的 创建IAM用户
而不是使用根用户
是的 我应该这样做
打开一瓶冷饮，看辛普森一家的重播
作为最佳实践，我很乐意这样做
但是遗憾的是AWS
认为这不是创建新账户的最佳实践
所以如果我们采取不同的方法
我们可以找到这项选项
这不是其中之一
这里有所有其他选项
所以我的最终答案是这个
继续前进
什么 亚马逊云追踪对我们的账户有什么作用
第一 它能保护我们的账户免受拒绝服务攻击
它实际上并不能保护我们
如果你记得云云轨迹记录东西
事实上
这就是答案记录
所有进出我们账户的api调用
这正是云轨迹所做的
它是否为我们提供了一种使用联邦账户管理登录的方式
不，它不是那样做的
它是否确保账户只能访问他们应该访问的东西
它并没有那样做
所以这里答案是我最终的答案
继续前进
提高在考试中选择正确答案的机会的关键策略是什么
仅仅依赖记忆
不，仅仅随机猜测
不 我不会说专注于只熟悉主题
你将会遇到你不熟悉的考试问题
所以这可能不是一个好策略
以艺术和科学来对待考试问题
是的 在我看来，这是一个好答案
跳过困难的问题
不 这不是一个好策略
然而 对于考试者，我建议的一件事
是你做一个单一的通读
对所有问题的第一次通读
并大致了解任何你可以立即回答的问题
去回答它们 任何给你带来一点困难的问题
标记它们，然后回来
但我不建议完全跳过它们并假装它们不存在
我们将在后续技能中讨论这一点
我会向你展示我是如何通过我的考试问题的
我希望这对你有所帮助 感谢您观看
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/008_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p08 1. Skill Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，美丽的学习者，来到另一个技能
我住在美国中西部
我们这里有很多农民
我对农民有极大的尊重
他们的工作难以置信的艰难
充满了风险
如果你幸运的话，到年底可能勉强收支平衡
事实上 如果你仔细看我的背景
这里有一张海报
火星上也需要农民
我们现在需要农民
假设我想成为一名农民
比如养殖 例如
奶牛 羊 鸡
这类东西 你认为呢
我计划中的第一件事
跑出去买一堆动物
不 当然不
我需要确保我为他们建造一个生活和放牧的地方
我不想确保他们受到保护免受捕食者
确保他们有床铺和东西
他们需要感到安全和无压力
现在 让我们用这个作为数据工程的类比
不幸的是 很多人只是开始积累数据
在他们意识到需要保持所有数据安全之前
但那不是你
因为你在这里
为了这个技能 我们将学习两种基本的我们在aws上保护数据的方式
在网络级别和身份级别 让我们开始
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/009_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p09 2. Shared Responsibility Model.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我们谈谈我们在aws方面的责任
所以如果我们在数据中心或公司总部安装了软件
无论怎样
很明显我们对该软件负责
我们也对硬件、备份和冗余负责
如果是关键任务应用
我们还负责补丁和确保安全设置正确
但我们也负责围绕它运行的所有基础设施
例如 数据中心的火灾抑制系统
空调
不间断电源
维护这些东西
你知道 火灾抑制和空调它们不需要进行一些维护就能无限期地运行
所以我们也必须对这些负责
如果我们迁移到aws
有一个理解那就是aws将负责火灾抑制
和空调
以及电力和维护
以及所有这些
因为这是交易的一部分
这是我们选择使用亚马逊网络服务时签订的
这真的很重要
aws希望我们确保我们确实知道责任线在哪里
让我们说
例如 我们移动一些数据到这里
无论是实例还是s3桶之类的
well aws不知道那个实例里是什么
或者那个桶里是什么
那可能只是垃圾数据
或者那可能是非常敏感的数据
那应该有个小锁在那里
所以那应该是非常敏感的
或者它 它应该受到那种保护
但亚马逊网络服务不知道
他们让我们很容易保护它 通常情况下
在存储中加密只是一个屏幕上的按钮选项
或者是一个复选框之类的
但那是我们的责任
因为我们才知道那是需要保护的敏感数据
因为aws希望他们对他们的责任与我们的责任非常清楚
他们制定了一个称为共享责任模型的东西
它定义了aws负责的以及我们负责的
总的来说
所有基础设施
运行我们现在使用的这些服务的所有必需品
在这些服务之上
我们还有责任
如果它是敏感数据
那么我们就有责任启用服务器端加密
以确保数据在存储时被加密
最终 如果这是客户数据
那么它绝对就是我们的责任
AWS本身并不负有责任
因为我们把我们客户的数据放在了他们的平台上
现在，这种共享责任模型有时候会变得有点模糊
让我给你举个例子
假设 例如
我们正在使用e
C two 它是一个虚拟机，运行在这台硬件上
这是aws为我们提供的硬件，是的
事实上 在那种情况下，我们对操作系统负责
任何类型的网络和防火墙配置
我们还对在那台虚拟机上运行的任何应用程序负责
当然
如果我们有客户数据
我们对系统中的客户数据负责
如果我们想，我们有选择启用加密的选项
当我们谈论e2机时，我们有很多控制权
二 机器
现在控制很多意味着我们有很多责任
让我们对比另一个服务
AWS或亚马逊工作邮件
什么是工作邮件呢
那只是电子邮件帐户的管理服务
所以我们可以订阅一个两个
三个一千个工作邮件帐户
我们可以把这些交给我们的用户
他们可以使用像Exchange或Outlook之类的产品来访问工作邮件
在那种情况下我们真的一无所知
我们是否需要了解操作系统，以便知道该系统正在运行的操作系统
我们所使用的只是邮件服务
我们不真正关心应用程序
只要我们可以使用Outlook或类似的东西
当然，我们可能负责将Outlook连接到工作邮件
但就工作邮件而言
只要它能发送和接收消息
那么一切都很好
所以AWS基本上会处理这里的所有事情
但这并不是说如果我们发送一封敏感的电子邮件给客户
然后AWS不对那个客户的数据负责
这是电子邮件
使用它存在一些风险
如果我们想绝对确保保护它
这是我们的责任使用称为pgp的东西
或者某种其他加密
这样我们可以加密电子邮件并发送给客户
这就是我的意思
当我说有时候
这个共享责任模型有时会变得模糊和令人困惑
但关键点是
这是一个管理服务
大部分情况下
如果你使用AWS
你真的想使用管理服务
为什么因为这是一个很好的例子
你越能外包给AWS
对你越好
我看这些事情就像嘿
那正好赚了我钱
因为我不用担心上面的任何麻烦
我可以只为我现在想使用的目的使用服务
为什么我们想要使用管理服务
很多人会问这个问题
因为他们来自一个本地的情况
在那里你必须自己做所有事情
他们对使用管理服务有点紧张
因为他们不相信任何人做这件事
让我用一个比喻来解释
假设 例如
我们有一个比萨店
我们镇上有很多比萨店
但我们以两件事最出名
第一 我们的面饼很好吃
我们已经花了很多年时间来完善这个面饼
我们有做比萨饼的完美配方
此外，我们还以木烤比萨饼而闻名
所以我们的木烤箱给比萨饼一种特殊的质地和风味
所以我们以这两点而闻名
所以这两点帮助我们在竞争激烈的比萨店市场中脱颖而出
使我们独一无二
好的 每天当我们来上班时
我们有一个餐厅职责清单
我们必须混合和准备我们的面团
这是一个非常重要的步骤
因为我们以我们的面饼而闻名
我们美味的饼底
我们必须切洋葱
我们必须切蘑菇
我们必须切意大利辣香肠
我们必须切番茄
以及可能还有其他人们可能想在他们的比萨饼上加的配料
最终我们必须做比萨饼
无论订单何时进来
我们必须在烤完比萨饼后清洗锅
或者我们可能有用餐顾客
我们用完后必须洗盘子
然后最终我们也必须照顾木火烤箱
那东西不会自己运行
现在我们必须用木头喂它
如果你是餐厅老板
如果你正在查看这家餐厅的职责清单
如果你知道你在这里以你的面团而闻名
如果你知道你在这里以你的木材和比萨而闻名
但如果你发现自己在一个资源有限，员工有限的情况下
你最有可能做得很好
可能我的钱花在了找一个能切洋葱的人身上
我可以找一个能切蘑菇的人
理想情况下，他们应该能在我们的后补仓库里切好
我可以找一个能切意大利辣香肠的人
我也能找到一个能切番茄的人
这些东西
我可能能在一个餐厅供应商那里找到一个能切好的人
一个能直接给我切好的东西的人
我为什么要考虑这个呢
那是因为作为一个企业很难区别自己
仅仅根据你切洋葱的方式
蘑菇 意大利辣香肠和番茄
在那边没有多少空间来区分
所以聪明的商业操作者会看看他们能让自己的服务真正区别开来的东西
所以我们看看其他的事情
我们没有多少可以做外包制作比萨
同样的事情与洗锅这里
所以我是一个聪明的商业业主
我可以找到一个合理的价格让别人做这件事
我不必担心做这件事
这对我来说是一个胜利
让我们从另一个角度看待这个问题
一个e C 两个实例现在
如果我们在aws上运行一个EC two个实例
我们需要负责升级
补丁 备份
可扩展性 监控故障容忍度
所有这些事情
这些都是我们现在的责任
如果我们看看rds这样的托管数据库
嗯 我们真的不对这些事情负责
因为aws处理这些事情
因为它是一项托管服务
这就是托管服务的价值所在
有些事情很难创建
增量价值和差异化
我认为升级
补丁 备份
可扩展性 监控和故障容忍度真的很难在业务中区分自己
如果你是一家公司
所以这就是为什么aws总是建议，可能的话
使用托管服务
我也建议这样做
因为你能多把工作交给aws，你就会越好
所以
让我来谈谈当我们谈论托管服务时，我经常听到人们谈论的一件事情 嗯
是不是托管服务让我们失去了控制权 不
它并没有真正失去控制权 它只是把那些没有差异化的工作交给了别人
理想情况下，是把它交给别人用自动化来做
这就是aws所做的，他们有数百万客户
他们不会到处跑着
手动安装软件和升级
他们会用自动化来做，通过自动化
你得到更多的一致性
更高的效率等等
所以失去控制权，不，我会说
这更像是专注于创造价值的事情
通过将这些没有差异化的事情外包出去
我们就可以将我们的精力，资源和金钱投入到创造价值的事情上 而这真正就是共享责任模型的视角
我们有这么多的责任
但如果我们能让aws在这里接管这些事情
嘿，这可能最终为我们赚钱
这就是共享责任模型应该被看待的方式
我们有这么多的责任
但如果我们能让aws在这里接管这些事情
嘿，这可能最终为我们赚钱
嘿 那才是重点
而且这也使我们自由了
专注于对我们和我们的客户来说更重要的事情
这就是一个共享责任模型
现在 那将发挥作用
当我们开始讨论我们可以保护aws资源的不同方式时 我们将在下个视频中这样做
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/010_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p10 3. VPCs and Network Security.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


使用aws，我们有两种主要方式来保护我们的东西
第一种是通过网络保护或网络访问
第二种是通过身份
首先，我们将探讨如何在网络层这样做
当我们谈论共享责任模型时，这里又出现了
如果你记得，我们越能推动aws s的责任向上
使用管理服务
这是一件好事
但现在我们将谈论这里的基础层
我们谈论的是区域
可用区和边缘位置
仅作复习
如果您对AWS全球网络不是非常熟悉
AWS全球网络由所有这些区域组成
这些区域通过高速数据连接相互连接
在很多情况下
AWS自己铺设了光纤，因为它们不想依赖第三方
它们通过光纤将许多这些不同的区域连接到一起，跨越海洋
一些这些区域是针对特定群体设计的
例如
政府云 你不能仅仅作为普通公民使用政府云
你必须是美国政府机构的一部分或与其有关联
例如 美国东部2号在这里俄亥俄州
我们可以使用它
如果我们有权访问
如果你去你的aws控制台
你可以看到所有的区域在这里
实际上它只是像做那个小型下拉菜单一样容易
你被神奇地传送到那个特定的区域
所以我们来做一个小的深入研究
这里有一些位于美国东海岸的区域
我们有ca一号中央
它在那里
我认为那是蒙特利尔
我们有us east one
它分散在弗吉尼亚北部
我们有us east two
它在哥伦布俄亥俄州附近
所以我们深入研究us east two
我们将在那里看到
我们将看到该地区不同的可用性区域
现在地图上的位置并不精确
AWS非常谨慎
关于他们数据中心的物理地址非常保密
因为他们想确保他们不会遭受破坏
或者类似的事情
现在这些us east to a to b to c
这些并不总是一个物理建筑每个可用性区域
一般来说，他们的物理建筑集合位于某种邻里或区域中
它们之间有非常高速的数据连接
在这些可用性区域之间也有非常高速的数据连接
为什么有可用性区域呢
如果因为可能龙卷风或其他事情经过这里并清除掉其中一个区域
如果我们在规划故障容忍和多可用性区域策略方面做得很好
那么我们完全不会受到影响
因为我们仍然有两个应该正常工作并支持我们的应用程序
这就是为什么有不同的可用性区域
这里有一些其他例子，新加坡有三个
我们有弗吉尼亚北部
美国东部1是创建的第一个区域
所以它里面有很多东西
而且他们一直在添加可用性区域
我们还有欧洲西部1
在那张初始地图上你看到了很多可用性区域
但这里才是它们发挥作用的地方
我们有亚马逊网络服务
我们有区域
然后在这些区域中我们有可用性区域
总的来说，我们在亚马逊网络服务围墙花园内受到保护
我们受到保护，因为我们在外面是公共互联网
外面充满了各种龙和恶劣的东西
试图获取我们的东西
我们可以决定允许任何东西进入我们的可用区，然后访问公共互联网
反之，我们可以允许公共互联网来访问我们的东西
但这取决于我们
这是我们的责任
我们可以选择是否开启或关闭
我们主要通过一个叫做虚拟私有云的方式来做出选择
让我们深入探讨一下
虚拟私有云也被称为vpc
vpc只是aws的一小部分
这是我们自己的私有云空间
它特定于某个区域
如果我们往回看
它将存在于这个区域
所以我们这里有一个vpc
但正如我们将看到的
我们可以将我们的东西放入这个区域内的任何可用性区
我们可以提供互联网访问权限
如果我们想要
但我们也可以选择完全孤立
如果我们想要
我们不必为那个虚拟私有云提供任何访问权限 如果我们不想
所以这里我们有亚马逊网络服务
我们有我们的区域
我们有我们的az
所以我会在这里设置一个VPC
然后我会在这里放一个子网
然后在我想使用的这些AZ中的任何一个
我可以有一个子网或多个子网
这不重要 所以这里这部分构成了我的虚拟私有云
我的VPC
让我们换个角度看
所以这里是VPC
这里是子网 子网A和B
现在我们定义了一个不同的可用区
当我们定义一个子网时
我们还必须定义它将使用哪个IP范围
我们将定义这个块
10.1.2.0/24
这将给我们从0到25
这个块中的地址
说实话 我们不能使用所有这些地址
因为有些地址我们无法使用
因为它们是保留的地址
但我们现在不会深入探讨
但定义一个子网时
你会设置一个地址范围
当你说嘿
在这个子网中启动设备时
它将使用这些IP地址中的一个
所以子网中的所有设备
你知道IP地址将是什么
它将在这个范围内
让我擦掉这个
这样我们看得更清楚 好的
所以第一种控制我们VPC和子网访问的方式 叫做网络访问控制列表
或者称为NACLs
至少酷小孩是这么叫的
如果你想成为酷小孩
你也可以叫它NACLs
那么什么是网络访问控制列表呢 嗯
它只是定义了一个IP地址和端口的范围
我们可以允许进出我们的子网 所以我们会定义一个NACL
我们可以说允许任何从17.0.2.4的流量
允许它到公共互联网
这意味着如果这里有一个资源
它试图在这里出去
它将被允许
这个节点将说嗨
是的 你在允许列表中
所以它现在允许这个出去
如果你恰好在这个地址范围内
它将出来说，不
你被拒绝了 所以它将把你送回去
实际上它什么都不会说
它只说，嗨
你不能出去 这就是网络访问控制列表
我们可以在我们的vpc中设置它
它可以保护什么可以进来和什么可以出去
接下来我们有一个叫做
安全组的东西
安全组有点不同
我们在vpc级别上不能定义IP地址范围
它们是在实例级别
我们将一个安全组分配给这个实例
我们可以把安全组想象成一个小的个人防火墙
那个实例的规则列表
安全组的酷之处在于我们可以允许其他安全组访问
如果我们有一个数据库在这里，让我们说一个应用服务器在这里
我们可以为我们的数据库创建一个sg
我们可以为我们的应用服务器创建一个sg
我们可以说，嗨，只允许这个sg的成员访问您
所以你不必担心定义IP地址或IP范围
如果这个应用服务器是该安全组的成员
它将能够访问我们的数据库
这就是安全组的独特之处
网络访问控制列表和安全组之间有几个其他差异
主要是网络访问控制列表是在子网级别
这意味着它们在vpc子网级别
我们可以根据他们的IP地址范围分配访问权限
正如你所看到的，安全组是在实例级别
安全组的另一个独特之处是它们只允许
我们可以只允许入站流量
或者我们可以允许出站流量
意思是 允许出站流量
但我们不会拒绝任何东西
我们没有拒绝的能力
而网络访问控制列表我们可以说，嘿
任何从这个IP地址进来的人试图使用此端口号您将被拒绝
然后有一些隐含的允许
在网络访问控制列表中，你有规则1
规则2 规则3
这些规则按顺序执行
尝试看看这些交通是否符合任何规则
如果符合
那么就会执行该规则
如果不符合 如果
例如 在这个列表的最底部，我们有拒绝
它不匹配这些其他规则
那么它将默认被拒绝
如果你对网络感兴趣
你可能理解这一点
网络访问控制列表被认为是无状态的
而安全组被认为是有状态的
这现在不是特别重要
这可能对网络课程或类似课程更重要
我只是想解释安全组和网络访问控制列表之间的区别 因为在本课程的后期，我们将使用这些来保护我们VPC中的各种资源
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/011_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p11 4. Identity and Access Management.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以在我们之前的视频中我提到了
我们可以通过两种主要方式来保护东西
一个是网络
另一个是通过身份
我们将特别讨论身份和访问管理
或aws
我是和iam是我们aws账户的一个非常中心的组成部分
它允许我们安全地提供对账户的每一个部分的访问
以及账户内的每一部分数据
我们可以将这些权限应用于用户
像我们这样的人会登录
但我们也可以将它们应用于服务
也许我们有一个备份服务需要访问我们的东西
以便能够备份
我们可以将角色或策略应用于该服务
我们也可以将它们应用于整个系统
因此我们可以将角色应用于e
C Two系统
然后它将允许在e
C Two系统上执行的东西继承该角色并采用那些权限并执行某些事情 我们还可以将它们应用于我们的本地系统
通过称为iam的东西
Anywhere aws
允许我们将这些权限应用于本地系统或硬件
然后它可以使用该权限与aws交互
现在iam提供的主要商业价值是身份验证
授权和护栏
当我说护栏时
我指的是它防止我们做事情
比如犯错误
或类似的事情
这是iam的一个重要部分
很多人认为它非常严厉
我们需要将每个人都锁住
这样他们就不能做任何事情
在大多数情况下
一个正确实施的身份和访问管理策略首先和主要是 确保保护您的资产
这意味着要保护它们免受外部威胁以及内部威胁
我在这里提到
身份验证和授权
这两个词
不幸的是它们经常被用作同义词
所以非常令人困惑
但实际上它们是不同的
所以这里是每个的定义
身份验证
你是你所说的人
你是 你已经授权了吗
你可以做什么或者你被允许做什么
让我们以 例如
这个小仓鼠在这里他坐在外面
或者他正站在一个非常受欢迎的夜总会外面
他有他的小记事板
我们来到仓鼠面前说嗨
让我们进入俱乐部
仓鼠说嗯
你是谁 我们说嗯
我们是
我们开始恐慌 我们开始冻结
我们说我们是查克·诺里斯
他看了看名单
确实查克·诺里斯出现在名单上
然后仓鼠说
让我看看你的ID
于是我们在口袋里翻找
我们拿出我们的ID并给仓鼠看
他说不
你不是查克·诺里斯 你骗了我
所以我们无法被授权
即使我们说我们是查克·诺里斯
作为好的门仓鼠说
让我们看看你的ID来尝试验证我们
如果我们是查克·诺里斯
我们会拿出我们的ID
我们会向我们的ID展示
实际上我们是查克·诺里斯
我们会验证
实际上
在我们排队后面的那个人是真正的查克·诺里斯
他走上前去 说我是查克·诺里斯
仓鼠说 让我看看你的ID
他被验证了
他现在被允许进入俱乐部
一旦你进入俱乐部
让我们说有一些区域
有一种公共区域
然后那边有一个VIP区域
有很多保安
也许他们有精致的食物和其他东西
查克·诺里斯被授权因为他是一个名人进入那个VIP部分
他有权在那个俱乐部里几乎任何地方去
这就是身份验证和授权的区别
通常你必须首先进行身份验证
然后一旦你被验证
那么系统将试图找出
你能做什么
你有权做什么
让我们看看身份验证的组成部分
首先我是
我们有用户嗨
那是我们 我们有一个组
那是我们中的一组
然后我们有角色，角色是
你可以这样想
它是我们可以打包分发的权限集合 所以可以想象它是一个钥匙环上的一些不同钥匙
那些不同的钥匙允许我们做不同的事情
如果我们看这些钥匙
它们被称为策略
至少在iam术语中它们被称为策略
每个策略都定义了我们能做某事的能力
请注意，这是授权
我们有授权去做某些事情
身份验证与授权不同
让我们看看不同寻常的常规
每当有人尝试登录
所以这里是我早期的日子
作为价格正确的主持人
那是我生命中的一个美好时光
这里
我是用户 我正在尝试登录
嘿
我是scott iam返回并说嘿 是的
就像那个小门仓鼠 我说这是我的密码
iam说，我还是不信任你
因为如果你了解我
你知道我使用uv钥匙
我使用多因素身份验证
每个人都应该使用多因素身份验证
密码是不够的
我说，嘿
这是我的ub钥匙和aws
iam说
好的 我会信任你一点
这是你的钥匙
AWS 我已经确认了我的身份
所以它知道我的身份是什么
它知道所有这些东西都是我的身份的一部分
它挑战我证明我是谁
一旦它确认了我是谁
它给了我一套钥匙
它给了我被分配给我的角色所包含的访问权限
这个角色由这些不同的政策组成，这些政策允许我做不同的事情
现在 让我们看看另一种方式
我们也可以通过这些第三方服务验证自己
你可能听说过样例登录
或者像联邦登录那样的东西
我们可以使用这些联邦登录系统
比如谷歌工作或OTA
我们可以通过那个登录
然后那些第三方登录系统会说是的
他是好的 他是他所说的人
然后我是就说好吧
我会相信你一会儿
这是你的钥匙
所以本质上，这里的整个验证过程在这里发生了
它可能涉及同样的东西
一个密码 以及我们登录AWSiam时可能会使用的多因素身份验证代码
但如果我们直接通过AWSiam登录，我们会这样做
IAM 但这仍然会发生
然后我们决定说嘿
我相信我们正在验证的系统
一旦IAM得到肯定的回应
那么它将向我发放一套钥匙 我可以用它们来做事情
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/012_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p12 5. IAM Roles and Policies.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 在我们之前的视频中我提到了AWS中有不同的服务组件
我是 其中一种组件被称为IAM角色
现在我们可以使用角色
当然可以给特定的人或组分配角色
我们也可以将这些角色分配给不同的服务
如果你熟悉AWS Lambda
例如AWS E
C2 我们也可以使用角色来在ssm中分配
并且基本上这个过程只涉及说嘿服务
我信任你执行在这角色中封装的权限
如果你记得的话，这个角色的作用有点像钥匙链
并且那个钥匙扣上的钥匙是不同的政策
不同的支持因素使得那个角色能够给予它所需要的安全，从而能够给予安全。
现在，这些被称为服务角色
因为它们适用于这里的服务
我们也可以使用这些服务角色为其他aws账户分配能力
因此，例如，我们可以设置一个服务角色
我们可以定义它指向一个e c two实例
并说嘿 e c two实例
这就是你的服务规则
在那个服务规则里面
我们可以说嘿
这个其他aws账户能够管理你
因此当那个其他aws账户
或者那个其他aws账户里的人尝试登录并尝试访问那个特定的资源时
我说是的
你被授权了 你有权限并且有政策去这样做
我们如何做到这一点
在细节中是通过iam策略
正如你所记得的
我使用了钥匙环上的钥匙的类比来表示iam策略
嗯 有几种类型的iam策略
一般而言
我们有基于身份的策略和基于资源的策略
现在两者都使用json语法定义
而且他们使用了完全相同的语法
主要的区别在于，身份策略是用于用户和组以及角色。
基于资源的政策被应用于资源
我们将在这里的几张幻灯片中详细解释这一点
但是首先我想专注于基于身份的政策
首先，身份为基础的政策
我们有一个称为aws管理策略的东西
现在 那是AWS自己创建的政策
他们已经把它放在那里供我们使用
现在我们无法更改那些aws管理策略
它们被认为是一种只读
但它们涵盖了广泛的主题
例如，如果我们使用dynamodb
这里有一个dynamo Dynamo db
全权访问策略
我们可以将该策略分配给一个角色
然后，任何在该角色中的人都将基本拥有对dynamodb或我们分配给他们的任何数据库的全权访问权限
现在我们也有自己的客户管理策略
这些是我们自己创建的策略
因此，我们可以在iam控制台中查找
我们可以创建一个策略，该策略包含对某些资源的某些权限
或者某些组或标签等
然后我们拥有该策略，它是我们的，我们可以将其分配给角色
然后该角色
当然将能够执行该策略定义的任何操作 现在我们也有一种称为内联策略的东西
内联策略是我们可以直接分配给可能用户的东西
我们不一定需要将策略分配给一个角色以使用它
我们可以直接将该内联策略分配给该用户
因此，该用户
例如将具有该能力
在资源策略方面，这些都是内联策略
例如，如果我们出去到s3桶
我们可以为该s3桶分配资源策略
但它将是内联策略 因此，我们将在管理控制台中查找
在管理控制台中有一个区域，我们可以在那里分配该策略
我们将编写该策略
然后该策略将应用于该资源
好的
所以让我们看一下策略的实际外观
所以这里我们上去 在这里的前面
这里有一个可选的小部分，称为版本
这是为了可能跟踪策略的版本
这是可选的
你不必拥有它
但我们有一些必填区域
首先我们有声明
首先在声明中，我们有主体
现在
主体是策略适用于什么
在这种情况下，我们有power用户角色 策略内容
这是策略的内容
策略权限
这是策略的权限
所以你可以看到，我们已经定义了一个名为power users的角色
这是我们的aws账户号码
而这是我们的角色及其效果
它将做什么
它将允许某种操作，即动作
通常来说
当你看到定义的操作时
它们将包含服务在这里
然后冒号
然后是这里定义的任何操作
你也可以做s3冒号以及通配符
这会产生影响，就是说嘿
我们允许power users做所有所有s3操作
但在这种情况下我们说
我们将允许他们获取对象和获取对象版本
我们可以进一步细化并限制它只针对资源
这正是我们在这里做的
所以资源说comic underscore books
斜杠通配符
这意味着我们说嘿
有一个名为comic books的桶
我们说这个特定角色将具有访问权限
以获取对象和获取对象版本以获取comic books s3桶中的所有对象
现在更详细了
我们还可以定义条件，考虑这有点像
如果那么条件，所以在这里我们有
如果字符串等于s3现有对象标签环境等于生产
那是什么意思
我们可以根据标签定义访问权限
所以在这种情况下
如果我们有一个被分配的环境标签
我们有一个环境标签在外面
并且我们设置它等于生产
然后这种i am策略将通过那里并说嘿
那个对象有一个生产标签吗
如果是 是的
那么这些规则将适用如果不是
那么这些规则将不适用
所以这种特定策略将不允许我们做任何事情
也就是说我们可以创建另一个策略并将它们放入同一个角色中以处理其他情况
也许我们只想让人在生产标签上只进行只读活动
但在沙盒中我们使用沙盒标签
他们可以做任何事情
这就是策略的结构
所以让我们更详细地看一下
所以，我在这里
我有我的角色
以及我们的comic books桶和我们的trading cards桶
这里我们称之为集合角色
这是我们的漫画书收藏和交易卡收藏
并且我们已经为这个角色分配了一个基于身份的策略
在这里它说我们将允许在所有s3桶中读取
我们将允许在所有s3桶中写入
所以从这里开始
我应该能够在这些桶中的任何一个中读取和写入
但不要这么快
如果我们对这些特定资源应用了基于资源的策略
那么这也必须成为游戏的一部分
在这种特殊情况下
我们有允许收集角色读取
这就是那个收集角色在这里
拒绝收集角色读取
啊哈
即使我基于身份的策略说我可以在所有s三个桶中读取和写入
本地资源基于策略
至少在交易卡中是拒绝我读取的能力
我要在这里出去
我将能够读取漫画书桶中的
但我将无法读取交易卡桶中的
让我们看另一个例子
如果我们定义一个基于资源的策略，拒绝集合角色对写入的访问
同样的情况在这里
拒绝集合角色对写入的访问
即使这里有这个策略
这个基于身份的策略说
我可以在所有S3桶中写入
我无法在这两个桶中写入
因为基于资源的策略与基于身份的策略一起生效
并且总是执行最严格的决策
所以这不会采取最宽容的
它会采取最严格的
在这个特定情况下
它剥夺了我写作的能力
这就是本质上将要应用的
所以让我们在这里更仔细地看一下
这里是效果
这里是操作 这里是资源
所以在我们的iam策略的json中
我们有这个效果
允许我们有这个行动
而这些都是我们所有的行动
让我们假设所有这些行动在这里一起涵盖了阅读的行动
这里有很多不同的行动
你不必记住这些
无论如何 文档对这些所有行动的描述相当详细
所以当你刚开始尝试做基于身份的政策时，它有点困难
有时候，当你刚开始尝试做基于身份的政策时，它有点困难
或者任何类型的政策
随意手写
但一旦你掌握了窍门
尤其是大多数AWS控制台的用户界面都有类似于代码辅助的功能
帮助我们
现在我们有了资源
所有S3桶
好的，我们看看，注意这只是通配符
这不是S3的通配符
而原因这仍然有效是因为我们只允许S3操作
所以即使我们可以
这个通配符可以代表AWS上任何服务
因为我们只允许那些S3操作
它仍然限制我们从事除S3以外的任何事情
让我们看看另一个
好的，我们看看
所以，我们有效果
允许我们在这种情况下我们有
我们有一个原则
我们的原则是我们的集合卷
所以再次在这里我们有我们可以做的所有操作
说实话这里
我认为有60多种不同的动作构成了read的活动
你可以在这里看到资源
在这种情况下我们限制了这一点为漫画书
并且我们有两个条目在这里
一个是通用桶
一个是桶内的对象
通常情况下你需要能够读取桶
为了读取桶内的对象
我认为这有点道理
所以通常你会在这里看到两个条目
但他们实际上是在调用不同的东西
我有点喜欢看这个
就像那个旧游戏战舰
一边是身份政策
另一边是资源政策
你差不多可以说嘿
你击中了我的战舰
或者你的权限
与我的权限相匹配
所以我能做点什么
在这里，中心有点像一个韦恩图
这是我们可以做的，只要资源政策和身份政策重叠
但等等 还有更多
有一种叫做会话政策的东西
会话政策允许我们程序化地动态控制访问
它们允许我们在会话期间暂时假设另一个角色
你可能使用这种策略
如果你正在编写一个应用程序
允许用户将数据上传到S3桶中
例如 你不想为所有客户创建一个IAM用户
相反，你可以让他们暂时获得写入S3桶的能力
一旦会话结束
他们将失去访问权限
但是 这里有另一个因素
这里有一个叫做权限边界的东西
这是我们可以应用到基于身份的策略的一种方式
这限制了我们身份可以做到其他资源的行为
如果你还记得这个技能的开始
我谈到了iam
它为我们提供了一种保护
这就是保护措施的一个典型例子
所以我们可以定义权限边界并说
在所有我们的身份策略中
我们将允许所有事情，除了删除数据
无论这个身份策略允许什么
只要我们在权限边界上设定了
嘿 没有身份策略不允许删除数据
那么它将覆盖该身份策略
总的来说这相当复杂
但我会尝试用例子让它更清晰
这里有一个例子 这里这里
我们的身份策略可能是一个s3角色
它允许我们创建s3桶
我们已经设置了权限边界
它说只允许在us west创建三个桶
结果是这个特定角色只能创建桶
在us west too
所以你可以看到这非常有用
如果我们有需要将活动限制在某个地区的情况
可能是因为法律或监管原因
我们可以定义权限边界
无论任何人在身份策略上注册或设置什么
权限边界将确保我们保持在正确的轨道上
但是等等，是的
还有更多，我们称之为服务控制策略
我将快速解释这个 因为不是很重要
但如果你有多个账户
这可能会发挥作用
所以AWS有一个服务叫做AWS组织
它设计用于管理多个不同账户
基本上
大多数使用aws环境的组织可能会使用aws组织
因为它们会有很多
许多不同账户
它是如何工作的
是你定义它 这里是管理账户
然后你所有的子账户都在这里
它们通常按某种组织单元排列
但基本上，服务控制策略可以做的是，从管理账户
我们可以说，下级账户
你们可以做所有事情，除了这些事情
这是很重要的事情
因为它没有允许所有事情
服务控制策略并没有真正允许任何事情
它明确拒绝东西
如果我们不希望任何下属账户
我们的下级账户无法使用
某个aws服务
我们可以应用一个服务控制策略，说，嘿
我们账户中没人可以使用lambda
然后
如果有人在这里的aws账户尝试创建一个lambda函数
他们会得到一个拒绝
他们会得到一个消息，说，嘿
你不被允许那样做
对于数据工程师认证考试来说，这并不是很重要
但我还是想在这里提到它
因为它有时确实会出现
尤其是如果你处理的是大型组织账户，拥有多个账户 分散在不同的组织单元
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/013_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p13 6. IAM Policy Structure.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好吧 让我们再多练习一下如何阅读和写策略
因为我们必须这样做
尤其是如果你是一名数据工程师
并且你负责保护某些数据资产
这是你应该熟悉了解的事情
我的意思是 当然你可以依赖
也许你的安全管理员
他们可能对此非常熟悉
或者至少他们应该 希望如此
但你也应该知道
所以这是我们的第一个策略
我们有允许
我们有这个角色
获取漫画书的对象版本
如果你要猜测这个特定的策略允许我们做什么
或者我们将允许我们做什么
如果你猜测在这个特定账户中的角色comic
允许该角色使用
S三动作 获取漫画内容的对象版本
你会是对的
让我们看看另一个
这里我们走 用户
我在这里是账户
哦 我们有用户homer simpson
我的动作
为资源生成凭据报告
让我们看看这说的是什么
允许homer simpson在账户一
二三四等等执行aim动作
为所有资源生成凭据报告
这就是这的意思
好的 让我们看看另一个
哦 这有一个允许和一个眼睛
所以我们有允许将对象放入我的桶上传
然后我们有一个拒绝对s three star in的所有其他
这是一个有趣的话题
所以你会想
这里有一个允许 但这里有一个拒绝
这不会拒绝一切吗
这不会拒绝吗
会覆盖 这允许不
实际上不 我在aws中寻找的是明确的允许
所以它内在地包含
如果我们政策中什么都没有
它不会给我们任何能力
这就是我们所说的隐含拒绝
如果我们有一个允许在这里
那么我们也可以在这里放一个拒绝
因为我们有一个允许
它将允许我们放置对象
但它将阻止我们做任何事情 else
这里我们允许主体将对象放置在此上传文件夹
但如果有人试图做任何其他操作在我的桶中的任何其他路径
那么拒绝他们
如果你仔细看这里
它说没有资源
所以没有资源
这就是为什么它试图做任何其他操作在我的桶中的任何其他路径
拒绝他们 它允许我们在此上传下做东西
它允许我们做一件事并且只做一件事
那就是放置一个对象
但在任何其他地方它不会让我们做任何事情 else
这引导我进入另一个点
这里我们有一个叫做基于角色的访问控制并且可以这样想
我们拥有一个在法国的QA工程师
并且我们可以将他们分解为工程师访问
欧洲工程师和QA工程师
因此如果我们创建不同的组或角色这里我们可以将这些指派给这个人
现在我们可以做到这一点
我们可以将这些角色指派给这个人
并且当他们偶然移动部门或某事
我们将不得不记住去那里并移除一个角色并且替换为另一个角色
这里有另一种方法我们可以这样做
那就是叫做基于属性的访问控制
因此我们可以指派属性
我们可以为这个特定个人账户指派标签并且说其工作类别是工程师
其部门是QA并且其位置是法国
然后我们可以定义政策说
如果工作类别等于工程师
则允许如果位置在法国
英国或意大利
则允许如果部门等于QA
则允许现在如果他们发生变化
也许他们被转移到美国
那么 我们将更改该标签
我们不必担心取消指派或指派新角色
因为我们这里有条件访问
所以突然 因为他们不在法国
而是在英国或意大利
他们将无法访问政策中定义的内容
这就是我们所说的基于属性的控制
你可以看到 这可能稍微更灵活，更容易管理
比基于角色的访问控制
但这需要一些时间来设置
让我们看一下其中一个基于访问的控制策略
或者现在我们看一下其中一个基于属性的控制策略
在这里我们有我们的效果
拒绝e
C 条件是停止实例资源
字符串等于e
C 资源标签环境prod所以
然后下面我们有允许停止实例的其他所有
所以这里在做的事情是，根据那个特定资源的标签
所以我们创建了一个e
C 两个实例 如果它是生产
也许我们有一个政策在执行，说的是嘿
如果你创建一个生产实例
你可能需要添加一个标签，标签内容是环境
然后给那个环境标签分配一个值prod
然后负责执行这个政策的人，会根据他们的角色来执行
或者可能是一项服务政策
然后他们会尝试阻止那个实例
然后他们会遇到一个警告或错误，上面写着嘿
你不能这样做
因为政策阻止你停止这个生产系统
这正是这所说的
允许主实体在所有资源上停止实例
除了一些e
C Two实例，这些实例的标签是环境等于prod
这就是一点预兆
我真的推荐任何人在做任何严肃的帐户时做一件事
制定一个标签策略
并且确保严格遵守这个标签策略
因为这样可以启用像这样的东西
我们可以使用策略和这些标签来非常精细地控制
或者对我们的资源进行非常精细的控制
让我们看看另一个策略
允许用户在t3和t4g家族中启动实例
你认为这个策略可能看起来像什么
如果你花一秒钟想想
那么这可能是这个政策的样子
我们有允许我们有运行实例
然后我们这里有一个条件字符串
一个像这样的字符串e
C 两种实例类型t四g星和t三g星
所以如果你被指派了这个政策 你只被允许启动实例和t三和t四g家族
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/014_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p14 7. Validation Preparing for Our Data.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们来做这些验证问题
首先 在下列哪种情况下你可能使用在线策略
我是 它说选择两个
只是为了我自己的练习
我通常喜欢这样做
我想非常小心
我正在看我应该选择多少
因为我以前做过问题
它确实说选择两个
当我只剩下两个选项时
我正在绞尽脑汁试图想
哪一个是它
它这里也说了
如果我已经缩小到两个
那么这可能就是答案
所以我只想指出这一点并且小心
并且确保你注意到我们应该选择多少
第一个选项是
我们想要快速给一个用户提供访问权限而不创建角色
如果我记得正确的话
是的 这正是在线策略
我们可以创建该策略直接对一个用户
例如 我们不必为该用户创建角色
所以我喜欢这个答案下一个
启动一个自动日志记录器来记录用户活动
不，这听起来不像我记忆中的在线策略下一个
选项在这里 防止互联网流量到达我们的ec
两个实例在我们的vpc
这听起来更像是一个安全组或网络访问控制列表
这真的与iam无关
为vpc端点应用资源基于策略
我可能不知道vpc端点是什么
但我肯定记得资源基于策略
我也记得资源基于策略使用在线策略
所以我喜欢这个答案这里
但让我们看看最后一个
确保我们的大脑中没有任何额外的疑问
构建出一个角色
这样它有足够的权限可以被分配给我们的高级用户
它这里说角色
并且我们不使用在线策略与角色
我们会使用正常策略与角色
所以我喜欢这些答案
这是我最终的答案
接下来在IAM策略中
以下哪项最能描述一个资源
让我们从这里开始
在IAM策略中没有叫做资源的元素
嗯 是的
有 但我确实想指出这里
这是一个我在AWS考试问题中见过的例子
他们有时会给出一个明确的谎言
他们想确保你真的了解这些东西
而不是只是猜测
有时候他们会在这里放一个答案
他们称之为干扰项
它是设计来让你偏离轨道的
但如果你了解得足够好
如果你了解IAM策略
以及它的组成部分
你会知道资源是存在的
所以我们继续
它是执行操作所针对的实体
是的 这听起来相当合理
让我们继续
它是试图执行操作的实体
我记得这不是真的
试图执行操作的实体通常被称为主体
我们继续 项目上
它是决定效果时所依据的条件
这不是一个条件
这是一个资源
它定义了给定操作的授权范围
再次不是
我认为我最喜欢的答案是
它是执行操作所针对的实体
这就是资源
所以我将坚持这个答案
作为我的最终答案
好的 所以我们被问到这里
以下哪项是错误的
关于安全组和安全组网络访问控制列表
我需要更改这个为
和安全组 稍后我会做
安全组应用于单个资源
是的
那是真的 但我们被问到哪个是错误的
所以这是一个情况，仔细阅读问题真的很重要
如果我们读得很快
我们会说，选择三个
是的 那是对的
你知道，所以 当然，那将是错误的
所以它要求我们选择错误的选项
安全组应用于单个资源，这是真的
那不是错误的
ncl和c
nacles和sgs提供入站拒绝操作
谁谁谁等一下
那不是正确的
我知道nacle提供入站拒绝
但安全组并不提供
提供入站拒绝
所以只允许那是错误的
nacles和安全组
安全组提供入站允许操作
那是真的
所以我跳过那个
安全组应用于整个子网
不 它们不 它们只应用于特定资源
我们分配那些安全组
所以那是错误的
nacles应用于整个子网，那是真的
nacles应用于单个资源
不，那是错误的
这是我三个答案
我希望这对你有帮助 感谢您观看
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/015_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p15 1. Skill Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，美丽的学习者，来到另一个技能
如果我必须命名云计算的一个特性，那就是真正使其成为突破性特性的特性
它的杀手级应用
就像我说的那样 我必须说存储
能够以几分钱的价格收集和存储大量数据的能力
就像视频游戏中的技术树第一级
这是一种能力，真的解锁了我们未来的大量选择
我的意思是，如果没有大量的数据集
大多数现代AI技术仍然只是空想 在这个技能中
我们将深入探讨aws提供的各种存储选项
然后，我们将通过收集和存储一些我们将在随后的技能中 我们将使用的数据集来结束这个技能
让我们开始吧 我们将开始
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/016_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p16 2. Why Storage in the Cloud.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在开始学习这个技能
我想指出，参加这门课程的人们来自不同的背景
也许你本来就有aws的背景
你只是想知道数据工程是如何进行的，或者想要通过认证
也许你背景是azure或gcp
也许你完全没有云背景
这没关系
我想花点时间来解释
首先 我认为存储已经成为云计算的杀手级应用
这就是那些默默无闻的英雄之一
从服务角度来看，它通常不会被注意到
但它确实促进了许多不同的技术发展
在我们深入探讨所有技术细节之前
我想先给你们一个概述
为什么云存储对云计算的发展变得如此重要
好的 我们在谈论存储
我不是在谈论一个破旧的仓库里存储
或者那些自助存储的地方
我们在谈论云存储
在云端存储之所以变得非常流行，有几个原因
首先最重要的是
这是有效的
我们将在下一秒看到，现在的快速检索速度非常快。
高速互联网
任何你放入云端的东西
你可以从中脱身
正如你放入它的速度一样快
它也现在安全了
我听说过一些事情，别人这么说：
它在云端 这不安全
事实并非如此
如果我们将某物放在云端
并且我们采取保护措施以确保其安全
它可以绝对安全
与在我们的本地数据中心一样安全
而且现在，它也同样经济实惠
这只是给我们提供一个基准
我要说1GB是我们的基准
这相当于大约256首歌曲
大约200张高分辨率照片
这也相当于大约4000份扫描文档
大约15分钟的高清视频
所以这是1GB
这些每项都将大约等于1GB
好的
如果我们看Netflix
例如 事实上
Netflix是最大的客户之一
最受欢迎或最活跃的aws客户
他们使用aws处理许多业务
一GB的视频大约是
如我所提到的，大约15分钟
一TB的视频是256小时的高清视频
一PB的视频大约是30年的流媒体视频
一EB的视频大约是30,000年的视频
如果你说的是30年或3万年，那么现在流媒体视频的存储量已经达到了这个水平
这是非常庞大的数据量
我在这里将数据分解成这种格式，是因为我已经确定了大约1GB的数据量
这些都是1024的乘积，1024的1024次方
你可以看到这些数据按照比例增长
所以，为了给你一个直观的感受
我们谈论的是GB，TB，PB和EB
所有这些尺寸的测量对于公司来说都很常见
在aws上的数据存储量
aws现在非常擅长处理这些事情
如果我们看一下成本
这里是成本
如果我们看一下最昂贵的s三存储，我们会了解一下s三是什么
这里是s三的价格
最贵的大约是两美分每千兆字节
记住，这大约是15分钟的高清视频
对于一PB，大约是23.55美元
大约两万两千美元
如果你在提供
或者如果你在 如果你在aws上有1EB的数据
使用最昂贵的s3层
这将花费你每月225万美元
说实话 如果你存储这么多数据
你将与aws谈判一个更低的价格
但这只是现在餐巾纸上的一些数字
如果我们在这里使用最便宜的方法，即使用aws
如果我们说我们将把它存储在冰川中
那么一GB的成本将不到一分钱
一TB的成本将约为1.30美元
一PB的成本约为1052美元
所以你可以看到这里巨大的价格范围
从最昂贵的到最便宜的
所以aws为我们提供了很多不同的选择
这取决于我们的用例
此外，如果我们在现场存储这些东西
我们必须建立一个大的NAS或硬盘或类似的东西
但我们也会承担所有这些东西的责任
所以机率是我们的价格会相当高
在AWS中存储意味着我们正在利用规模经济
我们不必自己支付所有这些东西
我们正在利用AWS必须为所有这些东西支付给许多
许多客户
他们将成本分散在所有这些客户中
所以 我们可以管理一个更低的账单
我想在这里做
是的 好的
所以继续前进
我说
存储检索快速
有多快呢，在古代，我们会将我们的备份发送到外地
我们会将它们发送到第三方仓库，在那里它们会被存储
如果我们需要从第三方仓库的磁带上恢复某些东西
我们必须拿起电话
我们必须找到管理那个仓库的人
可能是铁山
或者类似的人去他们的仓库里找那些带子
他们把带子装到卡车上然后送回我们的设施
然后我们在这里拿到带子，取决于这是不是计算机数据
我们可能会把它加载到我们的磁带驱动器中然后恢复它
如果是某种文档
我们必须拿到那份文档并交给需要它的人
他现在就需要 如果我们在谈论aws的话
那么在这里检索速度要快得多
因为我们把我们的备份发送到aws
我们所要做的就是拉取我们的备份软件
说嘿 去看看这个磁带
在aws的保险库中
它会拉取回来
我们满足了要求，即将我们的备份离线并远离我们的
我们的数据中心
但是现在快多了
下一个问题
AWS需要与许多不同的客户打交道，所以安全性有多高
如果这个词组对你有意义
那么你就知道我在说什么
有许多联邦法规和国际标准
在许多行业中，你必须遵守这些规定
如果你想与某些公司做生意，你必须遵守这些国际标准
因为AWS与许多不同的客户合作
谁必须注意并遵守这些标准
AWS已经在其平台上构建了大量功能和周边功能
以确保我们能够通过审计
无论何时进行审计，我们都会看到它们在其他技能中做了什么
但现在只需知道AWS已经通过了其他客户的大约一百种不同的标准和证明
所以如果你来到谈判桌上
需要这些认证或国际标准并需要支持
那么他们已经做过了
很可能现在
大多数情况下
此外 我们可以以加密格式存储我们的数据
我们可以以加密格式发送数据
我们可以在传输过程中进行加密
这意味着在两个点之间
如果我们通过互联网发送数据
在A点到B点之间，数据是加密的
我们最常使用称为TLS的东西来做这件事
在你的浏览器中，它更广为人知的名字是HTTPS
你总是要寻找那个s，那个x代表的是额外的安全
然后我们还有数据静态加密
这意味着如果数据只是静静地躺在那里
什么也不做
那么以非常普遍的方式对其进行加密
有许多不同的加密协议
但最受欢迎的是aes 256
并且这实际上直接内置于s3中
如果我们想在那里加密我们的东西
我们可以只是点击那里的单选按钮
我们写入的那个桶中的任何内容都将自动加密
现在 我们正在谈论数据存储的安全性
如果我们谈论的是S3
那是存储大量数据的最流行地方之一
那么AWS保证
或者至少他们说他们不保证
我想非常清楚地说明这一点
他们说我们可以期望99.9%
99.9%的可用性
他们也说我们可以期望99%
九九九九和一个持久性
现在 这里有什么区别
可用性是运行时间
这意味着如果我们走出去并尝试访问我们的数据
99.99%的时间
99.9% 9%的时间
我们将能够访问那个数据
基本上，这相当于在一个月中，
这意味着将有四分钟二十三秒的时间，
这相当于三个桶，
或者获取数据的机制将不可用，
但这就是他们现在的说法，
通常他们的可用性远高于99.99%，
但这就是他们公布的数据，
我们称之为服务级别协议，
我想确保，
我没有说aws保证这一点，
他们的sla，
他们的服务级别协议说他们的目标是，
这是他们作为顾客提供给我们的，
如果意外发生他们没有提供，
有补救措施，
有作为顾客的补救方法，
最常见的就是得到一些免费的信用，
而不需要我们支付任何费用，
但这99.99%，
9， 9%， 这叫做11个9，
这是耐用性，
那意味着什么，
如果我们把数据放在s3上，
例如， 那么数据将是安全的，
它将不会被损坏，
不会发生任何坏事，
99.99%的时间， 所以这在实际中看起来怎么样，
假设我们有十亿个文件， 如果我们在s3上保存这些文件100年，
那么我们可能会失去一个，
也许一个，
这就是在实际中的意思，
所以你可以看到，你的数据在s3上， 例如，
是非常安全的，
并且它将是非常可用的，
甚至在这之上， 我们有选择将我们的数据复制到不同的备份机制，
并且我们可以获得更高的耐用性或可用性，
如果我们需要这样做，
如果我们需要这样做
如果我们愿意为此支付费用
好的 那么，aws是如何实现这11个9的可靠性的
嗯 如果你还记得 aws被分成了区域和可用区
对于我们正在讨论的服务
s3
他们有每个可用区的服务副本
在每个可用区内部
他们投入了足够的设备和冗余来实现99.99%的可靠性
9个9
然后他们在这些可用区之间复制这些
所以你在这里上传一个文件
实际上，你不会看到它上传到特定的可用区
这一切都发生在幕后
一旦你上传了它
它就会被复制
在每个这些可用区内都有相同的99.99%的可靠性
9个9
如果你有三次
所以 如果我们这样做99.99的三次方
我们将得到11个9
这就是我所说的那些
这就是他们这样做的方式
他们没有使用任何魔法
特殊的成分
他们只是使用冗余来将可靠性因素乘以3，从而得到我们的11个9
这就是他们如何做到的
这就是为什么云存储已经成为我认为
云计算普及的杀手级应用和游戏改变者 之一
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/017_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p17 3. Object Storage.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们要讨论的第一种存储类型
我已经在之前的视频中谈论了很多 S3是物体存储
什么是物体呢
物体是一个文件
可能是一个mp3
可能是一个mov文件
是一个jpeg或任何类型的文档
这些是物体
这些物体存储在物体存储中
S3简单存储服务是物体存储平台或服务的一个例子
尽管它看起来像文件存储
当你打开你的资源管理器时
你看到这些文件和文件夹
S3有文件夹
里面有文件看起来有点像那样
但实际上它与数据库更相似而不是文件存储
稍后会看到
但基本上，我们会把想要存储的任何对象放入S3中
我们将它们放入S3中
这里有一些关于S3的重要信息
这是对您来说非常重要的信息
我认为那是服务编号
可能是四或五
可能是三
我忘了 但那是那些真正帮助建立其他服务和使能其他工作负载的服务之一
不仅在aws上
也为客户服务
它在aws内部被广泛使用
在幕后和显然现在也公开
将东西移到云存储往往是公司首先采取的第一步
当他们决定想要尝试这个云东西
原因第一点是它相当容易
第二点它是相当经济实惠
它相当便宜，第三点
它不是真正的单向门
所以如果我复制东西出去
S三 只要我不从我的当地一侧删除它
那么如果出了什么差错
如果我从S三桶中删除我的东西
我还是会保留它
所以这基本上是一个低风险的入口点
更何况，许多组织必须保留数据很长时间
而这些东西会累积起来
这会花费很多钱
尤其是如果你试图在一个非常昂贵的存储区域托管它
网络，那么s3已经成为一个受欢迎的选项
以将一些公司必须长期保存的东西卸载
出于监管原因
他们可以将数据上传到s3并基本上忘记它
正如我所提到的 它比传统的存储更接近数据库
这就是我的意思
所以让我们看看这里的s3桶
我们有上传的文件
s3的酷之处在于我们可以有这些文件
但我们也可以为这些文件关联很多元数据
默认情况下 你会得到一些标准信息
如创建日期和内容类型
例如，如果你上传一个jpeg
aws在s3中会说 我知道那是什么
我将其设置为内容类型image jpeg 我们可以上传另一个文件
是的，它也是jpeg
也会得到相同的信息
但我们可以添加自定义标签
我们可以添加主题
我们可以添加评分
我们可以添加与该对象相关的其他属性
所以你可以看到为什么这叫对象存储而不是文件存储
因为我们不仅仅是存储文件
但我们可以自定义与特定对象一起存储的数据
这在分类数据时非常有用
或者决定是否需要加密
或者作为敏感信息等
稍后在其他技能中我们会看到，所以s3有不同的存储类别
最昂贵的是称为标准存储类别
这些存储类别
它们都在同一个s3桶中
我们只需选择将哪个存储类别分配给我们的对象
我们可以到单个对象级别
我们可以到整个桶
或者我们可以非常选择性
也许只是这个子目录中的东西
或者这个扩展名
在这里我要非常小心
因为我不想调用文件夹结构在内部
S三个子目录
因为它真的不是子目录
如果我们回到这里
这个小的my picks slash doggy jpeg
那是对象的名称
事实是它这里有一个/斜杠
那是对象的名称的一部分
但是aws和s three将渲染它
你将会有一个小文件夹在这里
然后它会调用我的选项
然后在下面你会看到一个叫doggy的文件
这有点误导人
但请记住当我们谈论对象名时
如果我们想让它出现在子目录中
我们必须在前面加上我们想要的子目录
所以我们回到标准
这是成本最高的存储类别
这意味着对于标准来说，我们将在这里，在这里和这里复制
并且它将在这两个之间复制
现在还有一种存储类别叫做不频繁访问存储
那么它是什么意思呢
那就是字面意思
这意味着我们不会频繁访问这些数据
或者我们可以享受较低的支付或每GB的存储成本
因为我们不需要像以前那样频繁访问
所以如果我们需要访问
他们可能会收取一点
我猜测是一种罚款或者类似的东西
如果我们频繁访问
然后我们将更有道理地过渡回标准
但如果我们只是放这些东西在外面并访问它们
也许每月或每天一次，无论什么
那么我们可能通过使用不经常访问的存储类别来节省一些钱
下一个叫做一区存储类别
什么是一区 嗯
这意味着它只存在于一个区域
它不会被复制到这些其他区域
因此它将获得99.9%的可靠性
九九%的耐用性代替那些十一九
这是可以接受的
有大量的数据
让我们假设 例如
你有大量的日志数据
这些只是您在一段时间内积累的日志文件
它们并不真的提供很多安全性或任何类型的监管覆盖或类似的东西
但你只是想保留它们
因为也许你会用它们进行分析或其他类似的事情
如果他们迷路了
如果他们损坏了
这不是世界末日
如果我们现在选择一个区域，我们可以节省大量资金，标准费率下可以节省大量资金
在我看来 99.99%的耐用性仍然很高
但如果这些数据容易复制
或者这只是我们从其他地方复制的副本
那么我们现在可以选择一个区域，可以节省大量资金，我们还有
我们将在另一个视频中详细介绍
有一种叫做冰川类的东西，冰川是aws
S的冷存储
这意味着它为我们不需要非常频繁访问的文件和对象设计
事实上
如果我们将它们放在冰川中并完全忘记它们
这将是绝对完美的
因为冰川确实就是这样
如我之前所说
许多组织必须长期存储数据
由于法规或法律原因
因此，冰川已成为一个非常受欢迎的解决方案
因为它非常便宜
如果我记得在之前我做的那个比较幻灯片
从最昂贵的到最便宜的
我们谈论的是每GB存储的成本大约是0.02美元
而在最便宜的这里
大约是0.0009美元
实际上9美分
所以从最昂贵到最便宜，价格差异相当显著
所以关键在于你的数据需要什么样的存储环境
当它处于存储状态时
你是经常访问它，还是放在那里完全忘记它
现在，这里有另一个选项，就在这里
智能分层存储将根据数据的访问模式自动帮助我们做出决策
所以它会坐在那里，观察我们如何访问这些数据
如果我们经常访问数据
它会自动将数据发送到标准层
如果我们不经常访问数据
它会自动将数据发送到不经常访问的层
如果我们很长时间不接触数据
我们可以配置它甚至将数据发送到冰川层
现在，为了这种自动化，我们需要支付一点额外的费用
但如果你有大量数据需要管理，你不想费心去管理它们
这最终是值得的
现在我们有50个数据存储选项
每个选项都有其特定的用途和成本
选择合适的存储选项对于确保数据的安全和成本效益至关重要
了解这些选项，可以帮助你做出明智的存储决策
或者你没有管理它的能力
因为它实在是太多了
智能分层是一个非常好的选择，能够简单地分配
智能分层给所有数据 它自动优化自己的成本
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/018_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p18 4. Network Storage.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 现在我们来谈谈网络存储
通常当我们谈论服务器或系统时
我们会有一种叫做文件系统的东西
并且操作系统就位于其上方
它定义了我们如何沟通
以及如何存储文件等
如果我们从网络角度来看
这里有一台服务器
让我们假设这是一个Linux服务器
我们现在有一个文件系统
如果我们想让其他系统默认访问此文件系统
我们无法让这里的任何人访问此文件系统
我们需要做的是
我们需要在本地服务器上提供某种服务来暴露此文件系统
其中一种常见的方式是NFS网络文件系统
这是一个非常常见的文件系统
网络文件系统协议已经存在了很长时间
并且这允许我们将我们的文件系统暴露给更大的网络
假设我们这里有一个客户端
也许它是我们的桌面
该系统上也有一个文件系统
如果我们安装一个nfs客户端
我们现在可以使用这个nfs客户端连接到这个nfs服务器
假设 例如
我们这里有一个称为挂载点的东西
切分m和t
我们用NFS展示了这一点
然后我们可以将这个挂载点挂载到我们的客户端这里
它会以这种方式显示
每当我们在这个目录中写入文件时
实际上它将通过网络写入到这个目录中
AWS有一个做NFS的东西
它被称为Amazon Elastic File System或fs
你可以把它想象成你们部门或公司里的旧共享驱动器
也许你来自一个拥有共享驱动器的公司
一个H驱动器或者类似的东西，你可以在那里
放置文件
你部门的其他人可以访问它们
这就是所谓的云存储
我们按使用字节付费
如果我们只使用了1GB
我们只会为那一个GB付费
所以基本上你可以把它想象成一个无限的网络存储设备
我们可以把所有的数据放在那里
如果我们愿意
并且EFS有不同的存储级别
一旦我们将文件移动到那里
我们可以定义
比如 如果我们在7天内没有接触这个文件
14天
无论什么 那么会自动将其移动到一个不太频繁的访问级别，这将更便宜
如果我们仍然没有接触那个
也许我们可以定义在30天内
如果我们仍然在30天内没有接触它
它会自动移动到某种归档级别
这将更便宜
这相当方便
使用FS 我们还可以选择多AZ解决方案或仅一个AZ
正如你可能猜到的，多AZ具有更多冗余
它将更耐用
单一AZ将不那么耐用
但它将花费我们更少
所以你只需决定你的个人资料
你存储在那里的东西
你将如何使用它
它对你来说有多重要
等等 现在我们也有一些高性能选项
EFS主要用于Linux世界或Linux服务器社区
允许Linux系统共享文件系统
这些高性能系统也在Linux中使用
但我们也有一些Windows选项
我们有一个叫做Amazon FSx for Lustre的东西
Lustre通常用于这些巨大的集群计算场景
如复杂
复杂的模拟
或如做天气预报模型之类的东西
所以AWS有一个选项，我们可以部署一个托管的Lustre文件存储系统
我们可以使用
他们还有Amazon FSx for Windows文件服务器
如果你在Windows系统或你在Windows生态系统中
也许你用Windows为你的服务器
你会使用此Windows文件服务器而不是EFS
因为目前
EFS与Windows服务器不兼容
Windows系统 Windows文件服务器支持像B
NTFS 活动目录和去重之类的东西
现在我们还有Amazon FSx for Net App on Tap
如果你已经使用了Net App或其他NAS解决方案
那么这可能适合你
然后我们还有亚马逊fsx用于开放zfs
zfs是一个非常专业的文件系统
如果你恰好需要使用zfs 那么aws为你提供了一个托管版本
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/019_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p19 5. Archival Storage.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 现在我们来谈谈归档存储
在另一个视频中 我提到了冰川类别
这些只是我们可以分配到我们的数据中
然后我们将支付相应的级别成本
但它们有点特别
现在，有一个名为冰川保险库的东西，它独立于s3
如果我们谈论的是我们有磁带并备份磁带
我们正在备份磁带
我们将在另一个视频中看到
但总的来说
大多数人将使用glacier与其s3实现
所以它是我们可以在s3上分配的另一个存储层
这是aws上的最低成本存储选项
我们将按需支付
我们将按每GB支付
但我们也要支付检索费用
这是一小点注意事项
为了那个低价
亚马逊期望我们不要过多地干扰数据
不要触碰它
实际上
如果你谈论的是归档目的
一般来说，你将其放入冷存储或归档中
并且你不期望频繁访问它
所以为了阻止
亚马逊提供了按需检索付费
如果我们将TB级别的记录
日志或类似的东西发送出去并需要检索
假设我们有2KB的日志
那么我们会支付一小笔费用
我认为大约是两美分每GB左右
这是非常小的费用
但如果你需要检索特定的信息
那么这绝对是值得的
但真正的好处在于，大多数情况下
你可能永远不会需要检索任何信息
如果你需要检索
有一个保险政策总是好的
但是如果你不
那么它就会坐在那里收集灰尘
所以你想以最有效的成本存储它
现在也有有效存储期限
所以对于某些级别你必须让它在那里
至少九十天或者你将被收费如果你在九十天前触摸它你将被收费
下一个级别仍然覆盖它
还有一个级别
我认为深度归档要求你必须让它在那里
并且不触摸它大约一百八十天
在很多情况下这是完全可以的
因为很多组织必须长时间保留大量数据
但他们不会触碰那些数据
几乎从不
这就是冰川现在完美的用例
至于检索我们有范围在这里
现在 一些冰川层我们可以在毫秒内检索
而其他
更经济实惠的
可能需要几个小时
再次，这主要取决于你的用法模型和你的需求
如果你能等待几个小时
那么存储在那层可能是有益的
那层成本更低
因为大多数情况下你可能不会频繁恢复那些数据
再次 这取决于你的用法模型
我们还有能力放置锁定
使那些数据不可变
什么是不可变
嗯，不可变意味着它不能改变
我们为什么要这样做存档数据，例如，为了审计
例如 如果我们需要生成日志
并且我们需要存储那些信息一段时间
我不知道七年或类似的东西
如果我们回去并更改那些数据
那么我们可能会陷入麻烦
我们可以将我们的日志放在这个不可变状态
并保护它们不被更改或删除
例如 另一个原因可能是
法律保留，法律保留是如果我们公司被提起诉讼
并且我们被告之不要删除任何潜在证据
即使我们故意或意外地去删除
那可能是证据
那么这将是非常非常糟糕的
所以有一个法律保留
我们可以出去并应用这个不可变标签或这个不可变状态
它将防止任何人有意或意外地删除或更改那些数据
现在这里有三种主要的冰川层
首先我们有即时检索
这正如你所期望的
这是毫秒级的检索时间
我们有冰川冰川灵活检索
那可能是分钟
然后我们有冰川深档案
并且在小时
你可能会说，嗯
我为什么不总是想要毫秒呢
在某些情况下你可能确实想要
但请记住
你越往下限制这个列表
价格就越低
如果你有大量的数据可能需要
也许一年一次
甚至你可能不确定在未来十年内是否需要它们
那么深度归档可能是你的最佳选择
因为在你需要它们的时候
基于存储，几个小时算什么
也许几年内可以节省数万美金的存储成本
所以这取决于你的使用情况和检索情况 但这就是亚马逊冰川
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/020_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p20 6. Block Storage.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们来谈谈块存储
它们被称为块存储
假设我们有一个磁盘
让我们用自己的个人电脑为例
这里有一个磁盘在那个个人电脑
它可以是固态的
它可以是一个旋转的磁盘
你知道磁盘
现在像这样
在那个磁盘内部我们将会有一个分区
我们的操作系统在那个磁盘上创建了一个分区
我们可以有很多个分区
我们也可以只有一个分区
但我们将会有一个分区
在那个分区内部我们将会有一个被称为文件系统的东西
它将定义我们将如何存储我们的文件
现在 我们在计算机和服务器上使用的大多数文件系统
它们是基于块存储的
我指的是什么？嗯
我的意思是他们会分割文件系统或分区
更倾向于一堆这些不同的小方块
现在，这些不同的小方块有一个固定的大小
根据我们使用的文件系统，这些块的大小可能会有所不同。
让我们说现在是四 k
如果我们这里有一个文件
让我们假设这是一个16kb的文件，并且我们指示我们的操作系统
嘿，将这个文件保存到我们的文件系统中
好吧 它将在这里的文件系统子系统中说：嘿，我需要存储一个16KB的文件
这个子系统会说：好的，你可以用这个块
好的 你可以用这个块，这四个块加起来是16KB
这就是它在基于块的文件系统中存储数据的方式
如果我们想要检索它
我们会回到这里
它有点像一个目录或索引
它说嘿 我想访问这个文件，而这个索引说好的
我们会从这里这里这里这里获取
然后重新构建并返回给你
这就是块存储系统中的文件IO工作方式
AWS有自己的块存储版本
它叫做弹性块存储，你可以把它想象成
这最接近硬盘
你可能知道这是指硬盘或USB闪存驱动器
弹性块存储的使用范围有限
它用于与e
C 两台实例
EC two台实例是现在非常流行的虚拟机
在同一AZ中这里有一个警告
传统上，块存储可以认为是本地连接的
我用这个词加了空气引号
因为它并不完全是真正本地连接的
这意味着没有一根电缆直接从那个虚拟机连接到某个硬盘上
反而 它仍然需要磁盘无论虚拟磁盘在哪里都非常短的延迟
并且无论虚拟实例如何处理这一点
AWS表示，弹性块存储是Z特定的
所以每当我们创建一个ebs卷时
那么我们必须确保在同一个az中创建它
我们想要用它
无论什么e c
两个实例 我们将使用它
这里是一个使用弹性块存储的稍微不同的地方
如果我回到我的图表这里，你可以看到，每当我们定义一个文件系统时
我们不得不定义整个文件系统
我们不能说，嘿
我只想按需使用块存储
这就是为什么我们使用EBS时
我们必须为整个东西付费
如果我们认为我们需要40GB
那么我们就必须支付40GB
无论我们是否使用完整的
40千兆字节，无论现在如何，如果例如我们选择40千兆字节
然后我们后来决定嘿
我们我们需升级到80千兆字节
完全没有问题
我们可以简单地对40千兆字节进行快照
我们可以基本创建一个或恢复到新的EBS卷
该卷大小为80千兆字节
这就是那里为我们提供的一些灵活性
让我清除我的绘图
Ebs也有一个非常有用的快照机制
我是什么意思呢
如果我们想在某个时间点取得一个快照
该ebs卷的备份
我们可以这样做 我们可以去aws控制台
或者我们可以通过cli这样做并说
您对那个ebs卷进行快照
这在本质上是该整个卷的位元级备份
现在ebs被称为持久存储
我们也有另一个选项，使用e
C 两个实例
一些EC two种实例类型有一个叫做实例存储的东西
现在ebs是持久的，实例存储是临时的
那两个词是什么意思，持久的
意思是它会保留下来，数据不会丢失
临时的意思是它生命周期短，转瞬即逝
让我们仔细看看这两种存储选项的区别
持久的是弹性块存储
它速度相当快，数据会保留到你删除卷
所以我们可以将这个ebs卷附加到我们的e c two实例
即使我们关闭那个实例，数据也会保留
如果我们想断开那个卷并将其附加到另一个计算机
那个数据仍然会保留
另一方面 临时数据
叫做实例存储
至少在aws上，速度很快
数据在重启后会保留
但在关闭时数据会丢失
或者如果我们终止实例
或者如果我们改变实例类型
为什么你会想要使用临时数据存储，
当你可以使用持久数据时
在很多情况下，对于一些e
C
两种实例类型 它们包含一个实例存储，价格已经包含在内
例如，它们可能包含一个250GB的免费硬盘
那是实例的一部分
我们想要使用它，
特别是在数据处理场景中
如果我们需要下载一些数据
我们需要一些工作空间来处理数据
也许我们需要处理它，进行数据清理
或者进行一些转换
这个实例存储卷在这个场景中非常有用
因为我们不需要长时间存储数据
实际上我们不在乎数据是否会丢失
这可能是有益的
因为我们启动实例，下载数据
我们需要一些空间来处理数据
一旦处理完成，我们可以关闭实例
它会重置
也许第二天晚上，我们又需要处理数据
我们需要再次启动实例
进行数据清理工作
所以实例存储非常适合临时工作
当我们需要处理数据时
所以你可能会因为不同的原因使用持久性存储而不是临时存储
我只是想指出这一点的区别
所以之后 在这门课程中
每当我们遇到这种存储类型 那么你就知道区别
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/021_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p21 7. Even More Storage Options.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好吧 但是等一下
有更多的存储选项
我们有一个叫做aws存储网关的东西
存储网关实际上是各种存储上传机制或处理机制的集合
它是一个我们可以在本地数据中心安装的虚拟机
我们可以从aws免费下载
他们提供了一个虚拟机映像
有几种不同的虚拟机格式
它提供了一个本地存储平台
或者一个连接到aws存储的本地存储设备
如果我们想复制大量文件
并且不想通过互联网尝试
我们可以在本地设置存储网关
然后我们可以创建一个共享的文件共享
然后我们可以复制文件到该文件共享
然后幕后
存储网关会将其复制到可能s3桶
这实际上是一种相当不错的数据传输方式
特别是如果您想确保数据备份到云中
也许您想要一个本地副本
但您希望将其同步到云中
以防止本地发生故障
这对于将数据迁移到aws也非常有用
存储网关中有一些配置选项说嘿
您只能使用这么多带宽来同步到aws
这样我们就不会淹没我们的互联网连接的正常流量
我们可以将其调整
或者只是在夜间全速运行
当没有人使用我们的数据网络时
这里有不同的操作模式
我们可以使用存储网关的方法
第一种模式是文件网关
这将创建一个文件共享
NFS或SMB
我们可以在其他系统和服务器上挂载它
甚至客户端
我们可以将文件复制到此挂载位置
幕后
一切都将复制到我们配置的s3桶
另一种选择是称为卷网关的东西
这基本上就像ebs卷
就像块存储
我们可以为本地系统定义块存储
并且可以创建快照
这些将在幕后复制到aws
我们有磁带网关
这是一个相当不错的功能
因为我们可以配置它成为一个虚拟磁带库
我们可以配置存储网关成为一个虚拟磁带库
每当这个磁带网关运行时
我们可以将我们的东西存储在冰川中
冰川
因此，我们不必将东西放在磁带上然后寄出去
我们可以自动让它
直接进入冰川，它会在那里待着
我们希望我们不需要它来恢复它
但如果我们需要恢复它
它就在那里，可供我们使用
我们还有一个叫做fsx网关的东西
这就是存储网关可以运行的模式之一
如果你是一个Windows商店
如果你还记得 有一个FSX用于Windows文件服务器的托管解决方案
这将复制这些数据
从一个本地的Windows文件服务器到FSX平台
或者FSX Windows文件服务器
所以这些是你可以使用存储网关的模式
对于这门课程 知道所有这些模式并不是非常重要
但是我确实想提到这一点，以防你
你处于一种试图弄清楚如何将本地数据上传到aws的情况
这是如何将本地数据上传到aws的方法
这是一个方法
这些都是你可以这样做的一些方法
我还想提到另一种方法
我们有一个叫做aws snow家族的东西，雪家族
雪家族下有许多不同设备
但我们很快就会看到它们长什么样
但本质上，想想它就像一个便携式硬盘
这是一个硬盘
里面装满了存储
我们会使用普通快递公司寄送这个硬盘
我所说的普通快递公司比如UPS、DHL、FedEx
他们会把这个便携式硬盘寄给我们
然后我们会把它连接到我们的基础设施上
我们会在上面复制一些东西
然后我们会断开它 我们会把它寄回给AWS
他们会把它连接到他们的基础设施上，然后将这些东西上传到S3
所以我们可以用这个将东西导入到s三
但我们也可以导出它
假设 例如 我们在云中生成了大量数据
出于某种原因
我们需要将数据移回本地
所以我们可以让他们
将数据加载到雪设备上
然后将其运送给我们
然后我们可以将其连接到我们的本地系统并那样复制它
你可能在问
我们为什么要费这么多周折
嗯 有时你正在谈论大量的数据，这将花费很长时间
即使是在最快的互联网线路上
这就是为什么雪家族在这些情况下存在
当有大量数据时
有其他雪设备具有本地存储和计算
这相当不错
因为基本上想想这像一个小可用性区域
我们可以将其带到野外
如果我们在某个沙漠中进行测试
或者类似的事情 我们可以使用这些雪设备进行数据收集或对该数据的最小处理
这就是这样一台设备
这是它们提供的最小的雪设备
它被称为雪锥
它的大小相当于便携式硬盘
这里是一个更近的看这里
你可以看到它有两个以太网端口
我们可以那样将其连接到我们的网络
它也有两个USB端口在这里
一个是电源，然后是一个用于与它通信的端口
我们可以将其连接到我们的笔记本电脑
当我们在野外收集数据时
我们可以将数据存储在雪设备上
然后我们可以将其发送到AWS
他们将其上传到S3
这里是另一个例子
这被称为雪球边缘
你可以在这里看到一些规格
210TB的存储
它也有自己的内置计算
它有0和4个虚拟CPU
416GB的RAM
但它是一个相当强大的野兽
它的大小相当于滚轮包
重约50磅
我们可以按天付费
我们可以预付一个月
我们可以预付一年
我们可以预付三年
这种雪球边缘设备是例子，嗯
也许我们不仅会使用这种存储传输
但我们可能会更多地使用这种来在野外收集数据
一旦我们收集了那个数据
我们可以将其发送到AWS
这是另一个使用这种的公司的例子
他们发射火箭
他们将这个节点设备
将这个雪球边缘安装在他们的火箭上，以便在发射过程中收集所有数据
一旦它着陆并返回地球
他们可以分析那些数据
因为它内置了计算能力
他们可以实时处理那些数据
所以对于雪家族来说，关键点
如果你有大量的数据需要移动到或从aws中移出
这可能是一个选项
另一个选择是，你可以使用它进行离网数据收集
如果你在一个非常偏远的地方，想要收集那些数据
并且可能最终将数据发送到aws进行分析 这也是一个可能性
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/022_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p22 8. Walkthrough Retrieving and Storing Some Project Data.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 为了这次演示
我们将开始收集一些数据
我们将收集一些数据，以便在后续的技能中使用
但要做到这一点
我们需要出去获取这些数据
并将它们存储在s3空间中
并且我们将使用s3作为我们的存储空间
我们首先收集的数据是
我们将使用称为open aq的项目
这是一个户外空气质量项目
他们从全球数千个监测站收集空气质量统计数据
我们将使用其中的一小部分
数据的性质很有趣，很容易使用
我们将去获取一些数据
我们现在将去获取另一个数据源
称为松鼠普查
是的 没错
确实有松鼠普查
我们将前往松鼠普查的网站
我们将下载它是一个CSV文件
我们将前往这两个网站进行公开访问
这有点好因为他们已经在S3上托管了他们的数据
但我们将取一个子集
我们将将其复制到我们自己的桶中
为松鼠普查
我们将前往他们的网站并可以直接从那里拉取数据
我们将将其作为对象
放在我们S3桶的特定子目录中
那么我们开始吧
好的 现在我们在这里，打开aq网站
从这里我们可以探索数据
它给我们提供了一个漂亮的小地图
我们可以点击这里的任何一项
这些都是测量地点
我们点击一个
让我们看看这个
它将向我们展示什么
测量结果是过去24小时内的最后读数
这里给我们一个小图表
我们可以深入细节
我们可以得到关于任何种污染物的测量信息
但我们不会以这种方式获取我们的数据
因为打开队列
主机托管他们的数据
他们的原始数据存储在一个s3桶中
而且事实上 我们将从他们的桶中复制他们的数据到我们的桶中
那么我们现在就这样做吧
所以我们现在在aws控制台
记住我已经登录了
或者我们应该登录
不是以root用户身份
我们应该以我们的aim用户身份登录
希望你在其他视频中跟随了我
我们在那里能够创建这个用户并分配管理员权限
现在我们可以去s3，你可以在任何地区这样做
这真的不重要
当你来到这个控制台时
你将看到你账户中的所有桶
但是桶可以在特定的区域创建
这有点方便
如果你在某个特定区域工作
你可能希望桶在同一个区域
所以我要做的是
我将创建一个桶，我将其命名为
你可以给它起任何名字
这是我的数据工程师
游乐场
我将留下其他一切
你可以看到，我将公共访问阻止
因为我真的不需要提供公共访问
我真的不需要版本控制
这里和这里我们在谈论静态加密
因此默认情况下
服务器端加密已启用使用s3的amazon
s3管理的密钥
并且这完全符合我
如果我们不喜欢那样
我们可以在这里创建自己的密钥
或者我们可以做一点更复杂的事情
这里有一个选项
这是我希望你确保检查的事情
所以我们可以选择桶密钥
现在桶密钥与sse kms密钥有点不同
因为如果我们使用sse kms密钥
这个密钥
换句话说
我们在kms系统中创建的密钥 我们将其应用于这个桶
然后每次我们写入或读取
它需要连接到这个服务
并且这个服务是通过api访问计算的
所以我们每次这样做都需要付费
这可能会大大增加费用
所以我只是想确保你启用了桶密钥
这样你就不会在账单上感到惊讶
所以我将保持其他一切不变
创建我的桶
搞定 现在我已经创建了我的桶
我们将从开源的aqs三号桶中复制数据到我们自己的桶中
如你所见，目前我们的桶中什么都没有
我们的桶中目前没有任何内容
现在什么都没有
为此，我将使用名为云外壳的实用工具
这是一个小型实例，允许我们使用aws cli
让我们看看是否能将其放大
好的 我认为这就是我能放大的最大程度
不幸的是 我认为这就是我能放大的最大程度
让我们看看 哦，搞定
你看，搞定，好多了
我无法根据浏览器缩放来放大
你必须使用这个小齿轮，选择你想要的字体大小
希望这足够大
这是一个我们可以用来查看我们的整个账户的aws控制台
但我们现在只需要查看s3
所以我要使用这个命令s3 aws s3 ls
我忘记我叫它什么了
它叫做engineer playground
我可能又错了
我又错了
是的，我又错了
我将使用一个命令来复制来自开源aqs的桶中的数据
我需要从我的笔记中获取那个命令
好的 我将在这里清理
这是命令
所以我将解释
这是如此的aws
这是aws cli，我们说s3
使用s3服务
这通常是你访问其他服务的方式
aws cli的文档非常全面
所以这超出了本课程或本视频的范围
但是只要跟着走
如果你不熟悉使用aws cli
所以aws s three
那是一个服务cp说复制
我们说s three
这里是公共存储桶，open aq为我们提供了可用的
我已经探索过了，你可以探索它
你可以出去挖掘，如果你想要
但这是我们要去的路径
所以我要去斜杠记录
斜杠 csv 点 gz 斜杠 位置 id 等于七七一一
我首先选择的这个位置 id
一个非常具体的原因
因为它有大量的数据
但不太多 它是什么
它是内华达州的拉斯维加斯
所以我们应该能够在那个数据中看到一些有趣的事情
当我们使用这个数据在其他分析和可视化练习中使用时
但是现在你可以使用任何位置
你是如何获得位置代码的
如果你回到这里并探索数据
你可能能找到
我不知道 也许你想找到你住的地方
假设我在这里选择
我不知道 就在这里芝加哥
点击芝加哥 s wfp
我真的不确定那是什么
但是如果我点击这里查看详细信息，并在这里查看位置
看到1658
所以你可以替换我的77
一一与1658
如果你想要
但请警告，某些位置数据不多
他们没有 也许他们的数据不是很新
也许他们没有几年的数据
这个地点拉斯维加斯的数据可以追溯到2018年
所以我认为这已经是一个很好的部分
所以我们将复制到目的地
那不是我的名字
那是我使用的另一个桶的名字
哦 回到那里
我将删除这个并替换为数据工程师
游乐场/拉斯维加斯
然后我将有这个命令在这里说递归
所以这将要做的就是它会到这个目录去
再次我不愿意称之为目录
但基本上所有在这个下面的对象都有这个前缀
aws cli 会去那里
所有在这个前缀下面的对象
它将会拷贝到我们的桶这里，并且在使用前缀
拉斯维加斯 现在递归是说嘿
深入到这个特别前缀下面的一切
或者你可以这样想它是一个子目录，专业提示
如果你想让它更快
你可以在这里添加静音选项
如果我们不加入静音选项
它将会把所有这些东西放在这里的控制台上
但我有点喜欢看这些东西
因为它让我知道复制在做它应该做的事情
所以我将执行它
现在我们开始复制
在这里复制数据，你可以看到日期正在传输
我想现在差不多
这可能需要几分钟来复制所有这些东西
我只是暂停视频
然后继续
复制完成后
然后我们看看有什么
然后好的 复制刚刚完成
可能花了两三分钟
大约这样吧 我们已经复制了所有这些文件
让我给你展示一下我在这里使用的哪个站点
所以我要去探索
我跟你说过这是拉斯维加斯
这是拉斯维加斯的一个特定地点
因为拉斯维加斯这里有很多收集区域
而这里就是那一个
所以如果我去详细信息这里
你可以看到7711
所以我为什么使用这个呢
这个离aws re:Invent活动举办地点比较近
于是我想到
也许我们会看看这些数据
质量是在各个过去重新发明的时间周围
所以，我们会保留这些数据
我们将在整个课程的剩余时间里使用它来做各种事情
副本已经完成
让我们看看
让我们去掉这里的这个东西
让我们看看我们的桶
我将刷新
这就是我的拉斯维加斯文件夹
这里你可以看到
年份等于2018
年份等于2019
你可能认为我们会说2018和2019
这种格式有特定的原因
这在课程后面会变得明显
当我们开始使用一些aws工具查看这些数据时
这就是如何将数据分成不同的组
如果我们深入研究2018年
我们有月份
我们还有那个月份里发生的各种读数
所以你可以看到这里
我们从11月1日到11月2日，再到11月3日
一直持续到11月30日
这些是日期
这些是每天发生的读数，现在你可以看到
如果你对这个有点担心
这会让你花费一大笔钱
所以我们默认使用的是标准
如果我们在这里查看其中一个文件
你可以看到存储类别标准
但如果我们到这里
点击这里，选择操作，计算总大小
所以这里的总大小是917KB
请注意，我们可以以两美分多一点的价格存储一整个GB
所以这甚至不是一个MB
所以它可能不会显示在你的月账单上
所以我认为那里我们很安全
现在我们的数据在这里，我们可以在这里访问我们的桶
让我们在这里创建一个文件夹，用于我们的松鼠普查
命名为松鼠普查并创建文件夹
要做到这一点，我们需要出去或者获取文件
我们需要去一个有点有趣的网站
叫做松鼠普查网站
在这里我们有一只松鼠先生
他似乎有点不耐烦
我不太确定他在等待什么
但你可以随心所欲地浏览这个网站
这里有一些相当有趣的有点搞笑的东西
但这是松鼠普查
并且如果我们向下滚动这里
我们可以看到这些数据集
所以我们点击数据集
而我们要找的是松鼠数据
你也可以在这里获取用户指南
它会解释这些数据是如何收集的和如何使用它
简而言之 基本上他们有一个由72名志愿者组成的小组，他们是松鼠饲养员
他们记录了松鼠的活动
所以我们将深入研究松鼠数据
它为我们提供了一个链接到Dropbox上的CSV文件
这里是文件
你可以看到我们的标题在这里
下面我们有不同的数据
所以我们将下载这个
继续仅下载
这是开放数据
所以你不应该为此付费
如果你发现某人在出售松鼠数据
那么它是一场骗局
所以，我已经将其下载到我的本地硬盘上
我将回到我的桶
进入我们的松鼠普查文件夹
我将上传那个CSV文件
好的 所以我已经上传了
或者我至少选择了我的松鼠数据CSV文件
我现在上传它
因为现在我们在这里
我将回到数据这里
我认为将公园数据结合起来可能会有用
这是松鼠被记录的地方关于公园的一些数据
也许我们可以做一些组合查询和类似的事情
所以我只是继续去做
继续仅下载
我将做同样的事情
我只是上传这个文件
让我们看看上传并选择文件
我们去公园数据并上传
好的，我们就在这里
让我们关闭这个
如果我们回到这里，我们的松鼠普查文件夹
那么我们有我们的公园数据和我们的松鼠数据
我们将在以后的技能中使用它
这就是我们现在想要做的全部
我们正在使用AWS S3存储设施
这是大数据分析中最常见的
或者至少是数据分析
我想在这里向你展示的一件事
如果我在这里钻孔
假设我想改变这些文件的存储方式
也许我想将它们改为一个区域
如果我想改变存储类别
只需点击操作，到这里的存储类别
我可以将其更改为这些中的任意一个
如你所记得
我曾说过这里有一些限制
这里有一些最低存储时间，这里标有
但我现在不会详细解释
重要的是，我们已经收集了一些数据 我们将在后续的技能中使用这些数据
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/023_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p23 9. Validation File and Object Storage Options.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们来看看这些问题
首先，这些问题的存储类型
哪种类型在设计上冗余最低
实际上，这里就是一个排除的过程
我们可以从第一个开始，这里 s three 标准存储层
我们知道这是多区域存储
这意味着我们的数据在多个可用性区域有多个副本
让我们把这个作为基准
然后我们可以尝试找出
嘿，这些其他选项中是哪一个更冗余，还是更少冗余？
下一个 我们有冰川即时检索
如果我们考虑当我们试图存档东西时
因为冰川就是这样
它是存档存储
我们希望非常、非常确定它会在很长一段时间内保持不变
所以我们想在那里有一些冗余
所以我知道冰川也是一种多层或多区域类型的存储
下一个是s3一区存储，啊哈
所以，而不是多区存储
我们这里有s三单一区或一区存储
根据其设计
它只存储在一区
这意味着它只具有99.9%的冗余或耐久性
而不是我们跨三可用区存储时获得的11个九
99.999% 所以，我确定这就是答案
让我继续到最后一个
问题
这里有三个不常用的
不常用访问存储在这里很好
它有与常规旧s3相同的多az存储配置
我们同意
或者我们知道我们可能需要更少频繁地访问它
在所有答案中
这里有三个 一区域存储在这里是我的最爱
那就是我要坚持的下一个问题
以下哪项是对象存储的关键特性
如果我记得正确的话，对象存储更像是一个数据库而不是
它实际上是文件存储
但我们可以在对象存储中存储文件
没问题 我们可以去掉这里最后一个
块级访问 因为对象不是以块形式存储东西的
它们以对象形式存储
所以我们可以去掉下一个选项
高iops
如果你记得我们曾经比较过不同的存储类型
弹性块存储具有最佳性能
当我们谈论高IOPS时
这通常不是我们与对象存储相关联的东西
同样的情况在这里也适用，即低延迟
通常你不会看到访问数据的低延迟
在对象存储中
因为它必须通过其所有存储级别，并将数据视为数据库来处理
这里有一件事是突出的，即可扩展性
这是对象存储的关键特性之一
这就是我将要选择的下一个问题
以下哪些存储选项需要您支付您想要的最大存储量
而不是支付您数据占用的空间
所以本质上
这里它问我们要的是哪种服务不提供
按每GB或按使用付费的成本模型 glacier glacier
我们在冰川中支付每GB的数据
如果我们只使用了一小部分GB
我们只会支付一小部分那个GB的费用
或者那个GB的费用是弹性文件系统
那就是同样的交易
如果我们放一个小文件在那里
我们只会为那个小小的文件占用的空间支付费用，同样的事情
我们只会支付吉字节的一小部分
因为如果它是按吉字节计费的
我们只会支付那一个吉字节的一小部分
为我们的存储或对象占用的空间
然后我们转到弹性块存储
弹性块存储与这里的其他三种完全不同
因为它的本性
它是块存储
我们不真正管理文件级别
相反，我们以块级别管理它们
因此 我们对EBS卷上的数据量没有透明度
所以我们不得不为整个东西预留
然后 我们可以使用操作系统级别的指标来检查我们实际使用了多少空间
但无论如何我们都要支付
让我们说 如果我们准备了40GB的存储
我们必须为那整个40GB付费
无论我们使用了多少
所以这是我最终的答案
我希望这对你有所帮助 感谢您观看
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/024_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p24 1. Skill Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，美丽的学习者，来到另一个技能
这是Kiu
这是记录结构化信息的一种早期形式
古代的印加人用它们来记录从土地所有权到税收义务再到人口普查记录的一切
一系列结将被用来记录数据
主要是以数字形式
但Kiu也可以记录地点，就像我们今天使用邮政编码一样
在这个技能中 我们将深入探讨现代Kiu的几个等同物
我是在谈论aws上的数据库
仅仅作为21世纪的人类
我假设你一定知道数据库是什么
你很可能在每天工作中都会使用它
真正的技巧
虽然这里是理解不同类型的数据库服务
以及何时一种可能比另一种更好
给定的情况下
到技能结束时
我们将通过创建它来测试这些数据库之一
然后迅速迫使它失败 让我们开始
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/025_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p25 2. Amazon RDS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们将开始探索数据库选项的旅程
这可能是AWS上最著名的数据库选项
那就是关系型数据库服务或RDS
现在RDS是一个服务
但它是一个完全管理的服务，用于托管数据库平台
我所说的意思是我们有不同的选项来决定我们想要托管哪种类型的数据库
我们想要哪种品牌，哪种口味，如果你愿意的话
但它们基本上都被视为相同 并且它们使用RDS作为底层引擎或该服务的促进者
它们设置起来相当容易，基于向导的 你会进行并询问
你想要如何设置这个系统
你试图做什么
那是生产环境，开发环境之类的
你回答所有问题并选择所有这些选项
然后最终它会生成一个系统来满足你的需求
并且它相当可扩展
假设我们从一个小型试点项目开始 我们只拥有一个非常小的数据库，一切都进行得很好
我们决定嘿 让我们继续将其作为生产环境
这样我们就可以关闭那个实例
我们可以升级它
这意味着我们只是选择一个不同的底层实例类型，拥有更多的能力
然后我们可以重新启动那个实例
然后，嘿，我们刚刚升级了我们的RDS系统
现在我们也有多可用区选项
这意味着我们可以将我们的数据库分布在多个可用区
这样如果某个可用区失败或出现问题
那么它就会切换到其他可用区，并且，RDS的祝福与诅咒是
我们有很多灵活性，很多选项
这就是人们在开始使用RDS时遇到的主要挑战之一 因为他们有这么多选项
之所以有这么多选项，是因为AWS试图满足许多不同的客户需求
所以我们首先要决定的是，我们将使用哪种类型的数据库
RDS支持MySQL，支持MariaDB，PostgreSQL，Oracle，Microsoft SQL Server等
RDS支持多种数据库类型，满足不同需求
RDS的设置非常直观，用户只需回答几个问题，系统就会自动配置
RDS支持多种数据库类型，满足不同需求
RDS的设置非常直观，用户只需回答几个问题，系统就会自动配置
RDS支持多种数据库类型，满足不同需求
RDS的设置非常直观，用户只需回答几个问题，系统就会自动配置
RDS支持多种数据库类型，满足不同需求
RDS的设置非常直观，用户只需回答几个问题，系统就会自动配置
RDS支持多种数据库类型，满足不同需求
RDS的设置非常直观，用户只需回答几个问题，系统就会自动配置
RDS支持多种数据库类型，满足不同需求
RDS的设置非常直观，用户只需回答几个问题，系统就会自动配置
RDS支持多种数据库类型，满足不同需求 Sql server
And recently it now supports ibm db two
Now there is another flavor that's available
But that's more of an aws creation
And we're going to talk about that in a separate video
But as i said
One of the challenges here is trying to decide all the options
And all the features that we want to use
We have to first decide what sort of database flavor we want now
If we go with sql
服务器支持MySQL或Oracle数据库
甲骨文 IBM和微软都想要分得一杯羹
因此，那些平台的许可费用已经包含在价格中了
我们将现在支付一个rds实例的费用
如果我们使用 mysql
Postgres（PostgreSQL） SQL 或 MariaDB
我们没有在那里有任何直接的授权
那就是一种方式
有些组织能够通过使用这些其他类型的数据库来节省一些资金
而不是使用这些商业数据库类型
但如果你是一个微软商店或IBM商店或Oracle商店
那么你可能想使用这些数据库
因为你可能已经发展出了相应的技能集和管理工具
接下来我们需要选择的是底层硬件或虚拟硬件
我们是否想为我们的数据库配置
我们有一般的通用目的
这对于一般的通用目的很好
我们有内存优化
如果我们进行大量的查询和大量的分析
有时内存优化实例更好或性能更优
我们有突发稳定
这就是我们的t家族
所以这意味着如果我们有一种不可预测的工作负载
在大多数情况下
数据库基本上没做什么
但当我们叫它行动时
我们希望它高效地行动
然后突发稳定是一个可能的选项
然后我们也有一定的数据库类型，它们被优化为读取
如果我们写的不多
但是在做大量的读取的情况下，比如进行重分析的情况
例如
那么优化读取实例类型可能就是我们要使用的
下一个选择，我们必须决定什么样的存储
尽管这些数据库是全面管理的
它们仍然有提供
一些存储来存储我们的数据
你可以把这些数据库看作是某种类型的e的特殊版本
C2 这就是它们的主要功能
所以我们必须决定我们是否需要通用目的
SSD 可能是gp3选项或gp2选项
也许性能对我们来说非常重要
所以我们可能需要在这里配置一些IOPS并使用IO1类型的存储
如果我们根本不关心性能
那么我们就可以使用一些老旧的磁盘来节省一些钱
我们可以决定我们是否想在单个可用区中部署我们的数据库
或者在多个可用区
两者的区别在于如果我们只在单个可用区中运行
那么我们就没有那么冗余
我们没有在多个可用区模式下部署那么耐用
我们还有其他选项
这些在我看来就像是
你是否想要一些薯条
我们可以进行标准部署
我们可以进行读副本
稍后会解释这是什么
我们也可以选择部署RDS代理
稍后会解释这一点
但是等等，还有其他的，我们可以选择不同的定价模型
如果我们只是想要原型
也许我们只想要按需定价
这将根据
我们系统运行的小时数来收费 但是
如果我们确定
我们将保留这个系统运行一年或三年
我们可以选择预订实例数据库
这将为我们节省一些钱
因为我们在说，嘿
我将使用这个数据库一年
或者三年
根据你的选择，你将获得一个按需定价的折扣
对于大型数据库，通常在驱动关键应用时
它们不会经常上下线
它们通常会启动并保持运行一年或三年
因此，许多公司会选择长期运行的数据库需求
所以让我们看看一个选项，我们有标准部署
标准单AZ部署意味着我们有一个实例
在某个可用区
如果我们选择多AZ部署
我们将获得一个实例
但在另一个可用区，AWS还将创建一个称为备用实例
我们没有选择哪个可用区的灵活性
每个实例
我们将不得不接受 AWS的决定
这是我们为了完全托管服务做出的一种牺牲
但我们可以放心
AWS会将备用实例放在一个完全分开的AZ中
这个备用实例会做几件事
第一 如果我们的原始实例出现问题
这个备用实例将晋升为我们的主RDS实例
因此，我们的应用程序甚至不会注意到原始实例已下线
它们可能会经历短暂的连接中断
但总的来说，故障切换非常快
这还有一个好处
或者备用实例的另一个功能是当我们备份系统时
备份是从备用实例进行的
这是一件好事
因为如果我们的主RDS实例在这里进行大量读写操作
然后哦 是时候进行每日备份了
这会占用一些处理能力
因此，AWS使用备用实例进行备份
这是原始实例的复制
我们还可以部署多AZ数据库集群
这是指
我们在一个AZ中有一个写入实例，然后在其他AZ中有只读副本
这意味着每当我们对写入实例进行写入操作时
它会将这些操作复制到这些只读副本
然后我们可以将只读工作负载
指向这些只读副本
例如，将报表软件指向其中一个只读副本
以便只读副本可以处理查询活动
而不是返回写入实例，增加其负担
我们还可以在多个地区使用此只读副本模型
我们可以在这里的US East 2中有一个主实例
我们可以在EU West 1和AP South 1中有另一个实例
每当我们对这个实例进行写入操作时
它会将这些操作复制到这些只读副本
然后本地用户可以在AP South 1或EU West 1中引用它
他们可以将系统或分析软件指向这个只读副本
他们可以看到主实例写入的数据通常在几秒内或更短时间
我们还有RDS连接池
这是这样工作的
假设我们有许多应用程序在这里
四可能不是很多
但我们假设每个应用程序都有100个
那么我们将有400个不同的应用程序
它们试图访问这个系统
试图获取它的注意力
以便它可以做它需要做的任何事情
无论应用程序需要做什么
这就是对RDS实例的压力
所以我们有能力做一件事，叫做rds连接池
要做到这一点 我们可以在前端创建一些东西，叫做rds代理
这个代理将要做的事情是
它将充当这个后端的缓冲器和前门
rds实例
而不是有四百个不同的连接进入这个数据库
它将有这四百个不同的连接
然后将它们汇集在一起，进入
也许只有几个连接回到rds实例
这将节省我们的rds实例一些内存和性能
通过将连接负载卸载到rds代理
现在我们也有一个选项，我们可以在rds实例前面放一个缓存
我们可以使用redis缓存，并将其放在rds系统前面
而不是完全回到数据库获取一些数据
我们可以将数据加载到缓存中
这个应用程序完全回到rds实例
它获取一些数据，这些数据被发送到缓存这里
现在，这个小应用程序正在寻找相同的数据
所以它来到这里缓存
它说，缓存说，嘿
我已经有你的数据了
它将发送那个数据回来
第一，这将卸载那些连接和查询的负载
从我们的rds实例到那个缓存，第二
它将提高那个查询的性能为我们的应用程序
因为他们不必回到物理数据库中查找那个信息 他们可以直接从缓存获取
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/026_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p26 3. Amazon Aurora.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 当我谈论rds时
我提到了我们还有一个数据库引擎的选择
那就是称为亚马逊aurora的东西
现在亚马逊aurora是aws从零开始创造的
以利用aws基础设施并利用aws提供的一些功能
这些其他数据库引擎基本上就是遗产
它们来自我们拥有服务器的世界
在我们的数据中心 这就是我们所拥有的
但是亚马逊奥罗拉有一些特定的功能，利用了aws的优势，这使得它成为一个相当独特的产品
它将其与aws的其他服务集成，使其成为一个相当独特的产品
我们将深入探讨这一点
它提供了两种选择
两种兼容性选择
一种是my sequel
一种是postgres sequel
如果你有工具集，或者你现有的数据库是基于mysql或postgres sql
你可以将数据库迁移到亚马逊奥罗拉
并且有一个期望
它应该像以前一样运行
在这两个其他平台上
你为什么要迁移到亚马逊aurora
嗯 我们很快就会讨论这个问题
但是亚马逊aurora也有一个无服务器版本
我开始理解无服务器rds数据库花了我一点时间
但只是想象一下，你是通过一个api访问一个数据库
而不是连接到一个物理实例并运行sql查询
我们还有一个叫做亚马逊Aurora全球数据库的东西
我们很快就会讲到这一点
所以亚马逊Aurora
它是RDS的另一个选项
它支持MySQL和PostgreSQL
但它并不是MySQL或PostgreSQL
AWS非常小心地提到这一点
但他们只是说兼容
所以他们并没有保证所有特性
MySQL或PostgreSQL的所有特性都在那里
但大部分情况下
你的工作负载很可能会在那个平台上运行
如果它在其他平台上成功运行
现在，他们使用了一种非常独特的架构来组装亚马逊aurora
有趣的是
它可以是mysql和postgres sql的更高效、更高性能的版本
比那些原始的遗留设计平台
这就是为什么亚马逊aurora有点不同
通常当我们部署一个rds实例时
我们会在一个az中部署它
也许我们可以在另一个可用区有一个只读副本，现在区别在于
这些系统都是自包含的
所以存储和CPU以及所有那些东西都在同一个系统中
所有的东西都在同一个盒子里
这么说吧 所以我们这里有一个复制层
这是一个数据库的功能
所以数据库处理复制数据
所以在这里有一些负载
这里有一些延迟
因为这些通常是在数据中心运行的数据库
我们也可以将我们的数据中心中的一个mysql数据库
它可以复制到aws中的一个版本，我们当然可以做到这一点
但是再次 这是一个在数据库级别进行的复制服务
亚马逊aurora的不同之处在于
它已经将存储从实际的实例中抽象出来
所以这个实例只运行亚马逊aurora软件
并且aurora存储已经在不同的az中复制
所以考虑一下
几乎可以类比于efs
所以我们有一个多AZ存储设施，这是亚马逊aurora正在使用的
这里是好处
这里是这样做的好处
以这种方式
一旦我们将数据写入到这个复制存储
这个复制存储将发送数据
并将它提供给这些在其他可用区运行的实例
我们这里有整个存储子系统处理复制
并将这个负载从上面的CPU中移除
试图返回查询
这是一个相当酷的事情
它更进一步
我们可以将这些复制扩展到不同的地域
这里我们有us east 2
eu west 1 和ap south 1
并且这些aurora存储位置可以在幕后进行复制
我们不必担心这个
我们可以设置这个
并且它会将数据从这个复制到那里，从那里复制到那里
所以当我们在这里写入一些数据时，很短的时间内
这些数据可以在完全不同的地域中供我们读取
这里是另一个酷功能
称为本地写入转发
所以如果我们想要，例如
在这里写入一个值
也许不多
我们没有写很多
也许我们只是想更新记录
或者类似的事情
这次更新可以回传到这里，然后在我们的主实例中复制
所以本质上
我们这里有一个全球数据库
一个我们可以全球访问的数据库
我们可以全球读取
并且我们可以从我们的读取实例中向主位置进行有限的写入
这是一些RDS与传统MySQL引擎和亚马逊Aurora之间的区别
传统MySQL引擎和亚马逊Aurora
如果我们说MySQL引擎有一些基本的性能
无论那是什么 我们将其称为1x
然后亚马逊Aurora
至少AWS声称亚马逊Aurora可以提供高达五倍的性能
这种抽象存储的另一个好处是
当我们在传统MySQL中创建实例时
我们必须说
我们要为存储数据的数据卷分配多少
可能是100GB
200GB 无论多少
我们可以总是扩展它
但我们需要负责扩展它
除非我们选择其他自动扩展功能
但与亚马逊Aurora
我们根本不用担心 实际上
当我们创建亚马逊Aurora实例时
甚至不需要选择存储量
因为会自动根据我们存储的数据量扩展
MySQL我们可以在几分钟内恢复，亚马逊Aurora可以在秒内恢复
这是因为架构更设计用于利用AWS平台
而不是MySQL
MySQL在AWS出现之前设计 多年以前
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/027_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p27 4. Amazon DynamoDB.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以DynamoDB是那些在后台广泛使用的服务之一
在AWS内部
它在后台被广泛使用
许多用户每天使用的流行应用程序和服务
也是那些服务之一，理解它们是有意义的
这样你可以获得最佳性能
DynamoDB是完全管理的数据存储平台
它与RDS或关系数据库不同
它是一个NoSQL数据库
NoSQL数据库由键值对组成
我们将在第二个看看这意味着什么
我将比较和对比更传统的关系型数据库
我们没有实例需要管理
被认为是无服务器服务
我们通过API和SDK与之交互
它也是自动的可扩展的
所以我们可以设置它，只要我们能投入多少
它将能够处理它
因为它将扩展出以满足这一需求
现在我们也可以全球复制我们的数据
如果我们选择
这样它就可以在更快的速度下访问
或者对某个完全分开的区域或地球的另一边的人来说，提供更快的性能
如果我们真的需要速度
还有一种叫做dynamo db加速器
人们通常称之为dax
如果我们真的需要非常快速的访问
比如微秒级的访问
这相当于使用rds的现金
dynamo db加速器是dynamo db的缓存
现在，这里是传统数据库和nosql数据库之间的一些差异
传统数据库具有相当灵活的查询结构
但它们实际上并不特别擅长扩展
另一方面 nosql数据库具有相当有限的查询能力
但它们非常高效
这并不是传统数据库不好的情况
nosql是好的
它们各自有其服务的目的
这正是我的意思
在一个技能介绍中
关于关键点是试图找出你的需求是什么
以及哪个aws数据库
最适合这些需求
现在
通常 当你谈论传统或关系数据库时
你正在谈论模式
你可能首先听说过 正常第二
正常 第三范式之类的东西
现在，NoSQL数据库
通常，您会为将要使用的内容设计模式
在传统的关系数据库中
我们将基于
也许最大的灵活性
在关系数据库中，我们鼓励规范化事物
并利用这些关系
相比之下，NoSQL数据库
我们非常鼓励只有一个大而厚的记录
我会马上向你展示这看起来像什么
至于数据检索
我们将链接它
基本上从其正常形式重构数据
在NoSQL数据库中
数据理想情况下存储在一起
我们将出去那里
获取结构并获取该结构中的数据
并选择我们想要的
让我们以这里为例
获取客户记录从客户abc
让我们说我们要求我们的数据库
在关系数据库中
我们可能已经将事物规范化
像这样 我们将有一个客户表
可能有一个地址表
电话号码 订单历史表
它们将通过可能主键
外键 那种东西连接在一起
在NoSQL数据库中
我们有相同的所有数据
但你可以看到它以不同的方式存储
在这里我们有等效的客户信息
然后在下面我们有地址
然后我们有电话号码
也许您在这里有两个记录在这里的电话号码表
那是在这里表示的
我们有这里的两个记录
所有这些都在同一个结构下
所以这里整个结构包含我们所有的东西，如果我们要像
连接不同表
让我们更详细地看一下Dynamo DB记录
它由称为分区键的东西组成
那有点像主键 虽然然后我们有一个排序键
所以分区键定义数据将如何存储
因为DynamoDB将事物存储在分区中
而排序键只是另一种分组方式
这就是我们所说的DynamoDB
我们有不同的分区
它如何扩展是一旦我们填满一个分区到一定大小
它将会创建一个新的分区
它试图在所有这些分区之间均衡存储
例如 如果客户ID这里是我们的分区键
它将会取该值的哈希值
那么我们假设我们将数据存储在这里
我们将其设置为零 零到aa到cc，然后cc到f
这是值
Dynamodb将计算出哈希值
假设它落在了这里
然后在该分区中基于排序键
它将存储所有带有客户类型商业的记录
如果那是排序键，它将全部存储在一起
因此，当我们查看分区键时，检索速度会更快
当然 但也是排序键
选择这个分区键
虽然这是一个重要的决定
所以这是为什么
假设我们有这个记录
它可能来自某种机器或传感器
我们选择日期作为分区键
那么会发生什么
我们将使用哈希函数
它将从这里看到这些记录在这一天大量涌入
假设这一天有数百万条记录涌入
它会将它们都哈希成相同的值
所以它们会被放入同一个分区
所以这里这个分区会被严重冲击
而其他两个分区则几乎没有任何工作
这就是热分区的情况，会导致性能问题
所以如果我们选择一个分区键
这个键的分布更加随机
这个uid
我们仍然可以使用日期排序键
这里将要发生的事情
这个uu id 对所有传入的记录来说应该是独一无二的
或者至少是唯一的
然而 我们定义它以便理想地将负载分散到这些所有分区
而不是锤击一个分区
我们让所有这些分区一起工作来接收这些信息
然后我们想要查询它
我们仍然可以使用日期因为它是排序键
因为这些分区存储了所有内容
让我们谈谈Dynamo DB的另一个特性，称为二级索引
在这里我们有我们的主键，我们使用UUID作为主键
我们使用日期作为排序键
假设我们想要按传感器查询
在这种情况下，我们可能需要执行称为表扫描的操作
因为我们没有直接查询传感器的简单方法
因为它不是我们键字段之一
我们希望避免这种情况
我们不想进行表扫描
基本上，Dynamo DB会询问每个记录
问 我是否在寻找的值
这非常低效
因此，我们可以使用称为二级索引的东西
存在两种类型的二级索引
第一种类型的二级索引称为本地二级索引
在这种情况下，我们将继续使用uuid作为分区键
但我们可以将传感器定义为排序键
因此，在这个本地二级索引到位后
如果我们说嘿
给我所有等于c和c4的传感器
那么它会到这里去找
它会看到这本地二级索引
它会说嘿 我看到你的排序键在那里
它会非常迅速地拉回所有这个信息
但我们这里还有一个选项
我们有一个称为全局二级索引的东西
而全局二级索引
我们可以定义一个完全独立的分区键
所以我们可以说嘿
定义传感器作为我们的分区键
我们可以定义日期作为排序键
这将是查询传感器的最快方式
因为作为分区键
它将基本上像一个直接的读取
你可能在说嘿
全局二级索引看起来比本地二级索引更有用
为什么我们不 我们总是使用全局二级索引
这就是原因
所以，在本地二级索引中
我们没有改变分区键
分区键保持不变
我们只是改变了排序键
我们使用全局二级索引创建了不同的排序键
我们正在改变
或者我们正在创建基本一个新的集合
一个新的那个数据的副本
但是它的键是不同的
分区键是传感器
无论何时我们决定创建一个全局二级索引
Dynamo db将去那里并本质上创建一个副本
然后以传感器作为分区键
我只是对传感器进行哈希
然后丢弃传感器和任何对那个有意义的分区
本质上，拥有一个全局二级索引的成本是
我们基本上可以复制数据的大小
因为我们按数据大小付费与Dynamo db
然后它可能基本上会翻倍我们的成本
这是全局二级索引和本地二级索引之间差异的概述
本地第二项将使用相同的分区键
而全局二级索引可以使用不同的分区键
在两种情况下
我们可以有不同的排序键
在本地二级索引中
我们可以访问与基本表相同的所有属性
但这并不适用于全局二级索引
我们可以绝对拥有相同的所有属性
如果我们真的需要为那个全局二级索引的一个小属性子集
我们可以只创建一个小的子集
因此我们不会创建一个与我们的原始数据完全相同的副本
而是一个较小的子集
这里有一个警告
本地二级索引必须在创建表时创建
并且我们不能在创建后删除或添加它
我们可以在任何时候创建这些东西
并且我们可以在任何时候删除这些东西
这里有一些权衡
如果你真的了解你如何检索数据
你想在你创建表之前就对你的本地二级索引进行很多思考
并且如果你发现你在创建表之后
你真的需要一个字段在那个特定结构中的最快可能的检索 那么全局二级索引可能对你来说是一个选项
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/028_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p28 5. Amazon ElastiCache.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我们谈谈另一种类型的数据存储，即弹性缓存
弹性缓存是一种缓存
它是完全管理的缓存平台
它是一种缓存
所以它设计用于瞬态，不适合长期存储
所以不要把缓存看作是某种
你会建立并长期存储记录的东西
很长一段时间 那里有更好的选择
缓存实际上设计用于非常短期的高性能检索
AWS支持两种口味
一种是memcache d
它已经存在了很长时间
它非常成熟
另一种是redis，redis的功能比memcache d更多
我们会看看这两种之间的比较
无论是memcache d还是redis
我们都可以依赖亚毫秒级的延迟
所以检索时间在亚毫秒级
它们被广泛支持在许多开发语言中
可能大多数 几乎所有流行的开发语言都有某种方式与redis或memcache交互
区别在于memcache d就像那种无废话的老式
它只做一件事 而redis有更多的功能
它具有更多的功能
它有更大的脚印
这么说 所以它需要更高性能的资源来运行
redis提供快照
我们可以缓存的快照
这在我们需要关闭缓存时是有用的
例如 但我们不想失去缓存中的所有信息
但是，这里有一个警告
我们不真的把redis当作长期持久性存储
它更多的是一个缓存
这是我们可能重新填充缓存的机会
一旦我们恢复redis实例并执行一些维护或操作
memcache d没有快照
没有与redis的复制
我们可以将数据复制到可用区以获得冗余
memcache d基本上可以处理一次命令
而在redis中，我们有事务的概念
我们可以发送更复杂的事务
这些事情可以被处理并返回值给我们
memcache d没有提供的加密
而redis提供了加密
如果我们需要它
所以你可以看到这里有两种方法
一种是如果我们想要一种比较传统的可靠的
直白的缓存
我们可以使用memcache
如果我们想要功能更丰富的
我们可以使用redis
这两种都属于弹性缓存服务
当你设置弹性缓存实例时
你需要在这两种系统之间进行选择
这取决于你的需求
根据这些特性 这里
根据这些特性 在这里
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/029_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p29 6. More Specialized Database Options.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 这个视频中，我想进行一个快速的问答环节
我们有几个其他的专业数据库需要覆盖
但我不想花太多时间
为每个数据库制作一个视频
因为它们中的很多都非常专业
你可能不需要它们
但如果你需要
至少你会知道它们是做什么的
然后你可以进一步调查
Amazon Redshift 是完全管理的数据仓库服务
它专为大数据设计
关于它的名字有一个传说
当Redshift被创建时
Oracle几乎垄断了数据仓库市场
传说中，AWS将这个服务命名为Amazon Redshift
因为Oracle的主要颜色是红色，Redshift意味着红色偏移
get it
是的
它非常适合我们有多 terabyte 的结构化关系数据
并且我们可以直接从S3查询数据
下一个服务是Memory DB for Redis
如果你考虑Amazon Aurora
AWS是MySQL和PostgreSQL的自定义版本
Memory DB是AWS的Redis自定义版本
它是内存中的，兼容Redis
并且可以大规模扩展
我们可以将其扩展到100 terabyte的存储
如果你能想象100 terabyte的缓存 那是一个巨大的缓存
正如我所说
它类似于Aurora
但它是针对Redis的
我们还有Amazon Document DB
Amazon Document DB是AWS从零开始创建的
它试图模仿MongoDB
它具有MongoDB的兼容性
如果我们在premises上使用MongoDB
并且我们决定迁移到云中的全面管理服务
我们可能只需要将数据直接迁移到Document DB
并且可以使用它
它是Dynamo DB的替代品
AWS仍然希望你使用Dynamo DB
因为他们在该平台上投入了大量的技术和专业知识 但如果你只是想找一个MongoDB的替代品
那么Document DB可能是你的最佳选择
所以它的迁移非常简单
Amazon Redshift 是完全管理的数据仓库服务
它专为大数据设计
如果你在使用现有的mongodb实例迁移到文档db时
接下来我们有的是亚马逊密钥空间
这是一个完全管理的Apache Cassandra版本
Apache Cassandra是一个非常流行的NoSQL数据库
它有开源的起源
并且它只是同一个替代品的另一种形式
你会看到这种模式一次又一次地发生
AWS会创建一个开源产品的版本
他们的想法是吸引那些使用该开源产品的人
也许在预置环境中使用
选择完全管理的同一服务版本
另一个数据库选项我们有的是称为亚马逊Neptune的东西
现在 它是一个完全管理的图形数据库
现在 什么是图形数据库
嗯 这实际上是一种专门用于链接和关联不同实体的数据库
图形数据库的一个主要用例
现在是能够绘制社交媒体连接
以及社交媒体中不同实体之间的相互关系
它也非常适合关联事件和预测
因此，图形数据库用于威胁建模和威胁检测
它也用于欺诈检测等事情
因此，Neptune是AWS的图形数据库版本，继续前进
我们有一个数据库
或者我们有一个数据存储称为亚马逊Timestream，Timestream是一个完全管理的数据存储
针对时间序列数据进行了优化
想想从传感器或可能物联网设备收集的数据
它专门为存储该时间序列数据而设计
不仅存储它
而且还能检索它并进行分析
是的 在我们快速火轮的最后一个这里是一个称为亚马逊量子账本数据库的东西
量子账本数据库
尽管我深入研究了它
但我没有看到任何证据表明账本数据库中有任何量子
不是很确定为什么他们称之为量子账本数据库
它实际上只是一个账本数据库
它基于AWS的区块链产品构建
当你真的需要以不可变的方式存储数据时
什么是不可变的 不可变意味着它无法被更改
所以每个记录中包含前一个记录的哈希
这就是区块链的工作方式
这也是亚马逊量子账本数据库的工作方式
我们可以在其中存储记录
每个记录都将包含前一个记录的哈希
因此，这将使我们无法更改那些已经提交到区块链中的记录
任何记录
因为这会破坏整个链条
这里的用例是
当我们谈论金融交易时
也许供应链保险索赔
跟踪 那种事情
很多可追溯性
如果你需要跟踪某些事情
比如许多的药品 量子账本数据库可能是那里的一个选择
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/030_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p30 7. Walkthrough RDS Creation and Failover.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 在这个教程中，我们将创建一个rds实例
我们将创建一个aurora实例
然后我们将进行故障切换
换句话说，我们将迫使一个失败
然后观察另一个被提升
并且基本上测量时间
因为它非常快
如果我们使用另一种类型的数据库
这可能需要几分钟
但是，有了aurora
这基本上就是秒级
所以，为了到达rds
我们可以点击这里这个小东西
或者，我们可以在这里搜索，通过输入rds
它就在这里
它将带我们到达rds仪表板
目前，我们这里没有任何必要的数据库
正如你所看到的
rds是区域特定的
所以我会推荐选择一个靠近你的区域
你不必这样做 但这可能会使事情变得更容易
所以我们可以创建一个数据库
这里我们被提供了一个选项
我们可以选择一个简单的创建方法
这基本上只给我们提供了很少的几个选项
我们可以选择我们想要的
这有点像一个缩小的选择列表
或者我们可以使用标准创建
这是一门中级课程
所以我将使用标准创建
所以我们将选择aurora
MySQL兼容版
所以你可以在这里这个小面板中看到
它给我们一些限制
如果我们选择另一个数据库
你可以看到那里的一些不同的限制
但我们将选择aurora与MySQL兼容性
我将继续往下滚动
我们可以选择不同的引擎版本
所以如果我们正在与一个非常特殊的mysql版本合作
例如，mysql引擎
假设我们有一个在本地使用的数据库，正在使用特定的引擎版本在这里
我们可以选择这个引擎版本
只是为了确保 我们希望减少或消除任何不兼容的机会
所以我会保持一切不变
这里给了我们一些单选按钮
我们是否想要生产模板或开发测试模板
这就是预选了一些东西
如果我们选择生产
它会预选一些使它更健壮的东西
并且适合生产
但对于我们的目的，我们现在将选择开发和测试
在这里我们可以给数据库集群起一个名字
这里我们将其命名为我的数据库
我们可以为管理员用户选择一个用户名
如果我们的工作站上有mysql工具
我们可以实际上远程连接到这个实例
使用这个用户名和密码，我将选择自动生成
因为我不需要远程连接到系统
现在我们可以选择我们的集群存储配置
如果你记得当我解释aurora时
它具有非常独特的存储信息的方式
它不是为我们提供本地存储
就像会连接到那个实例那样
而是使用这种网络存储
所以我们可以使用aurora标准或aurora
I o优化
如果我们有非常IO密集型的工作负载
比如同时进行大量的读写
那么我们可能会选择IO优化
因为这样会给我们更好的性能
但对于我们这里的目的，我只会选择Aurora标准
所以我在这里滚动
它将给我不同的实例配置选项
所以我想要运行Aurora的硬件是什么
我可以选择内存优化类
或者突发稳定类
或者我可以运行aurora无服务器
你现在注意到这里有个小小的v2
在我与aws工作的这些年里，我看到的一件事情是
每当他们特意提到v2
任何类型的服务v2
这意味着他们可能对版本一不太满意
所以他们说，嘿
这不是版本一
那个臭旧的版本一
这是v2
我也会说是的
我同意，我尝试过无服务器v1
但它还没有准备好
所以我没有尝试过无服务器v2
所以我想象它应该更好
但我会选择突发BU
你可以在这里看见我的选项
中，大，中，大t4g t3
我会选择t4g
只是因为我喜欢t4g实例
现在我有了创建aurora复制或读者节点的选项
实际上我将这样做
这将使我们能够实现多可用区部署
如果我们选择 不要创建aurora复制
那么它将是一个单可用区部署
如果我们选择
不要创建aurora复制
它将不会为我们创建复制
但在这种情况下我想创建一个复制
因为我想切换到该复制
所以继续往下滚动
我不需要与之连接
我不需要双栈
那只意味着ipv4和ipv6支持
我真的不关心它放在哪个vpc，默认vpc就可以了
这个子网组允许我们定义我们希望这些实例创建的子网
这些实例将创建在这些子网中
如果你对vpc有特定的要求，并且希望在该vpc中的某个子网创建它们，而不在这个子网
那么你可能会设置它
我不在乎
我只是使用我的默认子网
我建议你现在这样做 我需要提供公共访问权限吗
不，我不需要
我不关心任何与安全组相关的事情
我不关心创建rds代理
我不关心安全权威
我不关心创建读复制
我不关心写转发
我不关心读复制
我不关心写转发 如果你记得当我谈论aurora时
我们有能力接受写的读复制
那些将回传到
主实例
这真的很方便
但我将不为我们的目的关闭它
我不设置任何数据库身份验证
我将留下所有这些东西
保持不变
如果我留下所有事情不变
如果我运行一个月，它将花费我五十五美元和二十九美分
但实际上我们将运行它
也许我不知三四分钟
所以这不会花费很多钱
所以我将创建一个数据库
当它正在创建数据库时
我将暂停视频
然后重新启动它
一旦数据库创建过程完成
在这里我将重新启动它，在它完全创建之前
我只想指出一个有点有趣的事情
当它正在创建新实例时
它将总是将新实例创建为只读实例
然后一旦这个景观设置完成
这个区域集群将运行
它将将其中一个提升为写入实例
那就是我们可以读写的实例
而另一个我们只能读
因为我们没有启用本地回写设置
所以说我们的集群已经创建
但它仍在创建实例的过程中
所以再次 我将暂停视频并在完成后重新启动
好的 所以集群已经创建
这可能花了每分钟大约15分钟
所以让我们看一下我们可以做些什么
如果我们在这里去操作
我们可以暂时停止它
我们可以添加一个只读实例
我们可以创建一个只读实例 如果你看下面
我们有写入实例 那就是我们的主要实例
然后我们有一个只读实例在这里
我们也可以添加一个aws区域
如果我们正在扩展到另一个区域
想在另一个区域创建一个只读副本
我们可以选择那里
然后我们可以选择要部署在那里的任何硬件
并且所有相同的功能和问题在那里
如果我们想要启用只读副本回写
我们也可以这样做
我们也可以创建这个系统的副本
我们可以创建一个快照
我们可以恢复到某个时间点
如果我们启用了点时间恢复
那么它将连续备份这个东西
我们可以说 恢复到5月21日的21点
这一天 那是一个非常有用的功能
特别是如果你想要创建一个用于生产系统的版本
也许质量保证或测试
或者类似的东西
或者你可以创建一个克隆，并在你的质量保证系统中使用它
所以我想向你展示一些有趣的东西
如果我们去掉这个
点击这个
我们将在这里下去
我们将看到它创建的端点
我们可以创建我们自己的自定义端点
但它自动创建了一些端点在这里
如果你仔细看，你可以看到
有所不同
这里有一支箭在阅读者上
我们没有骑手那个箭头
所以如果我们在分发这些端点
如果我们在设置我们的分析工具
也许我们的报告编写工具
我们可能想给我们的报告作者提供一些帮助。
无论他们是谁，这位读者都旨在与他们建立联系
这样他们的查询将在这个读者实例上进行，而不是在写入实例上进行
因为可能这个写实例与我们的OLAP软件或OLTP软件相关联
随便 所以我们可能希望尽量将更多的读取活动卸载到这个其他读取器
所以这里我们有两个实例
让我们模拟一个失败
假设我们想要失败这个
这将要做的是
这将会提升这个读取器成为一个写实例
并且将会降级这个一个为读取实例
它将会提升这个读取器成为一个写实例，并且将会降级这个一个为读取实例
所以我们可以在这里点击切换
我们是否想切换我的集群
是的 我们想
我们将看到这有多快
所以我将尝试在这里保持实时
尝试刷新
通常屏幕更新得很快
所以我们在这里要看的是
这将改变
所以它说，在这个过程中失败转换到一个只读实例
这将转换为一个写实例
所以它正在失败转换过程中，我们看看
所以我们又回到了可用状态
而我们之前的只读实例现在成为了写实例
而我们的写实例现在成为了只读实例
但是看这里
端点保持不变
这就是aurora的酷之处
这是我们可以将这些端点分发给任何需要它们的应用程序
即使我们反复失败
我们不需要改变任何事
因为端点名称保持不变
这就是能够反复失败的能力
总的来说这个过程花了
我不知道 也许三十秒左右
现在我们可以拆除这个
我们不再需要它
所以我们要上去
我们将去我们的读者实例这里，我们将要删除
删除我
它将会删除
我们将去我们的作者实例这里
我们将去删除
删除我
当这两个被删除
我们应该有能力
是的，在那里 是的，我们有能力删除这个
现在你无法删除这个集群
除非首先删除底层实例
所以我们要删除它
它会问我们
我们是否想要创建最终快照
不 那里没任何数据
我们不想保留任何自动备份
我承认在删除后
所有那些东西都将消失
好的 那没问题 删除我
删除数据库集群
几分钟后这将消失
将被删除
一切都会恢复正常
我们不会再产生任何费用
这很重要
如果你自己在尝试
确保删除东西
除非你想以某种方式结束
我不知道什么 这个特定的集群可能是每月53美元
所以正如它现在
我想你可能产生了大约25美分的使用费用 或者类似这样 所以这是在创建rds aurora集群并失败
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/031_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p31 8. Validation Database Options on AWS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我们来探讨一些这些问题
首先在dynamodb中提出问题
考虑到一个本地二级索引lsi和一个全局二级索引gsi
以下哪些是正确的
我们被要求选择
我们可以用两种不同的方式进行
这里有五个不同的选择
我们可以找到假的
并且这将给我们留下两个真相
或者我们可以找到两个真相
所以我们开始了
Li只能在表格创建时创建
在我们这里我们有几个选项
我们这里还有一个选项
这是说 Gis只能在表格创建时创建
然后我们这里有这个
一旦创建，无论是Gis还是Lsi都不能被删除
所以这些与我们创建这些二级索引有关
我记得gsi和li有不同的规则
我真的不认为这三个都可以是真的
事实上 它们不能为真，因为这将超过两个
让我们看看是否能简化或排除一些选项
gsi只能在表创建时创建
嗯 我知道这不是真的
因为gis本质上是原始表的副本
我们可以去掉那个副本
AWS不会强迫我们在复制某物后保留它
强迫我们永远保留它
所以这不是一个真实的陈述
GIS只能在创建表时创建
它们实际上可以在任何时间创建
我们可以在任何时间删除它们
所以LI只能在创建表时创建
是的 但是，这句话的意思是这也是一个错误的陈述
所以我们可以选择那个
现在我们只剩下两个了
我们只需要再找一个真或找一个假
Gsi可以包括原始表中子集的字段
Li也可以包括原始表中子集的字段
正如我们刚刚讨论的
我们可以随时创建和删除gis
因为它们本质上是原始表的副本
然后我们可以选择那些字段的子集
所以这就是我的真命题
所以这些都是我将用于回答这个问题的
下一个问题
你的公司正在尝试对客户的安全数据进行分析
以建立事件与安全日志条目之间的关联
哪种aws数据库最适合这项任务
这将是一个常见的问题
你可能会得到aws
aws希望确保你了解如何使用或如何最好地使用某些数据库
有很多数据库
它们中的一些现在具有非常特定的用途
这并不是说你不能用红移
例如 基本上任何东西
你可以用红移在线事务处理系统
你可能不会太高兴
成本 第二 性能因为它不是优化为在线事务处理数据库
它是优化为在线分析处理数据库
所以红移我们能做到
也许我们可以继续前进 dynamo db 好的
Dynamo db 基本上可以做任何事情
你可以让它做任何事情
但这是列表中最好的事情吗
让我们继续前进 document db 好的
Document db 有点像 dynamodb
它是 mongodb 的克隆
我猜你又可以说
你可能能做到
但是让我们继续前进，海王星啊，海王星
我记得海王星
因为海王星是那种图数据库
如果我记得图数据库是什么
它们确实擅长在不同实体之间建立关联，并以这种形式存储数据
所以如果我在这里选择
当然，我有没有上述选项
但我认为海王星可能是最好的选择，并以一个AWS问答者的方式思考
他们包括这个
这里的关联词
事件之间的相关性
这正是海王星应该做的事情
所以 那就是我最终的答案
下一个问题，你是在创建一个Dynamo DB表来存储
每天将组成数百万个记录的物联网数据
你将只会使用一个表格并且不会进行归档
以下哪一项将是您作为分区键的最佳选择
为了获得最佳性能，使用 DynamoDB 表的方式
尽量减少热分区问题的可能性
那么热分区问题是什么
如果我记得回到过去，Dynamo是如何被构建的
Dynamo db的结构
它是由
它是 它由分区组成
每当一个记录进来
进来被写入
我们取那个分区键
它会得到一个哈希
然后我们把那个记录放在包含
那些哈希范围的分区
它将存储在数据库中
让我们从这里开始
当前日期
这不是一个很好的方法来存储大量的记录
如果你记得我在视频中使用过
使用当前日期可能不是一个好选择
因为我们将收到数百万个记录
所有的都将哈希成相同的
所以它们都将试图落在同一个分区
这不是一个好日子
嗯 这可能不是一个更好的选择
因为只可能有七个不同的值
当你设计一个DynamoDB表时
你要确保你的分区键有尽可能多的不同值
不同的值越少
潜在的热分区问题或性能问题就越多
当前月份 嗯
这可能是我们能选择的最糟糕的选择 因为整个月份
每天数百万个记录都将撞击那个单一的分区
所以它对我们来说就不起作用
物联网设备名称
这是可以的
也许我们没有被告知我们有多少物联网设备
也许我们只有五个
也许我们有五百或五千
我们不知道 让我们看看
我还是喜欢这里
一个随机生成的32位字符串
对我来说这似乎是最好的选择
如果我们谈论大量的数据
那么作为分区键的随机字符串
可能会给我们 最好的能力来均匀分布这些记录到不同的分区
这就是我最终的答案了
顺便说一句
我们被告知您将只使用一个表，并且不会进行归档
实际上，AWS有最佳实践文档
Dynamo DB
这描述了如何处理时间序列数据更高效的方法
这可能基于当前日期
或者类似
但我特别想把这一点放在这里
以排除这一选项
基本上这意味着
或者至少他们在
最佳实践中描述的那样，每天创建一个新的表
要么归档旧数据
或者可能复制旧表的副本
从而为每一天创建一个表
这可以减少热点分区问题的可能性
我希望这对您是有帮助的 感谢您观看
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/032_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p32 1. Skill Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，美丽的学习者，到另一个技能和男孩
我们有一个技能为你
到目前为止，我们已经学习了一些安全措施，我们可以采取措施来保护我们的数据
我们已经学习了一些我们可以存放文件的地方
我们还学习了aws提供的各种数据库服务
这所有的一切都引导我们到达这个阶段
现在 我们准备好终于卷起袖子，动手
与一些真实数据
在本技能中 我们将从
可能是我最喜欢的aws服务之一，用于与数据捣乱
它是aws Glue没有进一步的废话 让我们开始
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/033_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p33 2. Meet AWS Glue.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 在这个技能中，我们将讨论一个称为ETL的工具，它代表提取
转换 或有时转换
我看到过两种方式
我将只使用转换然后加载
本质上这意味着
我们将从某个地方提取数据
然后以某种方式转换它
然后将其加载到其他目的地
我们可以有一个数据源
可能是一个文件
一个数据库
这并不重要
有很多不同的数据源
然后我们会将其加载到一个目标中，这种情况更常见
我的意思是 有时目标可以是与源相同的
相同的格式
相同的数据存储类型
那种事情 但大多数时候，情况并非如此
因此，将数据从这里加载到那里可能非常简单
也许我们只需要原样发送
但有时我们可能需要去其他小表查找一些值
也许我们需要将其转换为某种形式，或者运行某种函数来转换其值
例如 也许我们需要将摄氏度转换为华氏度
例如 也许数据并不干净，里面有一些错误
我们希望清理它们
那是扫帚
顺便说一下，那里有运动线条
然后，在其他情况下，也许数据有一些敏感信息
我们需要做一些叫做模糊处理的事情
数据意味着隐藏或伪装它
这样它就不会落入一些坏人手中
或者当我们不需要揭示它时不会意外地泄露
所以aws S的工具用于进行ETL类型的操作，有一个叫做AWS Glue的工具
AWS Glue是一个完全管理的ETL平台，完全管理的意思是
这意味着你没有需要维护或购买的东西
或者任何类似的东西，你按服务付费
服务的一些部分是免费的
它们属于免费层
服务的一些部分你需要付费
但这不像购买ETL工具
然后在一些服务器上安装它
事实上 A认为AWS Glue现在是无服务器
我在这里使用了小气的引号无服务器
因为内心深处
确实有服务器 但我们不关心它们
我们不必管理它们
我们不必升级它们或修补它们
我们只需使用该服务
AWS Glue有许多来源和目的地
大多数主要的AWS数据存储都作为来源和目的地可用
还有许多完全来自AWS的外部来源
如Salesforce和SAP HANA
这些连接器已经存在
这些是非常实用的情况
或者这些是非常实用的功能
如果我们发现自己需要将某些数据源连接起来
现在，那是外部的AWS
AWS Glue是一个套件产品
它包括您可能期望的ETL功能或核心ETL
但它也有数据探索工具
它有数据质量测量和修复
它有数据筛选
我们可以筛选数据
我们可以检测个人身份信息
例如 当数据流经时
我们可以要么过滤掉
或者我们可以用其他值替换它
我认为最有用的功能之一是称为数据目录的功能
如果您为大型组织进行数据分析和大数据类型的工作
您可能希望花一些时间构建一些称为数据目录的东西
因为这将有助于您跟踪所有您不同的数据元素
在大数据分析中，没有什么比这更令人沮丧的
两个人向您提交报告
他们认为他们从同一个地方获取数据
但报告数字完全不同
原因是
因为他们实际上并没有从同一个地方获取数据
也许有一些缺失的数据或过滤数据
或者类似的东西
数据目录有助于记录我们的数据元素
它有助于提供有关数据血统的一些信息
数据经历了哪些转换
直到它准备好用于编写报告
数据目录确实很方便
我们在这个技能中不会看到它
但我们将在下一个技能中探讨它
在这个技能中 我们将非常关注爬行
步行跑步方法
我们从非常简单的地方开始
然后 通过添加变换和其他东西，我们会让事情变得有趣 所以我们开始吧
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/034_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p34 3. Setting up Security Roles.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我们用aws glue来玩点有趣的
所以我们在这里我们的控制台
我们用
我是用户这里总结一下
如果我去s三
我可以通过导航到这里
或者我在这里输入
或者我的书签在这里
然后我们去s三
上次我们创建了一个特殊的桶,我们在我们的桶里放了一些数据
我们有拉斯维加斯空气质量数据
然后我们也有我们的松鼠人口普查数据
这是我们要玩的两个aws glue数据集
为了到达那里
我们可以点击这里
最初glue看起来有点令人困惑
这里有很多部分和部件
在这个控制台上,这里有很多条目
但你用得越多
你会意识到很多东西实际上是不同的方式
来达到一个目标
例如,在上面
如果我们谈论数据目录表
那么 我们也可以使用这个
所以这里的很多控制台都是重复的
然后让事情变得更糟
他们有旧的页面
所以如果你很久以前就开始使用aws glue
那么这些可能对你有意义
但我建议不要关注它们
因为它们真的不需要
好的 所以我们可以点击
开始 这带我们到这个页面
在我们设置glue时,我们要做的第一件事是
我们必须设置一些用户和角色
所以我们要删除这个
在这里我们选择角色 选择用户
我现在不会选择任何这些东西 因为我们是唯一的用户
将要使用这个
我们有行政访问权限
所以我们不需要任何特殊的访问权限
你可能要做的是,你可能要为
在你的组织中的数据工程师设置一个角色
你可以在这里添加那个角色
我们也可以选择用户
我们可以选择特定的IAM用户来使用这个工具
但我们现在不必这样做
所以我要点击下一步
如果你感到困惑
它会在这里给你一些指导
你可以总是阅读这些
现在它会询问我们想要授予访问权限的S3位置
我们是否想要授予访问权限
所以我要做的是添加一个特定的访问权限
因为实际上直接授予这个服务对所有AWS S3桶的全权访问并不很好
我只会授予我们使用的特定桶的访问权限
所以我要选择S3位置
我将选择我们要使用的这个桶
这是我们复制数据的那个桶，确认
然后到这里的数据访问权限
我们是想要只读权限还是读写权限
我将选择读写权限
因为我们的一些练习中
我们要做的事情是
我们将读取这些数据，然后将它们写入一个新的数据集
现在我可以点击下一步
现在它正在问我们选择默认的aws glue服务角色
我将在这里保留默认设置
这就是推荐的路径
这将创建一个标准的glue服务角色
我们也可以以自定义的方式做
如果你的安全人员有非常具体的规则和规定
你需要遵守的
他们可以选择
我们可以选择这个 然后我们可以设置一个角色
你可以通过点击这里的这个小链接来访问文档
它将给你提供关于你的安全人员可能希望如何设置该角色的详细信息
这是细节
我将选择创建标准角色
点击下一步
我们快完成了
我们成功到达确认屏幕
一切都看起来不错
所以我要点击应用更改
就是这样 我们已经设置好了aws glue的安全部分
现在我们准备好继续做一些其他事情 这就是我们将在下一视频中要做的
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/035_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p35 4. Our First ETL Job.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们现在回到了我们的aws glue控制台
我们已经设置了我们需要使用的glue的iam角色
现在我们准备好去创建我们的第一个etl作业
所以我可以点击这里etl作业
它将带我到这一页
你可以看到我没有etl作业
但这很快就会改变
我们可以有三种不同的方式来设置etl作业
我们可以使用一个可视化etl工具
这是一个拖放类型的工具
我们也可以使用jupyter笔记本实例来设置我们的作业
我们也可以直接脚本
对于我们的第一个作业
实际上让我给你展示这里的示例作业
如果你想要一个示例
我们可以创建我们的示例
这将创建一个可视化etl作业
这是一个多个来源的示例
这个使用spark笔记本使用pandas
如果你熟悉pandas
你可以用它来探索你的数据
然后我们也可以使用sql
对于我们的第一个作业
我们将使用可视化etl编辑器
可视化etl编辑器允许我们拖放我们的资源
这里有我们所有可以使用的来源
你可以看到有很多
其中有很多不是aws数据源
例如 sap hana
这是salesforce的一个好工具
我们可以使用google bigquery作为来源
snowflake
dynamo db等等
我将选择amazon s3
因为我们将从那里获取我们的数据
我们将运行这个在我们的open air quality数据上
所以我选择它
这就是我们的来源
现在我有了我的来源
我在这里还要做一件事
因为我如果不改变这个会抱怨我
我将这个从未命名作业改为open a queue
并命名为版本一
我们可以在这里到作业详情
我们可以看到大量的细节
它将使用glue 4.0
我们将使用python
并且每当我们定义这份工作时
实际上，我们需要有一种计算资源来为我们完成所有这些工作
在这里我们有我们的选择
消失的x
两个A，四个X
你可以看到它有不同量的cpu和ram
但是一个特定的x将会比合适还要多
对于我们现在处理的数据量
我们可以定义工人的数量
这些都是一些高级主题
我们不会深入探讨这些
因为我们处理的数据量相对较小
所以它不需要任何类型的自定义
所以这里有一个实用的小贴士
哦 我们还没有做任何事情
但我们会回到这个脚本
这里的脚本标签
因为如果你记得在主页上
我们有使用脚本或脚本创建工作的选项
并且那会自动生成脚本
当我们填写我们的变换、目的地和来源时
在这里我们有我们的aws
或者这里有我们的s3存储桶
我将此命名为open aq s3，这里是s3的位置
这是我想要的，我想要浏览并使用这个
我将选择这里的键
因为当然，在那个键下面是我们所要访问的所有年份
我将选择它并点击
选择现在
如果我向下滚动
我现在需要定义一个数据格式
这里有一个有趣的东西
让我回到s三这里，我们可以深入到我的数据这里
如果我深入到数据
你会看到它由不同的文件组成
所以这些都是那天从这个气象站或空气质量站收集的数据
但你会看到它确实以csv格式存在
但它是压缩的
所以我们在这里选择csv
因为我们的数据格式是这样的
但是下一秒
你将会看到那里
我们让我在这里折叠这些东西
aws正在做的是它正在读取那些数据
它正在尝试加载所有数据的预览
我不知道我能否在这里看到
让我缩小一点
我知道这在你的屏幕上可能很小
尤其是如果你在平板电脑或类似的设备上观看
但是它已经出去了，读取了所有这些文件中的所有数据
或者至少返回了足够的样本，我们可以看到这些数据的样子
它由位置传感器组成
位置日期
时间
纬度 经度
这里是参数
这里是正在读取的项目
这里是该项目的单位
这里是值
所以它已经出去了
即使它被gzip压缩
它足够聪明说
嘿 这是被gzip压缩的 我要解压它
我将其视为csv
我们有分隔符的选择
目前逗号工作得很好
如果我们这里有任何转义字符
我们也可以使用它
如果我们有任何特殊的双引号字符或引号字符
我们也可以选择这里
我们还检查了源的第一行包含列标题
事实上它确实使用了这些信息
aws glue根据我们的数据推断了模式
如果我们选择这里
我们可以看到位置id传感器
id位置日期时间
但它将这些所有东西都推断为字符串
这可能并不是实际情况
或者我认为这是有效的
所有这些东西都可以绝对为字符串
如果我们有机会
我们可能希望将其做得更好
因为我们可能希望将此纬度和经度
转换为浮点数或双精度数之类的
我们可能希望将日期时间转换为时间戳数据类型
以便将来更有意义
但是现在我们就这样
可以
现在我们已经定义了源
现在我们可以找些别的
现在我将使这个例子非常简单
我们只有有源和目标
我们不会有任何转换
但是我们将在这个技能中做一些转换
但是现在我将选择s3
因为我将要在这个etl流程中做
我将加载数据
然后我们将其输出到另一个位置
我们将使用相同的
我们将将其放入相同的三个桶中
但我将在这里做一些更改
我将在这里点击这个项
这是我们的目的地
您可以在这里看到
让我向下滚动
您可以看到这些框中有这些小连接器
现在 如果我们这里有多个框
我们可以在这些连接器之间拖放并定义操作顺序
但现在我们将将我们的数据输出到我们的 s3 桶中
我将点击它
我将其命名为
让我们看看打开 s3
我将以 parquet 格式存储此数据
parquet
好的 我不知道您是否
当我还是个孩子的时候 有一则广告
有一个说话的黄油盒
或者黄油盒或什么
人们会叫它黄油然后说没有公园
盖伊 那只是一边
也许我不记得确切的
但如果你知道
你知道所以现在
我们不需要定义任何节点
父母因为我们的来源是源
我们可以选择一个数据格式
您可以看到我们可以选择许多不同的数据格式
Avro orc parquet
Delta lake iceberg
我将选择 parquet
我还可以选择压缩类型
我可以选择不压缩
或者我将使用 snappy
确定那没问题
我可以定义我想要存储数据的位置
所以我将要做
我将在这里浏览我的 s3 桶
我将选择它
它将填充那个
但我将给新数据一个前缀
所以我将说 open a q parquet
所以这是 aws glue 将发送我们的数据的目的地
所以这不会改变我们的数据，只会加载它
以CSV格式加载它
然后它会转回来并以parquet格式写入
写入到这个前缀
或者这个所谓的子目录
但你知道我对在S3上称子目录的感觉
S3上的子目录
它们并不是真正的子目录
我们不会关心这里的任何内容
我认为我对所有这些都满意，它将再次使用相同的模式
我们没有真正改变我们的模式
我们只是基本假设所有事情都是字符串
这将会正常工作
所以它说嘿
我们没有保存这个
当然我会在那里保存它
现在已经保存了
我提到 我会重新审视这里的脚本选项卡
这就是脚本看起来的样子
所以你可以看到
这基本上是一种代码
是我们在这里做的东西的版本
尽管看起来有点复杂
有很多加载和准备
那是这部分的一部分
但实际上我们在这里
我们只是基本上打开了它然后保存它
这就是我们在这里做的事情
所以我认为我们准备好运行这个
那么我们点击运行
它做了什么
成功启动了我们的工作
我们可以去工作循环
没有在那里运行
这就是我想要运行的
所以这里它在运行，它在做的事情是
它启动了一个虚拟机
它正在运行我们使用视觉编辑器创建的脚本
可能在几秒钟后它就完成了
希望我们能得到一个成功的返回代码，你可以在这里看到
它有一堆统计数据
工人数量大约
有多少个并行任务将尝试运行
这是一个非常小的数据集
如果你处理的是百万级的行数
可能会有一些时间
你可能需要花时间来优化这些事情
稍后 在本课程中
我们将花一些时间来讨论我们可以对包含百万条记录的数据集做什么
因为处理那些数据
正如你所知道的
如果你曾经处理过那些
或者那是一个完全不同于处理一堆东西
那些仅仅在一个csv文件中
所以让我们在这里刷新一下
它正在继续运行
我只是在这里暂停视频然后重新启动它
当我们完成时
好的
我刚暂停视频不久 它完成了吗
花了大约一分钟十六秒
它成功了
我们可以看到一些输入
连续日志
这个日志中有很多垃圾
你真的不需要看它
我建议
虽然 如果你有问题
那么这个是一个很好的地方开始尝试找出发生了什么
当你的工作没有成功时
这确实会发生
可能是数据质量问题
稍后在技能中 我们将遇到一个数据质量问题
与我们的数据的一部分
现在我们可以转到我们的s3存储桶
我将回到工程练习场
那是顶级
这里是子子文件夹
不是子文件夹
这是我指定的前缀
如果我钻入那里
看 所以我们有所有这些parquet文件
你可以看到有很多
基本上它做了什么
是将数据导出到parquet格式
然后我们可以使用任何我们想要的东西
它可以读取parquet文件来读取这个数据
所以这是用aws glue的hello world版本
在下一个视频中 我们将把事情弄得复杂一点，通过添加一些转换
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/036_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p36 5. First Transformations.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们刚刚完成了对aws glue的第一次接入，那就是一个基本的工作，那就是从一个地方获取数据，然后在另一个地方以不同格式保存
所以我们要提高一点难度，做一些稍微复杂一点的事情
我们要开始引入转换
我还是会使用可视化ETL编辑器
我要在这里全屏
我要提前道歉
因为界面非常拥挤
UI这里会非常拥挤
我要做很多调整来展示给你看
但我认为你仍然能够跟上
我要给它起一个名字
打开qv2，就像我们定义的第一个工作一样
我们需要指定数据的来源或数据来源
我要使用相同的S3存储桶
哦不，再来一次
现在我可以选择拉斯维加斯
或者我可以选择我们创建的版本
这是同样的数据，只是以不同格式存储
我们没有做任何转换或任何事情
但我会选择我们的原始数据
我要说这是以CSV格式存储的
几秒钟后我们会得到一些样本行
所以这里我们开始
我现在要做的是改变一些数据类型
当glue尝试推断这些数据类型时
它可能会使用最宽松的
最宽容的
通常是字符串
但这些东西可能不需要字符串
作为事实
这可能会更好，如果他们不是字符串
例如 日期时间，这是一个日期时间戳
这可能会更好地存储为数据库中的此时间戳 或者我们将要发送此纬度和经度
这可能是一个十进制
所以我也会做其他事情
我将此日期时间戳拆分为单独的字段
因为我认为这可能对以后的分析有用
我们可以看到空气读数按星期几或按小时如何变化
在我们尝试在数据上进行一些分析时
最好现在就做，而不是在查询时间或我们的前端分析工具中
当我们有能力时
所以这就是我要做的，所以再次做
在这里重新排列以便于操作
我将添加一个节点
然后我将到这里的转换
这里有各种各样的转换
我们可以去掉重复项
删除字段 删除无字段
我们可以选择字段
我们可以做联合
我们可以查找
我们可以处理拒绝
提取器
我们现在可以操纵时间戳
如果你在这里找不到你想要做的事情
总有这个自定义转换
我们可以写自己的代码来做我们想做的差不多任何事情
我们也可以从数据酿造中选择数据准备食谱
我们将在不同的技能中涵盖数据酿造
但现在我想做的是更改我的方案
所以我要选择这里的选项
并且它掉落了这个小的转换框
现在，它已经加载了我们的 schema，或者至少是我们的起点
在这里我们有日期时间
我将其更改为时间戳
现在
你会注意到的一件事情 哦豁
哦天啊
看看我在说什么
好的，就这样，好吧
所以你会注意到的一件事是
我将此更改为字符串这里
字符串，好的，你可以看到此字符串包括我们的时区
它是一个ISO格式的字符串，包括时区
由于我们
将此更改为时间戳
时区被转换
至少是在渲染这里
它被更改为世界协调时间
现在这样是可以的
如果时区特别重要
如果我们需要
如果必要 我们可以推断出我们可能甚至可以使用纬度和经度在这里稍后
在未来确定这个原始测量是在什么时区
但这只是演示我们可以做的事情
我也会改变这些纬度和经度
我可以更改它们为
我不知道 他用十进制做什么
我想我会有一些截断
让我们看看 至少它在这里渲染的方式
是的 它截断了两位小数
这没关系
这只是在这里展示可能的事情
所以我已经改变了我的方案
所以我在这里去我的输出方案
我能看到我的日期时间戳
或者我的日期时间现在作为一个时间戳数据类型
和我的拉丁草坪是十进制的
所以这一步在转换中会为我们做这件事
现在我们可以做的是选择另一个转换
我将要做的是提取一些这些值出这个日期时间字段
为了做到这一点
我将使用格式时间戳转换
我们将在系列中做
因为我们有两个列想要填充所以这里它问我们
我们的来源列是什么
我们想要用哪个时间戳列
我们想要用日期
我们想要提取的模式是什么
或者我们想要怎么显示这些信息
所以我要使用 ampersand a
然后它会给我们显示一周中的哪一天
哦不
好的，我们继续
好的 我将这个星期称为
周几，或者更确切地说，周几
因为在所有本地化中，不是每周的每一天都被认为是工作日
那么，我们来谈谈周几
我们将把它留作一个字符串
这是完全可以的
我将滚动到这里，并且，我们就这样，我们就这样
我们有周几
星期六 星期日
星期一 等等
好吧，下一个
我想提取出小时
为了做到这一点 我将进行另一个时间戳格式化
你可以看到它只是添加了这些操作
在这里可能有些情况你可能想并行处理
但我不打算那样做
这算是另一种复杂性，我现在不需要
所以我选择时间
日期时间字段
在这里我们只使用hh格式
我们将其命名为小时
如果我们滚动到这里
好了 我们有小时和星期几
这些将是我们输出的新字段
所以我到目前为止都很满意
所以这是我们的成果
我们有数据源正在读取
我们将更改模式
我们将格式化时间戳
这将提取出我们的星期几
我们可以重命名这里
提取
星期几
然后我们可以到这里
我们可以实际上重命名这里
使其变得更容易理解提取
小时
好了 好的
现在我们对我们的转换和提取都很满意
哦我的天啊
这很令人沮丧
你知道一些这些gui
当你开始使用它们时
当你刚开始使用它们时
你会说嘿 这很酷
我可以很快地做我需要的事情
然后过了一段时间，这对每个人来说都会发生
过了一段时间，你就会对gui感到沮丧
然后你最终会使用某种脚本
如果我们点击脚本选项卡
这是根据此处视觉编辑器中我们做的生成的脚本
这是实际执行的代码
当我们进行转换时
所以让我们定义一个输出
我们必须定义一个目标
我们将其发送到相同的s3存储桶
哦对了
好的
来吧，好的
所以我们将其命名为s3输出
parquet格式，没问题，snappy压缩，没问题
我们将浏览
实际上 我只会选中这里的这个桶
因为我们将手动添加
添加一个前缀在这里
我们将其命名为open a q las vegas
V2，所以带有尾斜杠，我们不会做其他任何事情
所有这些其他东西都很好
如果我们滚动回出
缩小一些这个
你可以看到我们有小绿勾和所有这些小方框
这意味着它们所有通过了验证
我们现在可以首先保存这个工作
这样我们不会失去我们所有的努力
现在我们可以运行这个工作
我将点击运行
然后我们到这里的运行
你可以看到它刚刚启动了我们的工作
希望在几分钟内我们将会有一个成功完成的工作
我现在暂停视频并恢复它
当一切都恢复正常时
我们的工作已完成
它花了大约一分钟十秒
我没有看到任何错误
但我们应该检查输出那里
可能最容易的方法
没有引入任何更多的工具或新工具，我们还没有覆盖
只是启动另一个视觉etl工作并使用s3连接器这里
并拉取它并在数据预览这里看看
我将浏览这里
钻到我的桶
这是我们保存在那里的数据
open aq las vegas
V2 选择那个
我们以parquet格式保存，给它几秒钟
让我们看看有什么
让我展开它
从这里开始
那好吧
哦好吧 好的
所以我们要等到它加载出来
我们走 好的 所以如果我们去输出模式
这里是它的模式
它有来自那个数据的红色
是的 我们有时间戳是数据类型
时间戳 十进制
星期和小时
如果我们在这里预览
滚动到这里并查看星期日十二点
这正是我们想要的数据
数据的方式 数据存储的方式
我们读取数据的方式与数据预览完全一致
这正是我们想要的输出方式 因此，我们的转换似乎运行得很好
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/037_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p37 6. Using Notebooks.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们已经完成了我们glue etl工作的hello world版本
我们基本上只是将数据从一个地方移动到另一个地方
然后我们通过在这里添加一些转换使事情变得有趣一些
在幕后 当我们拖放并做所有事情时
aws glue在这里生成了一些代码
他们称之为脚本
我们可以下载这个脚本
我们还可以使用版本控制来管理这个脚本
但总的来说
如果你在使用视觉编辑器工作
你将在这里工作
我们还有一个选项
你记得
当我们能够添加我们的来源时
我们可以在那里看到一些预览数据
但我们的能力有点有限
我们不能对数据做太多
所以我们有另一种更好的探索数据的方式
那就是通过被称为笔记本的东西
所以这是一个jupiter笔记本的实现
所以让我开始这里
然后我会稍微谈谈jupiter笔记本
所以我要在这里重新开始
我们只是使用这个
我是角色
所以那正在启动
jupiter笔记本
如果你以前没听说过它
它是一个交互式编程平台
它在数据分析、机器学习等领域非常流行
这类东西 它允许你编写代码和进行文档编写
然后你可以进行可视化
这些都是不同种类的库
你可以下载和安装它们，以不同的方式查看数据
所以，如果你听说过它，那是很有可能的
如果你没听说过，也没什么大不了的
在考试中并没有详细覆盖
实际上 我记不起有任何关于jupyter笔记本的问题
因为它不是aws的产品
所以它只是我们可以使用的工具，用来探索我们的数据
你也可以通过jupyter笔记本做
你可以创建工作
你可以生成脚本
这里发生了什么
你可以看到有一些事情
所以我们做的
我们只是打开了这个默认笔记本
它会默认加载
你可以看到一些代码
我们有一些文档
有一些解释说明这段代码在做什么
在幕后
aws glue正在做的是编译这段代码
这将生成一个与使用视觉编辑器创建的完全相同的脚本
但这次我们使用的是笔记本
所以我不会为这个创建特定的作业
我将向你展示我们如何
与一些数据进行交互
我将点击这里
按Ctrl A来选中所有内容
然后点击删除，以便
我可以在这里重新开始
如果你曾经使用过jupyter笔记本
那么你可能会发现这个
这种界面可能看起来有点熟悉
不同实现的jupyter笔记本，如collab看起来会有所不同
但你在这里要找的是，当这个小圆点变灰时
这意味着内核
底层处理器正在处理
很多时候你会在这里看到一个星号
这意味着它正在运行
这不意味着它冻结了或什么的
所以不要担心
如果你 它看起来像是冻结了或没有做任何事情
那就是它在工作
所以首先，我们将导入一些库
这些将允许我们与我们的数据进行交互
代码提示功能可用
这非常有用
它是一个使用人工智能来尝试生成代码的工具
过去，我能够像这样
放入一个注释并说，嘿
加载这个数据集并获取最小值和最大值之类的
只是一个注释
然后代码提示会解析它
并生成代码来执行那个操作
它并不总是百分之百正确
但在它起作用时非常有用
有时它不起作用
现在我们开始 我们基本上在这里实例化了我们的会话
所以我们在后台有一个正在运行的进程
它将允许我们与我们的数据进行交互
首先，我们将做的事情是
我们将加载我们的数据
我们将说一个数据框
Df spark read
让我们看看 我们将加载一些那个parquet数据
然后我们必须提供路径
这将是我们的s三路径
如果我们回到我们的s三桶
嗯 让我们尝试加载这个一个open aq las vegas
这里有一个小贴士给你
如果你不想输入所有这个东西
我们可以进入一个单独的对象
我们可以点击这个小按钮这里说复制那个uri
然后我们在这里粘贴那里，好的
所以我要剪掉这里最后的部分
因为我想所有的递归在这个特定的键或文件夹下
然后我的朋友关闭
所以这将做是把数据加载到一个数据框
这是一个spark数据框
它与pandas数据框有点不同
它们用于不同的目的
但我将向你展示如何将这个spark数据框
如果你希望转换成一个pandas数据框
语法有点不同怎么看特定的数据框
例如 如果我们想看模式
我们可以做df print schema
这里有我们的location id传感器id
所有的东西在这里
包括我们的新字段我们在上次转换中添加的
所以我们有timestamp decimal
你可以看到现在我们如果我们想看一些记录在那里
我们可以做df show
最终它将返回一些结果在这里
希望如果我们那里，好的，你可以看到location
Id传感器
位置日期
Timestamp 所有信息
所以现在让我们说如果你想转换这个
或者你想用pandas
也许你更熟悉pandas
你可以这样做 所以我们可以导入pandas
我们称之为
pd
然后我们将做pdf equals
你可以看到那个小星星那里你可以看到
哦 它不见了
但你可以看到那个小方块
那个小方块在这里 与这里的这个小圆圈相比是灰色的
这意味着它在思考
所以现在它已经完成了思考
它已经导入了 那里那个库
所以 pdf 等于
df two
这就是你所要做的全部
所以我们正在将这个 spark 数据框转换为 pandas 数据框
现在正在制作副本 它并没有改变我们的原始内容
但它仍然是一个副本
所以只要这个完成
然后我们就可以使用所有数据框的常规操作
以便能够查看此特定数据框
搞定 它正在向我吠叫，因为它不喜欢我的日期时间数据类型
但你可以看到我们有一些值在这里
如果我们去 pdf
我说行三在这里
例如
搞定 所以这里有行三中的所有信息，所以好吧
所以让我们回到 我们的数据框这里和 spark 数据框
并且让我说我正在浏览这个数据
我看到位置 id
并且我看到位置
在这种情况下，位置只是前缀
rt 位置 id
假设我真的不需要看到这一列在这里
我想删除这一列
通常你可能会想，嗯
我可以只做 df. drop
位置并回来，说
嗯 好吧 我有一个小列表，这里没有位置
如果我回去看我的数据框
嘿 等等 这里有位置
嗯 是的 因为，我们做的是
我们就这样把它放在这里
我们没有把它分配给任何东西
我们可以做的是做一些递归
分配在这里
位置
哦，我忘了在上面的等号
因此，如果我们回到这里来展示
我按shift
按回车运行这些行，顺便说一下
如果我们查看这里的显示位置id自变量，好吧
位置不见了，太好了
我们已经去掉了我们的位置列
所以现在我们可以只把这些东西写回到它们来的地方，对吧
实际上不是 我会在下一秒告诉你
我们是gay或get
所以
只是关闭那个引号
在这里，我们做到了
我应该能做到这一点
嗯 不
它会在这里给我报错
它会说嘿 它已经存在
你不能重写它
这已经存在
我们可以做
然而，我们可以复制这个
并且我们可以将这个更改为v3
它会将整个数据集写入我们的s3存储桶
在这个v3文件夹或子目录下
如果我们回到这里，我们的桶
回到我们的顶级这里
正如你看到的，这是v3
在这个特定的转换中，我们只是删除了一个列，并且最终将数据保存在那里
在下一个视频中，我将向你展示
这个jupyter笔记本过程实际上可以救我们的命，并且使事情变得容易一些
与视觉编辑器相比 我们是gay或get
所以 只是关闭那个引号，我们在这里做到了
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/038_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p38 7. Notebooks to the Rescue.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的，对于这个视频，
我想向你展示一个案例，在这个案例中我们有一些数据问题，并且一个笔记本实际上是一个很好的解决这些问题的方式，相对简单，所以，
我们将在这里做一些工作，
我们将与我们的松鼠数据打交道，
我们还没有接触过它，
如果你记得，我们出去进行了松鼠普查， 我们访问了我们的数据集，
我们特别获得了公园数据和松鼠数据，
我可以点击它，
它只是带我到一个dropbox位置，
所以，我想在这里展示的一件事情是，
这个特定的数据有一些奇怪的字符，
看那里，我们走吧， 所以有一些奇怪的字符，
有时候这可能会导致问题，
至少对于这个可视化编辑器，
所以，我在这里要做的是，
我将尝试加载这个数据，
我将设置数据源，
我将选择浏览我的桶，
深入到我的松鼠普查数据，
我将选择这个松鼠数据，
我将滚动并选择csv作为我的格式，
通常应该发生的事情是，
我应该在这里得到一个记录预览，
但是在这种情况下，没有，
我们没有得到它， 我们得到一个大的空气，
当我得到一个大的空气，
这是一个大的空java堆栈转储，
这给了我们很多关于很多东西的信息，
如果你试图通过这查找原因，
试图找出为什么事情是这样的话，
祝你好运，
因为我在做这个的时候，
我在这里， 我真的没有找到任何指向我认为是问题的原因的东西，
所以这不会对我们有用，
不知为何我们无法获取数据的预览，
所以，我们可以回到我们的etl工作流，
我将启动一个笔记本，
创建一个笔记本，
当它开始时，
我们可以回到这个状态，
我认为它很快，
但你可以看到我们有其他东西在这里，
一些疯狂的字符，
我认为发生的事情是，这些特定的字符在渲染或加载到可视化编辑器时有困难，
我认为发生的事情是，
这些特定的字符在渲染或加载到可视化编辑器时有困难
所以我们如果从笔记本的角度来做
那么我们可能会得到不同的结果
所以我要点击这里
控制A并删除所有东西
因为我不需要所有这个东西
我将复制并粘贴我想使用的库
这样我们就可以开始我们的小会话
我不想使用u code whisper
当它正在加载时
我将启动另一个命令
所以我将点击这里
spark.read.options
我将告诉它这个特定文件有标题
然后我指定文件路径
如果我回到这里
点击文件路径或文件本身
我可以复制路径
这将节省我打字的时间
所以我需要这样做
好的 我们正在尝试加载它
我们将看看它会给我们什么
我们走吧，我将展示
让我们看看里面有什么
现在格式有点问题
因为它在这里把我们围了起来
但我们至少能够得到数据的显示
我们能够看到这些条目在这里
我在这里可以做的是
这里有一行
让我们看看df.collect
我认为它是325是行
如果我们回到这里，滚动条
滚动到这里
让我们找到行325
我们走吧，所以我们有一些字符在这里
让我们回到这里
它可能不是325
让我们看看这里有什么
这将拉起
好的 是的 所以我们有一些这些小字符在这里
显然当我在玩它的时候
如果我们使用拉丁一编码导入它
这些将作为ascii字符渲染
如果我们只是导入它
像这样 那么我们使用uf8并且渲染我们有这里的东西
这些奇怪的字符
所以我认为在视觉编辑器中发生的事情是
它似乎在处理这些特殊字符时卡住了
但现在我们可以做的事情是
因为我们已经能够加载它并且它看起来已经被加载
好的 我们可以把它写回去
我们可以去df点right
选项
标题等于true那个csv
然后我们可以给它一个路径在这里
所以我们要做的是
让我们给它一个不同的名字
所以粘贴出那里松鼠数据
然后我们会叫这个v2
然后关闭那个我们的文件在这里似乎已经写回来了
让我们出来这里那里
它是所以它创建了一个文件夹在这里
它放一个文件在下面
所以现在那个导出了
让我们回到这里我们的视觉编辑器
让我们终止我们的笔记本
进入视觉编辑器
尝试查找我们的数据再次
滚动人口普查
我们会选择那个在那里并且去这里格式化为csv
所以你可以看到我们现在能够得到一个预览
让我看看我能不能扩展这个一点
这是一个真的压缩
Ui这里真的混乱
但我们至少可以拉起我们的数据这里
仅仅是加载它到我们的数据框
我们的spark数据框
然后保存它回来作为一个csv
然后那似乎已经解决了
这个视觉查看器在渲染那个预览数据时的任何问题
所以那就是一个例子，你可能不得不使用笔记本来转换你的数据
预转换你的数据
我们将会玩这个data一点
我们将会深入一些这些东西
并且看看如何我们可以净化它
那将会是另一个技能 那就是要去做的
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/039_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p39 8. Validation Getting Started with AWS Glue.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们来看看这些问题
第一个问题是您正在创建新的ETL作业并需要选择数据源
数据存储在多个JSON文件在S3桶中
以下哪个选项将允许最直接的摄取和解析
我们的第一个选项是将数据转换为CSV
然后转换为Parquet格式进行摄取
嗯 我想我们可以这样做
但这似乎有点绕弯子
所以接下来，先将数据加载到aurora serverless中
然后使用aurora连接器
我想我们可以那样做
但让我们继续
这似乎是一种绕远路的方法来做
将数据加载到dynamodb中
然后使用dynamodb连接器
我知道我们可以很容易地将json文件加载到nosql数据库中
并且dynamodb是nosql数据库
所以让我们继续，看看这里还有什么
从s3桶加载数据
指定json路径
嗯 我知道从s3桶加载数据的选项中
我们有csv、parquet、orc
还有其他几种格式
其中一种格式绝对是json
所以我们应该能够直接使用这种格式，从s3桶中获取数据
而不是将其发送到不同格式或平台
所以这就是我最终的答案
下一个问题
我们有一个非常专业的数据源列转换
似乎没有准备好的转换选项
在这种情况下，AWS Glue 可视化 ETL 工具的最佳路径是什么
好的 这里有一个选项
使用自定义代码配置一个自定义转换步骤
嗯 我们可以绝对这样做
让我们继续阅读，看看是否有更好的选项
AWS Glue没有处理非标准转换的能力
嗯 这种说法是错误的
因为我们知道AWS Glue可以做到这一点
如果不能做到这一点，那它将是一个相当糟糕的工具下一个选项
将数据导入关系数据库并执行一个更改命令来更改值
音乐 我认为我们不应该这样做
这听起来像是一种相当糟糕的方法
所以我要消除这个选项
这是最后一个选项
使用aws glue将数据导出到s3桶中
然后使用一个自定义的python脚本进行转换
我想我们可以这样做
但是再次 这似乎是一个绕远的方法来做这件事
我认为这个选项
使用自定义代码转换
这是aws glue可视化etl工具的一部分
我们可以做我们几乎需要做的任何事情
这比尝试导出这个数据
以这种方式操纵它要合理得多
所以这将是我的最终答案
下一个问题
您需要从一个时间戳字段中提取年份的周数
并将其放在一个新字段中
使用可视化etl编辑器
哪种转换最适合这种提取
让我们从最下面开始
分割通常会取一个字段或一个值
并在某个地方分割它
可能是基于字符或字符数量
我们需要两个字符或两个字段
在时间戳中
我们没有年份的周数
所以分割不是我们可以使用的
我们可以使用自定义代码转换
我想我们可以发送时间戳
然后使用一些自定义代码
我们可以输出我们需要的周数
年份的周数
看看是否有更简单的方法
时间戳格式转换
我们在这个技能的视频中使用过
我们从中提取了
星期和小时基于时间戳
所以我们实际上可以使用它
我们可以使用python日期格式化
如果我们使用%u
这将给我们年份的周数
如果周从星期天开始
如果我们在一个国家或本地化中，周的开始日是星期一
我们可以使用w，这将给我们一年的周数
无论我们是使用u还是w
我们可以在这个转换中使用它
然后将该值分配给一个全新的字段
我们在这个视频中做过，过滤转换
现在更改模式转换
好吧，改变我们的模式也不会给我们提供这些信息
所以我们可以排除那个
所以我的最终答案是，我希望这对你有所帮助 我想感谢您观看
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/040_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p40 1. Skill Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，亲爱的学习者，来到另一个技能
在这个技能中 我们将继续探索aws glue
通过我们的数据样本做一些更多的事情
幸运的是，我们的空气质量数据看起来相当扎实
所以我们将只对该数据进行一些汇总
我们将使用另一个但相关的工具，称为数据酿造
是的 aws glue
数据酿造
我没有给这些东西命名
数据酿造也将帮助我们
尝试处理一些我们的松鼠普查数据
我认为这是最好情况的边缘
但是嘿 这是大多数数据集的情况
所以首先，不要同情
尽管我们将尝试构建一个数据目录
使用称为爬行的东西 让我们开始吧
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/041_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p41 2. AWS Glue Data Crawler.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 在我们上一节课中我介绍了aws glue
在那节课中我提到它基本上是一个etl服务
一个管理的etl服务
但它还做了一些其他事情
所以，在上一节课中，我们主要关注了前三个要点
在这节课中，我们将关注最后两个
我们将首先使用它来构建一个数据目录
让我们通过登录aws控制台来开始做这件事
好的 现在我们在aws控制台上
我将全屏显示
因为这将给我们提供更多的工作空间
让我们开始
我们要去aws glue
我在上一节课中提到过
如果你是第一次看这里
这很令人震惊
但你很快就会发现
这里的许多项目都是完整的复制
例如etl工作
这里的etl工作与上面的完全相同
视觉etl笔记本工作运行监控
这些都是相同的东西
不知道为什么他们把它放在不同的地方
这让事情变得复杂
但不要感到不知所措
如果你不想看这些东西
你可以总是把它收起来
当然，我们还有下面的旧页面
这让事情变得更有趣
但我们现在要找的是
我们将创建一个称为爬行的东西
我在上一节课中提到过
数据目录基本上是我们的数据集的集合
或者我们的数据字段，数据库和表
它描述了这些数据的元数据
有一个数据目录是很有用的
aws已经构建了一个名为爬行的工具
我们可以设置它
它可以出去查看数据源
并尝试找出数据源中有什么
它包含什么样的字段和数据
然后尝试推断一些关于它的信息
为了到达那里 我们将点击爬行
我们可以创建几种不同类型的爬行
我们将使用一个非常简单的
我们将让它爬行
我们的s3桶
所以我要选择一个名字
我将点击下一步
它将问我们这里
我们是否已经将我们的数据映射到Glue表
因为我们才刚开始
我们还没有将我们的数据映射到Glue表
我们的数据没有映射到Glue表
我们将说还没有
我们可以在这里添加一些数据源，你可以添加各种类型的数据源
这里有很多种可以选择
Dynamodb文档数据库Delta Lake
冰山 JDBC数据源
但我们将使用S3
我们不会担心网络连接
那是可选步骤
但我们将选择我们的S3桶
所以具体在我们S3桶这里
我将在这里深入挖掘
我将让它查看我们的松鼠数据
这是我们的松鼠普查数据
我现在要选择这里的这个项目
当然我也可以选择这个项目
这是我们整理的一部分数据
或者我们以某种方式进行了转换
但我只想选择原始数据
我将点击
选择，所以我已经选择了
它说我可以爬取子文件夹
如果你有需要这可能是有用的
例如 如果你记得
如果我们在这里打开我们的数据
我们可以去拉斯维加斯
你可以看到我们有多个子文件夹
所以我们可以选择最顶层的子文件夹
并告诉它检查这些子文件夹
但我只选择一个文件
这不重要因为我没有子文件夹
然后我点击
添加一个s三资源
好的 现在我们的数据源准备好了
我们接下来 我们准备好进行下一步
所以现在是我们选择iam角色的时间
这是一个非常重要的角色
因为它必须有能力询问我们所有数据源
如果你第一次这样做
你可以点击创建新的iam角色
你必须在这里给它一些后缀
我已经做过了
我在这里创建了这个角色，并为其添加了squirrel作为后缀
因为我们还没有开始使用湖形成
我们可以跳过这一步
我们会回来的
但这不在这个技能范围内
所以我们接下来有我们的目标数据库
我们可以选择哪个数据库
我们希望将此信息放入或与该数据库关联
现在 这是一个逻辑数据库
它不是一个真实的数据库
它不会存储真实的信息
它会存储信息
它会存储元数据
所以这将是代表我们的数据的东西
这是爬虫出去寻找东西时找到的
所以我要点击 添加一个数据库这将为我打开一个新的页面
我将称其为松鼠普查
因为这是我们的数据库
创建数据库
所以我们有一个逻辑上的东西叫做松鼠普查数据库
现在我需要将屏幕切换到我的另一个标签页
然后我可以全屏刷新
这就是我的松鼠普查
我将保留所有这些内容
在这里我们可以安排一个爬虫计划
我们可以选择每小时或每天运行
每周或某种自定义cron表达式在这里
我将其设置为按需
所以我将点击下一步
它给了我一个小的确认屏幕
我将创建我的爬虫
所以我的爬虫现在已经创建
所以它准备好了
我现在只需要点击运行爬虫
它将开始尝试查找数据
我已经设置了那个数据源
现在请注意
我可以设置许多数据源
它会去尝试寻找不同的表格或东西
看起来像是我们可能想要与这个松鼠数据库关联的表格
所以目前 我将暂停视频
然后继续
当这个爬虫完成它应该做的事情时
好的 爬虫已完成其工作
在这里大约花了一分钟十二秒
它发现了一张表格的更改或一张表格
所以我们可以回到现在
我们可以回到数据库这里，这将向我们展示我们的松鼠普查数据库
如果我们深入挖掘
看这里，它发现了一张表格
所以这里是表格名称
它从我们给它的CSV文件中推断出来的
这可能是一个安全的赌注，我们可以查看表格数据
这将我们带到athena
我们现在不在这个技能中玩athena
但这是我们将在不同的技能中覆盖的内容
这就是你可以实际查看数据的方式
如果我们在这里设置了数据质量统计
我们没有这样做
所以它没有进行任何数据质量分析
但现在它已经弄清楚了表格结构
这是一个CSV文件
所以它在尝试确定这里的真实数据类型方面能力有限
它识别出了这些字段
这看起来与我们预期的松鼠数据结构非常相似
对于现在，我们将将其放在一边
我们将继续到数据酿造
但我们将使用这个数据库和表格
以及后来在这个技能中的同一个视频中生成的数据目录 我们将使用这个数据库和表格
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/042_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p42 3. AWS Glue DataBrew First Look.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们来看看aws glue数据酿造
我们可以用几种不同的方式访问它
我们可以在这里的上方搜索栏中搜索
或者我最近访问过
所以我只是点击这里
它会带我们到这个屏幕
现在数据酿造是一个更先进的
一个更多功能 丰富的可视化ETL编辑器，我们在aws glue中玩过
但他们称之为假数据
最终我们将创建一个项目
但首先，先做第一件事
我们需要设置我们的数据集
所以我要点击数据集这里
我将点击连接新数据集
我将此连接到我们的空气质量数据
所以我称之为空气质量
我们有不同的来源
您可以看到红色偏移
Jdbc s three
但我们也能访问我们的aws glue数据目录条目
如果我们已经构建了我们的数据目录
我们可以在这里访问我们的表
你可以看到我们有一个名为squirrel census的数据库
这是我们在上个视频中创建的
所以我们可以很容易地选择它，如果我们想要的话
但我们不会这样做
我们将直接转到s three桶
所以从s three桶
我打算在这里钻探
我将此选为我们的数据输入
这是空气质量数据所在的地方
我们上次对它进行了一些操作
但我们不会使用它
这是以CSV格式
我们将第一行作为标题
我将滚动并删除这个
好的，创建数据，好的，所以我将点击这个
我将选择它 我将创建一个项目，使用这个数据集
现在我被要求输入一个项目名称
我们将这个项目命名为空气质量项目，数据胶水是aw has胶水
数据烘焙使用称为食谱的东西，食谱只是数据操作的步骤，我们将要做的
这个食谱将被称为
自动生成它
你可以更改它
如果你想要的话 你也可以编辑一个现有食谱或从另一个食谱导入步骤
空气质量项目食谱
我将使用我刚创建的数据集
我也可以创建一个新的数据集
如果我想继续往下走
我必须选择权限
因为这些
这将操纵这个数据
所以它需要访问
能够至少读取这个数据
我将创建一个新的
我是角色
我被要求输入一个角色
后缀 它将创建一个这个角色
就像这里一样
后缀是
我将其命名为空气质量，以便
我可以确定这个角色属于这个项目
我将点击创建
最终它将回来并说嘿
恭喜你 你有一个项目
如果你是第一次进入这里
你将得到这个小小的向导
它将引导你通过
各种界面的功能
我们可以跳过
这正在幕后构建一个实例
如果你记得当我们在做可视化etl编辑器时
它有些类似
这里有一个计算机或处理器
必须在幕后运行
实际上操纵这个数据并进行这些数据预览
这就是它正在做的
它正在设置一种幕后基础设施
以便我们处理这个数据
所以这有点无聊
它进展得很快
但我仍然会暂停视频
直到一切都稳定下来
然后我们继续 好的
终于设置好了
在我暂停视频后可能又花了大约两分钟
你可以在这里看到我们的数据
这只是对这个屏幕的简要介绍 我们在这里有一个项目
然后我们有这个工具栏
以及所有这些小图标
这些是不同的过程，我们可以应用到这个数据上
这是一张网格，展示我们的数据
它包含一个数据样本
它只选择前500行数据
如果我们向右滚动
我们可以看到有一个纬度列
经度列 这些最终会在参数中显示
单位和读数
现在 在顶部你会看到这些
它已经产生了一些统计数据
它正在尝试给我们提供一些关于我们数据的信息
这主要是因为我们使用了不同的标准
在这里我们测量了不同的事物
这些实际上并没有实际的意义
但这只是给你一个关于数据分类的想法
我在这里看到的是，这些记录中有332个是ppm记录
而其他的这些是每平方米微克
所以这很有用
我们将如何处理这些数据是，我想要限制这些数据
我想汇总这些数据
我只想看ppm的读数
我想把这些数据按天平均
如果我们回到这里的时间戳
你可以看到几乎每小时都有读数
我想把这些数据按天平均
所以我首先想做的是
我想去掉一些不会给我提供任何信息的列
因为现在地点都是一样的，这里的地点也是一样的
因为我们只取了代表单一地点的数据
所以这对我来说没有任何帮助
所以我打算
我会保留位置
我会把它放在那里
但我要做的是隐藏或删除
它并没有真正删除
它只是从我们这里移除
一些列
好的 所以首先
我要去这个会议
点击这个列
这很有趣
它截断了我查看完整的下拉菜单的能力
我把它放大了
我认为它被放大到了110%
这样你可以看得更清楚
但是再次 这是我们上次遇到的同样的情况
这是一个高密度的屏幕
你可能不会玩这个
在11x12x7x68上
你可能有两个显示器和所有那些东西
所以不幸的是
我不得不把它降到大约100%
真正的大小
如果我现在往上走
我可以看到删除这一列的选项
我将点击它
它已经创建了一个小的删除
这里删除列的选项
我可以选择我想要删除的列
我将保留位置
我将删除位置
我将删除拉丁草坪，这不会给我带来任何价值
这不会给我现在带来任何价值
让我们看看
好的 这目前很好
它正在生成预览
所以这里只有位置
ID 日期 时间
参数单位等等
所以你可以在这里看到这道菜谱
让我看看我能否把这个放大在这里这道菜谱
我们在这道菜谱的第一步
是删除这些列
所以我想做的下一件事是我想创建一个新的列
在这里我有按小时分解的日期时间
但我想要一个只包含日的列
最终我想做的事情是每天汇总这些信息
为了做到这一点
我有几个不同的选择我可以在这里提取
向下到日期时间单位
我可以提取年份，季度，月份，星期数
等等
但是我在这里没有提取日期的选项
换句话说 20191212
为了做到这一点
我不得不用一种稍微不同的方式去做
我得去函数这里
这允许我们做一些更深入的东西
所以我要去日期函数，日期格式
我在这里滚动值
这就是我们在这里看到的
基于函数 这是我选择的函数
它给我们一个小的定义，那是什么
我们的源列
这个人在这里是对的吗
我们要使用的日期格式
所以我想使用年-年-月-月-日-日
这里在问我们目的地列
我说我想创建一个新列
因为我不想干扰这个现有列
我可以直接覆盖它
这也是一种可能性
但我只想说日期时间，只取日期
它问我是否想过滤这个更改
换句话说 我想限制它只针对某些行吗
还是我想对所有的行进行操作
现在 我们可以在这里点击预览
这将给我们一个预览，让我们看看
它将看起来像什么 这里是预览
是的 这正是我想要的
所以我要应用这个
所以你可以看到我们现在有了步骤2
创建日期时间列
使用这个特殊的日期格式
我想做另一件事，因为我只对ppm感兴趣
或者pm2.5
你可以看到我们这里有其他条目
所以我想过滤掉它们
所以我要点击那个
我要去这里过滤
我的条件正好
这就是我现在想要的值
我可以输入这个
我可以在这里使用一个rex值
或者我可以从这里的独特值中选择
我想要的是pm two five
所以我要预览更改，我应该在这里看到的是
我们只有pm two five
一旦它更新了，好的，我们不在这里有其他选项
所以我现在开始应用
我可以在这里过滤值
我可能在任何地方在这个过程中做
但现在我们有第三步
这将把我们的值过滤到仅pm2.5
所以现在我们可以将这些分组在他们的日期桶中
为了做到这一点
我将点击日期时间日期
只有
在这里到组和我将要使用的列
这里只显示日期时间日期组
然后 我还想按位置进行分组
按围栏分组，我想这就够了
哦，按单位分组，值将是平均值
平均值，这就对了
所以我可以选择我想要的新列的数据类型
这是平均值
所以这正在做的是
它将按日期进行分组
因为这些物品在这里，也会在那里
这将包括结果中的那些
它提供给我们的
它们全都一样
所以那里不会出现任何子分组
唯一的区别是我们这里有日期和时间
所以我们在这里预览
这里说我们的日期
这里是我们的位置
这里是我们的参数
然后我们滚动到这里，这就是我们的值
所以这是那天的值
这是那天的平均读数
你可以看到有些日子比其他日子高
我们会期待那样
如果我们对此满意
我们可以点击完成
这将把我们带回到我们的食谱这里
这是步骤4
这将以这种方式分组这个数据
我认为我们准备就绪
我对这里的预览很满意
现在是时候将这应用到我们所有数据并创建一个全新的数据集
所以我要去这里创建工作
它正在这里给我一个工作名称
所以空气质量每日平均
它问我
我想把数据输出到哪里
所以我会说我们将其放回S3
我们也可以将其发送到另一个数据库
如果我们想要
所以 我将在这里选择我们的游乐场
我在这里给它一个分区
让我们看看这里 拉斯维加斯的日常平均怎么样
所以它将要做
是将它放在那个文件夹下面
所以我在这里滚动
我不会碰这里的任何东西
我在这里选择我的卷
因为这是要做工作的那一个
我认为这里的一切都很好
我只是要创建工作
我不确定它是否实际上创建了工作
所以我要去那里
看，没有，这以前发生过
这可能是一个好事
因为这也可能发生在你身上
我在这里创建工作
而不是实际上创建工作
它真的什么都没做
所以我要做这个
并且我在这里使用那个输出
我们将使用拉斯维加斯每日
平均值
并使用这里的权限
那个人，这次我们只是创建并运行工作
好的 看起来那里正在发生一些事情
我不知道为什么没有创建工作
这看起来更合理
所以现在如果我们在这里转到工作
希望我们能做到
这是我们的工作
如果我们在这里转到工作，好的
所以它正在运行
所以它将需要几分钟来实际构建这些或执行这些任务
你可以看到 它正在显示我们的输入
它说 这是我们的数据输入
这是我们的数据集
这是我们在数据集上执行的食谱
我们将有一个输出
我们只需要等到工作完成
然后我们将出去看看它产生了什么
所以我要暂停视频
然后我们将在完成时重新加入
好的 大约六分钟后
我们的工作终于完成了
让我放大一点
现在我们可以做的是检查结果
所以我在这里转到s3
我打开一个新的s3标签页
我将在这里全屏
以便我可以在我的标签之间切换
我将在这里滚动到我的桶
这是我的桶
或者这里是包含我们拉斯维加斯的文件夹
每日平均值，所以我们深入挖掘
你可以看到它有各种csv文件，这些文件在这里创建
查看这些文件的最佳方式
在没有引入另一个工具的情况下
我会说让我们去数据粘合剂或aws粘合剂
我们只是去那个视觉etl编辑器，我们上次用过
所以我要去视觉etl
选择那个，然后选择我们的文件，好的，我们允许它拉出一个预览
所以这是以csv格式
至少我们可以看一下数据，确保它看起来合理
好的 这是我们的数据输出
这里有日期或日期
我们有位置
Id参数
我们的单位 然后我们有平均值
看起来数据已经准备好了
或者数据准备工作做好了
我们想要的方式中的数据
所以在接下来的视频中，我们将深入探讨 我们将讨论如何使用数据酿造进行一些数据清理活动
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/043_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p43 4. Data Cleansing with DataBrew.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好吧，在这里 我们回到了数据酿造厂
让我们尝试使用数据酿造厂进行一些数据清洗活动
首先，最重要的事情
我们将建立一个数据集
点击数据集
连接到新的数据集
我将为这个数据集起一个名字
我将使用我的松鼠数据集
因为我们使用爬虫为这个数据创建了数据目录条目
我们不必担心太多
它已经在那里定义好了
元数据已经存在
数据目录已经知道了这一点
我们点击它，然后我们需要深入挖掘
我将删除这个东西
在这里我们需要深入挖掘
在这里我们有我们的表
这就是我想要使用的表
所以我将创建一个数据集
好的，我已经创建了
我将在这里创建项目，使用这个数据集
并为其起一个项目名称
所以松鼠清洗
它将默认使用我们将要使用的食谱
是的 我想要使用这个数据集
我将在这里到权限
我将创建一个新的权限
实际上我已经做过了
所以我将重用我已经创建的一个角色
如果我创建
它将问我一个后缀
我只是选择了松鼠作为我的后缀
我将点击创建项目
好的 就像以前一样
我们将不得不等待一段时间
以便在后台为我们提供计算资源
这将允许我们实时操作我们的数据
所以我将暂停视频并等待这个过程完成
好的 我们的数据集已经准备好了
我道歉 这是一个非常拥挤的空间
让我看看我是否能将其全屏，以便我们有更多的空间
在这里，如果我们看到这种界面
我们有所有不同的转换和操作
我们可以用这个数据集做
我们已经有了我们的数据集在这里
你可以看到它已经做了一些统计在这里来告诉我们有多少是中央曼哈顿
上曼哈顿等等
它也对所有的其余列做了同样的事情
所以这些列中的很多对我们来说实际上并不是非常有用
但我确实想在这里玩一个列，那就是这个其他笔记和观察
如果我们看来自数据源的笔记
它说这是一个字段，志愿者被允许基本上自由形式一些东西
但不幸的是，我们可以看到如果我们在这里下去
但我们在这里下去
我们这里有一些古怪的角色
我认为这可能是我们为什么在视觉ETL编辑器上卡住的原因
上次 因为这些古怪的角色
那里可能有一个转义字符
因此，在我们的清理活动中
让我们尝试去除这些角色
或者这里的这些字符
所以我在这里突出显示了源列
我将在这里去清理
现在 我们可以定义很多不同的清洁程序或步骤
其中一种可能适合我们的是特殊字符
我将点击它
它将为我们提供一个小食谱
在这里 它正在问我们
我们的源列是什么 它已经选择了那个
我想删除什么特殊字符
所有角色都是自定义角色
我们可以去除空格、引号或标点符号
有点实用
我想将此应用到所有行
我将预览并看看效果
好的 这里有一个预览列
我们可以看到，不幸的是，这种过滤器并没有帮助我们
我相信如果我们在这里上去
所以这里有一些一些英镑符号
它已经删除了那些这是可以的
如果我们正在做的就是那样
但我们真正要找的是这样的有趣字符在这里
所以我们可以做的是尝试一种更高级的侵略性
或者尝试另一种方法我们可以也许删除那些字符
所以我要点击 在这里取消
所以我在这里突出显示了我的列
我将前往清理
我将滚动到最下方
你可以在最下方看到它
替换值或模式
我将点击它
这将允许我们替换
替换列中的项
在这种情况下我们可以使用正则表达式
我将使用斜杠w并将其替换为一个空格
让我们看看这做了什么
斜杠w是一个正则表达式，表示嘿
清除所有内容 或者找到任何非字母数字字符
我认为这是拉丁类型拉丁一种类型编码
但是/ w 已经找到了那些项目
并且它用空格替换了它们
因此，现在删除它们，我们可以点击
应用
如果我们完成了数据集
那么我们就可以保存这个工作或创建这个工作并运行这个工作
然后它将输出这个数据集，该数据集已从这一列中清除了这些值
但是我们还没有完成这个特定数据集的处理，在下个视频中
我想向你展示我们如何尝试去做好
你可以在这里注意到
我们也有一些其他奇怪的字符
这是现在数据处理似乎做得不太好的一件事情
数据清洗似乎做得不太好
你必须要指定每个单独的列
如果我所知道的话
他们似乎没有一种方法可以跨列去处理整个表格
但是我们在下个视频中将要去做
我们将尝试处理这个活动列
在某些情况下，这些值是固定的或相同的
这些值中的一些是自由形式的
正如你可能猜到的
这简直是一场噩梦
当我们试图理解这些列中捕获的一些数据时 所以我们将在下一期视频中讨论这个问题
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/044_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p44 5. Data Shaping Approach.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 所以在上一集
确实，最近几项技能这里
我们一直在看这只松鼠的普查数据
你可以去松鼠普查网站
只需在谷歌中搜索松鼠普查
你会找到的 你可以在这里下载数据集
我对这个活动列感兴趣
在这里玩耍
如果我们深入挖掘
我们可以看到在数据集提供的文档中
你也可以在松鼠普查网站上下载
在这个字段中
它是活动 这是该字段的定义
志愿者应该输入跑步
追逐
攀爬
进食 觅食
或者他们可以自由添加自己的备注
如果我设计这个界面
我会将这些固定值
放在一个下拉菜单中
或者可能是一些小复选框
你可以选择多个
然后这里有一个自由文本字段
但是，不幸的是
这不是这里的情况 所以我们有这些有效的值在这里
但我们也有这样的小片段在这里
这些是由志愿者添加的
我们有些情况，松鼠在同一时间做多个活动
让我们说，我们感兴趣的是这些多个活动的相互作用
我们只对跑步
追逐 攀爬
进食和觅食感兴趣
我们感兴趣的是，进食是否与觅食相关
或者跑步是否与追逐相关
那样的事情
所以现在的挑战是我们要尝试找出，首先
允许的值，并过滤出一些其他值，并且做到这一点
尤其是当我们在同一列中有多个值时
我们将在这里输入的数据
并将其通过其分词器运行
分词器将做什么
它将把这些字符串分开
并将它们分成它们的单个单词
所以这将类似于那样，逗号
类似于那样 那样将会被解读为进食，逗号
挖掘，逗号 从那里来的东西
我们将要做的是，我们将会把这个数据集分割成独立的列
所以如果这是单词一、二、三
我们将会有单词一、二、三在那里
然后通过一些其他魔法
我们将进行一些去聚合操作
最终我们将进行一种称为独热编码的操作
独热编码在数据科学和机器学习中被广泛使用
这是为什么
这些词对机器或算法来说并没有太多意义
但这里有意义
我们有松鼠
假设在特定列中松鼠正在奔跑，逗号
这是他所做的一切
松鼠B在这里在觅食，逗号觅食
所以我们做的就是把1放在这个吃一列
来表示松鼠b实际上在吃东西
但他也在寻找食物
所以这里的数据
我们可以做这些值之间的相关性和算法
我们可以说如果他们在吃东西
他们可能没有在奔跑
如果他们在寻找食物
也许他们不会同时在攀爬
或者也许他们是谁知道
但这正是我们的任务，对于这个特定的练习来说
我们希望尝试将这个数据转化为我们可以用作原材料的东西
以便于进一步的分析 那么我们开始吧
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/045_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p45 6. Data Shaping with DataBrew.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们现在在我们松鼠的清洁数据集这里
这就是完整的数据集
我还没有对它做任何事情
我正从零开始
我想做第一件事之一
是让它在这里处理起来容易一点
所以我将删除所有列
除了那些我感兴趣的
那些就是松鼠ID和活动
所以我只到这里并删除所有其他列并应用
所以我应该剩下这里，我们只有松鼠ID
然后我们有活动，那么我们去那里
让我看看是否能扩大这一点
这样更容易看到
所以那是我的第一个食谱步骤
所以现在我剩下这个列
但我们说过我们不能做太多
所以下一步我想做的是这个列的词汇化
我将去活动这里
我将上去到文本单词词汇化
所以我可以做几件事
我可以扩展缩略词
我可以删除停用词
我可以做词干提取
我将向你展示词干提取看起来如何
我可以指定一个联合分隔符
所以我要做
我将使用逗号 我将预览
所以词干提取是它试图将单词简化到其最基本形式
所以你在这里看到吃
它说嘿只使用吃
挖掘挖
这在你有一个数据集时是有用的
例如
它说跑步或跑或跑了
它将其简化为同一件事 基本上它试图获取词根
但我认为这将混淆事情
所以我将立即删除它
我将向下到所有行
我认为一切都很好
我将预览并看看那看起来如何
我们去所以已经完成
它已提取出这些单词
它将这些单词放在这些单词之间
我们走 所以已经完成
它已提取出这些单词
这是一个很好的起点
所以我注意到
虽然这也是
它这里插入了一个空格
所以我有吃空格逗号
所以我想做的是
我只想删除那些空格
那么要做的就是
我可以点击那列然后可以去干净
然后领先和尾随空格
所以我可以删除所有的空格
事实上 我想我会那样做
因为我只想要这些单词用逗号分隔，里面没有其他任何东西
所以我认为看起来不错
让我们预览一下，看看它看起来如何
在这里保持警惕
然后我们出去在这里，我们就这样做
这就是我想要看到的
所以现在应用这一点，因为我把所有值都连接在一起
我能做的就是把这些分开
让我们来处理这些活动
在这个令牌化列
我将以单个分隔符进行分割
我的单个分隔符将是一个逗号
我不想包括分隔符本身
现在我们来到了如何多次分割的问题
我认为三次就够了，这里我们有一次
两次三次，三次可能足够了
我们将在这里看到
如果足够 我们将得到只包含空值的列
让我们从这里开始看看文件夹看起来怎么样
这将尝试将此列拆分为四个单独的列
基于该分隔符
在这里我们有四处空值空值等等
您可以在这里看到一些条目在这里进食挖掘某物没有
如果我们一直走到这里
看起来我们在这些外围列中确实有一些内容
但这些是我不关心的值
所以我更感兴趣的是这些值，这些值在我的原始文档中
挖掘，进食，我们的挖掘不是那些觅食，进食，奔跑，攀爬
这些是大致的主要五个
所以我更感兴趣保持这些
让我们只保留这些列
我可能可以将其减少到可能三个
嗯 让我们只保留它
我会向你展示我们接下来要做什么
所以我们接下来要做的就是
我们将要做一个名为 unpivot 的操作
这将使我们能够将这些列
我们将插入记录
或插入这些行，包括滚动ID和活动
它将会是多个
行，包括这些不同的活动
我认为一旦我们实际去做，将会更清楚
所以让我们尝试一下
转到 pivot 现在
我将列转换为行
我的不匹配列在这里
哦
我需要首先做的是拆分 然后保存，以便转换我们的数据
所以我想要调用那里和数量times将做apply
这将实际为我创建那些列，而不是只做预览
所以我在这里可以看到
这是我的列
所以现在我可以在这里做 pivot
我将做 unpivot
这将那些列
我将那些列转换为行
所以这里是我想要 pivot 的列一、二、四、五
六
所以现在我必须输入其他两个东西
我必须告诉它
我想要叫什么
我将那些列转换为行
哦 我需要给它不是一个破折号
它不喜欢破折号
我将 preview 现在
这将是它的样子
所以 如果我们稍微忽略这个列
在这里，我们更多是在这些列之后
所以，我在这里有一个松鼠
ID，我有 foraging
我有一堆 nulls
没关系 我们将能够处理这些
因为我们想做的是
我们希望将其简化为应该包含的内容
我们将过滤掉所有
我们不想要的其他内容
所以 我现在将完成这一点
我可以做的是因为我们
基本上完成了这一列
在这里 我们提取了数据
如果你记得的话
这有点像一个构建过程
这不是像电子表格
这里的这些项
引用了这些项
所以我们实际上可以再次删除这一列
我必须让它变小
删除应用
好的 现在我们有我们的松鼠ID
我们的词汇选择
这里有所有不同的列我们有活动汇总
我现在可以做的是让我让它在这里更大
我可以在这里过滤结果
我将过滤
并根据条件
并且让我们只选择正好
这就是我想要过滤的
所以我想限制一切
这不是这些批准的项目
这些前五个项目
所以我点击这个点击这个点击这个，好了
所以这些都是我更感兴趣的项目
所有这些其他东西只是那些随机的人们有能力输入
我真的不关心那些东西
所以我可以点击预览更改
所以我们应该看到的是
我们应该看到这些数据集大大缩小
它正在向我们展示它将删除这些记录在这里
并保留这些记录
我喜欢那样
我将应用
好的 所以现在我真的不再需要这列了
要么
我将删除这一列
随着我们做这些事情
你可以看到我们的食谱正在逐步增长
而且那完全没问题
现在我们完成了 我们有滚动ID
然后我们为这些活动每一项都有一个行
所以如果我们进行排序
让我们进行排序
也许我们在寻找这里
这里有一个
我们有像再次放大
这里有一个三一
他或她有一个追逐观察
但他们也有一个攀爬观察
所以这两项被观察到与这个特定的松鼠
所以现在我想要做的是对这个进行one hot编码
这在这里真的很容易
我们所要做的就是点击这个源列
点击这里进行one hot编码
它会问我们这里有多少个唯一值
它将会创建五个新列来表示所有这些不同的选择
我将点击应用，好的，我们这里仍然有我们的活动
如果我们滚动到这里
我们可以看到我们的one hot编码
所以如果这个项是追逐
我们在我们的追逐列下有一个一
但这仍然表示这些描述的多行
所以我想要压缩这些
我想要只有一个行在这里
它有一和零
一表示那个活动被那个松鼠观察到
所以我可以删除这个列
因为我不再需要它了
删除，应用，好的，你可以看到我的一些列仍然在这里
我有这个松鼠的两个条目，我想要做的是
我想要汇总这些项，按松鼠ID分组
为了做到这一点
我在这里进行分组，我将按松鼠ID进行分组
然后我将选择这些列
添加列，添加列
觅食
奔跑，使用求和
完成
好的
所以现在它正在尝试进行预览
所以我们应该看到的是
我们应该看到每个松鼠一个条目
但取决于那个松鼠的观察
所以这里我们有追逐和攀爬
所以在这种情况下，这个one hot编码说这个特定的松鼠进行了追逐和攀爬和进食
所以他是一个很活跃的
我将点击完成，那里
这将会最终确定
我们已经创建了我们的食谱
但这只是一个工作版本
我们要确保做的是保存它
以便我们以后可以欣赏或利用这份工作
所以我要点击发布食谱
你可以在这里看到所有步骤
点击发布，这就完成了
如果我们在这里转到食谱
我将选择那个
我将使用这个食谱创建一个工作并删除这个
我们不需要所有这些东西
这将是一个热松鼠
我听到你们在那里窃笑
你们应该感到羞耻 感到羞耻，感到羞耻
至于数据集
我可以选择项目或数据集在这里
我只是选择我的松鼠数据集
因为这是我想要使用的数据集
我将选择输出
这将输出到S3 我将选择Parquet
选择Snappy
并指定位置
热编码
搞定 我喜欢那个目的地
现在到这里选择我们的权限
松鼠权限
我希望现在每当我点击运行
它将实际上做一些事情
是的 看起来它在做某些事情
所以我认为它正在创建那个工作
所以让我们给它一点时间
然后我们去看看那个工作是什么样子的
它将花费一些时间来创建那个工作
我们已经创建了一个工作
然后我们可以转到工作
运行历史 我们可以看到这项工作正在运行
我将暂停视频
然后我们将等到那个工作完成
然后我们去看看数据
好的 我们的工作已经完成
它成功了
在我们这里我想向你展示的一个功能
是叫做数据线 所以这显示了我们的源数据
这表明它被转换为数据集
然后它显示了我们的食谱，这两者在这里结合在一起
然后它吐出了这个结果
这真的很方便
如果你有一些非常复杂的转换和通过多个步骤的数据
因为尝试找出数据来源通常非常困难
所以这是一个非常方便的方法
一种视觉导向的方法
所以现在让我们去glue proper看看那个数据
就像我刚才说的
这可能是最好的方法
我们可以用这种快速且肮脏的方式做
就是使用这里的可视化ETL编辑器
所以我要进去这里
去掉那个
添加一个数据源
我的数据源在数据工程下
我们创建了松鼠一号热
这就是我们的数据选择
我们的格式是parquet格式
所以它正在读取那个数据集
它将给我们一个预览
这将看起来像什么
好的 这是我们的数据
我们有我们的松鼠ID
我们有不同的列在这里
这已经被热编码出来了
这正是我们所期望的
所以我希望你做这个活动，因为我们稍后会使用这个文件
在这个课程中做一些分析
所以请确保你走完这个过程
首先，只是为了熟悉这个工具本身，其次，输出一个文件 以便我们稍后可以使用
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/046_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p46 7. Validation Advanced Features with AWS Glue.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我们来检查这些问题
第一个问题 你正在处理aws glue
你需要自动化发现数据模式和填充aws glue数据目录的过程
关于aws glue爬虫的以下陈述中，哪一个是不正确的
首先，从这里开始
aws glue爬虫可以定期安排运行
以确保数据目录保持最新
这是一个正确的陈述
但我们被要求找出哪一个是不正确的
所以我们必须绝对肯定地阅读问题
并确保我们理解了问题的真正意图
因为你可能会被诱惑
你可能会快速浏览这个不正确
阅读第一个项目并说，是的
然后继续
你可能会失去问题的分数
下一个答案
aws glue爬虫
可以自动推断数据集中的字段的数据类型
嗯
这也是正确的 我们看到了一个例子
它读取我们的数据集并说，是的
我认为这可能是一个双精度或浮点数
或者类似的东西
所以，是的
它可以推断数据类型 但它不是百分之百准确
它并不总是准确
但它会尝试这样做
下一个答案
aws glue爬虫可以检测并自动更新s3基数据和jdbc数据源的分区
我们讨论了并演示了s3基数据源
但我们没有处理jdbc数据源
但如果你不知道这是真是假
我们已经将选项缩小到了两个
所以让我们看看最后一个选项
aws
glue爬虫 需要你手动定义每个数据集的模式，然后才能运行爬虫
嗯
这与上面的陈述有点矛盾 因为如果它可以自动推断模式
那么我们为什么真的需要定义模式呢
这里的'手动'这个词是aws常用的伎俩
他们试图欺骗你，让你认为这个特定的服务
是不正确的
你必须手动做好某事
在背景中 他们微笑着
他们说不 它会自动完成所有工作
他们只是想看看你是否了解服务如何工作
如果我必须在这两者之间做出选择
我会肯定地说这可能是正确的答案
这可能是错误的
因为这里自动更新分区似乎绝对合理
所以我将选择这个作为我的最终答案
下一个问题
您正在使用aws glue数据酿造来准备数据以进行分析
以下aws glue数据酿造的哪些功能最合适
当你想自动识别数据集中潜在的数据质量问题时
第一个选项
使用数据酿造的概要功能生成数据统计并识别异常
嗯 听起来像是一个很好的事情
可以帮助我们识别潜在的数据质量问题
让我们继续
将数据酿造与aws glue数据目录集成
以注册您的数据集以供aws glue更广泛的使用
那是个好功能，一切都好
但这并不会帮助我们解决任何数据质量问题
所以我可以消除那个选项
创建一个数据酿造作业将准备好的数据导出到s 3以进行进一步分析
嗯，这不会帮助我们解决数据质量问题
或者 它只是生成一个新的数据集并将其发送到s 3
所以这里我们可以去掉那个选项
最后一个选项 应用转换以规范化和清理数据使用预定义的食谱
或预定义的食谱
嗯 这可能是一个选择
所以这里说的是
应用转换以规范化和清理数据使用预定义的食谱
这里问题是
如果我们想自动识别潜在的数据质量问题
所以如果我必须在这两个选项之间做出选择
使用数据酿造的概要功能生成统计数据并识别异常
我可能会选择这个选项
因为这里谈到使用预定义的步骤
如果我必须预先定义它们
那么这不太自动
所以这是我的最终答案
下一个问题
您正在准备一个数据集用于预测客户行为
这将预测客户行为
你的数据集中的一列是客户类型
它可以是新的、回归的或VIP
你需要将此数据转换为列格式
你需要将此列转换为模型可以更有效使用的格式
在什么情况下使用客户类型的一热编码是最好的
而不是其他方法，如为每种类型分配一个数字
让我们看看下面的答案
首先 您希望通过为客户类型使用单个数字来表示数据量减少
嗯 这正好描述了这个物品是什么
所以它说的是 也许我们
我们不想那样做
让我们继续这里
你哦
那里少了一个y
我必须回去修复那您想要
您想让模型识别订单或类型之间的排名
例如将vip视为高于回头客
这是一个称为序数排名或序数型数据集
这意味着我们想要它们中的一些值比其他值更具有影响力或价值
当我们谈论一热编码时
那不是一
一热编码所做的一切
所以我认为在这个特定情况下，我们应该说最好
你知道
也许vip等于三并返回 也许等于二
等等
所以这不会真正解决我们的问题
下一个选项是一热编码
你想要确保模型将新客户、回头客和VIP视为分开且平等的类别
在没有假设顺序的情况下
这正是一热编码所做的
它不会假设这些值中的每一个有任何特定的顺序
它只会将它们拆分为列
并在该列中放入一个一或零
取决于行中的值
所以这是一个相当不错的选项
我会继续前进
你想要保持简单
通过为新客户和VIP分配号码来保持简单
一个数字 比如一、二和三，嗯，又是
这基本上是一样的
那基本上是一样的
我们被要求
我们是否可以使用其他方法
所以所有这些选择中，真的
这是唯一看起来像热编码的
看起来像
它是唯一假设的
我们没有对这些特定值分配任何顺序
这是我最终的答案
我希望这对你有所帮助 我想感谢您观看
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/047_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p47 1. Skill Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，美丽的学习者，进入另一个希腊神话的技能
雅典娜是一位高度尊敬的智慧与洞察力的女神
她给了我们凡人烹饪和手工艺的礼物
雅典娜如此受欢迎，以至于希腊首都雅典以她的名字命名
然后 她知道如何在战场上找到自己的位置
她将美杜莎变成了戈耳工
并且她将某人变成了蜘蛛
因为她不喜欢他们编织的挂毯
不管所有这些，雅典娜
无论她是否知道
也借她的名字给亚马逊雅典娜
一个数据库服务，我们将在本技能中深入探讨 让我们开始
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/048_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p48 2. Introduction to Amazon Athena.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们来谈谈亚马逊雅典娜
我们这里只有几个幻灯片
我将解释产品的高层视图
然后我们将花费剩余的时间
大部分时间实际上使用雅典娜并让它做一些很酷的事情
亚马逊雅典娜
这是一个工具，我们可以使用它来使用SQL或Spark主要查询
S3基础的数据
它不限于S3基础的数据
我们有多种不同的连接器可以使用
可以将athena连接到多种类型的数据存储
但是大多数人主要使用它来访问s3
他们为什么这样做呢
因为s3是一个非常受欢迎的选项或选择
当公司建立数据湖时
我们将在另一个技能中涵盖数据湖
但是aws认为athena是无服务器的
我们不需要设置服务器
我们不需要担心后端的任何事情
我们只使用这个服务
我们按使用时间和使用次数付费
实际上
雅典娜实际上是一层抽象
雅典娜内部没有真正的存储
我们只将雅典娜指向不同的存储
后端数据存储
它充当一个顶层的层
我们可以使用SQL或Spark与该数据进行交互
现在有很多不同的连接到外部数据源
MongoDB转换为DynamoDB
转换为各种类型的正常关系数据库
这是亚马逊Athena的一个非常常见的用例
假设我们有一些数据正在积累在S3桶中
我们希望能够将这些数据与其他数据库的数据进行连接或进行联合
假设 例如 一个SAP HANA数据库
那么 我们可以使用Amazon Athena无缝地连接这两个数据集
我们可以看到结果
事实上
我们可以将我们的商业智能工具指向athena
使用可用的odbc或jdbc驱动程序
我们也可以通过aws控制台访问athena
通过cli或sdk
我们将通过aws控制台访问athena
但如果您的商业智能工具使用odbc或jdbc
或者如果您可能拥有使用相同类型连接的etl工具
您可以访问亚马逊athena
现在亚马逊athena有点有趣
因为它是aws创建的产品之一
它不是从零开始创建的
事实上
它大量借鉴
我的意思是大量借鉴
一个开源项目叫presto
presto最初由facebook创建
在他们还被称为meta之前
在他们还是facebook的时候
大约在2012年
2013年facebook开源了presto 他们只是上传到了github
任何想下载的人都可以 因此，现在有成千上万的组织
在使用presto来访问他们的跨不同数据存储的数据
aws就是那些公司之一
他们开始思考如何将这个项目
变成一个人们想要使用的服务
这在aws并不是新鲜事
在很多云服务提供商那里也不是 事实上
这是一个非常普遍的模式
有开源项目或产品
aws会获取这个开源
然后使它aws化
使它基本上一键式，易于使用
经济实惠
然后从aws的角度来看
每个人都是赢家
因为顾客想要使用这些开源产品
但他们不一定想处理
所有的麻烦，试图让他们运行高效
等等
另一方面，一些开源社区对此表示异议
他们说，实际上aws确实在利用了我们的努力
确实，这在某种程度上引起了争议
这不是特定的aws
许多其他大型组织
大型软件公司，或多或少地吸收了开源努力，并将其纳入他们的产品
这是用这些重新品牌开源产品的另一个潜在缺点
presto是一个相当复杂的平台
像许多其他开源努力一样
它使用许多 许多其他子项目
每个都有自己的开源社区和开发流水线
bug修复
aws在这方面值得称赞
他们试图成为你 顾客和这些开源平台之间的负责任的中介
许多时候，aws会向这些项目贡献资源，修复bug，添加新功能
修复bug
添加新功能
但有时aws也会受到底层缺陷的影响
或者仅仅是设计缺陷
当你使用athena时，你可能会遇到一些类似的小问题
但这就是现实
正如我们在另一个技能中看到的那样
有时这些开源项目会特别反感aws介入他们的项目 但这是另一个故事，留到另一个时间再说
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/049_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p49 3. Our First Athena Query.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 亚马逊athena
我们在控制台这里搜索它
我们可以在这里的上方搜索栏中输入athena
如果你是第一次使用athena
你会在这里
这是大致的起始页面
这里有一些关于价格的信息
这可能是你应该关注的事情
我们将根据每TB扫描的SQL查询收费
这是每TB5美元
我们还会根据SQL查询收费
以及它们运行的时间 以及Apache Spark运行的时间
但我们今天要做的事情
不会造成很大的费用
可能会几美分左右
我想首先给你一个简单的介绍
我们有查询编辑器
笔记本编辑器和笔记本探索器
这两者是专有的Spark
Pi Spark和Spark SQL
我们将主要在查询编辑器中工作
随着我们的进展，我们还会看到一些其他东西
我还想来到这里的数据源
因为这里是我们可以定义不同数据源的地方
你会看到我们已经有一个数据源
称为AWS Glue目录
如果你在最后一个技能中跟随我
我们已经创建了一个目录
实际上 我想做的是
我想创建一个新的表条目
为我们的天气数据创建一个新的数据库
为我们的空气质量数据
我现在想做的是
我要去AWS Glue
我要打开一个新的标签页
我要去数据库
我要创建一个新的数据库
这是对上一技能的回顾
我们将其命名为空气质量
这是我们处理的数据
我要创建一个数据库
现在我们有一个名为空气质量的逻辑实体
我想添加一些表，为了做到这一点
我要使用爬虫
我要创建一个爬虫
我们将其命名为空气质量爬虫
它正在问我们
我们已经将我们的数据映射到glue表了吗
还没有，我们还没有 我们将添加一个数据源
我们的数据源将是我们过去使用过的s3桶
这是我们过去使用过的s3桶
我们将深入斯科特·普莱彻
数据工程师 我们将使用这个拉斯维加斯作为我们的来源点击
选择爬取
所有子文件夹
我们只想保留其他所有东西
添加数据源，现在我将点击下一步
我必须选择一个角色
我将选择这里这个角色
应该可以
好的 现在我们被要求将我们想要输出的信息放在哪里
我将选择空气质量表
我们将保留其他所有东西
我将保留在这里按需下一步下一步
创建爬虫
现在爬虫创建完成后，是时候运行爬虫了
所以我们点击
运行爬虫
我们可以在这里看到
它已经开始运行
它将出去完成它的工作
与此同时 我将回到这个标签
我想向你展示一些我们可以使用的不同数据源
当然有aws glue目录
这是我们自动创建的
我们有apache
hives dynamodb
cloudwatch metrics
redshift neptune
还有很多很多很多
oracle microsoft
sql server
big data or bigquery
teradata
redis
ibm db two
如果我们想要这样做，我们可以使用lambda创建我们自己的连接
你可以看到有很多不同的选择
我们将继续使用我们的aws glue数据目录
所以现在我将等待
哦 时机正好完成
所以我要去数据库这里
进入我的大气质量数据库
这是我的表
现在我已经索引了这个表
它已经带来了一些信息
它做得相当好，试图确定这些数据类型
它说这些是大整数
这些都是双倍
值是双倍
没错 现在看起来都很好
你还看到这是年份和月份
我将在下个视频中讨论这一点
这与分区有关
但目前
我只是想向你展示如何使用雅典娜的基础层
所以我们要去查询编辑器
我们回到了雅典娜控制台
我要清理一些东西
现在 我们需要做的第一件事是
我们需要指定一个存储我们结果的地方
为此，我将点击编辑设置
然后我将被要求选择一个地方
我想存储我的雅典娜查询结果
我将不再使用我的桶
我将创建一个新的S3存储桶
因为开始变得杂乱了
我将创建一个新的桶来存放我的athena结果
所以我将回到我的s3控制面板
创建一个桶，我将其命名为结果
添加一个athena结果
我将保留所有默认设置
然后我将回到定义我们结果位置的地方
我将浏览并看到我的桶
我将选择它并选择它
所以我将选择它并选择它
现在我有了加密结果的选项
或者我可以从一个不同的账户获取一个桶
这有点方便
如果你在做跨账户类型的汇总或聚合
我也有将结果发送到另一个aws账户的选项
这有点方便 如果你在做跨账户的聚合或汇总
但我只想保存这个，现在我们的需求已经得到了满足
所以我要回到编辑器
我们将做一个基本的查询
然后在下一期视频中，我们将做一些更有趣的查询操作
我们可以选择我们的aws数据目录
作为数据源 我们在这里选择了数据库作为质量
在这里你会看到
它已经引入了我们的表
所以我们有一个表在这里
它说拉斯维加斯
现在我们可以做的是来到这里
我们可以预览表格
如果我们想要的话
并且它将在这里做
它将在这里粘贴一些sql代码
如果我们向下滚动
我们可以看到我们的结果
所以这里显示的是我们在队列中的时间
我们有运行时间
实际的查询运行时间
以及扫描的数据量
我们可以在这里转到查询统计
它将向我们展示基本查询运行方式的分解
它只花了1.1秒
并且花了6%的时间来排队
然后预处理和计划执行方式
然后执行并将结果返回给我们
我们还可以做的另一件事是点击这里的解释按钮
这将打开一个新的窗口
如果你熟悉数据库
通常情况下有一种方法可以让数据库解释
它是如何尝试满足查询的
这正是我们所要做的
我们可以向下滚动
我们可以在这里滚动
我们可以看到它进行了表扫描
限制返回的数字到10
最终将其返回给我们
这就是基本的一
零一的查询操作
使用athena进行查询 而不是查询这个表
请记住
这不是一个关系数据库表
这是存放在s3桶中的数据
在下一期视频中 我们将深入探讨一些这些查询
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/050_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p50 4. More Advanced Athena Queries.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 现在我们在这里我们的查询编辑器
我们有一个我们正在处理的表在这里
如果我们有任何视图
我们将在这里找到视图
这些基本上是您在关系数据库中期望视图将是什么
顺便说一下
我们可以从这里的查询创建一个视图，我们将在几秒钟内这样做 但现在让我们简单地玩弄这个数据
我们将这样做
从拉斯维加斯选择明星
你会发现它有一种自动完成功能
月份等于十二，这有点方便
我们可以运行它
如果我们向下滚动这里
我们可以看到该查询用了99毫秒，接近一秒钟
我们向下滚动
我们可以看到它为我们返回了11000项
我们滚动到最右边
因为我们说了嗨
在第十二个月给我们所有的东西
你可以在这里看到第十二个月
我认为我们没有
有一些好的 有一些18-20 18s
所以基本上数据库中的任何东西或者这张表中分配给第十二个月的任何东西
所以我们可以做一些别的事情我们可以这样做
我们基本上拥有大多数SQL命令
所以我们可以做一些像这样的事情
让我合并一下
所以选择计数作为值从拉斯维加斯超过100
Where参数等于
Pm 25并且值大于100
所以我们可以到这里并运行
并且这将最终返回
并且该特定查询用了7秒来运行
在这里我们有值超过100
因此这告诉我们在这个表中我们有10个值超过100
所以这ppm 25读取有10次发生
其中它大于100
现在我们也可以这样做
所以选择拉斯维加斯的星
Where参数等于pm two five按值排序
降序限制5
所以这将要做的是，它将选择前五个值
其中pm two five是最多的
所以它将从最高价值开始
然后按降序到5条记录
所以我们运行它
并且它为我们返回
如果我们滚动到这里
它显示的是我们有一个读数为175
这是在2021年8月
166
154等等
所以我认为你明白了
特别是如果你熟悉SQL
你应该熟悉SQL
因为这是在第一个视频中提到的东西之一
我说过你可能需要知道一些SQL为此课程
我希望这一切对你来说都讲得通
我鼓励你随便玩
我必须说，这里续集的情况与其他平台的续集不同
或者一些警告和类似的东西
大多数主要命令将起作用
有一些事情
一些保留字就不起作用或不适用
但总的来说
大多数主要选择爪
按组排序
通过那些类型的事情
这些都现在差不多起作用
我也说我们可以从这里创建一个视图
所以我们可以使用像这样的命令
创建视图拉斯维加斯pm二五为选择自拉斯维加斯
Where参数等于pm二五
所以这将为我们创建一个视图，位于这个表之上
这将只限制我们到
Where参数等于pm二五
所以我们将运行那个
就这样
所以我们的视图是成功的
你会发现在这里在我们的小列表中
我们现在有一个视图
如果我们扩大它
它看起来与我们原始的表格这里一样
实际上如果我们这样做
选择星号从拉斯维加斯
pm 2 5并点击运行
那么它将返回一些结果给我们
如果我们滚动到这里，我们就做到了
所以 所有这些项目在这个特定的视图中都在pm two five
那就是一些 这是一份特定的空气质量读数
思考这意味着颗粒物
那就是两点
五微米 或者类似这样的下一个
你认为在这里我们可以做什么来删除
当参数等于pm twenty five时
这样理论上应该为我们删除一些行，对吧？嗯
不，不
我们不能那样做
因为athena实际上只是一个覆盖层
我们的真实数据实际上存储在s3桶中
我们对原始数据的控制程度并不像在关系型数据库中那样
如果数据存储在像关系型数据库中，我们可以做什么呢
我们可以绝对删除这个视图
如果我们决定不需要那个
我们可以删除它
所以我们可以执行删除视图 操作
拉斯维加斯 pm two five，运行那个，好的
现在我们的视图不见了
你认为我们能删除这个表吗
当然可以，我们绝对可以删除这个表
因为这些是逻辑实体
这些并不是真实的表
它们只是某种元数据的集合
这就是使用亚马逊athena的基本操作
只是为了查询基本的数据
在下一个视频中 我们将深入探讨一下 并探索一个称为分区的东西
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/051_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p51 5. Athena Partitions.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 这里有一个有趣的小技巧，关于雅典娜的
我想我们做的是实际上进行一些大规模的查询
我在这里
让我们看看能不能扩大它
这是开放AQ测量
这是一个表 我们定义它是一个外部表
这意味着数据在某个地方，我们的位置是开放AQ数据存档
这是存储开放AQ数据的地方
我们在这里做的事情是
我们指向了我们获取其他开放AQ数据的原始仓库
我在这里想做的是
我想去掉这里的这些
我只想保留位置ID，年份和月份
这将做
它指向这个s3桶，远远的地方
在某个人的帐户中
我们可以对其进行查询
我将运行它
这已经完成
如果我们展开这里
我们看到我们的开放AQ测量表
它也已经分区
我们现在可以看到
让我们在这里做
如果你记得我们到目前为止所做的关于填充这些表和查看它们
你认为如果我们这样做会看到什么
让我们看看你是否正确
嘿
零记录 是的
因为我们必须添加一个分区
要做这个
我们可以使用alter table
所以我将使用这个命令
让我使用这个窗口
这就是我们所做的alter table open
Mq测量添加分区
年份2020月份01位置
7711
如果你记得我们选择数据的时候
这是我们的拉斯维加斯位置
所以我可以运行它
如果我回到我的查询并点击运行
我现在有数据
实际上 我有2192行
你可以看到，这些都是来自七七一一的
我们有所有数据
来自2020年1月
所以现在你可能说
嗯 这有点麻烦
我为什么不能添加所有这些东西
嗯，如果你使用这个特殊命令，这里
这将加载所有数据
我说的是所有数据
所有来自每个位置的数据
如果你记得，外面有很多位置
可能有几个太字节的信息
我们没有理由需要访问任何数据
目前我们没有需要
我们主要感兴趣的是位置
ID 7711
因此，对我们来说，限制自己
仅仅在那 如果你需要为每个这些添加表分区
每年、每月和每个地点
嗯 这会变得非常繁琐
我们可以把这些堆叠在一起
我们可以这样做
这在做什么
它指定了年份
但它也指定了月份和位置ID
所以我们继续
如果我执行这个
它将加载所有这个东西
因此我们将能够访问来自七个位置的所有数据
二零二零年全年七一
哦
分区已经存在
哦，它是告诉我这个的原因是因为我已经加载了一个分区
实际上，我已经将分区加载到这里了
所以它告诉我，嘿，艺术存在
所以我只是取出那一个
我将重新运行它
它将为我添加所有这些其他分区
哦，它实际上为我加载了所有其他分区
我怎么知道呢，我可以在这里查看详细信息
在这里查看表格详细信息
在表格下
在开放AQ管理下
进入分区
这就是我们所有的分区，我们都可以使用
尽管它给了我一个错误消息
它在第一个已经存在的那里给了我一个错误消息
但是继续进行 并加载了所有其他内容
我们现在应该可以使用的东西
基本上是2020年的所有内容
我们正在开放一个q
搞定了
让我们看看那里我们查看的内容
进行计数
所以20 两千零六十六
四十或四
那就是数据
让我们检查一下
因为我们的克隆可以访问所有年份
我必须要做的是
限制到仅仅一年
搞定了
我们有22664,这匹配我们从源数据提取的
这是某人的s3存储桶
是的 它匹配,所以我们的数字匹配
所以我们可以大规模管理
但我们只需要正确的工具
以便能够高效地做
所以这基本上是一个亚马逊athena的巡览
我强烈鼓励你去探索
从s3存储桶获取一些数据
或者你可以使用我们在这里使用的数据
这些都是公开可用的 只是去玩
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/052_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p52 6. Partition Projections.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 在上个视频中，我们讨论了分区的好处
现在 如果你无法创建分区会发生什么
或者你的数据不适合分区
那么 让我们看看类似的东西
所以，我们要做的就是
我们要创建一个全新的表
为了做到这一点，我想要获取我们现有表的创建语句
拉斯维加斯 并且要做到这一点
如果你熟悉sql
这里有一个命令我们可以使用
它将向我们展示创建此表的语句以重新创建此表
我将运行该命令，然后向下滚动
您可以看到它创建了外部表和这里的模式
按年份月份分区
以及这里的所有事情
这很好 所以我要复制那个，全部复制，好的
现在我要做的是
我将为这张表格创建一个新的名字
所以我们不想覆盖我们的表格
我们真的无法那样做
如果我现在执行得正确
我会给我报错并说嘿
假拉斯维加斯已经在外面存在了
所以我要说拉斯维加斯
我们将会说
我们将其命名为克隆
所以这本质上
与乌鸦爬行者构建或形成的结构相同
当它出去询问我们的数据时
它决定要创建这种格式
所以我将使用相同的格式
我将点击 运行那里
这是我的表格
它已经被创建
它被分区了
万岁
这样很好，因为它指向数据的同一位置
那么我应该可以查询它了
让我们看看
等一下
返回0条记录
这是怎么回事呢
问题在于当我们有一个分区表时
它不会自动将所有数据暴露给我们
它想让我们对如何选择数据进行选择
它现在正在帮我们一个忙
如果我们在这里使用爬虫并爬取该表
它将会添加所有那些分区
例如 如果我去表这里并这是拉斯维加斯
如果我在这里去分区
你可以看到嘿
它已经添加了所有那些分区和所有数据并将它们与该表关联
但这并不是情况
不过 如果我看看我的拉斯维加斯克隆
如果我去分区
你可以看到没有可用的分区
所以可以这样想
就像它在关联那些数据
所以它是可用的供查询
如果你在想
你知道 这些巨大的数据源它们以TB为单位
那么这是好事
因为 这使你在选择时更加有意识
你选择供你查询的数据
你可能说
我想要所有数据供我查询
你可以那样做
所以你可以做一个像这样的命令
这是一个特殊的命令
当我运行这个
这将会预加载或拉取我所有的不同分区
并将它们与该表关联
所以我可以继续运行
并且它会加载所有数据需要一些时间
并且它会在这里给我一些结果
它将会说嘿
这些分区不在我们的元数据存储中
换句话说它们没有被关联为可供我们查询
所以它所做的是
它将会添加所有那些
所以 如果我回到这里
并且让我们让我们打开一个新窗口这里
我们将做一个计数
运行并且我们完成了
这就是我们的魔法数字
这是我们在原始表中看到的数字
所以现在所有数据都存放在我们的元数据存储中我们可以对其进行查询
我并不真的需要那样做
我想向你展示一种不同的方法
我们可以如何做这件事
实际上我认为这更有效率
因为而不是加载所有此类信息
你需要在这里做的事情是
如果这个数据源发生了变化
例如 有一个定期的任务去那里添加新数据
让我们说
我们需要在这个任务运行后运行这个
以确保数据可供我们的查询和这里的athena使用
我真的不想那样做
我更愿意让athena
稍微聪明一点
按需
去那里找到自己需要的数据而不加载大量的此类信息
现在，这里有一个缺点
一旦我加载了所有此类信息
我可以绝对删除这些分区
我可以删除这些分区
但除了逐个删除之外，没有好的方法
或者我们可以编写一个小脚本使用API之类的
所以，我将要做的是
我将这样做
删除表，你可能在想
哦，天呐
他在做什么 他将删除所有数据
请记住，这不是一个真实的数据存储
这只是一些元数据
指向一些后端数据
所以实际上我可以回到这条小命令这里
我想我的结果还在这里
我将突出显示所有此类信息
我只是创建这个表
上到这里查询
粘贴
我需要做的是更改这里 我们将这个称为克隆
运行它，在那里我们做到了
现在我们有了我们的表
但由于它是一个全新的表
我们没有告诉它加载任何分区信息
如果我们选择星号从
将不会有任何记录返回给我们
所以我将要做的是添加一个称为分区投影的东西
要添加一个分区投影
我将去我们的数据目录这里，这是我的克隆
拉斯维加斯 克隆
我打算钻到那里去
清理这个地方或者最小化那个，给我们这里更多的空间
你可以看到 没有分区
我们这里有我们的模式和下面的两个分区键
所以进行这个分区投影
我将在这里的行动上到编辑表格
现在，这里很多技术术语的胡言乱语
如果你熟悉hadoop或hive或其他那些平台
那么这可能对你有意义
别太担心
我们感兴趣的是如果你滚动到这里
到这个添加按钮
所以我要做的是在这里添加一些特殊字符
一些特殊关键词和一些值
所以我要使用的第一个是投影月份类型
月份是我们的分区之一
所以我们必须声明它是一个类型
所以它将是一个整数
我要添加一些投影
范围
我将指定我想要这个特定的投影
能够从0到12的任何东西
所以这是从一月到十二月
哦
打字错误 那里
投影点月份数字现在
这是经过我的试验和错误发现的东西
一些东西并没有很好的文档
但这是这里重要的一步
因为我需要指定这个数字范围或这个月有多少位数字
这里有两个数字，所以我想使用它们
我将点击 再次添加
预测年份
这是另一个 我们的一个分区
我将使用整数类型
在这里设置范围，现在我认为我们只有两千十八到两千二十四的数据
但我可能想要做的是放一些稍微大一点的东西
这样如果未来的数据掉入我们数据存储那里在s3
不会被忽略
所以我将使用2015到2030
在未来的某个地方现在我也需要添加一些叫做存储位置模板的东西
这将做
它将把我的s3桶文件夹结构
并用它来帮助它交付这些预测
所以我想在这里
让我们出去吧
选择这些文件中的一个，我将复制该URL
将其粘贴在这里
然后我将删除
因为我不需要实际的文件
我在这里感兴趣的是
是这里的这一部分
所以我将要做的美元符号方括号月份
然后在这里
美元符号方括号年份
这将做
这将定义这里的投影，年等于年
月份等于月份，在这里
要让所有事情发生
我们必须输入这个特殊命令，启用为真
所以现在点击保存
如果我没有打错任何字
我们应该能够去我们的查询编辑器这里
如果你记得
我们刚刚运行了这个查询
选择星号从拉斯维加斯克隆实际上
我不想过于强调
不必要 我只是会用一个计数
然后运行
所以我们继续
所以奇迹
奇迹是数据源
或者我们的拉斯维加斯克隆表现在可以访问数据
这就是分区投影的工作方式
所以我们也可以这样做
选择拉斯维加斯克隆中的年份，按年份排序
如果我们往下走
这是我们可用的年份
二十八 两千九
二零二零
等等
现在 假设我们不再想使用那个
我们不再想使用那个了
我们不想使用那个分区
所以我们要去编辑表格
在这里往下走
找到那个分区启用在哪里
我们将其设置为false
保存
回到这里 再次运行我们的查询
瞬间我们没有可用的分区
所以它返回没有数据
那么我们让我把它重新打开
让我们来做个比较
让我们比较一下这种方法的分区
使用分区投影和自然分区
我们将启用它
它是真的
保存
如果我们粘贴我们在这里做的事情
选择拉斯维加斯的星
where parameter equals pm two five ordered by value descending limit five
这将找到pm two五的顶级五个值
我将让我复制这个运行它
在这里转到其他窗口粘贴更改我的表
克隆并这里运行相同的查询
让我们在这里的查询二看看它返回了什么
1.5秒
数据扫描917
如果我们在这里
我们应该看到非常相似的结果
我们看到几乎完全相同的结果
1.5秒
数据扫描917
请注意 因为我们使用了投影
我们不需要故意加载任何分区
它自己就处理了这个问题 这真的很酷
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/053_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p53 7. Athena at Scale.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我们探索一下分区
分区是一种方法，我们可以让我们的查询更加高效
很多
这里我们又回到了glue
如果我们深入研究空气质量
如果我们深入研究我们的表格
爬虫发现的一件事是，嘿
我们可以根据数据存储的方式进行一些分区
它怎么知道得很好
我们可以去s三那里，数据存储在那里
如果我们深入到我们的s三桶中
我们到了
如果你记得当我们复制这些东西时
看起来这一年等于20182020
2019 2020
当我们复制它时
我说 真的不用担心那个
如果我们在这里钻探
一月二月三月
这个数据以这种方式格式化的原因
它是以这种方式格式化的，是因为我们可以将这些用作分区，分区
将允许我们仅查询时间段
例如，我们可能只对2018年的数据感兴趣
这将允许我们仅查询位于2018年这个桶中的数据
因为我们不必担心其他任何数据
我们不必扫描其他数据
因为我们故意说，嘿
我们只对2018年的数据感兴趣
这将使我们的查询快得多，更有效率
所以让我来演示一下
为了做到这一点
我们将回到aws glue
我们将有意创建一个表结构或数据存储
它与此相同
确切的数据
这是拉斯维加斯的数据
但我们不会与任何类型的分区保存它
实际上，我们将给它一些好处的怀疑
我们将其保存为parquet格式
这 如果你记得这里的数据是csv格式
这可能是查询交互式时性能最差的格式
相反，我们将将此数据的副本保存为parquet格式
我认为这可能是查询s3时性能更好的格式之一
如果我们查询s3
为此
我们将在这里转到视觉编辑器
再次这里是一点复习
我将选择我的s三桶
我将指向
那个拉斯维加斯数据集这里，格式是csv
我将等待它这里出现
给我一点刷新
好的 这是我们的数据
所以我要做的第一件事是
首先我需要做的是改变这个模式
所以我要点击这里
我要更改模式
我要更改这个日期时间为一个时间戳
因为我需要做我想做的事情
我想要提取
如果你看数据模式
如它所示 我们没有年份和月份的字段
这些字段是自动添加的如果我们回到这里
到这个表格这里
这些下面的项目
年月已被自动添加到此模式中，从数据爬虫中获取
因为它识别出数据是如何存储在那里的
它说 我们可以使用年月作为分区键
所以它自动添加了这些
为了在这里有一个苹果对苹果
比较 我必须要做的是添加一个名为月份的字段和一个名为年份的字段
为了做到这一点
我需要将这一时间转换为时间戳
然后我将能够提取出日期或时间
提取出年份和月份
顺便说一下 我将继续将纬度和经度转换为
我们将使用双精度浮点数
这里的值也将使用双精度浮点数
我将使用大整数来表示位置ID和传感器ID
我正在尝试使这个模式与这里的模式相同
这样我们就可以至少进行苹果与苹果的比较
当涉及到查询这个数据时
所以我认为这看起来不错
现在 我可以格式化那个日期戳
搞定，格式化时间戳，我想在这个步骤中要做的是
我想选择一个列
我将使用日期时间，我先要做的第一件事
我将要做的是创建一个年份列
或者只叫这个年份
哦豁，如果我打开我的预览这里
滑到右边
这是我们的年份列
我将添加一个步骤，另一个日期时间格式
在这里我们继续，我将选择相同的日期列
然后在这种情况下，我们将选择月份
或者月mm
然后我们将此称为月份
这样我就可以滚动
哦不 等着这里刷新
我在这里可以滚动
这就是我们的月份
我们已经将我们的数据集增加了两列
年月 这将使它看起来相似
这里是这张桌子
最后我将其重新写入
并将目标写回到s3
因为我想要公平，我会给这个一个优势
我们将使用parquet
我们将使用snappy压缩
我将在这里浏览
看那里是拉斯维加斯
所以我要做的是我将创建一个
我将使用这个只是为了我可以选择它
然后我将创建一个新的条目在这里卡片称为parquet
我们将在我们的数据目录中创建一个表
在下一次运行中我们将更新那个模式
这将自动创建一个我们可以立即访问的表在这里
通过我们的AWS数据目录和我们的空气质量数据库
所以我们必须选择空气质量数据库
好的，然后是表名
我们称其为拉斯维加斯
没有分区我不会添加分区键
我可以 但我不会这样做
好的 我会给这个任务起个名字
好的，最重要的是我会保存这个
我不能告诉你
我多少次进来，做了所有这些事情
然后我回去尝试运行它
它说好的
我们刚刚完成 我们什么都没做
那是因为我没有保存
所以我要保存它
我要再保存一次
我要运行，希望如此
哦，我双击了它
所以我们应该在这里有一份工作
运行，我们继续，我们就得等这份工作运行
然后我们就去检查s3桶里它吐出来的东西
所以我只是暂停 这里的视频并等到它完成
好的，它完成了
大约用了一分钟十秒
让我们去s3看看它创建了什么
所以我要回到这里
这是拉斯维加斯的拼花地板
你可以看到它创建了大量的拼花文件在那里
这正是我们所需要的
如果我们去我们的查询编辑器并刷新
看看这个
现在我们有一个新表
拉斯维加斯
没有分区
是的
它看起来与我们已经存在的拉斯维加斯几乎完全相同
现在我们来做一些检查
我将选择计数
嗯 我们将做位置ID
我将在这里打开一个新查询
我们将运行它
我们有十二万两千八百五十五
让我们在我们的无分区数据源这里运行同样的东西
我们走吧
好的 所以我们每个数据集都有相同的行数
所以现在其中一个有数据分区或被分区
另一个没有分区
所以让我们做一些比较
所以让我们添加一些包括年份的内容
那么试试这个查询
我看看能不能扩展它
选择拉斯维加斯的星
where year等于2020年和perimeter pm 2.5按value排序
下降限制五
好的 这将获取2020年全年的前五个值
这是拉斯维加斯版本
让我在这里粘贴这个
我们将使用相同的查询来获取无分区版本
让我在这里缩小这个
我们将运行这个
然后我们将复制或移动到这里
我们将运行这个人
让我们看看差异
如果我们向下滚动
我们有时间在q 59毫秒
运行时间 975毫秒
扫描的数据167kb
如果我去这里查询2
完全相同的查询
只是这张表没有分区或没有分区
所以时间查询57毫秒
我们的运行时间是1.5秒
大约比预期长了一半
看看这个扫描的数据是564kb
现在我们处理的数据相对较小
所以不是很大
但如果你想象在TB级别的数据上
你可以基本扫描
看看这大约是20%
这个数据的25%
这个表没有分区
然后我认为这很好
所以让我们谈谈分区能做得好的地方
它允许查询优化器
只去查找符合这些条件的数据
如果我们指定年份为2020
优化器会去到我们的备份数据
如果我们在拉斯维加斯查找
它只会在这个文件夹里查找
因为已经被划分成2020年
而现在另一个查询正在做的是，因为它没有分区
它必须扫描整个数据集
因为它不知道可能还有其他2020年的记录在哪里
它们可能在这些文件中的任何地方
如果你回到这里的parquet
你可以看到只是一些文件
所以它必须扫描所有这些文件
因为这些文件中的任何一个都可能包含2020年的年份
所以现在分区的好处
正如我们在数据中爬行
AWS Glue 数据爬虫识别了数据
如果我们回到这里
这些爬虫在这里
他们确定数据格式适合分区
所以它继续添加分区
如果你的数据不是某种形式会发生什么
这有助于抓取
爬虫自动添加分区得很好 这就是我们在下个视频中将要讨论的内容
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/054_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p54 8. Validation Amazon Athena.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好吧 让我们来探讨这些问题
首先，你拥有一个存储在亚马逊S3的大型数据集，格式为CSV
你想使用亚马逊Athena高效地查询这个数据集
以下哪些步骤最能提高查询性能
我们正在寻找最能提高性能的
这些中的一些可能会提高性能
但我们正在寻找最能提高性能的
这里的第一个选项
使用亚马逊Glue将CSV文件转换为JSON文件
我不太确定这会有什么作用
除非JSON的性能比CSV好
这可能是事实 但最终，它们都只是文本文件
存储在S3中
压缩CSV文件使用gzip压缩
我想，如果我猜得没错
如果我不知道
我会说，仅仅将这些文件压缩为gzip，
只会在尝试访问这些数据时增加额外的开销
所以我不会说，期望gzip压缩会提高性能
它会提高的是它所占用的空间
这也是一个原因
下一个，将数据按分区存储在S3中，基于常见的过滤列
在S3中基于分区的数据，基于常见的过滤列
啊哈
我认为这会帮助我们提高性能
因为我们可以使用分区来获取我们需要的数据
所以我喜欢这个选项
我会继续下去，也许有一个更好的机会来提高性能
我会继续下去
因为我认为有一个更好的机会来提高性能
将数据集存储在多个S3桶中
嗯 这不会提高我们的性能
所以我会选择这个作为我的最终答案
下一个问题，你已经在亚马逊Athena中创建了一个外部表
该表引用了存储在亚马逊S3中的JSON数据
你想要检索特定JSON字段的记录，该字段的名称称为status
该字段的值为active
你应该使用哪个SQL子句来完成此操作
我们可能会在考试中遇到这样的问题
最终，这只是你是否理解JSON是亚马逊Athena的有效格式
以及你是否也理解亚马逊Athena使用了或多或少的标准SQL子句
所以你必须问自己
好的
哪一个这些标准的SQL子句将过滤行 不，不是选择
不是having
没有答案
就在这里
我希望你能够很快理解这一点
尤其是如果你有后续背景
我希望你在进入这门课程时有这样的背景
因为我假设你来这里时已经有了一些数据处理背景
下一个问题
好的 我们是负责设计和设置一些复杂的查询在一个数据集
我们将被给予的数据
数据的提供者已经询问
或者数据的提供者可以提供多种格式的原始数据并询问你更喜欢哪种格式
你更倾向于
以下哪种数据格式在使用亚马逊athena时性能最佳
现在我们有xml
我们有json
我们有制表符分隔的值
逗号分隔的值
parquet和csv
用gzip压缩
所以所有这些格式的性能
我相当确定
使用gzip压缩的CSV文件可能比纯CSV文件运行得更慢。
逗号分隔或制表符分隔
所以我现在可以立即排除那个选项
如果我在看XML和JSON的话
就性能而言，它们可能差不多一样
因为它们只是那些文件
并且我会说，制表符分隔和逗号分隔的格式差不多是一样的
这里的不同之处是 Parquet
那是不同的格式
那就是列存储格式
如果我真的不知道，我猜测
但在这种情况下，我实际上知道
我将说它是Parquet
这些中哪一个是不同的
嗯 那是不同的那个
那就是我最终的答案
我希望这对你有所帮助 我想感谢您观看
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/055_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p55 1. Skill Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，美丽的学习者，来到另一个技能
当亚马逊红移在2012年发布时
关于红移如何得名的有些争议
有一种理论是他们以物理学中的一个现象为其命名
当多普勒效应导致光的波长拉伸
当它们远离观察者时
导致它们向光的红色光谱末端偏移
现在 另一种理论
这似乎更可信
那就是它被称为红移，作为对甲骨文不太微妙的讽刺
在那个时候，谁在企业数据库市场占据了可观的份额
并且其主要营销颜色是红色
红移移向远离红色，而当红移被引入时，红移移向红色
获取任何较大规模的企业数据仓库的唯一真实方法
计划投资几百万在像teradata和natia这样的on prem平台上
现在，红移提供了一个非常不同的按需付费的基于云的模型
在当时这相当新颖
从那时起，OLTP和OLAP数据库之间的界限已经变得模糊。
多亏了像SAP这样的在内存数据库技术。
哈娜和其他动态可扩展的基于云的选项
但是作为一名数据工程师
您仍然应该了解红移，我们将深入探讨 让我们开始
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/056_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p56 2. Meet Redshift.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我们看看亚马逊红移的一些功能
所以亚马逊红移是aws的企业级数据仓库
它使用称为并行查询执行的东西工作
我们会在接下来的内容中解释这是什么意思
但总的来说，你可以选择不同的服务器配置
你可以选择集群
集群由节点组成，而这些节点
可以理解为服务器
或者你也可以选择无服务器配置
我们将在这门技能中专注于无服务器风味
因为AWS确实在推动这一点
而且这更经济实惠
这也是好事
当我们谈论Redshift的集群版本时
这意味着由节点组成的
我们必须选择哪种类型的节点
我们有一个选择叫做三节点
这将给我们提供管理存储
换句话说 我们不需要预置我们需要多少存储
它将根据我们需要的存储量自动增加存储
或者我们也可以预置称为dc two units的东西
这些是为计算优化的
它们被设计用于高性能情况
因为它们有本地连接的ssd存储
所以这就是集群的样子
首先，我们有一个领导者节点，就像头号人物
我们称之为领导者的东西 它是整个集群的头头
然后，在那位领导者之后，我们将会有一到多个计算节点
那么，发生了什么呢？查询在这里发送到领导者
领导者说
好的 我将会把这个分成几部分
你分得25%，你分得25%
你去获取25%，你去获取25%
像这样 然后每个计算系统都会尝试执行自己的查询
一旦它返回查询的一部分
它会将结果发送给领导者
领导者会将所有结果汇总
然后返回给执行查询的人
所有这些都在后台进行
对我们来说
我们只需要 可能像jdbc或odbc这样的连接
让我们的数据分析工具能够连接
我们只是发送查询，而这些查询返回得相对较快 现在我们进入控制台，实际使用红移做一些工作
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/057_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p57 3. Creating our Redshift Instance.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 现在我们来玩玩红移
我们在aws控制台外面
你可能想在离你最近的区域
因为红移是区域特定的
要得到那里 我们可以在这里搜索红移
或者因为我最近访问过它
它在我的近期访问中
我现在就点击它
如果你是第一次访问红移
你可能会看到这样的页面
我们要利用的一件事
是现在尝试红移服务器免费
因为原因在这里
如果我们向下滚动
我们有定价信息
我想鼓励你 就像我对每个服务做的那样
请出去
确保你了解
运行该服务将要花费你什么
但在我们的情况下我们有这个三百美元的信用
只要我们第一次设置红移
在任何你用的账户中
我们将获得这个三百美元的信用
我们不会为我们要做的事情花费任何东西
但现在如果我们在这里点击小汉堡图标
如果我们去集群
这是我们设置一种正常的传统红移部署的地方
传统的红移部署由服务器组成和集群服务器
如果我们向下滚动
我们可以看到我们有创建集群的选项
如果我点击它
现在我可以给它一个集群标识符
它将帮助我选择集群的大小
让我们说 例如
我将选择一个较小的大小
你可以看到它输出价格
每月365美元运行这个两节点的dc
两个大型集群
我可以容纳高达320GB的存储
我不知道你
但我真的不喜欢
花费365美元在这里学习红移
事实上 如果我们看看一些其他尺寸
如果我们去r316 x large
去这里
如果我们进行多可用区设置
你可以看到我们每月需要花费三万八千美元
但我们可以容纳高达250
六太字节的存储空间
现在，你可能在想，哇
这花了很多钱
如果你将这与某种本地解决方案进行比较
比如nsa或teradata
这些数字也非常巨大
因此，许多公司发现，能够按月支付数据仓库的费用
并使用红移，实际上与这些其他选项相比非常经济
我们不会处理集群
我们将直接转到红移无服务器
如果你看到红移服务，它是相对较新的
但aws确实在推广它
因为它是无服务器的
我们不需要管理任何服务器，它是按小时计费的
所以，这里是红移无服务器
现在我们可以设置一种默认入口
但我将要做的是创建一个预览工作区
事实上，我将创建一个预览工作区
由于我们启动了这个，我们将被授予300美元的信用额度
用于红移无服务器 请随意尝试并使用它，直到你满意为止
只要你注意这个信用消耗
我会向你展示如何检查它
所以我将点击预览工作区
关于红移无服务器的两件事，一个是工作区
另一个是命名空间
工作区就像它说的那样，只是一个资源的集合
允许我们与底层命名空间交互
而命名空间是我们的数据实际存储和组织的地方
对于工作区名称，我将其命名为预览
然后向下滚动，它将询问我关于性能和成本控制
我们可以设置基础容量性能
或者使用价格性能目标
这将给我们提供一个滑块
如果我们对性能非常感兴趣，但我们会牺牲一些成本
我们可以将这个滑块调到最高档
如果我们对性能不感兴趣，并且更关注成本
我们可以将这个滑块调到最低
如果我们对性能不感兴趣，并且更关注成本 我们可以将这个滑块调到最低
如果我们对性能不感兴趣，并且更关注成本
我们可以将这个滑块调到最低
如果我们对性能不感兴趣，并且更关注成本
我们可以将这个滑块调到最低
如果我们对性能不感兴趣，并且更关注成本
我们可以将这个滑块调到最低
如果我们对性能不感兴趣，并且更关注成本
因为我们处理的数据量非常小
我们不会从这种优化的性能中获得任何好处
所以我要把它降到最低
我们也可以指定我们的最大红移处理单元值
这就是红移无服务器架构的构建方式
每红移处理单元每小时3.8美分
我们不用担心这些
现在我们可以简单地使用
IPV4，没问题
这可能是我的默认设置
Vpc Yep That's right
It is my default Vpc
It is selected a security group there
It has selected the default subnets
All that stuff is cool
If i was more interested in securing this and creating this as a true enterprise deployment
I would probably create a specialized vpc and specialized subnets
所以我能更好地保护它
所以我要点击这里
它会说
嘿 我们需要一个命名空间
嗯 什么是命名空间
嗯 你可能在过去与一些数据库互动过
他们使用术语命名空间
这基本上是同一个概念
基本上这是一个存放你模式(schema)的地方
所以这个命名空间将会是
我们的表、视图和存储过程
以及所有其他东西都将存储在这里
记住这不是athena
这是一个真实的数据库，包含真实的数据存储
所以这就是数据将被存储的地方
我将其命名为预览
我们在这里被要求输入数据库名称
你可以看到它在开发中默认
这样做的原因是因为我们选择了使用预览组
我选择预览组的原因是
因为它将给我们提供一些高级功能
但这些功能可能会很快在生产或一般可用中部署
我想确保向你展示这些功能
我们将使用dev
因为aws在生产中不支持预览工作组
所以我们只留下它
现在轮到我们定义我们的角色
我将在这里创建一个IAM角色
我将指定我的桶
这将给Redshift提供访问任何我想要使用的桶的权限
我将选择这个数据桶
这是我们迄今为止一直在使用的那个
我将点击创建
它已经自动为我们生成了一个角色
我想这里其他都是好的
我不会进行任何加密
点击下一步，好的，一切都看起来不错
点击创建
现在它将创建我们的无服务器Redshift实例
所以我们只需等待，通常需要约五分钟
我将在这里结束视频 我们将从Redshift无服务器实例可用的那一刻开始下一个视频
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/058_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p58 4. Querying with Redshift.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们来看另一个功能，叫做笔记本
如果你猜测这是对jupyter笔记本的实现
那么你完全正确
那么，让我们来试试 我们在这里点击这个小加号
而不是编辑器
我们将转到笔记本，这就是我们的笔记本
这里有两个选项
我们有sequel和markdown，我将在这里添加一个markdown
我将要说
让我们
好的 所以我要做的是在这里输入一些markdown
创建一个临时表
这些笔记本很有用
如果我们想要转换数据并解释我们做了什么
以便下游的人可以也许使用相同的笔记本并分析数据
所以这里有一个SQL语句在这里
我将要做的是首先使用drop
我想做的是我
如果那里已经有一个临时表
那么我想删除它
如果没有我想创建它
所以我只是想主动删除表
临时表 A q
两千零十九
然后我将创建表
临时表
你可能在问自己
什么是临时表
嗯 它是一种像想象中的桌子一样的表
它并不存在在我们的模式中
是人们使用的
有时候 如果他们在操纵数据
他们想要一个临时的区域来处理它
这在大多数主要数据库引擎中并不是一个不常见的功能
所以机会很大，你们可能听说过这个
或者你们的数据库
我们如何创建一个与选择相同
好的 所以我们要做的是
我们要从2019年选择所有记录
我们将它们放入临时表
我在这里放drop table的原因是
当你通过一个笔记本时
有时候你必须回到开头并重新开始一切
这就是我们在这里做的事情
你可能在这里注意到的一件事是我们将我们的查询放在这里
但我们无法访问运行良好
那是因为我们还没有建立连接
我们可以双击这里
我们需要一个联邦用户
创建连接
只要我们在这里连接到我们的数据库
我们就可以访问运行
我将继续运行并腾出一些空间
所以我们这里有一个小区域
它正在显示我们的进度
我们有结果一，结果二，结果一在这里作为这个项目
结果二在这里
这两个看起来都很成功
好的
让我们把它放在这里
现在我将添加一个markdown
我们将其命名为
我们将添加一个sql步骤在这里
好的 这在做什么
我们将首先获取这个表中的行数
然后我们将删除表中所有不等于pm two five的行
然后我们将获取另一个计数并查看还剩下什么
所以我点击运行并这是我们的结果一
我们开始时有22000行
这看起来已经完成
是的
如果我们去看结果三 我们降到了75
26
所以删除了所有不是pm two five的行
所以我将删除它并关闭
我将添加一个markdown
因为我们需要对这些进行解释
好的
所以在这个单元格中，我们将查看rpm25的月度趋势
好的 所以在这个语句中，我们将选择月份和平均值
20个阅读和sql语句
好的 这个语句将选择月份和平均值
在每个月
我们将按月份分组并按月份排序
所以我将运行它 如果我向下滚动到我的结果
我有一月到十二月
这里有平均值
现在
就像我在编辑器里可以做的那样
我也可以做图表
所以我可以做月份和平均值
再次 这里有个奇怪的bug，没有这个更新
如果我刷新
我的笔记本回来了
我所有的值都回来了
好了
所以我要关闭这个
这就是我的值
所以我可以 然后到这个点我可以保存这个
保存这个版本
如果我想要
我可以复制它
我可以导出它
这将下载这个python或jupyter笔记本
然后我可以与别人分享
或者我可以到这里的笔记本
我可以与别人分享这个笔记本
所以他们可以加载它
他们可以一步一步通过我的代码
他们可以看到 好的
这里有解释 这里有实际做那件事的代码
所以你可以看到这是一个非常有力的工具 这就是使用笔记本
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/059_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p59 5. Charting Features.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们已经在红移上玩了一段时间了
我想花点时间向你展示如何跟踪
你消耗的RDPS或RP的数量
这是如何AWS跟踪红移无服务器成本或费用的
我们可以在这里找到我们的仪表板
我们有我们的仪表板
我们会在这里找到我们的总计算使用情况
我会选择预览
它会显示我最近一小时消耗的情况
这里有一个警告是实际上查找这些东西
会消耗RP
所以我不会经常这样做
事实上
如果我们向下滚动
你可以看到我正在消耗我的自由试用
到这个时候你可能不会消耗这么多
我在幕后做了一些其他事情
准备一些数据集等等
但这是你需要关注的事情
你可以免费使用这300美元的信用
但一旦你达到那个点
那么你就需要自掏腰包
如果你不想那样做
那么就确保不要使用它并删除你所有的东西
这个视频结束时
我不是说这个视频的结尾
我会向你展示如何基本上将一切都恢复到原来的状态
我们可以向下滚动
我们可以看到我们的使用容量
我现在不会花太多时间在那上面
我会回到我们的编辑器
并向你展示我们在查询编辑器中可以使用的一个功能
那就是图表
我们可以用数据制图
我将连接到我的命名空间
我将使用联合的用户
当然很好，我应该能够连接
我们走吧，好的
所以现在如果我看我的方案
我有一个表
我有一个视图
这就是我们的表在这里
我想做的事情是，我想获取一个列表
我想看看是否有某种模式
与一天中某些污染物读数的时间
为了做到这一点
我想看看这一时间
然后提取出小时
并获取每小时的平均读数，时间跨度为两到四小时
这就是我将要使用的查询来执行那件事
让我在这里带你走一遍
所以我们有选择日期部分小时
所以日期部分是一个函数
这是一个内置函数
它将查看这日期和时间
你会注意到这里日期和时间是一个字符
它不是一个时间戳
如果我们可能正在做完美的事情
我们将可能回到我们的数据目录
我们将更新那个
我们将刷新我们所有的分区
然后希望这将作为一个真正的日期时间数据类型
但这次我不会那样做
我将要做一些 那是有点懒惰的事情
我猜 但我只是想要展示我们实际上可以使用一个关键词cast
所以我们在这里将日期时间作为时间戳
因为这里这个函数只与时间戳的数据类型工作
它将从这里提取小时从这个数据类型时间戳使用日期时间
并将它作为字段或列称为our返回
我们也将得到平均值作为平均值
从这里从这个表
其中参数是pm 25
年份是2022年
我们将按小时分组按小时排序
所以我准备好执行这个
并且它去了
所以这花了大约220毫秒
让我腾出一些空间
好的 所以这是我们一天中的时间
然后我们有平均值
所以它从零到23
这里是平均值
这很好
但如果我们能有一个图表
那该多好
我们可以这个按钮在这里
我们点击它
现在我们可以有一个基于特定数据的图表
所以我选择线在这里
我们有不同的选择
派漏斗区域等等
所以我选择our作为x轴和平均作为y轴
这里是这个工具的一个特点
我不知道为什么它这样做
我在几个不同的浏览器中尝试过
但我在chrome中运行
我更新它，但什么也没发生
这里屏幕没有更新
所以我最终不得不刷新屏幕
然后一旦它回来
然后把它放在那里 再做一次图表，并且我们将做小时和平均
我们走吧，所以我不知道为什么它会那样做
而这是那些事情之一
如果你正在观看aws他们自己的训练课程
他们可能会悄悄地把这件事扫到地毯下
你会一脸懵逼
问我我做错了什么
嗯 我想在这里保留这个问题
因为我想展示一个事实，是的
我们在UI中有这些怪癖
你可能会遇到它们
所以这里我们看的是
这里是工作时间
这里是平均读数
所以我们可以点击这个并说平均值平均读数
在这里我们可以点击这个x轴并说一天中的小时
我们这里有其他选项
我们可以更改样式
我们可以更改颜色
使它成为不同的颜色
使它成为不同的字体
那种事情
现在 一旦我们满意了，我们可以做
我们可以保存这个图表
让我们说这是2022年的空气质量
我会保存它
现在这样做了
这保存了这个查询和图表
所以我的同事可以回到这里
他们可以去这里这个小图表
他们可以说嘿
我们有一个图表，我可以和他分享
如果我想要的话 所以我会双击那个
它拉起图表
如果我想再次运行查询
以更新图表
这正是我要做的，所以
点击运行并更新它
这里我们又遇到了这个错误
我不知道为什么它会这样
但如果我刷新
它将重置UI并让我回到图表
让我重新创建我的图表
再次
我不知道为什么它会那样做
但它确实会那样做，所以不管怎样
这正在创建图表
让我们让事情更有趣
所以 而不是只做一年
让我们做几年
我将删除这个
删除年份限制组
按年份小时排序
我将在这里添加年份
这样您可以看到什么我们在做
我们将有一个数据集
说年份 一天中的小时
以及我们的平均读数
让我们运行这个
哦，一件事
我会叫出这个
因为我应该注意到它
但你可能不会注意到
这是一个重要的事情
您将看到我们这里有一百条记录
您会说 看起来我们什么都有
但我们受到一百条记录的限制
我们需要禁用该限制一百
如果我们再次运行它
您将看到 然后我们有一百四十四条记录
这大约是我们应该有的数量
这就是您需要寻找的东西
这就是我们在这里的年份小时，然后是我们的平均读数
所以我们可以到这里的图表
我将这些对齐
看看我们的平均值
是的
没问题 再次
它并没有更新，顺便说一下
不要点击这个
这不是刷新
这是重置
它会清除您的所有内容
这是我必须以硬方式学习的事情
所以我们选择
我们的线条
哦 我甚至不能重新排列这里
选择那里的平均值，就这样
当然，我不得不刷新，出于某种奇怪的原因
回到这里的图表
是的，当然，好的
所以平均值
我对年份不感兴趣
我对现在感兴趣 在这里我们有所有数据集
但这并不很有帮助
因为我们真的不知道我们在看什么
所以我们可以到这里的变换并变换
它允许我们在这个图表上加入额外的层
我们可以过滤，分割，聚合和排序
我将分割并按年份分割
那样做的话
它将为每一年创建一个线条
我们可以看到在2019年，那些似乎有最低的集体平均读数
而2021年似乎有最高的集体读数，特别是这种污染物
这就是如何在Redshift Query中使用图表
我们可以保存这个
我们可以将其导出为png或jpeg
或者类似的东西 我们可以放大并查看它
我们可以注释它
我们可以调整样式
调整图例
所以你在这里有很多灵活性
这是一个相当不错的小功能 在下一个视频中 我们将看看另一个不错的功能，叫做笔记本
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/060_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p60 6. Working with Notebooks.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 现在我们已经完成了无服务器实例的全面启动
它现在完全活跃
如果我们在这里
预览工作组活跃 在上面显示我们已经成功创建了预览
并附加到预览
所以我认为我应该使用不同的词汇
而不是在整个板上使用预览
但无论如何，现在它就是这样
我们可以查看使用情况
在这里我们可以查看使用情况
预览是我们的工作组和命名空间
我们可以选择最后1小时或最后3小时
如果我们做了任何事情
但我们现在还没有做任何事情，这里有一些使用情况
而这很重要，因为无服务器红移是基于此构建的
如果我们在这里转到免费试用
这个小盒子给我们
这是我们获得的300美元信用
并将向我们展示我们是如何消耗它的
随着我们现在使用它
我要指出的另一件事是
因为这个命名空间实际上是一个数据存储
它有快照的概念
我们可以备份这些数据
事实上 如果我们在这里转到数据备份
我们可以创建命名空间的快照
如果你关心你的数据
你可能需要一个数据备份计划
但在这种情况下，我们将使用一些假数据
以便登录到我们的系统
我们将在这里转到查询数据
这将打开一个新标签
只是看看这个新标签
这里有红移查询编辑器版本2
我们有查询
笔记本
图表 历史记录
和所有其他东西
默认情况下，你可能会有这个深色模式
我不喜欢深色模式
这对我来说很难阅读
所以我将切换到亮模式
因为它记得我喜欢亮模式
所以它以亮模式为我保留了它
这个小齿轮也将允许我们编辑一些东西
我们可以放大我们的字体
如果我们想要 我们可以使用代码或代码补全
显示建议
我们可以更改我们的缩进大小
如果我们想要 我只会留下所有这些东西
现在你会注意到这个小型无服务器预览
这是我们的实例
或者这是我们将要玩的地方
有点灰暗
它被灰色高亮的原因是因为我们还没有连接到它
然而 要连接到它
我们需要点击它
我们可以选择不同的连接选项
我们可以使用IAM身份中心
我们可以使用联合会员
我们可以使用数据库用户和密码
或者我们可以使用秘密管理器
我就用联合会员
我会建议你也这样做
我们将创建一个连接
这将把我们连接到这里的命名空间
现在在这里的命名空间下
我们有一些样本数据，这是aws为我们预填充的
你可以随意玩
我们有dev
这是我们数据库的名称和public
这是我们东西的模式
那是什么意思？
这就是我们的表、视图、函数和存储过程实际存放的地方
你现在可以看到
我们现在那里什么都没有
但是看这里 aws 数据目录
这是我们的老朋友 数据目录
默认情况下自动
它已经从数据目录中提取了任何类型的表定义或数据库定义
所以我们可以展开空气质量这个数据库
我们可以展开这个 我们可以看到 aws 数据目录中的四个表
这就是我们一直在处理的表格和一些其他技能
我们可以从这里直接访问它
我将在这里打开一个编辑器并从aws数据目录中选择star
空气质量.拉斯维加斯
我将其限制为5行
我们成功了，它已经连接到我们的aws数据目录
它说：嗨
这是那张表格的定义
数据存储的位置
它出去获取数据
所以，因为我们可以访问它，我们也可以将我们的数据库中的数据复制进去
这里的数据并不在红移中
尽管我们可以访问它
这里的数据并不在红移中
所以我们不能做
或者我猜我们 我们在处理它的数量上受到限制
所以现在我要做的就是
我将复制这些数据并在我们的命名空间中创建一个全新的表
好的
那么这个查询会得到什么呢
它将在这里创建一个表，对吧
选择aws数据目录中的星
空气质量 拉斯维加斯
它将从这个表中选择所有内容
然后将其放入此表中
所以我要点击 运行
这将需要几秒钟来运行
花了2.4秒
所以这相当酷
现在我们可以过来看看
刷新
好了 这是我们的表格
如果我们点击这个
你可以看到它已经正确地填充了数据类型
因为我们花了时间
实际上我们让爬虫出去花费时间去获取正确的数据类型
所以它为我们拉取了所有那些东西
如果我们做类似的事情
选择 计数
并运行那个
如果你记得
如果你不记得 那完全没问题
但我们有十二万两千个
八百五十五行在这个特定的数据集
这正是我们在这个数据集中所拥有的确切数量
这告诉我所有数据都成功复制过来了
所以现在这个数据现在在我们的数据库中
它是一个数据库中的表
它也占用了空间
我认为它只占用
少于一个GB吗
我认为它是800MB或者类似的
但它并不是很多
但现在我们可以对它进行操作
例如，我将使用这个命令
我将关闭这个小于100的限制
它会做什么 如果你只是在四处摸索
在设计你的查询
它将限制返回的行数为100
根据你使用的SQL语句
它可能不会产生任何影响
但它可能会
如果你对确保获取所有成果感兴趣
你可以关闭它
我们还可以打开解释
这将尝试解释数据库如何执行此查询
这是有用的 如果你正在尝试优化查询等
这里这个语句将拉取top5参数的值pm 25
我们将运行它
好的
你会注意到返回的时间大约为31毫秒
当我们在athena运行相同查询时
它通常可能需要大约900毫秒
也许整整1秒
但这里返回的时间为31毫秒
这就是redshift的一个优点
因为数据是本地的
所以它的性能更高
这不是一个非常复杂的查询
但你可以看到差异
我们可以滚动到这里
如果你记得我们在之前的技能中玩的信息
这些是最值前5的值及其对应的年份和月份
另外
我们可以做的是
我们可以创建存储过程
我将打开一个新的编辑器
我将在这里键入
好的
创建或替换过程
sp reload a queue
这将删除我们的表
如果它存在red air quality
它将再次创建表
red air quality as select star from that same old thing now
如果我们运行这个
它将创建该存储过程
我可以在这里
刷新我们的存储过程
这是我们的存储过程
所以这将允许我们做
现在如果我想要做一些像截断的事情
红色空气质量
好的 所以那应该做了
让我们在这里检查一下
选择星从那里运行
那应该删除所有数据行
然后我们去
我们没有行要显示我可以做
现在调用那个存储过程
并且这将执行那里的代码
并且它应该做的是重新填充我的表在这里
所以如果我回到这里
选择计数
再次运行那里
所以我有我所有的东西
所以这相当方便
我们可以显示定义在这里
这就是你如何创建存储过程
你可以在你的数据库中使用它
就像你可以 在大多数其他数据库中
支持 存储过程
函数有点不同
我们现在不会覆盖那些
我想展示的另一件事
这里是创建一个视图
所以我可以做
让我们看看 创建视图
我们将说二零一九年作为选择星从红色空气质量
这里等于
二零一九年
哦
更改为十九
好的 让我们试试
我们成功了 所以视图已被创建
如果我们在这里刷新
这是我们的视图
所以我们可以查看我们的视图
我们可以实际上查询我们的视图
所以我要做
让我们说 选择星
并且我们应该只有两千零一九年的值
所以如果我们滚动到这里
我们看到的是53,000行
这是2019年
这创建了一个视图
这创建了一个存储过程 在下一个视频中 我们将探索一些其他功能
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/061_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p61 7. Scheduled Queries.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 只是为了完善我们的红移实验
我想和你分享两个小功能
所以我们这里有一个查询
我能运行那个
它将返回这些东西。
让我们举个例子
我的意思是我们的数据在这里是有限的
但是让我们假设我们想让这个查询以定期的方式运行
我们可以做的就是
实际上我们可以保存这个查询
我们可以点击这里这个小小的保存按钮
我只是说大于2022年并点击保存
如果我到这里我的查询
这里有大于2022年的查询
我有一个选项在这里
我可以将其添加到笔记本中
我可以与我的团队共享
这有点方便
如果你有一个相对复杂的查询
也许你想和别人分享这一点
或者你可能有一个人，他是写查询的专家
而且他们不想要费事去写相同的查询
一次又一次
所以他们可以保存那个
你知道这些相对简单的查询
但我确定如果你在数据科学或类似的领域工作
你已经看到这些庞大的、复杂的查询了
让我们说 例如
我们希望安排那个查询
我们可以点击这个按钮 这里显示的安排
我们必须选择一个角色
我只是选择相同的角色
我将选择我们的临时凭证
它将问我们哪个工作组
哪个数据库 预定查询的名称是2022年
这不是一个很好的例子
这里对这条查询
因为我认为这里它的价值有限
你可能想要做的是安排一些事情
也许你可以将数据转换为其他位置
或者类似的事情
你可以安排这条查询
我们可以选择运行的频率
或者我们可以使用cron格式
让我们假设我想在星期三运行
我不知道
十二点整，每周三都是十二点整
UC 是的
随便，当然
然后我还可以启用短信通知
每当它运行时，从这里
我可以点击安排查询
但我不会那样做
因为我真的不想设置这个东西并让它运行
但这是使用定时查询的一个例子
你可以在这里查看
但我们没有 但这会给你机会重复运行查询以实现自动化
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/062_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p62 8. Teardown.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 那么，如果你还记得
我曾说过，我将向你展示如何将一切拆毁
因为也许你只想让它恢复原状
也许你不想承担更多的成本
或者类似的事情
因为它是无服务器的
它应该只会产生费用
如果我们在运行查询和类似的东西 但从清洁的角度来看
我喜欢有一个非常干净的控制台
所以我喜欢关闭我正在玩的任何东西
所以我希望这个东西会停止在这里弹出
但要拆毁它
我们可以去工作组配置
是的
我知道 然后我们选择那个工作组
我们会到这里删除
它将删除属于这个工作组的所有组件
我们可以在这里输入并删除
如果我们在这里勾选这个框
删除相关的命名空间预览
我们可以点击这里
这将删除我们的命名空间
因为Redshift是一个真实的数据库
它问我们 我们是否想要创建一个最终的快照
我们系统的最终备份
不 我们不感兴趣
我可以点击或者我可以在这里输入删除
然后我可以点击删除
那么它将删除我们的工作组
然后最终它将在这里删除我们的命名空间
这就是在我们创建Redshift
无服务器实例后，我们如何清理一切 它将我们恢复为零，不会产生更多的费用
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/063_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p63 9. Validation Amazon Redshift.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好吧 让我们一步一步解决这些问题
第一个问题 以下哪些功能是
亚马逊红移让你能够无缝查询来自S3的数据
不需要将其加载到红移表中
现在我们的例子中，实际上我们将数据加载到了一个红移表。
但是，红移有一个特性可以让我们做得与雅典娜类似。
我们将从这里下去
亚马逊红移三节点
不
那不是它 我们的亚马逊红移光谱
啊哈，是的
如果你记得红移光谱是athena
或者athena在athena成为事物之前
所以我们可以使用红移光谱从s三中查询
让我们继续确保这是最好的答案
亚马逊红移无服务器rpus很好
我们可以使用rpus从s三中查询
但是红移光谱是我们实现这一目标的机制
现在我们的例子最终通过数据目录进行
这种类型的操作会通过athena到达s three
因此红移光谱仍然是这里的赢家
亚马逊红移增强vpc路由
我认为这与我们从s three查询的能力无关
所以我选择这个作为我的最终答案
下一个问题
以下哪种情况
你是否最有可能领导你使用红移服务器less笔记本
首先，当你需要通过Python脚本自动化ETL过程时，
每当我们想到笔记本或Jupyter笔记本时，
你可能想到Python
但在红移中的特定实现更偏向于SQL
所以下一个选项
当你需要从S3将大数据集加载到红移时，
我们刚才在上一题中看到了
这更多是Spectrum的工作，而不是笔记本
当你需要一个交互式环境来开发时
运行并可视化sql查询
Aha 这似乎更适合我们用笔记本
让我们转到最后一个选项
只是为了确保 当你需要将查询按特定时间自动安排运行时
这听起来更像是定时查询
所以看起来是我的最佳答案
下一个问题
以下哪项最准确地描述了亚马逊红移
所以我们这里有不同的选择
不同的红色描述
红移可能是什么
我们可以从那里往下走
让我们从这里开始底部
一个专有的数据库，只能通过aws api访问
这是因为我们通过sql访问它，所以这是错误的
所以我们可以立即排除那一个
一个优化了分析工作的mysql克隆
嗯 分析工作是正确的
但是mysql是错误的
因为它不是mysql的克隆
事实上 它起源于postgres sql
所以我们也可以立即排除这里这一个
基于mysql的数据库，优化了列存储再次
列存储是红移
但它不是，它不基于mysql
基于postgres sql的数据库，优化了列存储
好的，现在
我们在检查所有空格这里
我们有列存储
这是一个postgres sql数据库，或者一个基于postgres sql的数据库
这将得到这里最后的选项
一个基于postgres sql的克隆，优化了事务处理
它没有优化事务处理
它优化了分析处理
所以这是我的最佳答案
这是我的最终答案
所以我希望你们觉得这是有信息量的 并且我想感谢你们观看
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/064_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p64 1. Skill Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，美丽的学习者，来到另一个技能
回溯到早期的2000年代
谷歌的开发者在尝试找出最佳方式进行数据分析
在他们构建的庞大数据山之中
他们发明了一种将数据分割成小块的方法
并将它们分发到多个小型系统中
每个系统会完成它们的工作部分，然后将结果报告给中央协调系统
这种方法效果相当不错
一些谷歌开发者发表了一篇论文，题为《map reduce：大规模集群中简化数据处理》
然后 这篇论文促成了一个开源项目
最初作为开源网络爬虫Apache Nutch的一部分工作
最终，这个工作被分离出来，成为一个独立的项目
名为Apache Hadoop
以创始人之一儿子的心爱玩具大象命名
现在
自那些早期阶段以来
Hadoop生态系统已经显著增长
并为将所有数据转化为价值提供了非常有效
灵活且强大的工具
在本技能中 我们将深入研究AWS对Hadoop生态系统的实现，或者我称之为Hadoaverse
我们将动手操作这些工具
让我们开始吧 所以让我们开始吧
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/065_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p65 2. Meet Elastic MapReduce.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 开始之前
我想回答一个问题
或者至少解决一些事情
当我第一次学习弹性MapReduce时
世界上为什么会有人叫它弹性MapReduce
这是什么MapReduce业务
我的意思是，我知道弹性
因为AWS很自豪能够扩展
他们已经在其他服务前面使用了弹性这个词
像弹性计算
弹性块存储 那种东西
这个map reduce业务是什么
它涉及到我们如何实际处理基本数据，也就是说
大量的非常详细的数据
我们如何从数据中提取价值
假设我们有一堆数据
我们有一堆大东西 有些可能是结构化的
有些是未结构化的
半结构化的
可能是文本格式
可能是其他专有格式
但我们有大量的数据
通常当我们面对这堆大量数据时
我们可能需要进行一些转换
我们需要将其转换为某种格式
使其有意义
可能是翻译
也许它是在进行单位转换
所以我们处理的是苹果与苹果之间的比较
也许它是将其转换为某种格式
这是计算机可读的
例如
现在我们回到被称为函数式编程的时代
那里存在一个基本的函数叫做map
而map的目的是从形式a获取数据
然后将其映射到形式b
这就是这个过程，它类似于映射
数据来自 也许它开始的地方
到我们需要把它变成的地方
我们可以对它进行分析
现在我们有了数据映射
然后我们可以把它倒入大苹果压榨机
开始拧螺丝
我们希望能获得一些价值
这就是reduce的作用
这是另一个被称为reduce的基本函数
所以你可以把这个过程想象成从大量数据中提取精华
将其浓缩到纯粹的本质
纯粹的统计价值
理想情况下，一旦我们完成了这个过程
我们将从中获利
我们将从这个过程中获得一些价值
我们可以查看这些数据
它可能会告诉我们一些趋势或预测
或者类似的东西，这就是涉及大数据的任何事情的目标
你必须处理大量数据
将其浓缩为有价值的部分
你认为有价值的部分
然后你可以向人们展示这些数据
他们可以根据这些数据做出决策
现在，真正的mapreduce术语实际上来自一个叫做hadoop的东西
hadoop有几个主要组成部分
我们今天要讨论的组件之一
是我们的hdfs
那就是hadoop分布式文件系统
这就是hadoop中数据存放的地方
然后我们有yarn
这是这个系统的管理层
然后在这之上我们有map reduce
这就是map reduce的来源
它是hadoop的一部分
实际上执行工作
我们可以给它分配任务
我们可以说，嘿
在这个hdfs中的数据上执行这个任务
然后它就会去做
我们统称这一切为hadoop
但hadoop是更大生态系统中的一个单一部分
这就是我为什么称其为hadoop a verse的原因
我想没人会用我那个确切的术语
你可能会看到hadoop景观或hadoop生态系统
但那里有不同的工具在那生态系统中
现在 aws构建的一个工具
它是基于hdfs的
但它被称为e rfs
它的目的是让我们能够使用hadoop过程
或者这个hadoop框架
与我们存储在s three桶中的数据进行交互
我们也可以将我们的hadoop数据存储在那里的s three桶中
hadoop a verse有很多很多其他组件
我们有hive和hive
这是一个分布式数据库，允许我们用sql与我们的hadoop数据交互
我们有spark和spark
这是一个数据分析平台
它允许我们对hadoop数据进行一些复杂的操作
我们有'mahot'或'maht'
取决于你的来源
你可能有不同的说法
机器学习如何添加
在hadoop上
我们有一个叫做presto的东西
这可以让我们使用sql查询各种类型的数据
但等等，还有更多
有更多的东西
我们有一个叫做hbase的东西，hbase是一个nosql数据库
Hadoop附加组件
如果我们想以NoSQL格式查看Hadoop数据，或者以NoSQL格式与之交互，
我们可以使用HBase
然后它会使用HDFS作为存储层
并允许我们基本上重用我们的堆栈并提供
或为我们提供一个NoSQL数据库
然后有一个叫做Zookeeper的东西，Zookeeper帮助我们管理所有这些工具和服务
我们有一个叫做Flume的东西
它帮助我们收集大量数据并将其导入Hadoop集群
以供处理
然后我们有一个飞艇
这是一个基于网页的数据分析平台
我们有Flink
这可以帮助我们创建和管理数据流
我们有Scoop
这是一个可以帮助我们在Hadoop和其他结构化数据源之间传输大量数据的工具
现在官方上Scoop已经退役
但很多人仍然在使用它
我们有Pig
这是一个专门用于进行大规模并行处理的语言
它使用一种叫做猪拉丁语的语言
我保证
在hadoop中还有许多其他小项目和工具
现在，让我们来一首诗 我不想在这些上花太多时间
现在我们将深入研究一些这些内容，通过一门完整的技能
因为aws已经提取了一些这些特定的服务
他们已经为我们创建了这些管理的版本
现在 你会注意到的一件事是，这些管理服务有时
这些hadoop相关的服务看起来确实与aws已经拥有的产品非常相似
事实上确实如此
是的 这种情况绝对是事实
在某些情况下，原始的aws产品基于这个开源版本
这是Apache版本的那个工具
在其他情况下，这些产品各自独立发展
但它们有相同的需求，并且它们基本上做同样的事情
但总的来说
Emr和其他所有Apache托管的项目
AWS提供的服务真的非常适合那些已经使用Hadoop的公司，或者已经熟悉Hadoop的公司，他们只是想以一种管理的方式使用它。
稍后会详细介绍这些服务。
好的，接下来是亚马逊EMR。
总的来说，亚马逊EMR就是一个托管的Hadoop框架。
这里的商业理念是，如果你曾经尝试过启动Hadoop和一些其他相关的Hadoop项目，你会发现这实际上是非常复杂的，并且可能会非常耗时。 稍后会详细介绍这些服务。
好的，接下来是亚马逊EMR。
总的来说，亚马逊EMR就是一个托管的Hadoop框架。
这里的商业理念是，如果你曾经尝试过启动Hadoop和一些其他相关的Hadoop项目，你会发现这实际上是非常复杂的，并且可能会非常耗时。
稍后会详细介绍这些服务。
因此AWS已经通过使其变得更容易，消除了一些困难
基本上就是点击部署
这是大致的
EMR的主要目标是它基本上卸载了
维护所有这些东西并连接它们的必要性
所以它们可以很好地协同工作
它只是简化为一个托管服务
它是在2009年发布的
所以它作为一个AWS服务来说相当老了
记住 亚马逊云服务大约在2006年左右以强大的力量进入市场
这个产品大约在那个三年后发布
这就是为什么现在亚马逊云服务中有很多服务
这些服务看起来好像或者实际上在做一些Apache Hadoop产品在做的事情
这并不是简单的功能重复
亚马逊云服务想要尽可能的接近Hadoop Apache项目
来维护Hadoop Apache项目最原始的形式
然后在一些项目中，他们采取了一些创造性的授权，制作了一些更复杂的产品
他们做了一些更复杂的事情
现在我们可以在e上运行emr
C 两个实例 我们可以在eks上运行它
我们今天的技能也可以以无服务器模式或无服务器方式运行
我们将要看的是EC two版本
以及无服务器版本
好的 这是emr的组件
我们有一个叫做主节点的东西
这就是类似于控制集群中所有节点的头节点
现在，所有节点的行为都将由这个节点控制
我们也可以设置一个备用节点或次要节点
以防主节点出现问题
然后我们就有另一个节点可以说：嘿
我可以从这里接管
这给了我们一些故障容忍度
然后我们有称为核心组件的东西
这些核心组件存储数据
这就是hadoop景观中hdfs文件系统所在的地方
他们也为主节点工作
我们有一个工作，我们会说嘿
主节点 让你的舰队或集群去做这个
主节点会说好的
它会把这些任务分配给核心组件
我们也有称为任务服务器或任务组件的项目
这些是什么
这些是非常短暂的实例
一般来说
它们就会突然出现
当我们有一个相对较大的项目时
我们可以配置我们想要弹出多少个
这里核心与任务的重要区别是，核心持有
我们的hdfs数据
它们包括
我们的hdfs卷或数据存储层任务
不要考虑它们
仅仅作为工蜂
它们除了做计算任务外，什么都不做，而且只做主节点告诉我们的事情
所以我们可以
如果我们只想要有一个只包含核心实例或核心组件的分布
但我们也可以说嘿
每当你接到一个大项目
请随意启动一些任务
所以这基本上是一个高层次的概述
一旦我们进入控制台，这将变得更加清晰 这就是我们现在要去的地方
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/066_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p66 3. Setting Up Our First Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好吧，就在这里 我们在我们的aws控制台外面
让我们开始玩e r，首先做第一件事
虽然我想要去s三
在我之前的视频中我注意到的一件事
是这些中的一些让我摆脱它
移动这里过来，我们在这里
我们这里的一些文件在这里有点拥挤
所以我要清理一下
我将删除这些，删除这些
我将删除那只松鼠
我认为松鼠普查这是我们想要的
是的 所以我要恢复到开始的样子
如果你想跟着来
随意
点击删除
它将让我永久删除
这就是每个人都做的事情
而不是实际打字
他们只是复制并粘贴并删除
他们试图让我们免受伤害
是的 我知道
好的，关闭它
让我们看看这里还剩下什么
我认为我们可以删除这个打开队列
松鼠普查
这里有松鼠数据
v2 我将删除那个
猜猜看，永久删除仍然在我的复制粘贴缓冲区
回到这里
让我们删除那里
永久删除 是的
好的 所以现在我们已经清理了一下
在松鼠普查中，我想创建一个文件夹
它将作为我们输出位置的输出
我们的输出
我只是说
更多输出创建那个文件夹
我们走吧
好的 所以这改名了我们的文件夹
是的 emr输出现在我将回到我的控制台
我将进入emr
现在我们可以拉起来这里
Emr
如果之前您已经访问过这里
您可能会直接来到这里
但如果您还没有访问过
您可能会直接来到这里
它会给我们提供一些关于服务的信息
它会给我们提供一个小向导
在这里我们可以开始启动我们自己的集群
但我想先在这里转到集群
如果我们后退一步这里
我们有emr在e c two
它会在我们这里启动e c two实例
我们有emr on eks
我们会看到一些emr studio
当我们后面玩emr serverless时
我想启动一个集群
所以我要去创建集群
我们将分析我们的松鼠数据
所以我要更改为松鼠emr
我们可以选择要使用的amazon emr版本
在某些情况下，您可能需要选择一个较旧的版本
因为较新的发布可能存在bug
或者一些无法解决的问题
但大部分情况下
大多数人 如果他们刚开始
他们可能会使用最新版本
您可以看到在这里
这里有我们所有可用的小功能
您可能记得一些名字
在之前的演示中
我们可以点击这里
它将一些东西打包在一起
因为它们可以协同工作
但对于我们的目的
我们将使用spark interactive
并且我们还不会使用所有这些东西
但这没关系
我们将使用默认设置
我们将使用amazon linux版本
现在我们被问到集群配置
现在我们有灵活实例群集和统一实例群集
灵活给我们提供了更多的灵活性
但我们将使用统一实例群集 第一个是主
aws称之为主
领导者 我也听说过
所以我们必须选择一个e c
Two实例类型
现在您可以在这里选择您想要的任何东西
我就让它这样吧
M五x大
这对我来说正好
我们可以选择在高可用性下启动
如果你记得的话，我们可以有一个这个主副本的镜像
这样如果有问题，它可以切换
然后我们来到这里
我们还可以选择进行一些自定义节点的配置
我们不会费心去管那个
现在，这里是核心实例
我要说M五x大在这里正好
现在我们可以选择预定义或开始定义任何类型的任务实例
在我这种情况下，我不会使用任务实例
因为我们做的事情是如此之小
它不会用到任何资源
现在我来到这里，到集群扩展和配置
这是我们设置核心数量的地方
如果我们想要四个或五个核心，我们可以点击
如果我们想要用这些核心使用spot实例，我们也可以那样做
但我只想留一个
我们还可以选择让emr为我们扩展
我们还有一些自定义扩展
但我不会担心那些东西
现在，它已经为我预选了默认vpc
以及我在那个vpc中的一个子网
这对我来说正好
我们有选择定义步骤的选项
我们会稍后定义这个，所以我们不会费心去管那个
继续往下滚动
我们有设置安全配置和为我们定义一个密钥对的选项
以便我们能够登录那个EC two实例
在某些情况下，您可能需要这样做以进行管理
但我们不需要那样做
所以我不会费心去管那个
ec 2实例
在某些情况下，您可能需要这样做以进行管理
但我们不需要那样做
所以我不会费心去管那个
现在，我们来到一个非常重要的部分，现在，这对很多刚开始使用emr的人来说是非常困惑的
我们有两个角色需要定义
一个是服务角色，这是emr使用的iam角色，以便它尝试做我们所要求的所有事情，设置所有实例等等
然后我们有一个EC two实例配置
EC two实例配置是EC two实例使用的iam角色
以便它能够执行它所需的所有操作
然后，我们有一个EC two实例配置
ec
2 实例配置
这就是EC two实例的安全性
当它尝试操纵我们的数据或连接、读取我们的数据时
这是两个不同的事情
我将在这里创建一个服务角色
它预先定义了我的所有VPC和子网
当我们转到EC two时，一切都很好
C2 实例配置文件
我必须确保创建一个实例配置文件
我需要确保我们指定我们的S3桶
以便它有权限
或者我们的核心节点可以访问并与我们的S3桶交互
所以我将选择特定的S3桶
我将选择我的桶
这是我们的桶，Letcher数据工程游乐场
选择它，非常重要
点击添加，因为它会将其添加到下面的列表中
如果你没有点击添加
那么它不会添加到这个角色中
我将设置角色
当你尝试执行你的工作时
你会得到一个失败
因为它没有访问权限
我们可以指定我们想要的访问权限
让我们留下读写权限
我不会设置一个自定义自动缩放卷
但我会滚动到这里并点击创建集群
这将开始设置一个相对复杂的Hadoop景观的过程
一旦完成，我们就可以与之交互并发送工作
完成整个设置大约需要十分钟
你可以在这里看到
我们的集群已成功创建
这种类似红色听证会
因为它实际上只启动了创建过程
你可以在这里看到
它正在启动
根据我的经验，它通常需要约十分钟
十分钟来完全设置这个相对复杂的Hadoop集群
我们将在这里结束这个视频
在下一个视频中，我们将等到 这个集群准备好我们进行操作
完全设置这个相对复杂的Hadoop集群通常需要约十分钟
我们将在这里结束这个视频
在下一个视频中，我们将等到 这个集群准备好我们进行操作
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/067_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p67 4. Creating a Batch Step.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 终于
我们的emr集群已经完成
事实上，它处于一个叫做weighting and waiting的状态，这是完全正常的
它告诉我们，它已经准备好运行我们的任务
或者我们想发送到它的任何步骤
这就是emr称之为任务的地方
所以如果我们在这里滚动
它大约花了大约15分钟，12分钟，15分钟
然后我在看别的东西
所以，它已经运行了一段时间了
如果我们回到这里并深入挖掘
实际上 首先，让我出去e c
两 因为我们说我们要在e c two上运行它
猜猜看，它启动了一些e c two实例
我们现在不想做任何事情这些
这些是由管理的
是的 现在开始
这些里面一个是主要的
这些里面一个是核心的
所以我们就让它们自己
我将回到那边并钻入集群id
我们可以在这里获取一些信息
这里需要注意的一点是
默认情况下它被设置为一小时超时
这意味着如果我们在一个小时内没有给它任何工作
它将会自己关闭
这是为了防止我们产生一大笔费用
运行那些e
C 两台实例
当然我们可以关闭它
我们可以调整这个超时
如果我们想要的话
但我认为我们只是让它做一些工作
然后我们可能在那之前关闭它
在那一小时甚至到达之前
所以我也想看看
这里是步骤
这是我们可以发送的工作定义
现在 你可能有一个步骤
你可能有一个系列的步骤，它们被链接到一起并且相互依赖
在我们的情况下，我们将使用一步来做这一步
我们将使用Spark脚本
我们将使用一些Python代码来做某事
所以如果我打开一些代码
这就是这段代码要做的事情
这是一个Spark脚本
这里将加载我们从数据集从我们的松鼠数据中加载数据
CSV 我们必须定义我们的桶是什么
我将在下面的视频中附上这个
这样你就不必重新输入屏幕上的内容
然后我们将读取那个数据
然后我们将对其进行处理
我们将分析那个数据
我们这次只按主要毛色进行分组
如果你记得看那个数据
我们有权访问的一个列是主要毛色
这将给我们提供一个基于那里独特的毛色的计数
然后我们将对那个数据进行处理
将其写入到我们的输出位置
这就是我们的结果将出现的地方
这很简单
但你可以变得相当复杂
当你使用emr时
你可能在做更复杂的数据处理
你可以使用此过程进行大规模处理
当我说大规模时
我说的是吉字节和太字节的数据
并且能够非常有效和有效地处理
所以让我们出去获取我们的桶名称
如果我去s3
这里是我的文件
这里是我想要的数据
这是我们的数据源
我将进入那里并点击这个小小的复制按钮
这将为我复制一个路径
所以我可以在这里粘贴
所以这就是数据的路径
对于emr的输出路径
我可以取这里的前面一小部分
因为它们在同一个桶中
我们搞定了
现在我必须将这个上传到s3
我们必须将其放在一个emr可以读取的地方
所以我将上传这里
添加我的文件
这就是我的文件
我将上传
搞定了，这就是我们的emr步骤将要使用的文件
现在我将回到emr并添加一个步骤
我们有选择步骤
因为我们使用的是Spark
我将使用Spark应用程序，应用程序实际上在这里
我们可以将其命名为任何我们想要的东西
我只会把这称为松鼠
人口普查批次分析
我们开始并使用集群模式运行我们的应用程序
我们需要告诉它那个Python脚本在哪里
所以我要滚动到那里
在那里，鞋子，好的，如果我们需要添加额外的参数在这里
我们不需要
它正在问我们 如果这失败了，我们做
我们只是继续到队列中的下一步
好吧，对于我们的队列来说
这只是我们队列中的一步
我们可以取消并等待
我们可以终止集群
我们可以关闭一切
但我只是想添加一步
所以我们添加了这一步
它已经出去了
领导者或主人说：嘿，伙计们
我们有一个工作正在进来
准备去上班
现在正在尝试安排那个工作
然后几分钟后
我们会看到状态的改变
我会暂停视频
状态改变后
我会向你展示
好的 我们的步骤已经从待定变为运行
现在正在运行我们提供的脚本
这不应该花太多时间
我们就在这里待着，看看能不能抓到它
当它切换到最终状态时
它将会有 好的
所以工作现在已经完成
这是好的
我们没有收到任何错误消息
那项工作花了36秒
所以我们可以出去到我们的s3桶中
这就是我们告诉它把结果放在哪里的文件夹
哦，好的 我明白了
我可能把那个路径输错了
我不知道 好的
不管怎样 这里有毛色csv
这就是我们想要的
所以我们要深入研究那个
这是我们的CSV文件
我将下载它
让我使用表格查看它
好的，这里是
让我们在这里放大，放大，好的
让我们看看这里有什么
这是我们的CSV文件
它产生的
这里看起来我们有一个空白
我们有390个灰色记录
我们有2只六香红松鼠
我们有16只黑松鼠
这是一个使用emr进行批量处理的例子
如我所说，
这个例子很简单
在某些情况下，你可能会有很多
很多不同的阶段
各种数据转换和调整
最后你会输出什么
你也可以交互运行它，
但我不会花太多时间在那上面
我想接下来 我们要做的就是
确保我们终止这个集群
因为我们不想让它无谓地运行
我们已经对它进行了足够的操作
要终止它
我们只需要在这里点击终止
哦，对了，我们终止
它将关闭那个集群
它正在说终止
它大约需要5分钟来终止 所以在下一视频中，我们将讨论emr serverless
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/068_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p68 5. EMR Serverless.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的
现在我们尝试使用亚马逊emr serverless来做同样的事情
如果你记得我们终止了我们的集群
如果我们在这里到我们的EC two实例
你可以看到他们已经终止
我们不再为他们付费
这与emr on EC two和emr serverless之间的大不同
在EC two上我们会看到这些EC two实例
我们可以实际上登录它们
我们可以对它们进行低级别的维护，如果我们想要的话，可以做一些事情
这就是为什么它能够创建SSH密钥的原因
如果我们想访问他们
但是使用无服务器架构我们没有这个选项
它只是在后台启动那些服务器
我们不必担心这些事情
我们将点击
嗨 开始
它将要求我们创建一个并启动EMR工作室
这就是我们要做的，启动我们的EMR工作室
现在我们开始了
它带我们来到一个小巫师这里，他可以让我们创建一个应用程序
所以emr工作室
如果你看看这个上面的url
这是一个唯一的url
这是一个为我们建立的唯一系统
我们将创建应用程序
在其中我们可以创建应用程序来做很多事情
所以在我们的第一个应用程序这里
我们将创建一个应用程序来分析松鼠的毛皮颜色
搞定了，分析松鼠
我们向下滚动
我们将继续使用spark emr
使用7.2版本是可以的，这只是一个批处理任务
我将指定批处理任务的默认设置
我将向下滚动
我将开始创建并启动应用程序
现在 这将在后台做一些事情，为我们进一步处理数据做准备
并设置一些我们需要进一步处理数据的东西
而且当它开始时
我将深入研究这一点
我将在那里压缩这一点，所以如果你记得在我们的集群中
我们有一个标签说步骤在这种情况下
因为我们正在运行这个应用程序的批处理作业版本
我们有这个批处理作业运行标签
所以我们将等待
看起来它已经开始了
这是一件好事
好的 现在我们可以定义我们的批处理作业
所以我要提交一个批处理作业运行
如果我在这里向上走
我的第一个Spark批处理作业
那很好 运行时角色在这里
我现在要创建一个新角色
我被问到
我想怎么设置这个角色
所以我想指定这个角色可以访问的桶
所以我在那里放了一个桶
我只是要检查那个创建卷
它会自动为我生成所有我需要的规则
现在我可以为那个特定的桶指定我需要的权限
现在我可以指定我的实际文件
这是我们创建的python脚本
所以我要深入到这里
那里 它就在那里
选择它，所以我有我的脚本位置
我有我的emr执行规则
然后我们可以在这里指定一些额外的属性
如果我们想要的话 但我不打算碰任何那些东西
我将提交作业运行
所以它将立即安排那个作业
或者尝试安排那个作业
然后开始运行它
就像我们在集群版本的mr中看到的那样
现在我将暂停这个视频
一旦我们有了更新的状态
我会继续播放视频
好的 我们有了更新的状态
我们的运行状态现在是运行
让我们给它一点时间来运行
哦不 它失败了
我不能
我知道它会失败
我做这件事的原因是我认为这是正确的
每次展示顺利路径都是好的
但有时事情不会按照顺利路径发展
你需要找出如何实际调试这个问题
或者试图找出发生了什么
在我们这里
如果我在这里滚动并点击那个
任务未能提交到应用程序
或者它以失败告终
这并没有给我们提供太多信息
所以我们可以做的是深入到工作
运行ID
这将给我们提供更多信息
这就是我们要做的
好的 它说工作失败
请检查完整的日志
如果我们想要
我们可以点击这里查看日志
但这已经告诉我们问题所在
路径已存在
我们作为输出路径已经存在
所以我知道这将会发生，所以在我们的脚本中
如果你记得的话，我们在输出路径中指定了
当emr作业运行时
Spark处理器试图运行该作业
并说嘿
等等 那里存有数据
所以它不会覆盖那些数据
所以我们可以做的是，让我们删除那些数据并永久删除
删除那里的对象，搞定
好的 所以现在这个路径可用
它为我们打开发送工作回去的通道，要做这个
这真的很容易 我们所要做的就是点击这个克隆
它将会复制所有那些东西
一切都会保持不变
我们将提交工作运行
并且它会尝试再次运行那个任务
所以你可以看到它被安排了并且再次
我将在这里暂停视频
它可能需要
我不知道 也许两到三分钟才能收到结果
好的 我们成功了 花了大约两分钟，我们的任务成功了
万岁 所以我们可以滚动到这里
我们可以点击这里查看详细信息
它会在这里拉出一些统计数据
这显示了我们有多少个vpcp
vcpu小时
我们使用了多少 多少内存
多少存储以及我们在那里实际构建了什么
所以再次强调我们在这里做的事情非常小
土豆 这不会花费太多
事实上，它可能甚至不会显示在你的发票上
所以如果我们在这里去我们的s3桶
如果我刷新
这是我们的毛色再次
这是我们的文件再次
这是由无服务器版本创建的
所以我要下载它
让我们确保我们得到相同的结果
好的，我们得到相同的结果
主要毛色计数灰色
肉桂和黑色
现在我们已成功提交了我们的批处理作业
现在来点有趣的
现在尝试拆除这一切
这不如e2上的emr那么简单
我们有一个大的终止按钮
所以我们首先要做的是删除这个应用
要做到这一点
我们首先必须停止应用
我们可以看到无法删除它
所以我们必须停止应用
它将停止应用
它已经停止
然后我们可以删除这个应用，我们希望检查永久删除
是
好的 已被删除
然后你会想
好的 那么我怎么删除我的studio
嗯 我们现在仍然在这个studio中
如果你看我们的url
这是唯一的url
我们不能从这个studio删除studio
所以我们必须回到控制台
emr控制台
我们下到studio
我将关闭这个标签，这是我们的studio
我们点击小按钮并点击删除
你真的要删除它吗
是的 我们确实要
哦不 它仍然包含一个工作区
是的 我忘记了 我用向导设置这个
它会有一个工作区在里面
虽然我们没有使用它
因为我们没有使用或创建或启动工作区
我们只是使用了批处理模式
所以我要点击那里的那个小型单选按钮，然后点击
删除，好的，所以现在工作区不见了
我可以回去删除我的工作室
删除
我们走吧 好的
现在我们回到了我们找到东西的方式 我们找到了东西
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/069_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p69 6. Validation Amazon EMR.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我们来解答这些问题
亚马逊emr集群托管在e上的第一个问题
C二 以下哪些节点类型是可选的
如果你记得当我们设置基于EC two的集群时
我们设置了一种主要系统，也称为主导系统
或者可能是主节点
然后我们设置了一些核心系统实际上
在我们这个案例中我们只设置了一个核心系统
所以我们在没有这个领导者或主节点core的情况下无法设置集群，并且核心节点也处理hdfs存储
至少一个核心
而且如果你记得，核心节点也处理hadoop景观的文件系统存储
我们的文件系统存储
我们hadoop景观的数据存储
所以这里唯一的可选项是任务节点
如果你记得我们可以设置任务节点
它们不真正存储任何存储或任何东西
它们实际上只是种工人节点
如果我们想要的话，我们可以实际上使用spot实例来使用aws或emr
启动一些任务节点
每当有需要或需求时
在那之后处理完成
可以关闭所有那些节点
因为它们或多或少是临时的
它们真的不存储任何数据
这是一个好方法
如果我们想节省一些钱
所以这个问题的答案是以下哪些节点类型是可选的
这个列表中唯一的任务节点是真正可选的
没有叫hdfs的节点
核心节点托管hdfs
所以这里答案是不合理的
如果你了解核心节点和领导节点的要求
有时也称为主节点
这就限制了你只能选择任务节点
这就是我的最终答案
下一个问题，你需要在emr上运行一个夜间批处理任务
在emr集群上处理存储在s3上的日志文件
你想优化成本
在确保处理完成后，集群将自动关闭
我们也注意到有时集群会继续运行
即使所有成功步骤都已完成
以下哪个将确保集群终止？
当工作以最少的维护和配置完成时
现在在很多时候，你会在aws考试中看到
一个问题以这种方式形成
因为也许这里有四个完全合法的选项
但他们实际上是在测试你的知识以看
这是最具成本效益的类型
或者最少的维护和配置
所以你需要留意
尤其是当它提到最少的维护和配置时
你需要留意任何自动化的过程
或者你不需要手动编码或手动干预
让我们用那个视角来看这些可能的答案
在集群中启用终止保护以防止意外意外关闭
这不会帮助我们解决未能成功关闭的问题
它只是防止意外关闭
所以我们可以忽略这个选项
配置一个自定义脚本，在所有成功步骤完成后关闭集群
这可能是一个可行的选项
我们可以绝对这样做
但再次，我们要选择维护和配置最少的选项
如果我们这里有一个自定义脚本
我们将不得不维护那个东西
可能让我们继续下一个
使用aws lambda监控集群并在一段时间的无活动后终止它
再次，这可能是一个可行的选项
但我们仍然需要构建一些东西来做这件事
让我们看看最后一个选项
在配置集群时将自动终止选项设置为true
啊哈，现在
如果你记得，我们确实有设置自动终止的选项
默认情况下，它给我们一个小时的超时
所以我们可以设置成我们想要的任何时间
然后，当那个集群中没有任何活动时
它将会终止那个集群
所以这是一个非常好的选项
我认为 尤其是考虑到其他两个选项，它们是与编程相关的
这个功能是直接构建在这个平台上的
十次中有九次，或者十次中有十次
AWS通常会让你选择那些内置在平台上的功能
尤其是当他们在这里使用这些术语时
最少的维护和配置
这就是我最终的答案
下一个问题
你们的团队正在从传统的emr集群转向emr无服务器
用于运行Spark作业
他们想知道两种模型之间的成本会有何不同
特别是在资源使用方面
当没有作业运行时
使用emr无服务器的主要成本优势之一是什么
与传统emr集群相比
嗯，如果你记得的话，emr无服务器实际上并不涉及服务器
我的意思是 是的 背景中有服务器
但是无服务器服务的一个美丽之处在于我们只支付使用时间。
它实际上正在做一些事情
所以如果你回头看的话
当我们在我们的e上设置emr集群时
C两个 我们为那些e付费
C 两个系统 尽管它们只是坐在那里
闲置 这就是为什么自动终止功能非常有用的原因之一
因为它只是闲置
它并没有做任何事情，但却让我们在emr serverless上花费了钱
我们不必担心这个问题
所以让我们带着这一点来查看一些潜在的答案
emr serverless根据实例类型收费
即使没有运行任何任务
当我们设置emr serverless时，我们不会选择实例类型
当我们设置emr serverless时
所以我们可以排除与emr serverless相邻的这一项
需要设置并支付一定数量的预留资源
但那也不是真的
因为我们可以设置emr serverless，我们可以不用任何东西
我们可以选择不用任何东西
这不会花我们一分钱
是的 emr serverless只有在任务运行时才会产生费用
闲置时不会产生费用
啊哈 是的
我认为这就是答案
因为这是EMR无服务器服务的一个优点
让我们确保这里没其他答案
这可能是EMR无服务器服务比传统集群收费更高
由于无服务器架构，收费比传统集群高
这也不是事实
因为如果你看传统集群的收费
因为我们不需要支付系统闲置时的费用
但我们需要在传统集群中支付系统闲置时的费用
那么这将是净净的
这将会更贵
所以我选择这个作为我的最终答案
所以我希望这对你有所帮助 我想感谢您观看
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/070_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p70 1. Skill Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，美丽的学习者，来到另一个技能之夜
亚马逊Kinesis是那些你可能每天都在使用的服务之一
但你可能甚至根本不知道它
Kinesis用于收集实时观众情感
监测野生动物迁徙
处理来自自动驾驶汽车的流数据
甚至能在线游戏中检测作弊
如果你曾经坐在沙发上观看Netflix
迪士尼+或亚马逊Prime
你已经为强大的Kinesis漩涡做出了贡献
之前 我们在制作锡箔帽之前
收集关于我们的所有数据
让我们面对现实，kinesis实际上只是一个工具
而且它是一个异常高效的工具
它专门用于处理我们和其他人产生的大量数据
以便我们可以至少尝试对我们数字帝国做出一些意义
你也应该知道如何让它为你所用 所以让我们开始
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/071_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p71 2. Introducing Amazon Kinesis and MSK.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的
我们来谈谈流数据
当我谈论流数据时
我所说的是来自
也许来自物联网传感器的舰队
也许来自连接的车辆
也许有气象传感器在外面收集信息
温度湿度
我们的空气质量数据是流数据的一个好例子
也许你有一个车间或制造设施
在那条生产线上，有多种机械收集关于它生产的零件的信息
它也可能实时传输它的当前状态
可能零件在机器内部的位置或状态
所以 你可能在事故或问题发生之前进行预防性维护
我在这里区分流数据和其他类型的数据
假设批处理数据，因为这种数据正在发送
基本上实时
或多或少实时
这可能出于多种原因
第一 也许数据的及时性真的很重要
也许我们需要尽快得到这些数据
因为我们可能需要做出决定
或者我们需要向某人预警某事
例如
如果这些气象传感器检测到一股非常强的阵风
也许我们需要为那场气象事件的下游人员设置一个警报
或者就像我在工厂地板场景中提到的那样
知道一台机器开始磨损的时间是有益的
所以你可以进行主动的预防性维护
这样你就不会遭受计划外的停机时间
这就是流数据和我认为的区别
你知道 批处理数据
这是存储并可能每天或每周上传一次的数据
或者像这样 或者类似的东西
现在这些数据进来了
它们可以以不同的形式出现
我们可以有标准的
旧的零和一
我们可以有json文件
CSV文件
文本文件 我们可以有某种记录
我们也可以有视频数据
这变得越来越普遍
随着宽带连接速度提高
现在可以流式传输视频数据
例如
汽车捕获的视频数据中的一部分
可以流回某种总部或类似机构
这可以实时进行分析
我们也可以使用视频流进行质量测量或质量管理
也许我们有一个摄像头对准检查线
这些数据被发送到我们的收集点
然后使用一些机器学习技术进行分析
我们可以立即决定基于视频流一个部件是否符合规格
通常，流信息会回流到一个中央收集点
一个数据库或某种文件存储区域
或者类似
然后最终我们会使用这些数据来做事情或做出决定
或者类似
因为我们可能仅仅因为感觉良好就捕获这些数据
我们可能希望从这些数据中实现一些商业价值
现在，有许多不同的产品可以帮助我们捕获流数据
其中一个产品是我们可以使用的，称为Apache Kafka
是的
Apache Kafka是Apache基金会的一部分 Apache Kafka是一个开源产品，我们可以使用它来促进信息流
这个数据流的流动实际上在这里停止
然后这里有一些下游产品可以帮助我们处理这里的分析
AWS为我们提供了一个托管的Apache Kafka版本
所以我们可以安装
实际上我们不需要安装Kafka
我们只需要点击几个按钮
AWS将为我们启动一个Kafka实例
或者一个Kafka集群
当然，如果我们想要的话，我们可以手动安装它
这只是一个开源软件，公司已经在他们的数据中心运行了多年
但AWS为我们提供了托管选项 AWS是一个原生程序或服务来做这件事
流式摄入是称为Amazon Kinesis的东西
它与AWS的其他产品非常集成
如果AWS有他们的方式
我们也会使用它来进行一些分析
这里有许多服务可以帮助这些决策和运行分析
在流数据上
但我们也有像Green Grass和Amazon Kinesis的服务
视频流也可以帮助摄入这些数据
在数据流的更高位置
所以我们将在这个技能中涵盖Kafka和Amazon Kinesis
所以让我们从Kinesis开始
Kinesis实际上是一个完全管理的流式数据摄入服务
这就是它设计的目的
它只设计用于你向它投掷大量数据
这就是它设计的目的
它将处理它
它将根据负载自动扩展并处理数据
根据您配置的方式处理数据
它很好地集成到许多服务中
正如我们将看到的那样
亚马逊kinesis有不同的组件
有一个叫做提供库的东西
有一个叫做消费者库的东西
这些都是SDK的一部分
我们还有数据流
我们有视频流
可以接收视频数据
我们还有一个叫做火喉的东西
现在AWS已经改变了火喉的名字
它曾经是Kinesis家族的一部分
亚马逊Kinesis火喉
他们已经将其更改为亚马逊数据
火喉 我想大概是这样
但基本上它是我们进入控制台时会看到的同一个服务
并且它允许我们将通过亚马逊kinesis流入的数据流式传输
传输到S3存储桶或可能一个红移数据库
或者称为亚马逊opensearch的东西
我们将在另一个技能中覆盖
亚马逊kinesis的一个酷之处是
它非常可扩展
事实上 它基本上自己扩展
所以我们这里有kinesis接收我们的数据
这些数据可以是任何
也许这是天气数据或者类似的东西
数据正在输入
然后kinesis会将其交给一个叫做shard的东西
现在，shard只是用来处理数据的工具
当数据进来时，kinesis可以做的就是
它可以将自己分成多个不同的shard
这就是它如何扩展的
这使得kinesis能够非常有效地处理大量数据
因为进来的数据越多
它就越多地分解成不同的shard并将信息分散到那些不同的shard上
现在 一旦信息进入碎片中
这基本上是一种临时存储区域
我们可以配置在亚马逊Kinesis中的数据保留时间
我们可以说它可以存活一小时
三十天 三百六十五天
如果我们想要的话，它可以永远留在那里
但是最好的做法是
这不是一个长期存储区域
我们要做的就是尽快把那些数据提取出来并处理
处理数据的一种非常普遍的方式就是lambda
数据从这里进来
然后它通过shard
然后它可以被lambda函数捕获
我们不仅可以调用这些lambda函数
我们还可以做一个叫做fan out的事情
所以这个shard可以调用多个不同的lambda函数
它不必等到那些函数中的任何一个完成
我们可以以数据进来的速度以相同的速度调用同一个函数
因为lambda本身非常可扩展
它会启动新的这些函数的版本来处理数据
很快
我们有很多shard调用很多lambda函数
我们已经扩展了自己
等等 我们还可以扩展这些lambda函数到更多的lambda函数
所以也许每个小的lambda函数
然后启动一堆不同的lambda函数来做数据的不同事情
或者做不同的数据转换等
所以你可以看到这是一个非常可扩展的架构
你可能听说过术语扇出
这就是扇出确切的含义
这意味着我们将数据分散到许多服务、进程或设备上
AWS允许我们这样做，非常高效且简单
接下来是亚马逊管理的Apache Kafka流处理服务
这真是个长名字
通常简称为MSK
不要混淆与MSK3
这是完全不同的东西
如果你知道
Apache Kafka是一个开源的流数据引擎
它已经存在了很长时间
许多公司使用它
但我说
我认为我们可以通过在AWS上安装一个管理的版本来帮助人们
这样人们就不必担心服务器、管理和安装
以及所有其他事情
所以他们创建了一个管理的版本
在AWS上的传统Apache Kafka版本实际上就是在EC two上运行
C2 两台服务器 最近他们引入了一个无服务器选项
这可能对你有用，如果你想运行一个Pitch
通常来说
选择在AWS上运行Apache Kafka的人
通常是那些已经处于这个生态系统中的人
那些还没有流处理选项的新客户
可能会选择Kinesis
那么让我们看看亚马逊Kinesis和亚马逊MK之间的一些区别
亚马逊Kinesis会自动为我们扩展
而Apache Kafka我们需要进行一些手动管理
至少对于预配置版本
而现在的无服务器版本为我们提供了一些自动扩展功能
从亚马逊Kinesis方面我们可以按需付费
这主要基于我们最终通过那里推送的数据量
而现在这边的MK版本通常是基于我们选择的EC two实例构建的
这是为了预配置版本 服务器less版本有稍微不同的定价
并且它更接近于按需付费
至于我们如何与Kinesis交互
有一个专有库和一个专有协议是aws创建的
它允许我们将消息发送到Kinesis并从Kinesis中检索它们
对于亚马逊MKs
我们只使用开源的Kafka协议，这个协议已经存在了很久
现在
在设置方面
亚马逊Kinesis设置和使用起来要容易得多
我认为比亚马逊MK容易得多，MK确实需要你对Kafka有更多的了解 如果我们谈论设置速度
我们可以很快设置并启动Kinesis
但是当我们设置亚马逊MK时，我们还需要做一些其他事情
我们需要对Kafka有更多的了解，而不是对aws
所以，当我们谈论设置速度时，我们可以很快设置并启动Kinesis
但是当我们设置亚马逊MK时，我们还需要做一些其他事情
我们需要对Kafka有更多的了解，而不是对aws 所以，当我们谈论设置速度时，我们可以很快设置并启动Kinesis
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/072_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p72 3. Setting up Amazon Kinesis.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们创建我们的kinesis数据流
接下来我们要做的就是在这里有一个小的数据生成器
这将是一个简单的Python程序
这是一个相当简单的Python程序
我会一步一步解释
你可以看到各个部分
我们将使用该Python程序生成数据
数据将每10秒发送一次
我们将将数据发送到这里的kinesis端点
一旦数据进入kinesis
我们可以对其进行多种操作
在我们这个案例中 最终我们将会做
我们将其发送到一个s3存储桶
我们将使用fire hose来完成
让我们进入控制台开始工作
我们已经在ada控制台，首先
我们可以创建一个kinesis数据流
我们只需在这里输入kinesis
点击那个小图标，我们将进入这个界面
我们可以创建一个数据流
我们可以创建一个firehose
记得以前它被称为kinesis data firehose
不知为何改名了
我们还可以选择创建apache flink
以前称为kinesis data analytics
现在我们不做这个
我们将转到数据流这里
你可以看到还没有创建任何数据流
我们将解决这个问题
我们将创建一个数据流
我的程序将生成笑话
无尽的笑话
每10秒一个
我迫不及待了
这将被称为
让我们看看数据流笑话
我们可以用不同的方式做
我们可以使用按需容量
或者我们可以预置容量
我就用按需容量
不同之处在于
我们可以指定不同数量的分区
我们可以使用这个小分区估算器来帮助我们
帮助我们确定最佳的分区数量
对我来说
我只想保持简单，我们将使用按需容量
我想按需容量初始化4个分区
如果我没记错的话，仅仅是因为它不知道你将要输入什么
所以我们将使用按需
我们将来到这里的数据流设置
这里我们真的什么也做不了
但它告诉我们的是，在我们创建后什么可以编辑
所以这里所有的东西在我们创建后都可以完全编辑
所以如果我们不喜欢这里的某件事情
我们就必须等到我们创建它
然后我们就可以去改变它
所以我要点击创建数据流
它将需要几秒钟来创建那个数据流
所以我只暂停视频，直到我们的数据流可用
哦不
比我按暂停键还要快
按钮数据流变得可用
让我们在这里看看
去掉那个东西
这是我们不同的应用程序
我们可以注册不同的应用程序
如我所说 我们有不同的方式发送信息
我们可以使用亚马逊Kinesis代理
我们可以使用亚马逊SDK
这就是我们将要使用的
我们也可以使用称为kinesis生产库的东西
我们也有消费者
一旦数据进来，这些就是事情
我们可以从kinesis中获取数据并对其进行处理
所以我在这里提到了flink
我们可以将数据发送到数据火喉
这就是我们最终在这个数据流中要做的事情
然后我们也可以使用这个kinesis客户端库
从kinesis中提取数据
所以我们可以在配置这里查看
查看不同的写入能力和读取能力
我们可以编辑并更改它，如果我们需要更多的能力
但现在我们不会做任何这些事情
如果我滚动到这里
我们也有事件桥
所以如果数据正在输入
这将做其他事情
所以这有点方便
但现在让我们只留下我们的数据流
就像现在这样，我将打开另一个窗口
这里打开哪个窗口并不重要
让我新建一个标签页
因为我要找的是这里的这个东西
这个东西叫做云外壳
现在我们要做这个Python代码
我将在下面这个视频中包含它
所以你可以直接下载并使用
但要在自己的机器上这样做
或者从一个我们可能需要启动的EC two实例
这将涉及到我们
设置Python需要的一切
这里真的很方便
在控制台中我们有一个小系统
在我们点击鼠标按钮这里
它被称为云外壳
当然它已经加载了Python
实际上它也加载了Node
所以Node.js
如果你运行Node.js程序
那么你也可以使用它
这是我们将要使用的Python程序
它导入了一些东西
我们将使用boto客户端
Kinesis客户端
我们需要指定我们的区域
如果你在你的Kinesis数据流中启动了不同的区域
请随意更改此设置
这是我们给我们的Kinesis数据流的名称
我想我在这里叫什么
如果我翻回去
嗯，在这里
数据流
爸爸笑话
所以我会复制它 并粘贴在那里
所以数据流爸爸笑话
这是我们获取爸爸笑话的方式
我们将使用一个名为我可以有爸爸笑话的网站的API
它会发出API调用
它为我们获取一个笑话
然后将其打包为一个JSON数据类型或JSON格式
然后将其发送到Kinesis
您还可以看到，我们正在这里添加一个时间戳
只是为了在那里发送一些额外的东西
在下面
是实际发送记录的地方
我们有几种不同的方法可以发送记录
取决于您发送的数据量
这将影响您选择的方法
但对于我们，这是一个非常简单的过程，我们每10秒发送一个
所以我们根本不会压力系统
实际上
你可以把这个改成1秒 你可以并行运行
你可以启动500个不同的实例
这个
你仍然不会重视kinesis
因为它是为那个量级设计的
所以我要选择这里的所有内容
这是我们使用我们的云外壳的一个缺点之一
我们没有一个容易的上传文件的方法
所以我要使用nano
你可以使用vi
你可以使用Linux中你想要使用的任何编辑器
如果你不熟悉Linux
只需输入我所输入的内容
我们将其命名为jokes
点py
我们的屏幕有点拥挤
因为我把这个字体设置得非常大
通常情况下
你可能在这个屏幕上会有更多的可见性
我将粘贴这个
我们会在这里收到一个警告
它说 我们是否确定要粘贴
是的 我们确定要粘贴
所以 这发生了什么
将其粘贴到我们的编辑器中
现在我们按下ctrl x并保存修改的缓冲区
我们按下yes
保存修改的缓冲区
它问我们 这是我们想要使用的文件名吗
是的 那就是我们要使用的文件名
所以我刚刚创建了jokes
点py
从这里我们可以运行我们的笑话生成器
这将向kinesis发送东西 这就是我们在下一个视频中要做的
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/073_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p73 4. Streaming Data to Kinesis.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 上次我们离开时
我们已经将我们的代码复制到我们的笑话python中
我们已经在这里设置了我们的数据流
我们准备开始发送笑话，为此
我只需在python中输入
然后输入我的程序名称
它将开始发送笑话
事实上 它已成功发送我们的第一个笑话
它给我们提供了一些从kinesis返回的信息
当我们发送时
这基本上是收据
它说，是的
我们收到了那个消息
它给了我们很多关于那个的信息
我会在这里称这个东西为shard id
所以我们需要关注这个
因为我们需要它来找到消息
如果我们回到我们的kinesis streams标签这里
我们可以去数据查看器
我们有这个下拉菜单
shard zero 一、二、三，因为我们说了三
我将选择shard three
因为那是接收我们数据的shard
这基本上是轮转
或者更多是随机决定
作为kinesis试图均匀分配
我认为因为我们没有产生太多信息
它每十秒才进来
实际上没有多少shard之间的竞争
我认为实际上四个shard对我们正在做的事情来说太多了
但我们只是使用on demand方法设置它
如果我们使用provision方法设置它
我们可以选择我们要多少shard
所以我将选择shard three
然后我将点击并获取记录
我有不同的选择
最新trim horizon
任何像这样的东西
我将点击获取记录 如果我向下滚动这里
它可能会给你发一条消息
没有找到记录
我可以点击重试
获取记录，然后我们就去了
所以这里是最新的记录
所以这可能是这里记录
如果我们点击那个
我们可以看到它看起来像玩笑
牛铃铛的魔法是什么
牛相信
穆度
嗯
好的 我们在这里有时间戳
如果我们实际上修剪地平线并获取记录
它将向我们展示一堆
更多的记录在这里
所以我们可以查看
这里有一个在之前收到的
你叫什么名字戴着耳机的大猩猩
你喜欢的任何东西
它听不见
哈哈哈
好的 所以我们肯定有笑话进来
你可以在这里看到
它们相当一致地每10秒进来一次
好消息是
我们有数据进入我们的kinesis数据流
但我们并没有真正做任何事情
我们有配置在这里
如果我们向下滚动到底部这里
我们有数据保留
所以我们可以编辑
所以数据进入kinesis
我们可以决定我们想保留多长时间
可以是一天
可以是七天
所有这些天
我们也可以做自定义
我们可以说嘿
只保留这些东西一个小时
如果它超过一个小时
那么它将被删除并被清理出去
所以我们将保持在一天这里我们将取消
我们不需要更改
下一步将在下一个视频中
实际上我们将设置数据火喉到s 3
成为我们流笑话的终点
所以我将取消这现在 我们将设置数据火喉下一个
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/074_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p74 5. Amazon Data Firehose.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 到目前为止，我们已经能够发送我们的记录
我们可以在我们的kinesis数据流中看到这些记录
我们将为这些记录定义一个着陆点
所以我们可以到这里数据火炬
这是我们将要点击的地方
它将带我们到这个小的输入屏幕
哦 让我清除那里，我们可以创建一个火炬流
所以我们这样做
创建一个火烈鸟流
它正在问我们数据源
所以数据源可以是kinesis数据流
它也可以是mk s或mk
我现在选择kinesis数据流
对于目的地我们有几个不同的目的地
我们可以将其发送到红移开源
开源搜索 S3
然后我们可以将这些数据发送到这些其他第三方应用程序
这还挺实用的
我选择第三个
它会问我想用哪个流
我想用
我只有一个流
我选择那个
我在这里滚动并给我的流起个名字
我们叫它消防水带笑话
这里有一些其他非常实用的选项，现在我们可以转换这些记录
当他们进来时
我们可以告诉火栓发射这个lambda函数或者一个lambda函数
我们可以将数据转换为其他格式
也许我们需要将单位转换为测量或其他
我们也可以转换记录格式
我们可以将其转换为parquet或orc
这将更容易，更性能查询
如果我们在s3上查询
如果我们使用像athena的东西
如果我们想查询这个数据
使用parquet或orc可能会受益
只是作为json数据发送
这正是它接收的方式
我们可以这样做
我们可以在这里进行解压缩
我现在不会这样做
现在它正在问我们的s3桶
所以我在这里浏览一个桶
嗯 你知道的
我要去创建一个新的桶
因为我不想污染这里的这些桶
要做到这一点非常简单
我们将在这里的s3上移动
打开一个新标签页
跳到这里
我将创建一个新的桶
我们将称其为
它有letcher火喉输出
我将保留所有其他设置，创建并返回这里
到我们的kinesis火喉
我将浏览并刷新
好的，火喉输出
选择那个，我们可以更改分隔符
我们可以进行动态分区
这非常有用
如果你有大量数据输入
如果你记得我们的户外空气质量数据
你可以看到
实际上 我现在可以去看看
我将在这里
拉斯维加斯
你可以看到它按年份分开
我们可以在这里深入挖掘
它按月份分开
如果你记得我们的athena视频
我们使用分区来加速查询速度
我们可以进行动态分区
当数据输入时
它可以查看数据并将其放入特定文件夹
或给定特定前缀
以便以特定方式排列
但我们不会这样做
我们将保留所有这些设置
我将进入缓冲提示
压缩和文件扩展名
缓冲提示
这是如何数据火喉工作的
你可以这样想
就像一个小桶
如果你去过水上乐园
他们有那个小型游泳池区域
水正从那个顶部桶倒入
然后当那个桶装满
它会倾倒并溅到孩子们身上
他们疯了
这就是缓冲大小和缓冲间隔
这设定了我们必须输入的最小数据量
或必须输入的最小数据间隔
在打包并将其发送到我们的s3桶之前
所以这说的是 现在，5兆字节的数据需要传入
才会批量处理并发送到存储在
S3或每300秒
我不喜欢等那么久
我将其更改为30秒
一旦触发任意一个
很有可能会是这个缓冲区和间隔
因为我们谈论的是我们笑话中非常小的数据量
所以不太可能触发这个
因此，每30秒的数据火线将打包从kinesis获取的数据
并将其发送到我们的存储桶
我也可以选择在这里压缩数据
这非常有用
因为某些数据
你知道 如果你从高吞吐量的生成过程中获取 可能会占用很多空间
通过选择压缩数据，你可以节省很多钱
正如我们在其他视频中看到的
我们可以查询那些数据
即使它们被压缩
所以我们可以给它一个文件扩展名
如果我们想要的话
我们可以选择加密选项
我不会去碰那些
我将创建一个火线流
这将需要一点时间来创建
现在，它是活跃的
我们可以发送一些测试数据
我不会那样做
因为我们刚刚创建了它
我们实际上没有任何指标
那些需要一些时间来填充
所以我认为我们准备好回到我们的小小生成器这里
我必须让它重新连接
并且我们仍然有我们的笑话
让我清除它
让它看起来干净一点 好的
所以，我们已经开始将我们的笑话发送到kinesis 我们要关注的是这里我们的存储桶
并且这是火线输出，目前这里什么也没有
我现在不会期望我们很快看到任何东西
因为需要达到那些我们配置的门槛
所以很有可能在30秒后
我们应该看到一个文件被放入那里
它在某个目录中
我们可以下载那个文件并查看里面的内容
现在，我将让生成器继续运行一段时间
那么让我们看看，我想象着这可能已经过去了三十秒左右
让我们去看看那里
我们走吧，这里是一年时间，一个月，一个小时的时间
所以这就是默认放置在这里
所以我要点击这里
这就是数据火枪在这里掉落的文件
所以我要下载它
让我们看看那个文件
让我打开它
这是我下载的文件
你会注意到这里有一个笑话和一个时间戳
然后我们有一个笑话和一个时间戳
和一个笑话和一个时间戳
和三个笑话和时间戳
那倒是说得通 因为我们每十秒发送一个笑话
而我们的小数据火枪发送我们的数据的时间点
三是三十秒
所以三十秒后，它收集起这三个笑话
将它们全部放入同一个文件并发送
如果我们回到我们的桶这里
如果我们回到十二
二二
你可以看到 我们有一些文件在这里逐渐建立
它们很普通因为我们选择不在那里放任何类型的扩展
我可能应该做的
它使用像json扩展一样
那样至少可以识别文件类型
但这就是用数据火枪从kinesis将信息发送到s three 发送信息
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/075_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p75 6. Kinesis Teardown.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 这里有一个简短的视频，向你展示我们如何清理我们的kinesis流
我们将回到kinesis这里
如果我们不想让这个东西到处乱跑
我们所要做的就是点击那里那个小框
到这里行动并点击删除
所以我要删除这个删除
如果我想删除实际上进来的数据
并落在s3桶中
我在这里可以做的就是去s3
我可以点击这里这个小项目
那个小按钮并点击清空
它可以永久删除我的数据
所以我要去做
因为我要清理一切
因为我们将来会使用相同的设置
我真正希望你下次我们到这里
需要或想要使用kinesis数据流
你可以尝试完全自己创建
我不必走遍它
然后看看是否能做到
所以在下一个视频中 我想给你一个小的msk走通
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/076_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p76 7. Amazon MSK.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 所以在这个视频中，我想给你展示一下MSK的设置过程
为什么我们不创建一个集群
嗯
Kafka是一个非常难以捉摸的生物
设置它非常复杂
尤其是在AWS上使用管理的Kafka服务非常难以捉摸
根据我的经验
设置一个集群至少需要约一个小时
这不仅仅是点击所有小按钮
但你必须真正启动它并让它发挥作用
然后你必须在那之后调整它
然后你也必须设置所有类型的iam角色
如果你希望安全地做事情
并且如果你想要将数据发送到s3
您也可以通过firehose
您也必须正确设置某些网络连接
以及所有那些东西都远远超出了数据
工程师助理考试的范围
所以我真的不想把你拖入其中
所以我们要做的是
我们将出去假装设置一个
只是为了向你展示平台的一些功能
所以我们在这里aws控制台
我可以去mk下面
它将带我到一个小屏幕
一个小着陆屏幕，我有创建集群或连接的选项
你可以看到 我们也有s3交付，带我们到火喉
我们可以使用mk作为发送至火喉的来源
但是正如我之前所说
这相当复杂且麻烦
这涉及到超出本课程范围的事情
所以我要上去到集群
我要创建一个集群
我有几个不同的选择
我可以快速创建或自定义创建
快速创建将基本为我默认设置
但我可以告诉你，从我的个人经验
你将不得不出去并更改那些事情
如果你想使用它
如果你想正确使用它
这会增加设置过程
现在我们可以选择无服务器或预配置
就是预配置这些代理或这些
我不知道你们想怎么称呼它们
我猜它们是e c two实例类型
但它们实际上并不是e c two实例类型
所以我们可以选择这些下面的所有选项
你可以这样想，每一个都可以这样理解
就像一只小工蚁，要去收集
或者现在接收消息
如果你只是为了开发目的这样做
你可以总是选择这个小的三个
将会很好
所以我可以收集那个存储
一百千兆字节是完美的
然后它告诉我现在将要设置哪些东西
一旦这个集群设置好，有一些东西我无法改变
你可以看到vpc
我不能更改vpc
不能更改子网
不能更改很多这些东西
这就是为什么如果你在设置它时
你可能有一些非常精确的
我猜设置，是你想要完成的
这不是你随意设置的东西
只是为了给它一个
给它一个尝试 我想你可以
如果你想要浪费一天去做那件事
但我们也可以点击这个自定义
这将给我们更多的控制来设置所有这些设置
在这里我们可以指定我们的经纪人
然后我们可以指定我们想要的经纪人数量
每个区域的经纪人数量
如果我们想让我们的经纪人分布在三个不同的区域
那么这将肯定给我们最大的可靠性
在这里你可以看到
当我们做定制
它会默认为EB存储的1TB存储
所以请注意这一点
因为你需要在配置中为存储付费
我们可以进行自定义配置
这就是我做的
当我早些时候尝试这个时
我选择了一个自定义配置
使我能够向代理发送新主题
而不必抱怨
我必须从行政角度设置它
让我给它起个名字
我的集群
点击下一步
它会问我们的vpc
当然 很好
选择我的子网
所以典型地
当你设置这个时
你可能有一些非常精确的事情
如果你想要公共访问
它是默认关闭的
如果你想要重新打开它
你将不得不设置你的分布式系统
等等 我不知道
我不到五分钟 三十分钟
它说十五分钟
但在我的经验中，需要更长的时间
一旦它运行起来 然后你可以回去
更改此启用此
然后设置大约另三十分钟
等待期 这有点烦人
我们可以选择我们的安全组
然后我有不同的访问控制方法
默认情况下，他们推荐的是IAM角色基本文凭认证路径
这是一个很好的路径
它非常安全
问题是，从演示角度来看，设置这相当复杂
也设置S3交付的VPC设置via数据
Firehose也非常复杂
我真的没想过这对课程会有帮助
所以我们也可以进行未认证访问
我们可以进行TLS
我们可以设置我们的加密
点击下一步，现在我将获得监控选项
如果我想用Prometheus监控
如果我想把这些代理日志交付给Amazon CloudWatch
我给你一个提示
如果我在这里不选择任何内容
这将非常困难
了解你的代理关于消息的情况
所以我推荐至少将它们发送到CloudWatch
甚至可能发送到S3
我认为这可能是一个更好的选择
你也可以使用Firehose将它们发送到S3
所以我们可以点击下一步
它将给我一个选项来审查一切
如果我真的要创建这个集群
我将点击这个按钮
它说最多需要十五分钟
但它撒谎
它需要
至少今天
搭建集群花了三十多分钟
我尝试了五到六个不同的设置来搭建集群
其中一个集群在更新时卡住了
直到三个小时后，它才决定解冻并恢复正常
然后我可以删除它
所以 不管怎样
这就是亚马逊msk的快速教程
如我之前所说 如果你还没有完全投入kafka，我会强烈推荐使用kinesis
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/077_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p77 8. Validation Amazon Kinesis and Amazon MSK.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我们来分析这些问题
第一个问题 你的任务是构建一个实时数据处理系统
该系统可以摄入大量来自物联网设备的数据
数据需要在接近实时的情况下进行处理，并存储在亚马逊的S3中以供进一步分析
以下哪种服务组合对于这个用例来说最有效率
现在，你将
你将在考试中遇到这样的问题
现在，让我们思考什么是成本最低
或者最有效率的，或者最容易的
这是我从来不喜欢的那种问题
因为那非常主观，有时候
但是，这个问题问的是最有效率的
所以我们可能会寻找一个答案
这将是最自动化的
它将迫使我们做的最少的手动工作
所以，让我们从上面开始
亚马逊Kinesis数据流和AWS Lambda
我们可以通过Lambda函数将数据上传到S3
但是，让我们继续看看是否有更有效率的方法
亚马逊MK和亚马逊RDS
我真的不认为这会非常有效率
如果我们的目的仅仅是从物联网设备获取实时数据
首先，我们需要设置一个MSK集群
然后，我们不知道如何将数据发送到RDS
但这并不是我们的最终目的地
然后我们需要将数据发送到S3
所以，这似乎是一个疯狂的路径
来完成我们需要做的事情
我们有亚马逊SQS和亚马逊DynamoDB
但是，这似乎是一个绕远的方法
来将数据发送到S3
我记得有一个服务，它是专门为将流数据发送到S3而设计的
那就是亚马逊Data Firehose 以前称为Kinesis Data Firehose和S3
所以，这是我的最终答案
我将坚持这个答案
下一个问题
一家零售公司使用亚马逊Kinesis数据流来摄入来自各个商店的销售数据 他们希望在实时处理数据
使用AWS的一个服务，该服务可以自动扩展并处理不同流量的水平，而无需手动干预
他们应该选择哪种解决方案，以最小的管理要求
好的
我们有配置了自动扩展的亚马逊MSK 我不太确定这是不是一个功能
如果我们谈论配置的MSK
然后，如果你记得，MSK的一个缺点
至少是在非无服务器形式使用时
是
现在我们基本上只剩下手动扩容了
我们又有了kinesis数据流，手动分片扩容
那个词，手动，告诉我，我的管理要求会上升
所以我会继续寻找
亚马逊SQS标准队列，再次手动扩容
这些都是明显的迹象，你可能不会选这个
如果它要求最少的管理
kinesis数据流，按需容量模式
这正是我们所要的
因为我们设置按需容量后
它就会根据我们需要进行扩容
那就是我的最终答案
接下来，到我们的最后一个问题
一家公司想要构建一个解决方案，收集和处理来自各种应用的日志数据
实时处理 他们计划使用亚马逊kinesis数据流来摄入日志
然后使用一个服务，实时查询流以查找异常并发送警报
你会推荐哪个AWS服务来实时查询数据呢
如果我们看下面
亚马逊MSK与Lambda消费者，嗯
我想那应该可以
让我们继续查看亚马逊SQS与Lambda函数，嗯
我们在谈论流数据，日志处理
SQS真的不是那个工具
而且，在我们这个技能中，我们只讨论了kinesis和MSK
所以，这会给你一个提示，这不是答案
亚马逊数据火炬
嗯 我们可以将我们的数据发送到S3，使用数据火炬
但这并不是真正的实时数据分析
最后我们有亚马逊管理服务，Apache Flank Studio
啊哈 这是一个我们可以用来进行实时分析的服务，流数据
这是新版本
或者新产品的名字，以前的服务叫做亚马逊数据分析
或者亚马逊kinesis数据分析
但他们正在引导所有人使用这个服务
现在 亚马逊管理服务，Apache Flank Studio
他们说，设置更简单，功能更多
那就是我的最终答案
我希望这对你有所帮助
我想感谢你观看 所以，我会继续寻找
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/078_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p78 1. Skill Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


问候
你们这些美丽的学习者
欢迎回来，另一个
记得那些美好的旧时光，当集成应用程序意味着与一堆自定义代码搏斗
交换数据 转储文件
就像宝可梦卡片一样
以及为这些浪费的轮询作业设置新记录
所有这些都加上那些深夜批处理运行
这给我们的尝试带来持续的压力，使我们难以获得充足的睡眠
这是怀旧
然后事件驱动架构出现了
让我们能够实时处理数据
每当需要时
所以现在这种低强度的压力伴随着我们，白天和黑夜
就在这个技能中
我们将深入探讨亚马逊事件桥接，这是一个aws服务
它是亚马逊云观察的衍生品
与大多数衍生品不同
它实际上比原版更好 让我们开始吧
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/079_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p79 2. Meet Amazon EventBridge.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们来谈谈ventbridge
但在我们讨论vent bridge之前
我想谈谈云观察
云观察是一个相当流行的服务
在aws的所有服务中都很常见
它是我们监控事物的方式
它是aws的主要监控服务
我们为什么在这里讨论它很快就会变得明显
但我不确定将这个服务放在哪个技能中
我想这作为任何服务的选择，或者作为任何地方来放置它，都是相当不错的
因为这里我们将使用CloudWatch
来大致了解我们在这个小小的EventBridge管道中发生的事情
正如他们所说，称之为管道
所以CloudWatch是种中心中央监控枢纽
它是种中央监控工具
它与大多数AWS服务非常集成
我想不出有任何一个单一的服务
没有某种CloudWatch指标
我们可以设置它
在所服务的所有领域，就有成千上万种不同的指标。
那就是我们可以将它们放在一起
在日志组中
或者在仪表板中
就像小网格和图表一样
一些东西 就像那样我们也可以使用云观察来做到这一点
但我们不会花太多时间在建造那些东西里面
云观察 因为这超出了数据工程师助理课程的范围
所以我们将使用它在这个技能中，以查看我们的lambda函数在做什么，所以
Cloudwatch也有一个代理，我们可以在我们的本地系统上安装它
并且它也可以监控我们的本地系统
现在，Cloudwatch有不同的部分或组件
第一部分是度量
这些只是我们可以设置的测量
我们可以在像线图或条形图之类的图表中查看它们
然后我们有警报，警报非常重要
因为如果我们的度量超出范围
如果我们的容忍度超出了
然后我们可以拉响警报
这个警报可以是任何形式的自动化过程
或者它只是一个发送到某个电子邮件地址的电子邮件，上面写着嘿
这个值超出了范围
我们还有日志
所以每当aws服务运行时
有很大几率他们会生成某些日志
至少如果你把它打开
他们会生成某些日志
并且它会将这些日志通常发送到云观察
我们可以访问这些日志，搜索这些日志，在云观察日志中过滤这些
然后还有一个叫做洞察的东西
而洞察是什么
它试图使用一些人工智能和数学原理
来为你自动生成基于这些指标的一些总结
现在最终有一些叫做云观察事件的东西
并且那个服务的功能已经转移到我们现在称之为亚马逊事件桥的服务
我们完成了，万岁
所以现在我们正在谈论事件桥
这就是我为什么想要介绍云监控的原因
让你们了解事件桥的起源故事
因为云监控中的事件非常灵活，AWS决定将它们从云监控中移出
使它们成为一个独立的服务
这是有道理的
因为它可以被用来
或者事件桥可以被用来做很多
很多不同的事情
那么事件桥是什么
嗯 Eventbridge 只是一个我们可以用来处理事件的框架
大致如其字面意思
事件桥接 它可以将一个事件的桥接至我们想要执行的其他事物
我们可以使用它来编排
这就是人们所说的编排的技术术语
我们可以将这些过程串接起来
根据一个过程的结果
我们可以让它执行其他过程
现在，AWS 内有其他服务可以做这些事情，并且做类似的事情
我们将把它们留到另一个技能再使用
但目前而言 只需知道EventBridge是我们用来串联某些事件的有用的编排层
事件桥的一个不错的地方是
那是他们已经在非常受欢迎的服务中构建了大量的第三方集成
并且日志监控服务和类似的东西
每当在那些系统中发生某事时
Entbridge可以检测到如果我们设置该集成
然后我们可以触发一些事件或响应，无论第三方系统中发生了什么
并且我们可以在这些事件发生时触发这些事件
基于意义的事件
或者我们可以定期安排这些事件
好的
一切都始于被称为公交的东西
这并不是你上学的公交车
或者类似的东西，这只是
如果你熟悉电子或电力，你可能会听说过总线条，这其实就是一种
通常它是一个金属条，多个电线可以插进去并从中获得
流动的电流
这就是巴士的原理，事件桥
在这个巴士上我们可以有一个事件
我们可以配置事件桥在特定区域监听
或者监听一个服务
然后我们可以用规则处理这个事件
一个例子可能是我们检测到事件
但是该事件发生在非生产设备上
所以我们不想做一堆活动
所以我们可以过滤掉
这是一个例子，即在特定事件上应用一个规则
如果这个规则说是的
那么我们就处理这个事件
然后我们通常有一个目标
我们会有一个事件发生
然后我们会有些事情需要对外界的其他事物发生
这就是目标
现在所有这些东西放在一起
这个小链条被称为管道
这就是我们将从kinesis设置的东西
我们将设置一个管道，该管道将从kinesis获取我们的数据
它将最终将数据放入dynamo db
但我们也会使用某种中介来增强数据
在将其放入dynamo db之前
你有多个公交
你可以有一个事件驱动的公交
你可以有一个基于日程的公交
公交只是帮助我们将事件分类和区分
也许你不想关注一些其他事件和这些其他系统
所以公交车可以让我们分开处理 让我们去控制台开始构建一些东西
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/080_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p80 3. Getting Started with EventBridge.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我先花一点时间画出我们要构建的东西
这样会更容易想象
我们将从一个我们在其他技能中用过的程序开始
kinesis技能
我们将生成些老爸笑话
我们将使用这个Python程序
它将生成老爸笑话
它将将其发送到kinesis
一个kinesis数据流
在那个Kinesis数据流的另一边
我们将使用Event Bridge连接它，简称Ventbridge
然后，我们可以利用这个数据流做很多事情
这是一条充满爸爸笑话的数据流
我们首先将要做的事情之一是创建一个Lambda函数
将这个数据提交或插入到DynamoDB表中
我知道我们还没有在实践操作中接触过DynamoDB
但我们将要做的事情非常简单
这将是一个很好的练习，让你有机会在实际操作中体验DynamoDB
获取一些使用DynamoDB的实际操作经验
这将成为我们的第一步
第二步是使用通风桥
我们也可以在这里设置一些小的中间步骤来实际增强这些数据
所以我们要做的是当数据进来时
我们将调用另一个lambda函数
我们将使用aws翻译
我们将把那个原始的笑话翻译成另一种语言
我们就选西班牙语吧
所以它会返回一个包含英语和西班牙语翻译版本的消息
这个消息中既有英语的翻译也有西班牙语的翻译
然后我们会把它发送到这条线上
到这个lambda函数
然后进入dynamo db
那就是我们的第二阶段
不多说了 让我们开始构建
我们在aws控制台
首先 我们需要创建一个kinesis数据流
所以我要去kinesis这里
如果你还没有看过kinesis技能
我强烈推荐你去看一下
但创建数据流非常简单
所以我将要快速地走一遍
所以我要去创建数据流
然后在数据流下
我将其命名为笑话
我们只是使用按需
你可以使用预留容量
因为按需真的给了我们比我们真正需要的更多的容量
但我只想为了简单起见就这样了
如果你改变主意
我们可以改变所有这些东西
一旦我们创建了这个数据流
所以我要点击创建并开始处理
现在 我将切换到另一个标签页
我们需要做的事情之一是
我们需要一个生成器来为我们的笑话
仅仅使用kinesis对我们没有帮助
因为我们需要一些数据流入kinesis
所以我们将使用一个python程序
它将生成笑话
你可以使用一个
C Two系统来做这个
你可以甚至设置它在你的家或工作
无论你在哪里观看这段视频
然后你将不得不处理iam规则和策略
并设置以便你可以从那个系统访问
相反我们可以做
这稍微简单一点
是使用这个小家伙在这里
它叫做云外壳
云外壳只是一个小电脑
它坐在那里
当我们想要使用时 云外壳的好地方是
因为我们在这里登录在控制台
它将假设或采用我们的权限
我们不必担心设置iam角色
仅仅将笑话发送到kinesis
如果你看kinesis技能
我们使用了一个python程序
我将向你展示这个程序
我将在下面附上它
我们不会详细讲解
因为我们在那项技能中已经覆盖得很好
但我确实想做的一件事是
我想确保这个流实际上正确
我的流名是dad jokes
我在us east two并且快速
这向外界发送请求 有一个api网站
它给我们一个笑话
它将把这个笑话放在一个叫做data的结构中
并添加一个时间戳
然后将这个记录发送到kinesis
并且每10秒发送一次
我现在可以做的是
我将控制控制C
现在我可以回到我的控制台
在这里我们走吧
我将这段代码粘贴到一个文件这里
云壳的一边
是这里没一个有效的方法来获取文件
当然我们可以将其上传到github或一些公共网站
然后使用git或curl将其下载
但这只是更容易
我将使用nano
你可以使用vim emacs
无论你想要生成什么
py现在一旦这上去了
我将粘贴
它会在这里给我一个粘贴的警告
是的 好的
它已经将那段代码粘贴进去了
现在我将按控制X保存修改的缓冲区
是的 这是我想要的文件名
按回车 然后我的文件就出来了
所以现在如果我的数据流可用
是的我现在应该能够启动这个
我们应该开始发送一些笑话
在那里我们去
我们成功发送了我们的第一个笑话
现在 重要的是你想要关注 shard id
它将告诉你你可以找到这个消息的地方
现在 这里这个小部分类似于收据
我们的程序发送了这个
这个dad笑话和kinesis说非常感谢
这里是你的收据 它确实收到了
所以它给了我们这个东西
所以如果我们在这里去kinesis并去数据查看器
让我腾出一些空间好的
我将选择一个分片 这里是为我们设置的分片
那条消息落在了第三个分片
所以我将去这里并点击
获取记录 不知为何它总是给我发这条消息
我再次点击
它说嘿 我刚找到一个记录
所以这就是那个碎片上最新记录
我看到我的丈夫在搬运装满铁衣服的洗衣篮时跌倒
我目睹了这一切
哇 好的
所以这里有一个时间戳
这是我们发送到kinesis的数据
所以我现在要结束这个视频 在下一个视频中 我们将设置一个dynamodb表和一些lambda函数
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/081_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p81 4. Preparing DynamoDB.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们现在又回到了控制台
如果你还在运行这个小程序
我建议 这里是点击那个小窗口
按Ctrl + C退出循环
因为我们不想只是将烂笑话发送到kinesis
只是让他们堆积
我们还没有真正处理它们
所以我们现在可以按Ctrl + C
如果你想关闭它
你点击这个小小的X光这里
如果你在任何时候想提起它
从控制台的任何地方
你只需点击这里的这个小图标
所以创建DynamoDB表
我们只是去DynamoDB并点击创建表
我们在这里给它起个名字
搞笑老爸，这个名字适合我
现在我们被要求指定一个分区键
如果你记得在Dynamo DB的技能中
我们在谈论分区键
选择合适的分区键非常重要
因为它会影响DynamoDB的性能
如果我们谈论的是高并发
我想要在这里做的
理想情况下是使用完全随机的分区键
我现在正好这样做
如果你记得我们的结构只有笑话和时间戳
所以我在将此插入到此表中
添加一个UUID或好的
所以我将此字段命名为玩笑ID
笑话ID
它是一个字符串
没问题，这样可以
我不会设置排序键
在这里我们有默认设置或自定义设置
我将选择自定义设置
如果你在DynamoDB数据库表中有数据
你不经常访问，可以通过选择标准低频访问来节省一些钱
你不经常访问的数据
但我们谈论的数据量只是九牛一毛
所以我们不会处理那个
我们将在这里读取
写入容量 你可以预先配置它
或者你可以按需进行
我只是选择按需，让它为我调整
我不会在这个上面创建任何类型的索引
我不会处理加密
数据保护资源策略
我不会碰那些东西
我现在只需要点击创建表格
这将创建我的表格
这将花费几秒钟来完成
所以我要结束这个视频
然后在下一个视频中我们将创建函数
Lambda函数 它将将我们的记录插入这个数据库
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/082_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p82 5. Target Lambda Function.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们创建第一个lambda函数，以便这样做
我们必须去lambda
我们可以在这里搜索
或者我最近访问了它
所以我将点击那里，我们前往lambda
我们将在函数下，创建一个函数
我们有一些选项
我们可以从头开始编写
我们可以使用一个蓝图 这有点像我们可以使用的内置函数，并进行调整
或者你可以使用一个容器映像
我将从头开始创建一个
我们将这个命名为right to dynamo
对于我们的运行时我们将使用python
你可以看到有许多不同的运行时
我们可以从这里选择
但我们在这里的项目中只使用python
现在我被呈现了我的架构
现在我可以选择x八十六或arm六十四
我们所做的并不重要
建筑风格实际上并不重要
你将会节省一点
如果你选择六十四臂
如果你正在处理某事
那并没有真正遵循特定的架构
所以我要设置arm六十四
现在我将要选择这个小的更改默认执行规则
我想要创建一个全新的角色，使用我的lambda权限
现在，为什么这对我来说很重要，是因为一旦它创造了这个
这就是这个lambda函数将要使用的角色，以便尝试执行
现在我需要做的是也让这个角色能够将数据插入到dynamo db中
因为这是我们将在这个函数中要做的事情
但我会稍后设置
所以我们将创建一个具有基本lambda权限的新角色
我将创建这个函数
当它正在创建时
我将切换到我们将要使用的代码
所以这就是代码
我们这里有一些导入
实际上这并不是一个编程课程
但是，我会把这个包含在内
作为下载链接放在这个视频的下方
如果你想下载并粘贴到你自己的lambda函数中
那完全没问题
我们这里有dynamodb的定义
那就是笑话
你必须把这个改成你的数据库
它会做这些事情
它会接收这些发送的事件
那就是一个lambda的事情
这些事件会被发送进来
那就是我们将要处理的数据
所以我们首先必须解码数据
因为事件桥将数据发送到lambda
它将以基六十四编码格式发送给我们
实际上这是一件好事
这相当安全
如果里面有一些奇怪的字符或者什么
这是有效地保存所有那些东西
所以我们将解码那个
我们还将分配一个独特的笑话
ID 这就是那个uu id
我刚刚告诉你的
所以 这里基本上是一个随机数生成器
然后我们将我们的项目放入我们的表格中
如果一切顺利
我们将得到这个返回代码
这意味着所有内容已成功插入
所以我要做的是控制C
然后回到我的屏幕这里，到lambda
它在那里，它就在那里
这是我们的lambda屏幕
所以我们有不同的标签
这里有我们的代码标签，测试监控
配置别名等等
所以我要选择所有这些东西并控制C或Ctrl V
这样我就把我的代码粘贴进去了
现在我已经测试了这个代码
所以我知道它将符合我们的目的
但让我告诉你
当你第一次尝试使用lambda时
试图构建一个函数
会有很多试错
我已经经历过所有的混乱
所以幸运的话
这将会正常工作
我可能刚刚诅咒了自己
所以我要保存这个
我们将部署它
部署实际上会将其发送出去，使其对其他进程可用
所以我要做的另一件事是转到配置
我将转到权限
目前您可以看到，我们只有云监控日志
作为在这里具有访问权限的资源
当这个lambda函数将要执行时
它将使用此安全设置
我想做的事情是
我想添加一些东西
我想添加一些Dynamo DB的东西
允许它与Dynamo DB进行交互
因为我们正在使用的表
我将点击这个滚动条
它将带我出去，我是
然后我可以到这里来添加权限
我只是要附加一个策略
因为AWS为我们提供了许多预先制作的策略
这可能不是最好的安全措施
但我只想使用DynamoDB的全面访问
通常 你可能需要锁定到仅此函数所需的活动
但只是为了方便
我将使用全面访问
然后我将
它实际上已经在添加策略时保存了
把它放在那个策略中
所以现在这个角色有那个权限
如果我回到我的执行角色标签这里
让我刷新
我们现在正在查看更多的访问权限
更多的访问权限是Dynamo DB策略的一部分
所以现在，正如我们所看到的
现在 这个函数有能力写入我们的数据
应该写入我们的数据
现在我们必须回到EventBridge
并设置这个函数作为桥梁的目标 我们将在下一个视频中这样做
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/083_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p83 6. Configuring EventBridge.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 所以我们已经创建了我们的lambda函数或者插入到数据库中
Lambda函数 我们已经创建了我们的dynamodb表，现在正是时候创建我们的vent bridge pipe
我们可以回到kinesis这里，然后回到我们的数据流
这是我们的笑话数据流
如果我们在这里滚动
让我让它变得更大在这里，然后我们在这里向右滚动
我们有一个叫做event bridges pipe或者event bridge pipe的东西
它给我们一个小的这个小图表在这里
它说它将从这个数据流开始
我们有机会过滤掉某些记录
如果我们不想处理某些记录
我们可以然后丰富或转换那个数据
我们将在我们设置这里的第一块之后做这件事
我们将丰富那个数据
然后我们可以将其发送到目标
在这个情况下目标是一个lambda函数
所以我要往下到这里将kinesis数据流连接到管道
它将在这里弹出一个小窗口
哪个是事件桥接
我将给它起一个名字
老爸笑话
π
并且我们这里有一个小的流动
所以这种类型的近似我们见过的图形
因此，我们的来源在这里已经被预先定义了
我们的起始位置是
我们就只使用最新的那些
我们不想回到过去处理所有已经公开的事情
所以我们将使用这里可以过滤的那部分
我们可以丰富，我们将在几秒钟内完成
但对于我们的目标
让我们继续设置
我们的目标服务将是aws lambda
现在我们也有在这里转换输入的选项
现在 我不会在这里选择任何东西
因为撤销起来有点困难
一旦我们设置好
这有点麻烦 但基本上这样做可以让我们发送消息时，而不是发送完整的消息
我们可以发送与我们继续跟踪相关的片段
这有点像缩小规模
我必须选择我的功能
我将选择到 DynamoDB 那里
至于调用类型
我将选择同步
我们也有异步
同步非常方便
当你试图调试问题时
因为事件桥会做的事情是
是将那个指令发送给函数
然后等待响应回来
如果选择了异步
它就会发送、发送、发送
所以我们就选择同步
然后我点击创建管道
所以我们完成了
我们的创建请求被发送
这将需要几秒钟来创建
如果我在这里源和目标
我们可以看到这些设置得当
现在，一个重要的步骤在这里是执行规则
这定义了什么权限
这个事件桥接管道必须处理我们的数据或连接到我们的服务
所以如果我们深入研究
我们将看到它已经被填充了两个策略
这里 一个是这个kinesis管道源
这允许它访问我们的笑话
Kinesis源
然后它也会填充我们的目标这里
我们的lambda管道目标，并允许这个管道能够调用那个函数
直接访问Dynamo
我拼错了
但这没关系
所以当我们回到这里，在这里添加一个中间步骤时
有时它会自动被拉取，大多数时候
你必须亲自出去，手动将该函数添加到该角色
这次执行的角色
所以我们将要这样做
当到时机成熟 但我认为一切就绪
这个已经运行起来 这非常好
所以我们可以做的是
我们可以开始我们的流
开始我们的老爸笑话生成器
你们会注意到的一件事情是
如果我们回到管道
它在运行 我们可以选择这个
如果我们想要
我们可以停止它
这样就会停止管道处理任何数据
这非常有用 如果你注意到有空气
嘿停止去修复空气
然后重新开始
但我们将继续在这里运行
我们将出去，看看这里
好的 我们将在这里出去
打开我们的云外壳，我将重启我的笑话生成器
它将把东西发送到kinesis
在这种情况下，kinesis应该将该信息发送到事件桥
哦 让我们清除那个
我将前往云观察
因为我想看看当那些信息进来时发生了什么
所以如果我们去云观察
我将在日志组下面，然后我可以查看
这里是管道 我可以查看它
或者我可以查看这里的函数
正确 所以动态
所以我将点击它并点击开始尾随
这将给我一个实时视图我的函数的执行
有时这里需要几秒钟才开始流动
所以我们等一下，好的，我们就有一个
至少有一个部分
看看另一个是否进来，好的
所以它说，嘿
它解码了这个笑话
在那里
看起来因为我们这里没有收到任何错误消息
我想我们成功地插入了那些
所以这里我们可以做，出去动态
我将打开一个新标签
这里有我们走吧，去表
那是笑话
从这里我可以点击探索表项
如果我在这里下去，嗯
这是我们的数据 所以
如果我刷新这里
有九个
所以应该有一个每10秒左右进来
如果我们钻到这里
我们有我们的笑话id
这是我们的分区键
我们有笑话本身
然后我们有时间戳
看起来我们的来源正确实际生成那些东西
我们的目标正确实际插入它们
让我们提高赌注，通过插入
或者至少将这个笑话翻译成西班牙语
使用中间函数
我想我发音是正确的
使用增强功能
以便能够做到这一点 这就是我们在下个视频中要做的
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/084_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p84 7. Enrichment Lambda Function.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 现在是时候创建一个另一个lambda函数了
所以我们要回到lambda这里
这是我们的right to dynamo
我将创建一个函数
让我们称其为translate
这将是python
我将选择arm六十四
我们将让它创建一个默认执行角色
我们可能不得不去那里事实上
我知道我们必须出去添加访问权限
访问aws翻译api
所以我们要创建它
好的 所以它在那里创建了函数
我们会到这里的代码
让我看看这里的代码
所以这里是我们要在这里读取的代码
因为它必须解码那个数据
因为记得ventbridge会以base64编码格式发送给我们
所以我们要解码那个
我们要提取时间戳中的笑话
然后我们试图翻译那个
我们使用AWS SDK中的翻译API
然后我们会得到响应
希望它能正确翻译
然后我们会重新设计数据的外观
我们将其作为笑话:发送回去
然后我们将在笑话中
然后是es
西班牙语翻译笑话的语言代码是什么
我们应该在这里有一个笑话项目
有两个子项目
一个是英语的 一个是西班牙语的
然后我们有我们的时间戳
在这个流中，我们还没有我们的笑话ID
所以它将在我们为目标使用的同一函数中动态分配
我们使用DynamoDB插入
所以到这一步我们不用担心笑话的id
所以它会发送这条消息回去
它会将其编码回base64并发送出去
所以我要选择所有然后复制
然后回到这里
然后粘贴，这样我们就完成了
现在我需要保存文件
保存或command或control s
然后我们需要将其部署
现在，这是至关重要的一步
因为我经常创建一个lambda函数
然后我会忘记部署它，疑惑为什么它不工作
为什么没有反映我最新版本
嗯 因为我没有部署它
你得部署它
好的 所以现在我们还得做一件事
如我所说 我们必须给它访问翻译的权限
API 你现在可以看到
它具有非常少的访问权限
所以我要出去到权限这里
点击这个
它将带我到“我”这里
我可以在这里向下滚动并添加一个策略
我只需搜索“将文本翻译为目标语言”
在这里，我们将使用“全权访问”
现在我们将使用全权访问
我们可能只需要只读权限
因为我认为此可以满足我们的需求
但我选择全权访问
所以我会添加这个
现在我们的角色已经准备就绪
它应该能够为我们进行翻译
所以我们回到这里
我们会刷新一下
现在我们可以看到不同权限已经被授予了那个策略
现在我们可以回到事件桥
如果我能在这里找到它
事件桥那里，我们到这里管道这里
这就是我们的爸爸笑话
管道，我要在这里进去
我现在要停止它
我们不需要它运行
然后一旦它停止，它就去了
我要编辑它
所以现在我们可以添加一步丰富步骤
所以我要点击那里那个框
我将在这里选择aws lambda
对于函数
我将选择翻译笑话
当我们谈论增强步骤时
它们总是以同步方式进行
我们没有异步或同步的选项
这是有道理的
因为你正在翻译或转换
你在数据流中改变数据
你不想让系统随便扔出去并希望最好
好的 我们还可以选择更改输入或转换输入
所以我们只能发送我们想要的某些字段
我们不会那样做
我们只是发送所有内容
然后我会点击这里的下一个
它会带我到那个目标
然后我会点击更新管道
这里它说嘿
确保您更新这些权限
这样你就可以调用你的新函数
我现在就要这样做
我还没有做
但我将要这样做
所以我要点击那个并点击更新管道
好的 我们已经更新了我们的管道
我们在这里添加了我们的增强步骤
但我们仍然需要向执行角色添加lambda函数
以便我们在管道中实际运行该函数
所以我们在这里查看我们的执行日志
点击这里，然后我们在这里查看
你可以看到我们还没有为那个lambda函数设置政策
所以我要做的就是重新使用这个政策
我将使用它来包含我们的翻译函数
所以我点击编辑
这将带我进入这个小屏幕
我将转到我们的翻译函数并复制ARN
然后返回这里，好的
我在这里有一个拥挤的屏幕
被炸毁 所以我要去这里
粘贴
必须围绕它
让我滚动并放一个逗号在那里
逗号，好了
所以我添加了那个功能
我们创建的新功能，使其能够调用
我将点击下一步
保存更改并更新了我的角色
现在我如果回到这里的事件桥接器
你会注意到我的事件桥接器已经停止
我将开始它
我将回到云外壳
我将启动我们的Python生成笑话程序
然后我将尝试去
让我们去云观察，看看我们是否能捕捉到那个翻译器实际上正在运行
哦，我有一件事想做
我将取消它
我将在这里到lambda或不是lambda Dynamo DB
因为我们这里有这些数据
这些数据是以旧的格式存在的
我们的新格式会有所不同
所以我想做的是
我想删除这里的所有数据，为了做到这一点
我可以只选择
我希望我没有很多
选择那个，删除项目
刷新 哦，还有五个
选择那个，删除项目，好了，我们现在有一个空的表格
所以现在我要回到这里
重新启动我的笑话生成器
转到云观察
我想我已经启动了我的事件桥
是的 它在这里运行到云观察
向下滚动，刷新
在那里，在那里
我们有翻译笑话
你会注意到，直到真的有东西被执行，它才显示出来
所以，我在这里，我可以看一些日志
这里是进来的笑话
看起来它能够处理它们
好的 我真的没有看到任何错误
所以现在我们可以转到我们的DynamoDB表
刷新
嘿 看，我们有七个记录
如果我们点击其中一个
这里 我们有我们的笑话ID和我们的分区键
我们展开它
我们有在这里
为什么工人被橙汁工厂解雇了
缺乏集中
然后是西班牙语的翻译在这里
无论它是否准确
我不会说西班牙语
足够好来说我不知道那是否准确
但是不管怎样，你可以看到我们如何增强数据
当它通过Kinesis流入时
这就是使用事件桥和Kinesis来增强我们的数据，并将其存储到 在这个案例中 一个DynamoDB表
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/085_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p85 8. Teardown.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 所以我们想拆除这个
如果我们只是让它继续运行
那么它就会一直生成笑话并不断翻译
并且可能会让我们花费一些钱
所以首先，第一步
让我们取消我们的小流程
在这里，按Ctrl + C来退出
我们已经完成了
所以我们现在可以关闭它
接下来，我们要去停止我们的事件桥接
这将停止其处理
好的
我想它已经成功停止了
我们可以删除它 因为我们不再需要它
删除
它将删除它
现在，我可以在这里查看我的功能
如果你想保留这些功能
这样做不会有任何伤害
它们不会让你花费任何费用
记住，关于lambda
只有在它们执行时才会产生费用
但我喜欢保持工作环境整洁
所以我要删除这些
在这里删除它们
我们走吧 好的
现在我要去我的DynamoDB表
我要删除它
最后，我要去Kinesis并删除我的Kinesis流
因为我不再需要它
删除 就这样
我们已经将自己重置回了正常状态
或者回到了我们的起点 所以现在你知道如何使用EventBridge和Kinesis一起进行数据流
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/086_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p86 9. Validation Amazon EventBridge.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 现在我们来一起解决这些问题
首先看一下
这里有很多单词
有时候AWS会试图这样做
问题中包含了很多单词
但如果你仔细阅读
有些内容实际上并不重要
所以你可以忽略它们
我通常的做法
当我试图阅读这里的问题时
然后我会往下读答案，试图理解它们
看看哪些是完全荒谬的，完全不符合
然后我可以缩小范围，可能剩下两个选项，甚至一个
首先，我们在配置一个kinesis流，使其触发
一个事件桥接管道，将文件写入s3，采用专有格式
通过自定义lambda函数
数据需要从redshift数据库补充
通过sql查询，然后再保存到s3位置
数据需要从redshift数据库补充，通过sql查询，然后再保存到s3位置
以下选项中哪一个是正确的
更合适的 选择最佳选项来正确处理
处理这个需求
这很有趣，因为他们说最佳选项
而且他们也告诉我们以下选项
通常当我看到这一点，这意味着也许最好的推荐方式
所以aws推荐的做法可能不在这里
所以他们在测试我们的知识，以确保嘿
如果我们不能选择最佳推荐方式
还有什么别的方法会起作用
那么我们开始逐步处理这些
使用事件桥接存档临时存储事件并重新播放它们
在从dynamo db获取附加数据后
嗯 我们可以立即消除这个
因为我们谈论的是红色偏移
我们不会从dynamodb获取任何数据
所以那里有一个毫无意义的项目
下一个 我能看到这也涉及到了DynamoDB
所以这可能是同一种情况
设置一个事件桥规则，首先触发一个DynamoDB流
然后将增强的事件传递给Lambda函数
好吧，再次
这与DynamoDB无关
我们可以完全消除这个答案
太好了 这就给了我们两个答案
就像那样
我们已经把它缩小到了某个范围
所以这里这个选项使用事件桥输入转换器来修改事件
以便在将其发送到lambda函数之前包含额外的数据
如果我们记得输入转换器
它们允许我们可能筛选或或不发送某些数据到中间件
或增强事件
或也许最终的目标lambda函数
我们可以控制这一点
但它们实际上不允许我们添加数据
它不会允许我们走出去和
从红移数据库通过SQL获取一些数据
当然，最后一个步骤是将Lambda函数配置为目标
在Lambda函数中
从红移数据库获取必要的数据来丰富事件
这不是我认为的最佳实践
也许这不是最好的做法
因为我们在目标函数中进行了增强和改进，而不是在EventBridge的丰富步骤中进行
所以我可能会在丰富步骤中进行
但在这种情况下，当我说'被要求'时
但是，在这种情况下，当我说'被要求'时
或者我们被告知以下选项
最好的是什么
我会说那是最好的选择
我会 那就是我最终的答案
继续下一个问题
你被要求设计一个系统，用于实时更新库存状态
DynamoDB更新由各种应用程序组件生成
并且你想要集中管理
你想要集中管理并监控所有流入的库存更新
哪种架构能确保实时更新 Dynamo DB
这里有一个项目
首先设置事件桥接总线来记录事件
然后定期将更新批量写入 Dynamo DB
如果我们定期将更新批量写入 Dynamo DB
那不是实时更新
所以我们可以排除这里
直接从应用程序将库存更新发送到 DynamoDB
而不涉及 Eventbridge 以减少延迟
这与这里的愿望有点相反
我们希望有一个集中的方式来管理和监控所有传入的库存更新
因为它们来自所有应用程序
我们不知道有多少应用程序
也许有上百个应用程序
它们直接发送到DynamoDB
这对我们来说并不是很友好的管理方式
下一个问题
我们的下一个答案 配置事件桥以将库存事件路由到SQS队列
然后哪个或哪个Lambda拉取并更新DynamoDB，再次
如果我们正在调查某事
这种类型的调查告诉我们我们没有实时做任何事情
所以我会把这个放在一边
这个最后的选项
使用事件桥规则捕获库存事件并触发lambda函数
这将处理并将数据写入dynamo db
这将为我们完成工作
我们可以实时捕获库存事件
它将触发lambda函数
它将直接写入dynamo db
因为我们使用ventbridge
我们有一个集中的地方可以管理和监控所有这些东西
这就是我最终的答案
我会坚持这一点
最后一个问题 一家零售公司使用亚马逊事件桥来路由客户订单事件
他们需要确保在处理这些事件时，如果出现故障
不会发生任何事件丢失
并且他们可以稍后重试事件处理
他们需要确保在处理这些事件时，如果出现故障，不会发生任何事件丢失，并且他们可以稍后重试事件处理
他们也希望容易地监控任何失败的事件
哪些功能的组合最能满足这些要求
首先在这里列出的项目
设置一个通风桥来记录失败的事件听起来合理，监控云日志
并手动检查日志以查找失败
不，不 我们不必手动检查日志以查找失败
它不会告诉我们通过多少数据
但我在想
如果我们谈论一个零售公司
然后他们会有大量的订单
所以我们不想手动检查所有数据
使用entbridge将失败的事件路由到sqs队列并在延迟后重试处理
我想这可能会起作用
让我们继续看看是否有其他更好的选择
启用事件桥接存档以存储所有事件
并使用事件桥接重放以在稍后时间重试处理
嗯，这可能会起作用
但问题是存档
如果我们只恢复所有事件并重新处理所有事件
然后它会在我们的事件上翻倍
所以这一个完全不合逻辑，这个正确的
最后一个配置一个死信队列
Dlq和事件桥存储失败的事件并使用云观察器警报监控失败
对我来说听起来像个好计划
因为我们可以在事件桥中配置死信队列
我们也可以设置它触发警报
如果我们有任何失败的事件最终进入死信队列
然后我们可以去那里看看那些订单需要什么
所以这是我最后的答案
我希望这对你有所启发 我想感谢你观看
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/087_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p87 1. Skill Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


小测验，热手
什么才是第一个被公开发布的aws服务呢
大多数人可能会说它是e
C 两或三
也许只是一个简单的数据库
这是aw S的没有在Dynamo DB之前续订选项
那些服务分别在二十六和二十七年发布
但是，有一个aws服务在11月份就已经发布了
两千零四年
这是一个简单的查询服务
或者简称SQS
这是酷孩子们给它起的名字
简单查询服务和简单通知服务是AWS中两个开创性的集成工具
时至今日，它们仍然被数千名客户广泛使用
在本技能中 我们将深入探讨这两个奠基服务，并为我们所用 让我们开始吧
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/088_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p88 2. Amazon SNS and Amazon SQS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们来谈谈sqs和s和s
首先，我将从这里开始一个类比
假设我们有一条溪流
每当下雨时
这条溪流会变得非常大
它 事实上，洪水
因此，我们需要找出一种方法
我们如何处理额外的雨水
那就是正在流入并导致这条溪流泛滥的东西
因此，土木工程师通常会创建一个称为集水区的东西
什么是集水区
它是一个允许水流入的大面积空间
但此处的小出水阀尺寸固定
因此，无论集水区内的水量如何
流出的水量始终保持一致
因此，这个集水区在某种程度上起到了缓冲或调节的作用
水就像排队等待轮到它一样
以受控的方式流出
这就是如何我们思考简单的q服务或sqs的方式
这只是一种方法，使我们能够创建一些队列
这样我们可以吸收变化
我说的是什么意思呢
如果我们回到我们的图片
那么雨水可以以很快的速度进来
它会填满我们的集水区
但在正常的一天里
或者可能根本没有多少水进入那个集水区
它就在这里的某个水平上
但是尽管如此，出来的水将是一致的
所以它将允许我们处理那水
或者以有规律的方式处理那水
这正是SQS的含义
这就是我说的吸收变化的意思
因为某些情况下变化是可以接受的
你不介意 但在其他情况下变化有点烦人
因为也许你正在有序地处理事情
或者你的工作风格非常尖锐
你需要找到一种方法
你可以如何平滑处理以使事情更加一致
嗯 简单的队列服务或SQS是实现这一方法的一种方式
当然，按照典型的AWS风格
它完全由管理，并且非常可扩展
并且它对于我们想要实现轮询架构非常有用
当我说轮询架构时
我的意思是我们有一个过程在这里
它可能去这个队列
他说嗨 你有给我什么吗
队列说没有，今天没有
在某些情况下 也许它会出去说嗨
你有给我什么吗
当然有 这是你的工作量
然后我们可以处理那个工作量
所以我们可以独立地处理前端发生的事情
我们的队列可能会填满
而且它可能会很快填满
但我们将继续以相同的速度处理
继续前进
我们来谈谈亚马逊简单通知服务
或亚马逊SNS
假设我们这里有一些设备
向使用诺基亚333的人致敬
你知道你是谁
如果你知道你知道，所以我们有一些苹果手机
一些安卓手机
我们有各种设备
我们有某种事件
也许它是 这是一个浪潮正在来临
也许它是海啸
也许它是雷暴或类似的东西
我们需要将这些信息迅速发送到所有这些设备
所有这些终端设备
所以每当我们有警报或警报
或者类似这样
我们希望将这些信息发送到这些设备上
这些设备可以是电子邮件
可以是短信
可以是移动推送
这就是SN简单通知服务可以帮助我们的
服务
它可以帮助我们看到一些事件正在发生 然后可以将其发送到多个不同的目的地
然后我们可以将其发送到多个不同的目的地
非常快速和高效
简单的通知服务允许我们监听这些事件
然后我们可以做一些事情
每当那个事情发生时
通常我们将这个消息转发给一些其他aws服务
来做一些工作
当然它是完全管理的
并且它是完全可扩展的
我们不必担心任何服务器或遇到任何限制
我的意思 是的 理论上，短信通知的发送数量有限制
但数量相当高，这真是个实用的服务
每当我们有活动
某种我们需要了解的情况
或者我们有事情需要立即处理
这时短信服务就是最佳工具
与SQS相比
SQS更多用于异步处理
或者不是实时的东西
这两项服务在当前的各种架构中都有着非常重要的地位
对于简单的通知服务
有超过六十种不同的aws服务可以将通知发送到s和s
然后 当然我们可以将这些发送到多个不同的终端目的地
s的工作原理是基于一个发布/订阅模型
我们将会有主题
可能是鸡需要水
谷仓 门 打开 大门打开
类似这样 然后我们会订阅这些各种主题
无论是通过手机
也许是推送通知或通过电子邮件地址
每当这里发生什么事情
也许我们有一个小传感器检测到嘿
鸡需要水
好吧 它将立即向这两个目的地发送通知
所有订阅了该主题的人都会收到通知
他们将知道在该主题和通道上收到的消息
幻灯片已经足够 让我们去控制台开始构建
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/089_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p89 3. Our Architectural Design.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我花点时间来描述我们要做的事情
让我画出它的图解
我们将与我们的松鼠数据一起玩
我们的松鼠人口普查数据
现在我们将假装
就像我们在积极收集新的松鼠报告
所以假设这里有一个应用程序，在手持设备上
有人在公共公园外面
他们注意到一只新的松鼠
所以他们会用他们的设备输入有关那只松鼠的数据
然后他们会点击发送
我们将会把数据发送到这里一个s3的存储桶
所以我们会将这个消息发送到一个s3的存储桶
一旦我们将这个消息发送到s3的存储桶
然后我们就会触发一个sns的消息
所以这个s3的存储桶
会告诉sns我有一个新的文件给你
当它到达sns这里
我们将会用s3来做两件事，首先
我们将把它发送到一个电子邮件地址并说嘿
我们收到了一条新消息
嗯 也许我们不会确切地说那样
但你会看到我们将如何设置那
这样我们就可以使用s将消息发送到电子邮件端点
现在我们要做的第二件事是从消息中取得一条消息
我们将把它发送到s q s sq sq a q在sqs中
通过将消息发送到队列
它不会是我们最初发送的消息
我们将指向那条消息
然后在那个队列中
我们将有一个lambda函数
去那里获取那条消息
它将查看这条消息
它将回到这里的s3桶
读取由我们这里的观察者发送的原始消息
我们将将其插入到dynamo db数据库中
或者dynamo db表
这就是我们收集松鼠观察的方式
首先，最重要的
我们需要找到一种方法来生成这些松鼠报告
为了做到这一点 我们将使用一个小的Python程序来模拟
向S3存储桶发送松鼠报告 让我们开始吧
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/090_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p90 4. Preparing our Testbed.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好吧，在这里 我们在我们的aws控制台外面
如果你记得 我说我们将在这些新消息中降落在一个s3桶中
首先，先做第一
让我们出去看看并创建一个新的s3桶
让我们创建它
让我们确保它在我们工作的同一地区创建
我们在俄亥俄州工作
我将扩展这一点并创建一个桶
我将其命名为
刚在弗莱彻那里看到了松鼠，好的
我将保持所有其他内容不变
我不会乱动其他任何东西
这就是我们的桶，就在这里
斯科特·弗莱彻松鼠目击
好的 现在我们的桶创建好了
我们需要模拟发送那些松鼠报告
就像它们是来自某个移动设备一样
或者类似那样 要做到这一点
我们将使用一个python程序来生成它们
我们可以做的事情是
我们可以启动一个EC two服务器
我们也可以在本地从我们的机器上进行
但如果你看过我的其他技能
你知道我对这里的这个小云壳很着迷
它很酷
它允许我们执行一些aws cli命令
但它本质上是一个预加载的完整Linux机器
其他事情之一 Python因此我们可以运行Python程序事实上
在我们的上一个技能中，我们必须生成笑话
Python程序在这里
所以在这个技能中
让我看看我们要使用的程序
这就是我们将要使用的程序松鼠报告py，我将其附在下面的视频中
所以你可以下载并使用它
你不必使用这个程序
如果你愿意，你可以用你自己的用例来制作一些手工艺品
也许你在工作中正在做一些东西
这可能适合这个结构
随你的便
如果你愿意，你可以使用对你来说意义重大的东西 但这将做的事情是
它将从我们下载的csv文件中读取一些数据
我们将使用这个来模拟松鼠报告
松鼠调查或松鼠普查数据将读取那些记录
它将逐个发送那些记录
在一个个json文件中
下面我们有一些参数，我们将传递给它
我们要使用的csv文件
我们将传递它
我们希望将此消息放入的桶名称
我们将传递一个前缀
这与你想要的文件夹名称类似
然后我们有一个限制在这里
我做这件事的原因是因为当你测试东西时，很多时候真的很难测试。
如果你想一次性导入四百条记录。
所以设置一个限制，
我们可以说，
只发送一条记录。
然后我们可以看看这是怎么工作的。
我要退出qq，qq真的很想帮我，
但我现在不关心qq。
如果你还没有听说过队列。
这就是队列。
有点像辅助编程的AI
所以你可以告诉它
或者你可以问它
嘿 我想写一个程序来做这件事
它会尝试生成一个程序
但有时它会妨碍我
所以我们在这里要做的是
我们将读取CSV文件
我们将那个格式化或整理成一定的方式
然后我们会把它发送到我们的s3存储桶中
如果你记得那个数据集
松鼠csv数据集中有一些奇怪的字符
它并不真正与unicode兼容
至少不完全兼容
那里面有一些奇怪的字符，有时会导致进程崩溃
所以我们这里有个小小的程序
它会清除一些奇怪的字符
这样我们就不会遇到任何问题
顺便说一下 我们以拉丁文形式读取它
而不是以Unicode形式
这样可以避免一些奇怪的字符
让我们看看正发送的内容
我们将这个文件上传到S3这里
这里一小部分
收集我们的参数
我将要做的是
我将选择控制所有和控制C
我们将复制这个
我们要回到一个控制台这里
让我全屏
好的，我将使用nano
我们将称其为squirrel reports
Python并将粘贴
它将给我一个警告
您确定要在这里粘贴吗
是的
我将按ctrl x保存
是的
这就是我现在想给它起的名字
我们需要做的事情之一
我相信我们需要做的事情是，如果你注意到
如果你回到这个程序
我们正在使用pandas，我不认为pandas
它不是默认安装在我们aws云外壳上
所以我们必须使用pip install pandas来获取安装
所以我们完成了pandas的安装
它还安装了numpy和一些时区库
现在 我们需要在这里做的事情之一
我们需要获取那只松鼠的人口普查文件
现在有几种不同的方法可以做到这一点
我们可以访问我们的网站
松鼠人口普查网站并找到它
因为我想把它下载到本地这里只是为了简单起见
但是因为我已经把它放在我一个s3桶这里
然后我将从那里拉取它
所以我认为我认为它在这个一只松鼠的普查和松鼠数据之下
我认为这就是我们想要的
所以我要做的就是打开这个
我将获取这个URI
我将复制它
然后我将回到下面
让我清除，我将使用AWS CLI
所以这将做aws s三复制
然后这里是URI它将出去尝试找到
我将将其复制到当前目录那里我们走吧
如果我在这里列出
我们有我们的松鼠数据csv
现在我需要确保几件事
哦我给了那个错误的名字
我将更改那个松鼠报告到那里我们走吧
好的 所以我有我的松鼠数据在这里
我知道我想发送到哪个桶我的松鼠消息
我想你可以说让我清除这个
我们走吧 这格式有点奇怪
因为我空间有限
让我们看看能不能让这个东西运行松鼠报告
如果我什么都不发送
它会在这里给我一些帮助信息
它会说我们需要提供CSV
我们需要给它一个桶
这两项是可选的
前缀和限制
所以我们试试这个
让我们看看，我给我的桶起了什么名字
回到那里
滚动查看并复制
粘贴，搞定
我要添加一个前缀
我们将其命名为新发现或新报告
我将其设置为1
好的，让我们开始
让我们尝试执行它
搞定 所以它上传了一个松鼠json文件
松鼠一json到松鼠目击
斜杠新松鼠一json
所以这很多哦
所以我会出去到我的桶
我要刷新并且那里是我们的文件夹
新并且那是我们的文件
让我们看看那个文件
让我们下载它看看它
这就是它的样子
它只是读取CSV文件中的一行
然后将其转换为JSON格式
然后创建一个JSON文件
然后上传它
这将代表一个松鼠报告
现在我可以发送多个松鼠报告
如果我想要的话 只需更改那里的限制函数
所以我在这里向上走
我回到下面
限制 假设是十
它会在里面发送十个不同的消息
所以我可以回去并新
然后你可以看到
哦
你可以看到我所有的文件在下面
现在我要做的就是
我只是清理这个
因为我们现在不需要这些东西
好的 现在我已经清理了我们刚刚生成的东西 我们将继续创建我们的s和s通知
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/091_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p91 5. Our First SNS Topic.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我们创建短信通知
现在我们有了一个模拟的松鼠报告生成器
它正在正常工作
我们可以创建一些通知
我们到这里s和s去简单的通知
我们将被要求创建一个主题
如果你记得，主题是订阅者听的东西
他们订阅这些主题
他们收到这个主题的消息的通知
所以我要在这里创建一个主题
现在我有一些选项
首先，先进先出或标准
首先，先进先出将确保每个消息严格按顺序传递
这是因为，让我们假设
例如 您正在处理库存或某种财务系统
您希望按它们进入的精确顺序处理这些消息
因为也许之前的一个对它后面的一个有影响
先进先出非常有序和标准
只要信息通过，它就会发生
这被认为是尽力而为的消息
所以，如果一条消息比另一条慢
那完全没问题
这正是我们的情况
就在这里 标准消息是可以的
我将这个新的消息称为新松鼠报告
我可以使用显示名称
那是可选的 我不会去碰其他任何东西
所以我只创建了一个主题
就这么简单
我们可以将消息发布到这个主题
但现在的问题是
我们没有任何订阅者
我们没有任何消息订阅
所以我们需要创建一些订阅
所以我要创建一个订阅
这是我的ARN
我将要使用的协议
你可以看到 我有很多不同的选择
我只会使用电子邮件
我的端点在这里
我将使用我的电子邮件地址
在我创建了这个订阅之后
它将向我发送一封电子邮件验证
然后我必须说是的
我选择加入
你不能只是订阅
不想被加入这个s和s通知的人
所以我要创建订阅并等待通知
所以在此期间
所以我们的订阅已经创建
但要让它实际上起作用
我们必须确认它
所以我要去到我的另一个屏幕这里
看看是否能找到我的消息，在那里它，我正在被询问
让我在这里弹出它
这就是它看起来的样子
它正在说嘿
您需要确认此订阅
所以我可以点击它
它弹出了另一个屏幕
它说无法显示
但它弹出了另一个屏幕
说嘿 您已成功确认订阅
所以这就是我们的状态
如果我回到这里
我看到状态现在是确认的
所以现在我可以向这个s和sq发送消息
我可以发布一条消息
我现在就做这件事
我们称之为测试消息
我可以给它一个生存时间
这是可选的
这意味着如果它不能在一定秒数内发送或送达
那么我们不会发送它
我现在会把它留空
现在我可以为所有协议发送相同的负载，或者为每个交付协议定制负载
通常来说，你可能会为每个交付协议使用自定义负载
因为你看到了有很多不同类型的交付协议
但对于我们的测试来说 这是一个测试消息
好的，我们不会有任何消息属性或类似的东西
这基本上意味着无法在变量中发送或创建消息中的变量
所以我要发布这个消息
我们刚刚向我们的队列发布了一条消息
我们可以回到我们的电子邮件
我们应该是的
让我打开它这里
把它带入这里
这是一个测试消息
当然，这里有一个小东西
如果你觉得这很烦人
你可以停止接收这些通知
你可以点击它
但我们的测试信息是成功的
所以我们知道我们的电子邮件通知正在起作用
所以，我们已经创建了我们的第一个s和s主题
现在我们要做的是将这个s和s主题与
三个水桶
以便当东西掉进那个三桶里时 然后我们将被通知
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/092_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p92 6. Connecting S3 to SNS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 现在我们在这里，回到S3和S3服务
是时候将我们的S3桶连接到这个S3服务
或者这个主题
我将打开一个新的标签页
在这里 我将转到S3
并进入我们的Scott Fletcher
松鼠
侧翻桶
我要清理这些东西
我们将进入属性
如果我们向下滚动这里
我们将获得一个叫做事件通知的东西
这将帮助我们现在发送通知
我们也可以使用事件桥
如果我们想要的话
但对于这个我们将只使用S和S
我可以创建一个通知
这将要做什么
这将会触发我们的事件
无论我们定义它是什么
并且它会为这个事件做某事
我将会说是新报告
我现在被赋予了一个前缀
在这种情况下我想要使用一个前缀
因为这将会做
它将只会寻找具有特定前缀的事件
所以我们的前缀是新，后缀是.json
这就是我们文件名的名字
我现在想要做这个的原因是，因为后来在路上
我要做的就是
每当有新项目进来
我们将从这个新文件夹中获取新项目
这个新前缀
我们将把它放在一个不同的前缀
一个不同的文件夹
所以我不想这个事件也在那里触发
所以我限制它只到这个前缀
所以继续往下走
我想做的是现在放
这些都是可以发生的不同操作
我们可以删除东西 我们可以恢复东西
我们可以有事件
触发这些类型的事情
我们可以做所有事件
但在这种情况下我只对放感兴趣，放就是把东西上传到
或者直接放在我们的桶里
所以我要往下滚动
我不会担心这些其他东西
这些都是不同的事件
我们要求一个目的地，我们有几个不同的目的地可以选择
我们可以选择一个lambda函数
我们可以选择一个s主题或sqs q
我将选择一个s和s主题
现在我将选择我的s和s主题，我们可以保存更改
现在
我们得到了这个神秘的错误消息
这告诉我们的是
我们没有授予这个s three桶发送消息的权限
所以我们必须改变我们对s和s主题的权限
我们需要在这里改变我们的权限
我们必须改变我们对s和s主题的权限
我要回到我们的主题这里
我要深入它，我要去
让我创造一些空间
我要到这里的访问策略
默认情况下，它允许任何人访问
这个账户的所有者负责做好事情
这是非常受限制的
这意味着只有所有者可以做
这意味着只有管理员用户可以做
所以我要做的是改变这个策略
让我们在这里编辑
我在这里滚动到访问策略
我将更改此源所有者为
这将告诉我
这将告诉aws
这个账户中的任何服务都有权向这个主题写入吗
向这个主题发布
所以我要向下滚动到这里
我将保存更改并返回到我的事件通知创建
我再次尝试保存更改
这次成功了
所以我出去到我的桶
你可以看到里面什么都没有
我将在这里打开另一个标签并
去我们的小模拟松鼠报告生成器
我将生成
这里很清楚
我将生成一个
然后它将发送一条消息出去
如果我们去我们的s3桶
有新的
这是我们的松鼠一
现在 如果我们做得对
这将触发一条消息到这个
而且这应该导致我收到一封电子邮件
所以我让我过来这里查看我的电子邮件消息，好的，让我提取出这个
把它带入画面
这是我的消息
所以我收到了一条包含事件的消息
发生的信息
它告诉我这个文件
新松鼠json
让我看看我是否可以放大它那里
那里我们放大了一点，好的，在这里
这里是文件
这里是对象
它说嘿
这个东西被创建了
所以现在我们可以使用它在我们的s和s通知链中发送邮件给某人
我们也可以将其发送给某人的短信通知
或者我们可以将其发送到推送通知
或者下一步我们将实际将其发送到sqsq 首先，让我们在下一个视频中创建我们的队列
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/093_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p93 7. Adding in a Queue.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 现在我们将创建一个dynamodb表和一个lambda函数
首先我们需要处理来自sqs队列的条目
我将创建一个dynamodb表
我将在这里访问亚马逊dynamodb并创建表
我们将此表命名为squirrel reports
我的分区键
因为我想控制分区键
如果我们谈论dynamodb的分区键
理想情况下，它们应该是完全随机的
这将使记录在分区中均匀分布
这给我们最好的性能
我将在我的代码中生成一个名为report id的id
这将是一个良好的或UUID
我们将使用字符串
我们不会使用排序键
我将自定义我的设置
我将使用dynamodb标准
我将使用按需
我不想要尝试计算
我需要多少容量单位
我将选择按需
理想情况下 如果你在高流量下工作
你可能有足够的信息
你将能够计算你的预留容量单位
你可能节省一些金钱和性能
也许
但我将使用按需
这更昂贵
但这更简单
我将滚动到这里
留下所有其他东西不变并创建我的表
该表将需要几分钟才能创建
与此同时
我将前往lambda
我将创建我的功能
首先让我看看我将使用的代码
这是记录报告dynamo
我将包括此代码作为附件，位于视频下方
它将使用boto three sdk
这就是aws sdk for python
我们将使用环境变量来包含我们的dynamodb表名
这将使我们不必在lambda函数中硬编码信息
我将向你展示我们如何设置它
它将从sqs接收消息
它将从消息中提取内容
然后它将在消息中查找某些部分
然后它将从我们的s3桶中获取json文件
它将加载该文件
我们将添加一个UUID和一个报告ID
这是我们的分区键
然后我们将尝试将此插入到我们的表中
如果你记得当我设置SNS触发器时
我说只需关注新子目录中的内容
这样做的原因是我知道我会复制那些文件
到另一个子目录称为processed
所以我在这里使用那个前缀来复制那些文件
并删除原始位置的文件
这就是整个Lambda函数的作用
我将按Ctrl+C
回到我的Lambda终端
我的Lambda屏幕创建函数
我将从头创建一个
我们将其命名为新松鼠报告
我将使用Python和ARM64应该没问题
我将创建一个新角色并使用基本Lambda权限
我们将不得不回到这里
因为我们需要这个Lambda函数
能够从SQS获取信息
并且能够将数据插入到DynamoDB表中
现在我们有了我们的函数
我将向下滚动
我将粘贴我的代码按Ctrl+A删除所有内容
按Ctrl+V粘贴
我需要做的是
我想在这里设置环境变量
我将复制这个
我将回到配置
首先 让我保存
回到配置
我将向下滚动到环境变量
编辑 添加环境变量看看
我忘记了我的数据库表名为什么
让我们回到这里
我们去松鼠报告
复制它
回到那里
我将粘贴松鼠报告
而不是在代码中硬编码
松鼠报告
我可以使用环境变量
我认为这看起来不错
我将点击部署
我们的函数已成功更新
它已被创建和部署
但我们还没有完成
因为这个函数需要读取SQS，它也需要读取和写入S3
去权限设置
这是创建的角色
我将点击那个
这将带我到
我是，我将分配一些权限
我只是要添加策略
这可能不是你想要的方式
但我会这样做因为它快速且简单
并且我们只是在演示一个功能
我将选择亚马逊S3全访问策略
并且动态数据库是其他政策之一
然后是SQS作为其他政策之一
所以我已经添加了这些
所以现在我们有这三种政策，使我们能够与S3、SQS和动态数据库进行交互
S3、SQS和动态数据库
好的 如果我们回到执行角色这里
如果我们刷新并查看所有权限
你可以看到 我们有大量的权限在那里
所以我认为我们现在可以添加触发器了
这就是触发这个lambda函数的
所以我选择SQS
搞定了，我的队列是新报告
对于批处理大小
我只会把它降到1
因为我不想一次性发送多于一条记录
到这个特定的lambda函数
我会留下其他所有东西
就这样
所以我们创建了一个触发器
但现在默认情况下触发器处于禁用状态
这个触发器正在被创建
如果我们往下走可能刷新一下
它可能在那里被创建 它被启用了
所以现在我们已经将该lambda函数与我们的sqs队列连接起来
每当有东西进入那个队列
它将触发这个lambda函数
并且希望这里直接到达dynamo db
那么我们回到这里看表格
我们将深入研究这里 我们将探索表格项
如果我们现在运行
你可以看到我们的表格里没有任何项目
我将转到我的生成程序这里
我将发送一个作为测试
让我设置一些东西在这里
这样我们就可以尝试观察
我将前往s三
我们将在这里重新开始我们的滚动监视
我将删除这里作为测试的那个
好了 好的
所以我们 我们现在干净了
这是我们的新文件夹
这是我们的表格
我们没有记录
我将回到我的模拟器这里
我们将发送一个进去
我发了一个进去
回到这里
看看能不能抓到它
有一只松鼠
它不见了
它突然不见了
所以它的速度很快
它已经降落在处理文件夹里了
在这里刷新我们的亚马逊动态数据库表
好的 这是我们的记录
这是我们从值中插入的记录
既然看起来我们所有的工作部分都像我们想要的那样工作
在下一个视频中 让我们真的加大马力 看看用这个我们创建的小服务可以处理多少
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/094_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p94 8. DynamoDB and Lambda Function.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 这里简单回顾一下
到目前为止我们已经创建了我们的小程序
这是我们的小python程序，它将报告发送到我们的s3存储桶
一旦该文件在s3上落地
我们配置了s3
使其触发一个事件
该事件向简单通知服务发送通知
我们已经看到如何将其发送到电子邮件地址
例如 现在我们要做的下一步是添加一个SQS队列
所以我们将要发送一个通知
不仅发送到一个电子邮件地址
还会发送到SQS进行进一步处理
所以我们来设置那个SQS队列
好的 现在我们回到这里，开始在控制台设置SQS
我们只需在这里输入SQS
或者点击它
就像我刚刚做的那样，因为我最近访问过
Sqs 是那种我认为非常优雅简单的服务
看看这个
它就是队列
这就是唯一的选择 我们可以将其与那些其他服务进行比较
我们可以在这里创建一个队列
创建队列，我们有与 s 和 s 相似的选项
我们可以选择标准或先进先出
这些项的处理顺序并不重要
但在你的用例中，它可能是重要的
我打算给这个新报告命名，然后到这里来配置
这里有一些选项
消息保留期
我将其设置为一天
我们可以选择消息保留的天数
可见性超时有点意思
它做的事情是发送它们
将它们发送到队列中
让某些客户端在30秒内可见
然后如果有人没有接收
然后它将对其他客户开放
这只是一种防止消息竞争的方式
或者类似的东西
我们不会碰那个
我要在这里指出
最大消息大小
SQS的最大消息大小是256KB
这并不大
我是说，对于一份JSON文件或类似的东西来说已经足够了
但如果你试图处理一张图片或照片之类的东西
你不会通过sqs发送那个
你只会发送一个指针
你会通过sqs发送一个指针
所以我们将要做的事情
由于我们将我们的数据上传到s3
然后我们在sqs上发送一个触发器
包含文件路径的指针
这与我们处理大型文件的方式非常相似
例如 我们不会处理这里的其他任何东西
我会留下所有这些东西
我将创建队列
就是这样 我们刚刚创建了一个队列
我们可以在这里发送和接收消息
这里是接收消息，这里是发送消息
我可以输入测试消息
如果我想延迟几秒钟
我可以这样做
我现在点击发送
如果我在这里拉取消息
看 我们的消息就在这里
我可以点击它，看看测试消息
有一些属性和详细信息
诸如此类
现在我们定义了队列
我们需要将此链接到我们的s3流
而不是从这个侧创建新的订阅
我将从sqs侧做
我将钻入那里
这里有一些不同的选择我们可以做lambda触发器
我们可以做事件桥接管道
在这里我们将做s3订阅
我将订阅该主题
我将上去
订阅亚马逊s3
我将选择我的主题
新松鼠报告并保存
所以那做了
它创建了队列和s3订阅之间的链接
如果我回到这里
哦，主题在这里，我们去看看
我从q侧做原因，我可以很容易地从这个侧做
主题侧是它自动为我创建权限条目
这样允许我
它允许那个特定的s3队列发送消息
如果我从s3侧做
它不会这样做
我必须手动做
这样节省了一点额外的步骤
现在我可以回到这里发送和接收消息
我将在这里准备我的程序
我们将在这里发送一份报告，所以我将发送它
然后我将迅速跳过来这里拉取消息
在我到达这里之前
消息已经成功发送到队列中
这非常快，几乎秒级
我们可以在这里向下滚动，我看看是否能放大一些
好的
这就是我们拥有的
这里有通知
它来自这个主题
这里是主题
这里是消息
这是由S3事件生成的实际消息
在下一个视频中
我们将使用Lambda函数处理这个消息，去获取文件
然后将其插入DynamoDB表中
我们将在下一个视频中进行操作
但现在我将清除这个消息
我想删除它
因为我不想有任何陈旧的消息在那里
让我们困惑
我们将构建我们的Lambda函数 现在
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/095_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p95 9. Full-Scale Testing.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我们开始拆除东西
首先，我将去 dynamodb 这里
我将在桌子下面点击删除
确认 删除完成
所以我可以出去把我的控制台放在这里
我可以直接退出
我不需要真的删除任何东西
我将去我的 s three 桶，首先在删除我的桶之前
我必须清空我的桶里的东西
所以我只是删除并永久粘贴
删除，搞定，现在我可以回到我的桶这里
然后我可以点击滚动观察并删除
它将说 你真的想这样做吗
是的 我想这样做 好的
现在我可以到我的 sqs q 这里
点击新建报告
删除
然后到我的 s 和 s 这里
到我的主题这里
删除
有时候它们会搞混
有时候是确认
有时候是删除我
在我的订阅这里，我将删除这个订阅
因为我不再需要它
好的 现在 s 和 s 恢复正常
我将去 lambda 这里
然后删除我的函数
删除，搞定
好的
我们现在恢复正常
你可以做一件事
我们将回到 iam 这里
每当它创建角色时
让我们看看 我们称那个服务为什么
新建报告
它是新的滚动报告角色
它不会删除这些
当你删除函数时
所以我喜欢去那里并删除它创建的所有角色
但我们不再需要
我的强迫症又犯了
好了，就这样
好的 我们又恢复正常了
我们已经删除了一切
我们已经清理了一切 所以这是用s和s以及sqs进行一些集成
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/096_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p96 10. Teardown.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们一起尝试所有事情，看看我们能多快处理这些事情
所以我认为我们的文件在这里
我认为我们有433行数据
这是我们的松鼠普查数据
为了帮助事情处理得更快
我将删除打印文件的步骤
那就是上传文件的步骤
这将使它更快
我认为
我现在只是注释掉了那条线
我在这里将要使用time命令作为这个命令的前缀
它将执行python
然后我不给它设定一个限制
这意味着它将处理整个东西
与此同时 我将尝试跳转到我们的简单q服务视图
我们将发送和接收消息并下来这里拉取消息
我们将尝试实时捕获这些消息
实时
我们也会看看这个新子文件夹
看看能不能找到他们
现在我们在dynamodb中
我们从没有记录开始
所以每个人都在这里，让我们开始
我真的不知道这将需要多长时间
但执行这里运行以拉取消息，好的
这里有消息进来，再拉取一次
你可以看到消息
我不知道你是否看到了，这是非常快的
他们进来又出去
如果我在这里去S三
在这里做几个刷新
我们可以看到消息和物体进来那里
我想如果我们在这里去
所以花了十七秒
十七点一过十七秒那里
我想我们的队列可能空了
是的 现在空了
如果我们去我们的新文件夹
这里它是空的
如果我们在进程下
我们应该看到433个项目
所以我们每个记录都有一个文件
如果我们去dynamo db
我们应该看到
它分页了
我已经设置为300了
让我们 我们转到下一页
它应该显示那里我们完成了
这就是在这张 dynamo db 表中我们有多少条记录
四百三十三
所以在十七秒内，我们能够扩展并处理四百三十三条报告
所以我们甚至没有触及 sqs 和 s 和 s 的限制
所以你可以看到，使用这些工具是多么的方便
如果你正在创建一个基于事件的动态应用程序，那么在下一集中
我想向你展示如何将一切都设置回我们进来之前的状态
这样我们就可以有一个干净的白板，以便继续前进 所以
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/098_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p98 1. Skill Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎回来，所有美丽的学习者，我们又开始了一项技能
当我还是个孩子的时候 我绝对最喜欢的电视节目之一是一部动作冒险系列
叫做A队
A队是一群前特种部队
士兵，他们被判犯有犯罪
但他们并没有犯下
他们设法逃脱
现在成为法律的逃犯
当他们躲避当局时
他们总是有时间帮助那些需要帮助的人
通过复杂的计划
充满了爆炸和特技，使我这样的孩子
几乎每一集的结尾都让人着迷
当故事弧结束
所有线头都被整齐地系好
这个团伙的领导者
约翰 汉尼拔史密斯将雪茄咬在牙齿之间
微笑并说
我喜欢 当一个计划成真时
在这个技能中
我们将覆盖aws提供的一些服务，帮助我们
完成我们的故事弧，使计划成真
我们将探索工作流工具 让我们开始
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/099_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p99 2. Meet the Workflow Gang.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我们从讨论一些工作流程选项开始，来启动这些技能
我们有应用程序集成选项
到目前为止，我们有很多选择
我们谈论了亚马逊SQS
我们谈论了亚马逊SNS
我们尝试了亚马逊事件桥
现在我们将遇到三个更多的候选者，我们可以从中选择
其中之一是亚马逊步进函数
另一个是亚马逊管理的Apache Airflow工作流
如果按这个名字你认为它是经管理的版本
是亚马逊管理的版本，一个开源产品叫apache airflow
你百分之百正确
类似于亚马逊将其他产品转变为管理的解决方案
这样我们不用担心后台服务器
和其他所有那些东西
我们可以通过按钮部署它，基本上就是这样，并且直接使用
我们还有叫亚马逊app flow的东西
对于这一特定技能
我们将主要关注step functions
因为这就是AWS专门为工作流程创建的产品
这是一个从零开始由Adha创建的产品
旨在在Nabis平台上表现良好
这里显示的产品
Amazon Apache Airflow完美无瑕
也 Amazon App Flow在某种程度上有限制
但我们会在我们实际上进行一些测试驾驶时看到
所以Amazon Step Functions和Amazon管理的Apache Airflow有共同的许多事情
有很多共同点
两者都是为非常复杂的工作流程或应用程序集成场景设计的
它们也有很多钩子
与第三方应用的大量集成
它们主要是非常全面的编排平台
Apache Airflow有点不同
我真的不打算深入探讨
因为这是一个开源软件，它恰好在AWS上运行
或者AWS 创建了一个在AWS上运行的版本
我将主要关注步骤功能
但是我确实想提到Apache Airflow，因为它已经存在，可能会出现在一些考试问题中
但是，嘿
那又怎样，我呢 亚马逊简单工作流
它被缩写为SWF，是的
有一个叫做亚马逊的服务
简单工作流
在AWS的Step Functions和Airflow出现之前，它就已经被创建了
它是AWS最初的工作流产品
它的理念是您可以使用如SQS和S3等服务将应用程序连接在一起
但如果您需要插入一个人工任务
例如，向某人发送消息并让他们批准
通过某种电子邮件或类似的方式
然后继续那个工作流
这就是您会使用简单工作流
但我担心Amazon SWF的日子有限
因为在这里AWS文档中
它说AWS客户应该考虑使用Step Functions为新的应用程序
如果Step Functions不符合您的需求
那么您应该考虑使用简单工作流
因为简单工作流配置和实现起来非常复杂
要做您想让它做的事情
它相当复杂
这就是Step Functions试图解决的问题
很多人说嘿
这太复杂了
我需要一些更简单的东西，并且Step Functions确实符合这一点
就我个人而言
您可以在Step Functions中做在简单工作流中可以做的任何事情
几乎没有问题
所以让我们看看一些不同的情况
为什么我们会使用Amazon Step Functions呢
它是一个很好的通用工作流选项
它非常灵活，并且有一个相当友好的用户界面
它还具有直接编码的能力
如果您只是想通过编程来实现
它是一个基于AWS的产品，专为AWS设计
并且它与许多AWS服务有广泛的集成
许多AWS服务
对于那些您无法集成的原因，您可以调用Lambda函数并从那里使用您想要的集成
Apache Airflow管理的Amazon工作流
这是一个很长的名字
如果您已经在Airflow中投资
如果您已经在组织内部使用Airflow
并且您正在迁移到AWS
并且您不想创建大量的新工作流
或者将它们移植到其他工具
那么Apache Airflow on AWS可能是您的最佳选择
因为您可以使用您已经构建的工作流
几乎不用更改
回到Amazon Step Functions
它们都从一个叫做状态机的东西开始
只需认为状态机是持有状态，进展工作流的东西
它包含一些信息
它把这种信息传递给
工作流程的下一步
我们有一个选项
我们可以创建一个标准工作流程，标准工作流程可以运行长达一年
这是一个很长的工作流程时间
但假设你正在构建
也许一个大学录取过程
或者类似的东西
这个工作流程将运行几个月
那么也许这就是你想要的
也许这就是你需要的
一般来说
标准工作流程
或用于较长的运行过程
我们有非常强大的调试和日志记录
我们还可以重放这个工作流程
如果我们想要分析和查看它内部的情况
我 kind of 考虑标准工作流程
豪华选项
然后我们还有快速工作流程，快速工作流程
真的那些较小的
更紧凑的
更少的复杂工作流程
他们可以运行到五分钟
如果你有什么东西将要运行超过五分钟
那么你将不得不使用标准工作流程
通常您使用快速工作流程进行那些小而快的任务和作业
现在它有云观察日志
它不是标准工作流程日志那么强大
我 kind of 考虑这是经济选项
因为它没有所有功能
它便宜一点，管理起来简单一点
现在我们开始构建我们的工作流程在这个技能中
我们将使用快速工作流程
仅仅因为我们不预期我们的工作将运行超过五分钟
不再多说 让我们进入控制台并检查一些这些服务
```

### /content/drive/MyDrive/bilibili/CBTNuggets–AWSCertifiedDataEngineer–Associate(DEA-C01)part1/100_CBTNuggets – AWS Certified Data Engineer – Associate (DEA-C01) part1 p100 3. Amazon AppFlow.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的，我们又回到了控制台
我想通过Apache Airflow或管理Apache Airflow来操作
并给你展示它是什么
我们不会设置任何东西或创建太多
我们只是给你展示它是如何设置的
这给我们一些如何的上下文
它如何工作 Apache Airflow通过使用称为DAGs或有向循环图的东西来工作
这只是一些代码，它告诉工作流程
如何行为和如何处理它
这就是你创建工作流程的方式
在这里我们可以创建我们的airflow环境
我们可以选择我们要使用的版本
我们也可以选择指定维护窗口
我们想要决定
现在 在这里它要求我们指定一个s三文件
以加载我们所有的信息
实际上有三个桶非常重要
我会在这里展示给你
我将向你展示为什么在这里
所以，一旦我选择了我的s三桶
那么它将在这里创建所有这些其他条目
它说 好的 哪些子文件夹
你将在这里放下所有其他东西
一些东西是可选的
一些东西是必须的
所以，一旦我设置了那些东西
它将尝试创建一个Apache Airflow的版本
使用此S3桶作为输入
因为我不会定义我想要使用的路径类型
这将阻止我继续前进
接下来的屏幕真的很无聊
这只是一些高级功能
除非你已经在使用Apache Airflow
否则这对你来说可能没有意义
但我至少想展示给你
这就是向你展示这与其他Apache服务相似
AWS已经转变为管理服务
基本上它就是Apache产品的脚本安装
然后我们可以使用AWS控制面板来管理它们
至少可以部分管理它们
我们经常需要使用内置在Apache产品中的管理工具
这就是全部
我将向你展示如何使用Apache Airflow进行MWA管理的工作流程
但我只是想让你知道它是如何设置的，以及它看起来什么样
在下一个视频中，我们将直接转到步骤函数 所以在接下来的视频中，我们将直接转到步骤函数
```