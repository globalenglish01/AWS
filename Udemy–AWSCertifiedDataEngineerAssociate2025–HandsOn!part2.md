### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/001_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p01 08. AWS Transfer Family.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈aws传输家族
所以，想法是你想将数据发送到或从亚马逊
S3或efs
但你不想使用S3 API
所以你不想使用fs网络文件系统
你只想使用FTP协议
在这种情况下，你需要使用aws的传输服务家族
所以它支持三种类型的协议
它支持传输FTP
所以文件传输协议
FTP FTPS，这是经过SSL加密的文件传输协议
所以加密形式
或者FTP，这是安全的文件传输协议
也许你不需要是这些方面的专家
只需知道FTP是非加密的
而FTP和FTPS在传输中是加密的
想法是使用FTP协议
你可以将文件上传到S3或efs
传输家族有一个完全管理的基础设施
它是可扩展的 可靠且高度可用
所以你不需要管理任何这种能力
至于定价，你将
你将支付每端点每小时的费用，加上每GB数据传输费用
到传输家族的费用
你可以在服务中存储和管理用户的凭据
或者你也可以与现有的身份验证系统集成
例如微软
活动目录 LDAP
Okta 亚马逊
Cognito 或任何自定义源
这种使用的明显目的是提供一个FTP接口到亚马逊
S3或efs
以便共享文件
共享公共数据集
进行crm erp
等等
所以，这是一个图表，以便您了解传输家族有三种口味
用户可以直接使用FTP端点访问
或者 可选择地 你可以使用名为route 53的DNS
为你自己的FTP服务提供主机名
然后FTP服务
虽然 所以传输FTP服务将
一个将被假设用来发送或接收你的角色
来自亚马逊的文件是免费的或者亚马逊es
这是无缝进行的
你不需要设置很多东西
最后，如果你想要安全地传输文件，可以使用家庭服务
那么你可以使用外部身份验证系统来验证你的用户
比如活动目录
LDAP 或者我所提到的所有东西，就像我在之前的幻灯片中展示的那样
好的 假设我只需要知道这个功能在高层次的信息
我希望你喜欢它 下次再见
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/002_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p02 01. Intro Compute.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


数据处理的重要部分是分配给它的计算资源
尽管数据处理将在本课程的其他部分中进行讨论
同样 理解亚马逊e的基础
C AWS Lambda
AWS无服务器应用模型和AWS批处理是你需要了解的事情
斯蒂芬 我将在这部分不同的主题上切换
我将假设你知道e的基础
C 两个了 只需关注它如何融入数据工程这里
尽管数据工程考试是关联水平
但它绝对是一个更先进的关联水平考试
我假设你至少拥有云实践者水平知识进入这里 所以让我们深入探讨考试中你需要了解的计算服务
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/003_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p03 02. EC2 in Big Data.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以我们一直在使用的一个服务，我们没有真正了解它是EC two
C Two
C Two在大数据中扮演着重要角色
尤其是在启动模式中
所以我们了解按需
Spot和预留实例
但让我们快速回顾一下
所以Spot意味着它将是一个非常便宜的实例
但是AIS随时可能夺走它
所以你需要学会接受损失
这意味着如果你使用AIS进行大数据处理
需要有某种检查点的功能
所以机器学习算法
Spark hive
所有这些都意味着可以容忍损失
所以它们将是运行大数据处理任务的绝佳选择
或者你的机器学习任务在AIS上
类似地 如果你的集群运行时间非常长
或者像emr这样的大数据库运行超过一年
或者rds数据库运行超过一年
那么你需要开始预订你的实例
因为你提前预订实例
你将会获得一个巨大的折扣
所以这适用于那些在未来会非常稳定的东西
最后对于所有不确定我们是否可以失去的工作负载
或者我们不确定你是否可以连续使用一年
那么按需将是我们想要启动的实例类型
我们也来谈谈自动扩展
所以，自动扩展可以用于emr或其他服务
DynamoDB的自动化
自动扩展组等
随着时间的推移，这可能非常有帮助
或者这是季节性数据
或者这是每日数据
每天的数据 也许你在白天有更多的负载，晚上则较少
或者扩展对于这一点非常有用
如果你将自动扩展与按需结合
这可能会给你一些非常好的价格和成本效益
最后e
C Two是bi是在你的emr集群后面
所以想想你要分配的节点的类型
你的主节点的类型
和你的计算节点的类型
所以计算节点记得包含数据
任务节点不包含地址
太 完成任务
就是这样关于e C
只是做一个快速的回顾
希望您能更好地理解e
C Two如何融入大数据生态系统 下次课再见
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/004_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p04 03. EC2 Graviton-based instances.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


关于aws graviton的快速说明，这在亚马逊是一个相当大的事情
因为它是他们自己的处理器
他们自己设计的处理器系列
这种力量 几种e
C 两种实例类型，您可能想要知道
有通用计算优化
内存优化 存储优化
甚至加速计算家族，特别用于安卓游戏流
以及机器学习
关于具体的字母和数字，不必太担心
你知道，知道它们不会伤害
但我不会太担心
但他们确实自豪地提供这种服务
当然，这带来了在专有处理器类型上运行的代价
你需要特殊的软件才能运行在它上面，对吧
所以它不能运行任何东西
然而
许多数据工程服务在本课程中涵盖，可以运行在aws Graviton实例上
所以，他们的内部服务可以运行在它上面
Graviton可以是一种非常有效的选择，用于运行实例的类型
例如
Msk Rds
内存db Elasticcache
Open search Emr
Lambda 和fargate都可以部署在aws graviton实例上
上面列出的服务，所以请记住
Graviton是亚马逊自己的处理器系列
它提供了非常好的价格
性能比率
并且我们列出的许多数据工程服务可以无额外努力运行在它们上面
没有你的部分努力 没有额外的努力
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/005_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p05 04. AWS Lambda.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们来谈谈aws
Lambda是一种无服务器数据处理工具，你可以使用它
让我们谈谈这意味着什么
那么lambda是什么
基本上，它是在云中运行小代码片段的一种方式
如果你有任何语言的一小段代码
你可以想象
lambda可以在你不必担心运行它的服务器上是为你运行它
所以你不必去配置一堆e
C 两台服务器运行你的代码
Lambda会为你处理这一点
它考虑你代码的实际执行
你只需要考虑代码本身做什么
所以 这是一种无服务器的方法，实际运行小块代码，它会不断扩展
你不需要做任何事情
Lambda会根据需要自动扩展它运行的硬件
取决于进入它的数据量
所以你可以看到这如何适应大数据世界
如果你有大量数据流入
Lambda会自动根据处理数据的需求调整自己的处理能力
在大数据处理中
它通常用于处理数据在服务间传输的过程
因此，AWS中某些服务不会直接与其他服务通信
但Lambda可以作为任何服务间的桥梁
它可以被其他服务触发
例如，通过Kinesis数据流向其发送数据
然后将数据格式化为其他服务所需的格式
并将数据发送给其他服务进行进一步处理
也许可以检索数据并返回
Lambda只是像云中运行无状态的代码片段的方式
在大数据中经常被用来连接不同的服务
让我们看一个例子
Lambda并不总是用于大数据
Lambda的一个常见用途是用于无服务器网站
实际上完全有可能构建一个没有自己管理的服务器的网站
这被称为无服务器网站
现在可以使用AWS Lambda来实现
通常不能在非常动态的网站上这样做
但是如果你可以通过只使用静态的HTML和ajax调用构建你的网站，
那么这些HTML中嵌入的ajax调用就可以满足需求，
那么你就可以从s3或者其他地方获取这些内容，
然后你只需要处理ajax调用就可以了，
所以也许你有一个api网关在亚马逊上，
它充当了外部客户端和你的系统内部之间的屏障，
在那里，
假设，
例如， 你有一个需要登录的网站
那个登录请求可能会通过API网关
然后它会被发送到AWS Lambda
网站想要这个人登录
它可以转而向亚马逊Cognito发送请求
询问是否验证该用户
亚马逊Cognito会回复说可以
这里是您的令牌 Lambda然后将该结果格式化并返回给网站
所以Lambda是网站、API和后端Cognito之间的粘合剂
在那里
同样地，假设我们在前端构建一个聊天应用，可能我们需要获取该用户ID的聊天记录。
一旦他们登录后。
所以，API网关将接收该请求。
Lambda将通过API请求触发并说：
好的，我需要获取该用户ID的聊天记录。
我需要在DynamoDB中构建那个请求。 然后它会转向在DynamoDB中构建那个请求以获取聊天记录。
它会转而在DynamoDB中构建那个请求以获取聊天记录。
然后将其通过API网关发送回去
回到网站
在这里你可以看到 Lambda通常扮演着不同表面之间的粘合剂角色
这正是我们将在本应用中使用的方式
很快我们将构建出这个
你还记得在我们的订单历史应用中
之前我们使用了一个在EC two主机上运行的Kinesis消费者应用程序
这种类型的服务作为Kinesis数据流和Amazon DynamoDB之间的粘合剂
我们不想为这样一个简单的任务管理EC two服务器
我们可以使用lambda
而不是使用EC two，这是一个更好的选择
我们将构建一个lambda函数，它实际上坐在我们数据流和我们接收服务器日志之间
数据流接收服务器日志
并在DynamoDB中存储数据
长期来看，lambda只是坐在那里
等待来自数据流的事件触发
每次触发事件实际上都有一个事件批次需要处理
并提取每个单独的记录
然后将其写入DynamoDB
所以再次 Lambda只是数据流和一些其他服务之间的粘合剂
在这个案例中是dynamodb
课程后期 我们将构建交易率警报
它的唯一目的是在我们系统发生异常时通知我们
在这个案例中我们有一个kinesis数据流接收事件
表明系统出了什么问题
需要某人的注意
Lambda将由这些数据流事件触发
然后它会转身创建一个
S请求，实际向你的手机发送消息
通知你某事需要你的关注
所以又一次 Lambda是连接两个不同服务的胶水，它们之间不会直接交谈
但由于Lambda只是代码
它可以做任何事情
它可以以任何方式转换数据
你可以与后台的任何服务交谈
你可以想象它可以做任何你想要的事情
所以 它基本上是一种魔法胶水
它使你能够以创造性的方式将不同组件放在一起 这只是一个例子
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/006_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p06 05. Lambda Integration - Part 1.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


那么关于无服务器处理你的代码有什么大不了的
为什么不直接在服务器上运行
嗯 我的意思是 即使lambda仍然在底层运行在服务器上
它们不是你管理的服务器
而这是一个很大的区别
所以亚马逊有专人24小时待命确保那些服务器的正常运行
以及处理所有补丁和监控
以及硬件故障等
所以你不需要
你根本不用考虑这些
当你运行aws时
lambda 你所考虑的只是你在运行在上面的代码
是的 即使单个服务器可能很便宜
扩展那些服务器可能会非常快且昂贵
如果你扩展一个舰队来运行你的功能以管理你的峰值容量
你将会 支付很多处理时间而你甚至没有用到
当你在低谷期使用lambda时
你只支付你实际消耗的处理时间
这可以从成本节约的角度来看是非常大的
尽管在大数据世界中这并不是什么大不了的事
如果你处理的是一个无服务器网站
这可以使你的前端开发和后端开发分开进行
所以当你的后端工作可以在lambda函数中进行时
而你的前端开发者可以专注于前端客户端网站的静态内容
lambda有几个主要应用
一个是实时文件处理
所以新数据进入s3或其他aws目的地时
lambda可以被触发并实际处理那个文件
无论你想要怎样 这可以包括做一些基本的etl，即提取
转换和加载
lambda只是代码
你可以在里面做任何你想做的事情
包括对传入数据进行etl
你也可以进行实时流处理，正如我们所讨论的
只是监听kinesis流或kinesis fire hose流上的新事件
你也可以将lambda用作cron替代品
这很有趣 你也可以使用时间作为lambda函数的触发器
就像你可以使用cron在linux系统上触发事件
你也可以在固定日程上触发lambda事件
如果你需要每过一天或每小时
或每分钟 或你想要的任何日程
你可以设置它定期调用你的lambda函数
这可能对启动日常批处理任务有所帮助
比如这样那样的任务
它也可以处理来自aws服务的任意事件
正如我们所见 有很长一串aws服务可以生成lambda的触发器
几乎你想要开发的任何语言都被lambda支持
这很好 不是js
Python Java C# Go PowerShell Ruby
所以你可以用你想要的任何语言处理数据并做你想做的任何事情
这非常重要
因为这意味着任何系统只要有这些语言之一的接口
你的lambda代码就可以利用
所以记住lambda的代码和代码可以做任何事情
所以你不必局限于作为aws服务之间的胶水
或转换
如果你愿意，你也可以与aws之外的其他服务交谈
或者其他库
这可能使你能够对数据进行更有趣的转换
所以真的，你可以用你的想象力
当你使用lambda做很多事情时
它确实可以做很多事情
所有这些都不需要管理服务器
这太棒了
如果你使用lambda作为aws服务之间的胶水
这里有一个很长的服务列表实际上会为你触发lambda事件
所以这些服务中的任何一个都可以向lambda函数发送触发器
每当这些服务中有趣的事情发生时
这只是他们中的一部分
所以也有alexa
你也可以手动指示lambda函数
让我们点名几个与大数据相关的更常见的
虽然 那就是s3 kinesis
Dynamo B s s和sqs和iot
所以你可以集成lambda与s3
所以当s3中对象发生预定义的事情时
可以作为事件数据触发lambda函数
所以 例如
新对象创建在s3中
你可能有一个lambda触发器来捕获该对象的创建
然后 解析该信息
并将其放入redshift或类似的东西中
同故事与dynamodb
每次在dynamodb表中发生更改时，都可以触发事件数据并触发lambda函数
这使得实时事件驱动的数据处理成为可能，适用于流入DynamoDB表的数据
您可以将Lambda与Kinesis流集成
然后，Lambda函数可以从流中读取记录并根据需要进行处理
在底层 Lambda实际上是从Kinesis流中拉取数据
流不会向Lambda推送数据
在某些上下文中，这可能是一个重要的区别
所以，当你有Kinesis流与Lambda集成时
Kinesis实际上不会向Lambda推送数据
正如架构图可能所示
Lambda实际上会定期拉取流并按批处理方式从其中收集信息
您也可以将Lambda与IoT集成
因此，当某些设备开始将数据发送到IoT服务时
这可以触发Lambda并对其进行相应处理，根据您在Lambda函数中定义的方式
它与Kinesis Firehose集成
在这种情况下 Lambda可以转换数据并将其交付
将数据转换为S3或Redshift或Elasticsearch
或者你想要的任何东西
它可以输出到大约任何东西
因为你可以编写代码来做你想要做的事情
对这些触发器的响应
所有AWS服务都有API
您可以调用它们，所以实际上天空才是极限
至于您可以从您的Lambda函数调用的下游内容
您可以做任何您想要的事情
这些只是您可以自动触发Lambda函数的服务之一
只要这些服务都在同一个账户下
您可以设置IAM角色以允许Lambda访问它们
只要您正在使用的所有服务都在同一个账户下 只要您设置了适当的IAM角色，Lambda就可以与之通信
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/007_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p07 06. Lambda Integration - Part 2.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以这里有一个有趣的方法
你可以使用lambda来连接s三和亚马逊的开放搜索服务
开放搜索基本上是亚马逊的版本elasticsearch
为什么它叫不同的名字
以及为什么它分支出去是一个更长的故事
我们将在后续的课程中详细讨论开放搜索
但现在你需要知道的关于开放搜索的事情是它很好
它是一个搜索引擎 但它也可以被重新用作像分析引擎一样
所以它是一个摄入大量信息的方式
无论是文本还是数值并且以有趣的方式搜索和查询数据
并且可视化它
一个常见的用例可能是
构建你自己的本地版本谷歌分析
你可以输入大量的日志数据
例如那可能正在被倒入s三的数据湖中
并且你可能想可视化这些日志数据
以查看服务器延迟的趋势
或者报告的错误数量
或者类似的事情
所以事情是
你不能直接将s三的数据湖与亚马逊的开放搜索服务连接
你需要在中间有一个东西并且lambda在这种情况下将是那个胶水
所以这将工作的方式是
你将把数据从你的日志倒入s三
你知道通过kinesis
火枪或任何你想要的机制
并且当数据被倒入那里
lambda可以自动触发来处理新数据所以s三可以说嘿
lambda我有一些新数据给你
我的lambda函数可以然后从s三摄入那个数据
转换它 我需要改变它的数据类型
以open search期望的结构化方式
并且然后转身将那个信息以接近实时的方式
喂给亚马逊的开放搜索服务从那里
我可以使用open searches工具来可视化那个数据查询它
做我想要的任何事情
这就是lambda的一个有用的应用作为与其他服务的胶水
在这种情况下s三的日志数据
和一个可视化和搜索工具在亚马逊的开放搜索服务
使用lambda的另一个例子将是与数据管道
所以数据管道再次是aws
我们连接的过程
用于处理和导入和分析你的数据到一个数据管道
所以这里的想法是再次
你可能想要这样做当新数据被接收时
正常情况下数据管道
您安排这些管道
检查在s三中是否有新数据在之前启动
但是仍然会按照正常的时间表运行
但是通过使用lambda
你可以按需启动
如果你在s3中接收到新数据
并且你不知道何时会收到
它不是按照某个固定的时间表到来
使用lambda可能很有意义
这种方式会再次发生
你会有s3 当接收到新数据时会向lambda发送触发器
lambda然后转身启动该数据管道一旦数据到来
因此你可以在数据接收后立即处理
而不是在预先定义的固定时间表上
它也可以用于red shift
red shift是亚马逊的数据仓库产品
并且考试中的重点
加载数据到red shift的最佳实践是使用copy命令
你知道基本上一个命令来说
去复制数据并从这里放入red shift
但是再次 如果你需要响应任何时间出现的新数据
你可能想使用aws lambda自动完成
当新数据到来时
再次 想法是s3会在新数据到达时触发lambda
然后lambda可以转身将新数据加载到red shift
问题是lambda是一个无状态的服务
它无法自己跟踪你离开的地方
所以如何
lambda如何知道在哪里开始和结束
当从s3导入数据到red shift时，嗯
为了做到这一点 你需要使用像dynamodb这样的存储
因为aws本身是无状态的
如果你需要维护持久状态
例如我在导入这个数据时离开的地方
你将不得不连接到像dynamodb这样的存储来跟踪
所以想法是
dynamodb将跟踪已经加载的文件
以及加载到哪个表
这些数据将用于批量新文件并将它们复制在一起
使用copy命令将数据导入red shift
lambda也可以与kinesis一起使用
我们已经见过这种情况
假设你有kinesis流数据
你的lambda代码将接收一个事件，其中包括一批流记录
OK，正在到来 你可以指定批大小，最大值为10,000条记录
这看起来足够直接
你知道，流数据会进入lambda
你可以将其发送到亚马逊Kinesis进行处理，进一步的处理将在那里进行
但在这个过程中可能会出现一些小问题
考试的重点之一
他们喜欢向你抛出这些场景
在你的应用程序中某事会出大问题
你需要找出最佳解决方案
这些小细节可能非常重要
你需要意识到如果你的批处理大小过大
可能会导致你的Lambda服务超时
Lambda函数运行时间有限制
默认情况下最大值为900秒
但这可能会被配置得更低
如果你的Lambda函数处理时间过长
可能会导致失败
这将导致整个系统堵塞
这可能是一个问题的来源
批处理也可能会被分割
超出Lambda的负载限制
除了超时限制
还有限制为6MB
嗯 你需要采取特殊步骤
确保你的批处理不会超过6MB
这也是需要留意的一点
另一个需要注意的是Lambda会自动重试流数据批处理
直到成功
或者数据过期
这听起来是一个很好的功能
但也可能造成问题
如果你没有正确处理错误
可能会导致整个分区堵塞
这也是一个需要留意的失败模式
如果发生这种情况 那么
增加分区数量可能有助于解决这个问题
你可以确保在遇到错误时
你的处理会不会被错误完全阻塞
如果你遇到一个批处理被反复重试
并且总是遇到错误的情况
增加分区数量可能是解决这个问题的一种方法
记住Lambda是串行处理流数据的
它不是将数据分割并并行处理
所以你需要考虑你的批处理大小
一次发送多少数据
并确保
Lambda能在短时间内处理完 不会导致超时
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/008_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p08 07. AWS Lambda - File Systems Mounting.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈文件系统挂载
这样你的lambda函数就可以访问你的efs文件系统
如果他们在vpc中运行，这样做
我们刚刚配置了lambda将efs文件系统挂载到本地目录
为了实现这一点，需要在初始化期间进行设置
你必须利用s的efs访问点功能
假设你有一个efs文件系统并创建了一个efs访问点
然后 如果你的lambda函数部署在私有子网中
并且该子网可以通过私有连接访问vpc
那么你就可以正常工作
这种方法的局限性是，对于每次出现的lambda实例
你将会多一个连接到你的efs文件系统
所以你需要确保你不会达到efs连接限制
并且如果你有很多
很多不同的lambda函数会同时出现作为爆发
那么你也可能会达到连接爆发限制
所以我想花些时间来比较lambda的存储选项
这样你就可以根据情况了解哪个是最好的
所以 临时存储/tp的最大大小为10GB
这是一个很大的持久性是临时的
这意味着一旦你的lambda函数实例被销毁
你将失去存储
这就是为什么它被称为临时的
内容动态
你可以随意修改它是一个文件系统
它支持任何文件系统操作
它包括在你的lambda函数中至多512MB
然后你为额外的付费如果你超过512MB
并且只有你的函数可以访问它
因为这是基于你的lambda函数的存储
这是数据检索的最快级别
并且它不会共享所有你的lambda函数调用
现在你应该明白你的lambda层
最大尺寸为每个函数5层
总共不超过250MB
不超过最大lambda包大小
并且它是持久的
因为它是不可变的
你不能更改放入lambda层的内容
所以它是归档类型
它是静态的 它包括在你的lambda函数定价中
要访问一层
你需要确保你有正确的权限
它也是访问数据的最快速度
因为它作为存储附加到你的lambda函数
并且它共享所有你的lambda函数调用
所以他们都共享它 记住你不能修改lambda层的数据
如果你使用亚马逊s3
那么你可以无限制地扩展大小
它是持久的 它是动态的
源类型是对象
所以你需要使用s3 api来访问s3对象
然后你有原子操作
所以你可以得到put post等等
对于定价你有版本控制
当然对于亚马逊是免费定价
所以存储加请求加数据传输
要获取访问s3的权限
你需要确保你有适当的权限
这是一个基于网络的存储
所以我们有快速的访问
因为我们有专用的aws带宽
但它不是最快的
亚马逊是免费的 它是共享的，所有lambda调用
因为，这是一个外部存储数据
最后对于amazon es
我们有弹性 它是持久的
它是动态的 存储类型是文件系统
所以我们使用任何文件系统操作来访问它
我们将为存储付费
数据传输和吞吐量
因为它作为一个网络系统在你的lambda函数上挂载
你将有快速访问你的数据
最后，这是一个网络类
这将共享所有lambda调用
希望这使lambda的存储选项有意义
我希望你喜欢它 我将在下次讲座见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/009_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p09 08. AWS SAM.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈aws ssm
所以sam是这个小松鼠
但它也是无服务器应用模型
这是一个实际的框架，用于开发和部署无服务器应用
也就是说，你会编写你代码
然后你会使用yaml格式的配置
这将符合sam框架，并自动
sam将从简单的sam yaml文件中生成非常复杂的云形成文件
它支持任何转换
所以你仍然可以使用输出
映射 参数资源
在你的sam yaml代码中的etc
现在，幕后的一些也可以使用代码部署部署lambda函数
并且sam可以帮助你运行lambda api网关和dynamodb本地
所以sam真正是专注于无服务器应用程序
并允许你在本地调试它们并快速部署
使用云形成到aws云
所以sam是由食谱组成的
并且在你的模板的最顶层
你将添加一个转换头来指示这是一个SAM模板
这是转换头，它将告诉云形成
将其转换为云形成模板
然后你编写代码
但你不使用云形成构造
你将使用SAM构造
例如无服务器函数，即lambda函数
无服务器API，即API网关
和无服务器简单表，即DynamoDB
但你使用这些构造来使它更简单、更容易
编写你的无服务器应用程序
然后将其打包并部署到AWS上
你将使用相同的部署命令
它曾经是to命令
它曾经是一些打包和一些部署
但现在你可以只做一些部署
它将为你打包
还有一种快速将你的变化同步到AWS Lambda的方式
使用一些加速
命令是sam sync
然后减去减去观察
我会在这节课中给你更多的信息
那么我们来看看一些部署
所以你有你的应用程序代码以及你的sam模板在yaml格式
然后你在本地使用一些构建构建应用程序
这将转换为云形成模板和你的应用程序代码
然后你可以使用sam deploy部署它
所以你压缩并上传所有内容到一个桶中
然后自动执行一个变化集对云形成
然后你的云形成堆栈可以由不同的无服务器组件组成
例如lambda api
API网关和DynamoDB
让我们来谈谈一些加速功能
一些加速功能是一套用于减少延迟的特性
在AWS上部署资源时
您希望尽可能快速地部署
命令将使用some sync声明项目
并使用some模板将其声明到AWS中
它将实际上绕过CloudFormation
如果您仅进行代码更改而不使用服务API更新基础设施
这将是一个非常快速的部署
您有一些模板、应用程序代码和已部署的Lambda函数
您将运行samsync并自动
随着您更改应用程序代码，自动
因此 如果您想在云中测试Lambda函数将非常容易且快速
不同选项 您可以运行samsync来同步代码和基础设施
或samsync
仅同步代码选项而不同步基础设施
您将不会
确认将像这样
更新将在秒内完成
然后您可以指定特定资源以说
我只想更新我的Lambda函数及其依赖项
或者我想更新此特定Lambda函数使用资源ID
或者您可以使用watch选项
它将监控文件更改
它将在后台运行并自动同步任何内容
当检测到更改时
如果您包括配置
使用samsync 如果是代码
将使用samsync和代码选项
我希望这有道理
我希望这是对sam的好概述，我们还将进行一些实践
这将使您更清楚地了解它是如何工作的
我希望您喜欢这门课 我将在下节课见到您
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/010_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p10 09. AWS SAM - CLI Installation.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


那么我们开始安装相同的cli
所以你在谷歌上搜索安装相同的cli
然后你到达atos文档
它提供了关于linux的信息
mac os和windows
对于linux，有一个命令行安装器
对于x86-64和mac os有一个
对于mac os，有一个图形界面安装器，还有一个命令行安装器
对于windows，你也可以选择安装一个安装器
这样你就完成了，所以对于我来说
我将使用这些安装器中的一个
所以我将点击与我对应的那个
然后在mac上，你将被引导通过安装器
所以你每次点击下一步
这样你就可以正常工作了
为了验证一切正常，我可以在我的命令行界面中输入一些减号版本
然后你会得到一个cli版本
随便 这意味着sam已成功安装
就是这样 希望你喜欢，下次见 我会在下次讲座见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/011_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p11 10. AWS SAM - Create Project.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以，为了初始化我们的项目
我们可以运行一些东西
我在这里会有一个帮助函数
seminit基本上会为你生成一个sam项目
你可以指定运行时
你想要的是python 3.6
但你也可以说2.7 node js等等
seminit会为你创建大量文件
我不想让你感到不知所措
那么我们就一个一个来吧
我们不在里边使用它们
我们将逐个文件创建代码
这对我们来说很有意义
为了方便起见
我将有一个源文件夹
源文件夹将包含我们的应用程序
我将创建一个新文件并命名为app.py
我将用python编写我的应用程序
这将是我们的源文件夹
我们还需要一个模板文件
所以我们会有模板
Yaml和这将是我们的safile
所以sam文件就在这里
最后我们必须在某地方写我们的命令
所以我会保存它们 我会有一个命令点sh文件
所以现在我们需要去填满app点py
样本那个yaml
一个模板点yaml和如此等等
让我们直接走到服务器应用程序模型示例应用程序
因为这样对我们理解所有事情来说会是最简单的方式
让我们找一个hello world python three，听起来很棒
这里有一个lambda函数py和一个模板.yaml
让我们打开这两个文件在两个标签页中
让我们首先看一下lambda函数
这是一个非常简单的lambda函数
导入相邻的包并返回一些事件
所以让我们去导入这个函数
我将其放入我的source app.py中
所以这里 这就是我们之前拥有的基本lambda函数
实际上，我将从这里移除大量内容
我只会说返回Hello World
就是这样
好的 一个非常简单的lambda函数
让我们做更有趣的事情
那就是处理这个模板
Yaml文件
所以模板 Yaml文件
让我们回去
这就是它
这将是描述您的功能应该怎么做的YAML文件
让我复制整个内容
我们将逐行分析这些行
我将复制yo到正确的位置，这就完成了
首先 第一行是aws模板格式版本
它说2010年
所以这仅仅表示
这是一个云形成模板
第二件事是一个转换
而这个转换表明我们正在使用一个sam模板
因为转换说aws无服务器
两千零十六十月三十一日
这就是你在考试中会看到的
任何时间你看到一个转换
说这那意味着这是同一个模板
现在我们可以有一个描述
这就是我们的开胃菜
这是lada功能
我们可以有一些参数
但我实际上要删除这些参数，使其非常简单
所以我们有第一个资源
这是hello world python three
它的类型是无服务器函数
这是某些特定类型的资源
我们可以看看属性属性
我们的lambda函数将具有处理程序
现在处理程序应该是文件名点
函数名结果是我们的文件名是app点py
不是lambda函数
所以我只将此更改为app
在我们app点py中
我们定义的第一个函数是lambda handler
所以我将保留app点lambda handler
运行时是python 3.6
并且代码不在此目录中
它在源目录中
所以我会把源代码作为代码，你代码，你本地代码
代码在我这里
它在源代码目录中
描述是我们的启动函数
我们可以从我们的参数中直接定义内存大小
所以我们可以为超时和策略定义
我需要删除这个
因为它现在不起作用
这是我们最简单的一个文件
Sam 看起来非常简单
我们有一个模板
一个应用 点 py
现在我们需要部署它 我将在下一节课进行部署
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/012_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p12 11. AWS SAM - Deploy Project.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我首先需要做的事情是去评论
我将基本创建一个s3桶
为此我们知道命令行现在是s3创建桶
我将只是说定义代码sam应该是一个唯一的名称
显然你需要将s3桶更改为一个名称
我有一个终端在这里
所以我将只是继续粘贴
它是s3创建桶
现在我们已经创建了我的桶
斯特凡代码sam
如果我去我的s3并刷新
现在我们看到有斯特凡代码sam作为一个创建的桶
并且现在是空的
所以我们需要开始上传代码
因此我们首先要做的事情是打包我们的云形成
因此为此我们将做risk云形成打包
并且我们需要指定正确的参数
所以我们做这一点云形成打包按下回车
并且我们将做帮助以获取一些信息关于我们应该做的事情
并且如果我们滚动下来
我们看到我们需要指定我们在的s3桶
所以我们说s3桶是斯特凡代码sam
然后您需要指定我们有的模板文件
因此模板文件我们有的是模板
点yaml
然后生成模板文件因为我们生成某物将去under一个gen目录
所以我只说有一个gen目录
因为我们生成代码
所以我说gen
我在这里说生成的模板文件
好的 所以这将是我们的转换云形成模板
所以这看起来是正确的
这看起来我好像需要为此一切
所以让我去
退出这个并运行我的命令行按下回车
并且它成功完成
所以这实际上上传了代码到s3
所以它说成功打包了艺术作品并将输出产品模板写入文件
gen模板生成文件
并且它说我需要执行以下命令
以便不部署我们的代码
顺便说一下 我们可以运行sam和打包
呃命令 并且它将会是完全相同的
好的 所以sam只是打包的缩写
好的 所以我们已经完成了第一件事情
我们来看看这个生成的模板文件
这样我们就可以好好看看它
这样它就看起来一模一样
现在代码i现在引用了s三
结果发现我的代码上传到了s三
而这个代码you uri就在这里自动更新以引用那个s三桶
所以那个文件被更新了
这是云形成包做的一个小巧思
当然你可以做更多
但这只是一个小样
我们开始部署
在那之前也许我们应该查看我们的桶
确实，是的
一个文件被上传
现在我们将部署我们的模板
所以它说你应该运行这个命令
所以它是云形成部署
然后模板文件是
我不需要整个路径
我可以只使用生成的模板和堆栈名是hello
世界
Lambda和世界sam
听起来差不多
所以我将运行这个命令
这将基本创建一个变更集
它现在正在等待变更集被创建
当它准备好
它将
嗯 执行变更集
这里它说
变更集的 生成失败了
原因是它需要能力
我是所以这是烦人的事情
当你运行这个包命令时
它不一定会给你正确的答案
实际上你需要添加能力
这里我添加了能力
然后我将复制这个能力
我是因为我们正在生成一个
我是角色
所以我将再次运行这个命令
这次它将等待变更集被创建
这次一切都应该创建成功
堆栈在创建
更新 状态
所以我们去云形成
所以我们去这里我们可以去服务云形成
让我们确保我在正确的地区
所以我在巴黎地区
这是我配置我的cli工作的地方
在这里，正如我们所见，hello world sam正在创建中
所以自动地为我创建了这个转换堆栈
现在我只需要等待创建完成
正如我们所见，创建函数已经完成
因此，如果您查看创建的资源
正如我们所见，我们的hello weld python lambda函数已经创建
还有hello world python three
我也创建了一个角色
所以我们创建了一个lambda函数和一个iam角色
我们可以查看事件，我不知道为什么我收到这个错误
我们可以查看事件，查看发生过什么
现在，如果我们转到lambda
我们在这里输入lambda
我们应该能看到我们部署的函数
它是hello weld sam，由simon管理
我们得到了由云形成生成的名称
如果我们查看函数代码
正如我们所料，它是我们本地环境中的代码
它返回hello world
我们可以测试这个函数
点击创建
哦，测试事件
现在点击创建
我点击测试
我们的函数确实被调用，显示hello world
这非常基础
总的来说，我们展示了我们如何在计算机上编写代码
只用两行
直接将其上传到云形成，云形成为我们创建lambda函数
显然，sam的功能更强大
在下一讲中
我们将看如何更新这个标签，为我们做更多的事情 所以下一讲见
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/013_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p13 12. AWS SAM - with API Gateway.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们真的希望在我们的函数前面有一个API网关
就像我们之前所做的那样
因此我们需要修改那个模板的yaml文件
为了添加与API网关相关的内容，这是非常容易的
我们回到示例中
我们应该寻找一个API网关
例如 有一个API网关授权器
这不是我想要的
我更感兴趣的是微服务类型的东西
那么我们就往下滚动
这里我们得到微服务端点
Python 3 这听起来差不多
那么我们就关闭这些东西
所以这里我们有一个lambda函数
一个模板yaml，让我们打开这两个文件看看它们是如何工作的
所以lambda函数本身现在非常简单
它将返回一些更多的内容
它使用dynamo db表
所以我们现在不需要dynamo db
但我想做的就是复制整个文档
我们会根据需要编辑它
所以我们来复制这个
然后我们去我们的应用
点py，我将粘贴它
所以我们有一个lambda处理程序函数
这对现在来说是很好的
我现在不需要dynamo db
也不需要一个表名 在下一堂课中，我们将使用这个
我将删除它
有一个重生函数来返回错误或结果
我会保留这个函数，因为它工作得很好
这正是API网关期望我们有的格式
我会保留这个响应函数
对于Lambda处理程序
我将删除所有注释，我们可以在日志中打印
接收到的事件是什么
我将删除任何操作
我现在唯一要做的就是说回应
没有错误
所以我说没有
结果是等于hello world
好的 我们只是返回hello well
但这次以API网关喜欢的格式
即json格式
带有状态码 一个主体和一个头部
我们必须修改那个模板yaml，以便包括一些api网关相关的东西
那么我们回去看看，这里我们有模板yaml
但现在我们有更多的东西在里面
让我们看看我们需要什么
如果我们看一下我们的lambda函数
现在添加的是什么
是这里添加了一个事件
并且这个事件添加了一个api
那么我们就一个一个复制这个事件
所以我们正在创建一个api
所以我们称之为hello
Weld apa sam api
它的类型是api
路径将是hello
方法将是get
所以我们在说
每当我们去/hello使用get方法
我们应该调用我们的lambda处理程序
我们没有添加很多东西
我们只是添加了这个事件并修改了app.py的代码
但是我们已经有一个API网关被同一个框架安装好了
让我们继续运行我们的命令
我们不需要重新创建桶
因为桶已经为我们创建好了
但我需要再次运行云形成包
我将运行云形成包
它已经上传了艺术作品并更新了我的生成模板
所以现在如果我们查看生成的模板本身
再次，S3 URL已经自动为我们更新
让我们关闭这个
现在我们将要运行云形成命令
部署命令，让我们现在就开始运行云形成部署
现在我们正在等待更改被创建
然后当它被创建之后
它将会被执行到我们的云形成中
所以现在让我们去云形成，在这里它已经完成
如果我刷新
我看到现在正在进行更新
所以现在会有更多的东西被创建
我将暂停直到所有事情完成
更新已完成
但现在我们发现创建了大量更多资源
记得之前我们只有两个资源
而现在我们有lambda函数权限
IAM角色API
网关阶段部署和REST API
因此，自动创建了大量东西
再次
你知道的，非常简单易读的模板
像这样的YAML文件
所以这就是萨姆的全部力量
它确实允许我们生成一个复杂的云形成模板
这个模板由云形成执行，并为我们生成了所有这些东西
不仅仅是空谈 让我们去看看在api网关
发生了什么 所以我们输入api网关并到这里
我们将看到我们的api网关
现在有一个hello weld
为我们创建的一些api
我们发现这里有一个获取资源的路径
我点击获取
我们可以看到它是如何被配置的
显然我们可以在沙盒中有更多的配置参数
但我们有相当少的配置
如果我点击测试，我们应该看到一个内部服务器错误
在这里
显然我的功能中有些地方出错了
结果是，是的，确实如此
我忘记了return关键字
基本错误 但我会保留它因为它没问题
我会有一个返回值缺失但这没关系
这对我们来说是更多的练习
现在我们将再次运行命令
所以我将一次运行包命令和部署命令
一个接一个 这样会快一些
但我们可以看到我可以快速迭代我在云形成中犯的错误
嗯，在我的，在我的无服务器环境中
Python 文件 只需使用此 Cloud Formation 包和 Cloud Formation 部署命令
所以，基本在此更新后，我们应该一切顺利
所以，让我们回到 Cloud Formation，看看更新是否完成
是的 更新已完成
所以现在，如果我回到我的 API 网关并重新测试我的功能
现在我们得到了一个好响应
调用了 Hello World
所以，我现在解决了我的问题
我们有一个API网关
我们可以看到该阶段已部署
我们有一个生产阶段
现在我们有一个调用URL
如果我去斜杠生产
斜杠hello
那么我们将得到hello weld，一切都运行正常
就是这样
我们刚刚在不到一分钟的时间内将我们的SAM文件添加了一个API网关阶段 让我们看看如何添加DynamoDB
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/014_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p14 13. AWS SAM - with DynamoDB.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们想将我们的无服务器股票应用程序与一个DynamoDB表结合起来
以便真正实现我们的应用程序的无服务器
所以让我们回到之前我们有的代码
正如你所看到的
我们已经在代码中有DynamoDB
这听起来差不多
所以让我们继续复制这里的代码
我们将加载函数
在这里我们创建我们的三个客户端
所以你会注意到我们创建了客户端
在外部处理程序
再次 如果你记得的话，最好的实践之一是将客户端放在你的处理程序外部
这是非常非常重要的，我们在这里加载表名
我们还要做的另一件事是设置这个
这三个客户端都位于我们正在的区域名称中
所以我会说region_name等于os.environ
我们获取了另一个环境变量
这次它将被命名为region_name
然后我会说region_name等于region_name
所以基本上我们设置了我们的 dynamodb 客户端
要与我们部署函数的地区相等
正如你所见 现在我们正在引用两个环境变量
区域名称和表名
因此，我们必须将它们放入模板yam中
那么我们该怎么做呢
让我们看看示例文件中是如何做到的
所以这是我们的lambda函数
我们不再需要这个了
这是我们的模板yamo
现在我们需要引用dynamodb
正如你们在底部看到的
有一个为我们创建的表格
这是一个简单的表格类型
让我们去复制这个
我们将复制这里的简单表格
好的，表格类型是简单表格
但我想设置一些属性
因为我不想让这个表格花费你很多钱
所以我们输入简单表格
无服务器选项
应该是在这里我们简单表格
我们得到资源类型
所以我们得到不同的参数
我们可以使用其中之一是
如果我们滚动到提供通量
我想要提供通量等于一和一
只是为了我们有限的提供通量
我们不会过度支付
我们也可以设置一个主键，所以我们也可以复制这一点
所以我们得到一个完整的简单表，将开始工作
所以我们复制这些东西
我们有主键
并且通过提供吞吐量
主键将被称为问候
它将是一个字符串
所以我们只有有分区键
并且对于提供吞吐量
我说 让我们使用222
这是我们的简单表
所以现在我们需要设置环境变量为我们的lambda处理程序
对于这一点，非常简单
让我们回去看看它是如何做的
我们使用一个叫做环境的东西，在那里我们可以引用我们要的变量
所以让我们去环境，我们将在这里设置它
环境，我必须要缩进
好的 变量
第一个将是表名
我将做一个引用函数
所以re像一个有趣的云形成函数到我们之前创建的简单表
所以这里有表，我说对了
和其他的uh uh
我设置的环境变量是区域名称
对于这一点，我将设置一个引用到aws区域
这是一个伪参数，如果记得的话
这是一个伪参数，直接由rest
云形成模板提供
模板
好的 所以现在我们有表名和区域名
并且我们已经创建了一个表
但如果你记得当我们这样做时
我们需要给自己lambda函数
访问这个简单表
我们继续并需要我们添加策略
这些策略将是iam策略
我们可以添加到我们的lambda函数
所以让我们去并写策略在这里
并且我们想要添加的策略是dynamodb crude策略
并且表名我们引用的必须是为创建的表名，所以ref表
这将基本上给我们
一个dynamodb策略，允许我们做粗略操作在创建的表
所以现在我们有完整的iam策略，至于我们的应用程序所做的
也许我们不想返回hello
也许你想做一些更有趣的事情这里
所以我们可以使用扫描操作
所以dynamodb
所以DynamoDB扫描
然后我们必须引用一个表名参数
所以表名等于，我们必须传递我们从之前提取的表名
所以表名等于
表名 这是我们的扫描结果等于这个
这就是我们要传递的
我们将以扫描结果响应
所以这是一个非常简单的功能
我们只是扫描创建的表
这就是我们函数的扫描功能
你知道 我们的表是从这个简单的表格创建的
现在让我们去部署我们的函数
所以我将在我的终端运行这两件事
现在我们在云形成中
更新已完成
如果你看正在创建的资源
我们现在有一个在dynamodb中被创建的表
我们在dynamodb中有一个表
让我们进入dynamo db，看看情况如何，dynamo db
这就去 我们去到表
这是我的hello weld sam表
它有一个分区键名为问候
它有两个读取容量单位和两个写入容量单位
让我们添加一些项目
所以让我们创建一个项目
我将说创建hello
然后点击保存
这是我们的第一个问候
也许我会创建一个叫做bonjour的另一个问候
这是在我的语言法语
所以我们有bonjour的问候
但你可以有其他问候
如果你想要的话
所以我们在dynamodb表中有两个元素
这相当酷
现在我们要做的是确保我们的函数正常工作
所以让我们去我们的lambda函数
我们会刷新这一页
现在我们向下滚动并查看环境变量
我们可以看到区域名称自动设置为eu us three
我们的表名称自动设置为由dynamodb创建的表名称
我们的环境变量正确设置
因此我们的代码应该能够引用它们
我们的代码以某种原因没有更新
让我们看一下
没有 代码没有更新
因为我可能忘记保存代码了
所以我们再运行一次
好的
我的代码没有更新
这很好
顺便说一下 如果你向下滚动并查看
嗯 我的角色
它创建了一个hello world sam
你好世界 我是角色
所以我们可以去 我是那里看看那里做了什么
所以让我们去iam点击那个角色
这里是由hello weld sam创建的角色
它包含一个lambda基本执行角色
这很好 还有一个内置策略
允许它执行dynamodb操作
获取项目删除 添加输入
项目扫描 等等，所有这些都是我的hello weld sam表
这太棒了
sam框架为我们生成了dynamodb
api gateway lambda函数
iam角色 一切都完美
所以现在我希望如果我去我的资源获取路由并测试它
我将点击测试并再次点击测试
我将得到一个200
所以它工作正常
现在它将返回响应体 问候你好
问候Bonjour
所以无论我向dynamodb添加什么内容
都会被api gateway返回
总的来说
我们做了很多事情
但我们只是构建了api gateway
lambda函数
dynamodb表
iam策略
例如我现在有一个可以直接从prod调用的api
例如 如果我去这个url
prod/slash/hello
它将返回我所有的dynamo db表中的问候
你可以在这里添加或删除更多的问候语
如果你想 这就是关于dynamodb表的讲座的全部内容 希望你喜欢 我会在下一个视频中见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/015_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p15 14. AWS Batch.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你需要了解的下一个服务在较高层面上是aws批处理
aws批处理允许你运行批处理作业，正如其名所示
它们基于docker镜像构建
因此，使用批处理你不需要预置实例
实例的动态预置已经为你完成，无论是e
C 2实例还是spot实例 批处理会找出最优的实例数量和类型
基于它需要处理的作业数量
和类型
并且您不需要管理任何集群的要求
这是一个完全无服务器的服务
就像胶水一样
您只需为底层支付费用
这两台实例是由批处理本身创建的
您可以使用CloudWatch Events安排批处理作业
以便您可以在预定时间运行
或者您可以使用Step Functions编排批处理作业
在下一节课中我们将会看到
但是您可能又会有疑问，批处理和Glue之间的区别是什么
再次粘合 再重复一次
它是Apache Spark代码
只使用Scala或Python
它专注于ETL
你不需要担心配置、管理资源
现在有批处理数据目录
它有点不同 它是任何计算任务的
无论工作
好的 这不仅适用于etl
这适用于任何批处理导向的任务
只要你有一个适用于批处理的docker镜像
你就可以进行批处理
例如 适合批处理的工作
可能是清理任务
然后是一个空的桶 所以你在免费桶上运行一个脚本来清理它
你定期运行这个批处理作业
而在glue中
它真是关于处理数据
转换它 然后将其放在其他地方
所以资源在你的账户中创建
并由批处理管理
所以你在使用e C 账户中的两个实例
所以你可以在账户中访问所有这些内容
如果你需要进行数据转换，实际上需要ETL
那么Glue可能是一个更好的选择
但如果你的工作与非ETL相关
但它仍然是批处理导向的
那么AWS批处理会更好
这就是关于这个服务的所有内容 希望这对你有帮助 那么下次讲座再见
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/016_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p16 01. Intro Containers.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


如果你有一些自定义代码或自定义应用程序来处理你的数据
你需要一种部署和管理该代码的方式
这就是下一节要讲的内容
使用亚马逊弹性容器服务
亚马逊弹性容器存储库
以及亚马逊弹性Kubernetes服务来打包你的数据处理应用程序
并将它们部署到云中 Docker和Kubernetes是重要的知识
不仅限于数据工程领域
在整个开发世界中也是如此
这是另一个相对较短的部分
Stefan将带领我们完成这一部分
并像往常一样给我们一些实际的例子
我们会在结束时有一个测验来巩固你所学的知识 开始吧 Stefan
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/017_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p17 02. What is Docker.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你好，欢迎来到关于容器的这一部分，我们将讨论docker
Ecs和e
所以docker是什么
Docker是一个软件开发平台，用于部署应用程序
理念是它是一种容器技术
因此，应用程序将被打包到容器中
这些容器是标准化的
因此，它可以在任何操作系统上运行，这意味着您的应用程序
一旦它们被容器化
以相同的方式运行
无论它们在哪里运行
这可能是任何你不兼容的机器
没有兼容性问题 行为可预测
这意味着您要做的工作更少
更容易维护和部署
并且应该与任何语言一起工作
任何操作系统和任何技术
因此Docker的使用案例是微服务架构
所以这是一个好的关键词要记住
将本地应用程序提升到云端
无论何时你想运行一个容器
那么容器在操作系统上是如何工作的
嗯 我们有一个服务器
在我们的情况下，它可能是e
C 两个实例 但它可以是任何类型的服务器，你将运行一个docker代理
然后你可以从那里开始运行docker容器
因此，你的第一个Docker容器可能包含一个Java应用程序
而你的第二个Docker容器可能包含一个Node.js应用程序
Docker容器可以多次运行相同的
因此，你可以有多个相同的Java应用程序的Docker容器
或者有多个相同的Node.js应用程序的Docker容器
你也可以在Docker中运行数据库
例如 MySQL等等
因此，Docker非常灵活
从服务器的角度来看
所有这些都是Docker容器
那么你在哪里存储Docker镜像
嗯 你将它们存储在一个称为Docker仓库的地方
我们有多个选项
第一个是Docker Hub
它是一个公共仓库
你可以找到许多技术和操作系统的基础镜像，如Ubuntu
MySQL等等 它非常受欢迎
然后对于更私密的集成
你有亚马逊 ECR
亚马逊弹性容器存储库
你可以在那里运行你的个人镜像
但在亚马逊的etr上，还有一个公共仓库选项可用。
公共画廊现在开放
好吧 Docker 和虚拟机有什么不同
嗯，Docker可以说是一种虚拟化技术
但并不是纯粹的人会试图打我
如果我这么说 所以资源与主机共享
这意味着您可以在一个服务器上共享许多容器
所以如果你看一下虚拟机的架构
您有基础设施
然后你有主机操作系统
然后你有hypervisor
然后你有你的应用程序和你的宾客操作系统
这就是例如e的架构
C 两个工作 例如当你收到一封电子邮件时
C 两台机器 实际上这是一款在hypervisor上运行的虚拟机
这使得亚马逊能够为许多客户提供多种服务
C两台实例
所有这些实例
所有这些虚拟机都是独立的
他们不会分享资源
他们将会被孤立
但对于一个docker容器
您仍然拥有基础设施和主机操作系统
这可能是这次在e中
C 两个实例 然后你就有docker守护进程
然后在它的上面
你可以有很多轻量级的容器
在 Docker 守护程序之上运行
这使得容器能够真正一起共存
它们实际上可以共享网络
分享一些数据等等
所以，它稍微不那么引用了
引号 安全得像一台虚拟机
但是它允许你在单个机器上运行更多的容器
一个服务器 这就是我们为什么真的很喜欢docker容器
那么，你是如何开始使用docker的
嗯 首先，你必须写一个docker文件
它定义了你的docker容器的外观
我们有一个基础的docker镜像
我们添加一些文件
然后我们构建它
这将成为一个docker镜像
这个docker镜像你可以存储在docker仓库中
这叫做推送 你可以将其推送到docker hub
这是一个公共仓库
或者亚马逊r
这是亚马逊的docker仓库版本
然后你可以从这些仓库拉回这些镜像
然后你可以运行它们
当你运行一个docker镜像时
它会成为一个docker容器
它运行你从docker构建的代码
这就是使用docker的整个过程
那么，如何在aws上管理docker容器呢
第一个叫做amazon ecs
它是亚马逊的弹性容器服务
这是亚马逊自己的docker管理平台
我们将在深度研究中详细查看
然后我们有amazon eks
这是亚马逊的弹性kubernetes服务
这是亚马逊管理的kubernetes版本
这是一个开源项目
我们将快速浏览一下
我们有aws目标
这是亚马逊自己的无服务器容器平台，fargate工作
它与ecs和eks一起工作
我们将在深度研究中详细研究fargate
然后我们有amazon ecr，用于存储容器镜像
就像我之前展示的那样
我们有一个docker和docker在aws上的工作概述
现在我们将深入研究amazon ecs和其他内容 现在让我们深入研究amazon ecs和其他内容
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/018_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p18 03. Amazon ECS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 现在我们来谈谈亚马逊ecs
我们将概述它的所有不同方面
我想和你们谈论的第一件事是EC two启动类型
ecs代表弹性容器服务
当你在aws上启动docker容器时
你在aws上启动了一个ecs任务，一个ecs集群
一个ecs集群是由一些东西组成的
以及EC two启动类型
well
这些事情是e c
两个实例
在这种情况下，如果你使用ecs集群与e c two启动类型
你必须规划和维护基础设施
这意味着你的亚马逊ecs集群将由多个e
C 两个实例组成
现在，这些实例有点特别
因为它们中的每一个都必须运行ecs代理
然后，这个代理将注册每个e c
在亚马逊ecs服务中启动两个实例并指定ecs集群
现在，一旦你完成了这些
然后，当你开始启动ecs任务时
然后，aws将开始或停止在容器中
这意味着，每当你有一个新的docker容器时
容器将被放置在每个e
C 两个实例随时间推移
正如你看到的这里
你可以开始或停止ecs任务
它会自动放置
这就是e C 两种启动类型和Docker容器将被放置在提前准备的亚马逊EC two实例上
好的 现在存在第二种启动类型，称为Fargate启动类型
你仍然可以在AWS上启动容器
但这次你不需要准备基础设施
因此不需要管理任何EC two实例
一切都是无服务器
因为我们不管理服务器
但当然后面有服务器
所以在目标类型中
如果我们有一个ecs集群
我们只需创建一个任务定义来定义我们的ecs任务
然后aws将根据我们需要的cpu和ram运行这些ecs任务
所以我们想要运行一个新的docker容器时
简单点说，它将会被运行，我们不需要知道它在哪里运行
我们不需要关心服务器
和没有e
C 后台将创建两个实例，在我们的账户中运行
有点魔法
然后要很好地扩展
只需增加任务数量
你不需要管理更多
C
两个实例
考试喜欢告诉你使用fargate
因为fargate是无服务器
比e容易管理
C 两个启动类型
好的 我们已经看到了amazon cs的两个启动类型
让我们谈谈ecs任务的角色
让我们以e为例
C 两个启动类型中我们有一个e
C 两个实例运行ecs代理在docker中
在这种情况下我们可以创建一个e
C 两个实例配置文件
当然只有有效
如果你使用e C
两个启动类型，它将被ecs代理使用 然后ecs代理将使用e
C
两个实例配置文件通过ecs服务进行api调用进行注册 实例将通过云watch日志进行api调用发送容器日志
将通过eccr api调用从eccr拉取docker图像
也将引用敏感数据在秘密管理器或ssm参数存储中
然后我们的ecs任务将获得ecs任务角色
因此对于e
C
两个启动类型和fargate 我有两个任务
我们可以为每个任务创建特定角色
我的第一个ask将具有ecs任务角色a
我的第一个任务b
第二个任务b将具有任务b角色
为什么我们有不同的角色
因为每个角色都可以链接到不同的ecs服务
例如
ecs任务a角色允许你的任务a运行
对amazon s3进行api调用
任务b角色允许你的任务b运行
对modb的API调用
你在ECS服务的任务定义中定义了任务角色
记住这一点
EC two实例配置角色和ECS任务角色之间的区别
ECS EC two实例配置角色和ECS任务角色之间的区别，以及下一级负载均衡集成
在这个例子中，我在EC two实例配置中
EC two启动类型 但这也可能是Fargate
当然，可以运行多个ECS任务
这一切都在这个集群中
我们希望将这些任务暴露为http或https端点
因此我们可以在集群前面运行一个应用程序平衡器
然后，我们的用户将访问alb
在背后直接访问ecs任务
在这种情况下，alb是支持的，并且将支持大多数用例
这是一个很好的选择
网络负载均衡器仅在您有非常高的吞吐量或高性能用例时推荐
或者，正如您稍后所学到的
在本课程中 如果你用它与aws私有链接一起使用
或者如果你想使用较旧的经典负载均衡器，你可以这样做
但这绝对不推荐，因为你无法获得任何高级功能
你也无法将你的弹性负载均衡器链接到farts
而如果你使用的是应用负载均衡器
那么它就能正常工作 当然，与fargets一起使用
那么关于在amazonas上的数据持久性呢
你需要数据卷 并且有多种类型
但是，其中一个是显著的
那就是efs
所以，假设你有一个ecs集群
在这个例子中，我代表ecs集群中的ec和c实例，也包括frigate启动类型
以及我的ecs集群中的ecs任务
我们希望在ecs任务上挂载一个文件系统
以便共享一些数据
在这种情况下，我们使用亚马逊efs
因为它是一个网络文件系统
它将与ec兼容
C 两艘和快速帆船类型
并且它允许我们将文件系统直接挂载到我们的ecs任务上
为什么很好
然后，在这个亚马逊文件系统中运行的任何az任务将共享相同的数据。
因此它们可以通过文件系统相互通信
如果他们想
因此，终极组合是使用fargate以无服务器方式启动ecs任务
亚马逊FS用于文件系统
嗯 坚持不懈，因为EFS（弹性文件系统）也是无服务器
我们不需要管理任何服务器
它是按需付费的 它只是在预先配置后即可使用
因此，使用EFS与ECS（弹性容器服务）的场景是用于持久化
多共享存储以供容器使用
需要注意的是，亚马逊的S3无法作为文件系统挂载到ECS任务中
好的
就是这样 希望您喜欢 期待下次再见 我将在下次练习课再见
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/019_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p19 04. Amazon ECS - Create Cluster - Hands On.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


那么我们通过进入ecs控制面板来练习使用亚马逊ecs
然后在左上角启用新的ecs体验
然后您将选择集群并创建我们的第一个集群
所以这个将被称为演示集群
我们保持默认命名空间不变
对于基础设施
我们有三个选项
第一个是aws fargate
它允许我们提供容器的地址
然后操作系统将按需运行这些容器
所以这很好，因为这是一种无服务器选项
我们不提供计算资源
无论是我还是我们都可以根据需求提供计算资源
但我们甚至看不到它
其他选项是亚马逊EC two
两个实例中我们提供自己的亚马逊
两个实例为我们的容器提供计算资源
我们在使用ecs的任何地方都有实例
例如 在自己的数据中心运行ecs容器
所以到目前为止，忘记将其启用
我们也将启用亚马逊ECS
为演示创建两个实例
所以我们将创建一个新的弹性伸缩组
然后，我们必须选择操作系统
所以亚马逊Linux 2非常好
或者你可以选择最新的版本
亚马逊Linux 2023
无论你选择什么类型的ECS ECU实例
我将使用t2微实例
这是免费的符合条件的
然后对于所需的容量
我会选择最小值为零，最大值为五
所以不会配置SSH密钥对
并且将保留根EBS卷大小
保持不变
现在我们已经指定了亚马逊EC two实例类型的基础设施
我们需要为亚马逊提供网络设置
EC two实例
这是默认VPC
除了可用的三个子网之外
对于安全组
我们将使用现有的安全组
即默认安全组
最后对于自动分配公共IP
我们将使用默认的设置，使用子网设置
我们不会触碰监控和标签
然后点击创建来创建我们的集群
所以当这个过程进行时
让我们看一下在AWS上创建的自动扩展组
然后在左边，我会点击自动扩展组在这里
如你所见，它显示在这里
为我创建了一个名为infra ecs cluster的自动扩展组
这些是零容量，最小值为零，最大值为五
这是由我的ecs集群直接创建的
创建正在进行中
这个集群将有，可能这两台实例供我启动任务
And then on the left hand side
I will click on auto scaling groups right here
And it's showing as you can see
所以你可以看到它有三个可用的选项
我们知道ecs任务将分布在三个可用区
我现在要做的就是等待这个集群被创建
这正是现在的情况
所以现在我可以探索这个演示集群
如果我点击它
我在演示集群中
我可以去服务查看是零
任务也是零 因为我们还没有启动任何东西
然后我们转到更有趣的基础设施
如果你在这里转到基础设施
在这个ecs集群中，我们有三个容量提供商
第一个是目标
这意味着我们可以在ecs集群上启动任务
第二个是fargate
但那意味着我们可以在spot模式下启动fartask
选择e的spot实例
C 二 最后一个是asg提供商
这意味着我们可以直接在这个集群中通过asg启动EC two实例
所以现在是管理的扩展
当前的大小是零
但我可以改变它
如果我想要的话
让我看看它会是什么样子
我在这里 我去到详细信息并编辑所需的容量为一
只是为了向你展示它是什么样子
那么这将会发生什么
一个e c two实例将被创建
好的，当它被创建时，它将会注册到自己的demo集群中
然后我会在容器实例中看到它，这意味着当我们创建一个ecs任务时
它可以要么在farspot容量提供商上启动
或者它可以在我的faror作为部分这个启动的容器实例上启动
Asg 所以现在我将要做的就是等待这个实例处于运行状态并注册到我的ecs集群中
所以我现在刷新一下
所以让我刷新一下
我的实例正在运行
这是由于微服务导致的
如果我回到我的亚马逊ecs集群
它将被注册为一个容器实例
目前它当然没有运行任何任务，但有一千零二十四个cpu可用
有九百八十二个内存可用
这告诉我了这个实例的容量
我可以在它的容量耗尽之前在上面启动任务
所以我们可以继续
我们有一个e集群
我们看到了两个容量提供者
我们看到了三个容器实例
所以现在让我们去运行我们的第一个服务 在下一节课我会向你展示
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/020_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p20 05. Amazon ECS - Create Service - Hands On.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来创建一个ecs服务
但在我们做之前
我们需要创建一个任务定义
所以我将从任务定义面板创建一个新的任务定义
我需要给它起一个名字
我将其命名为nginx demos
Python hello
这来自于Docker镜像
名为nginx demo
在Docker Hub上
这就是我们在演示中使用的
这就是为什么我将我的任务定义命名为demo-hello
好的 接下来我们需要选择基础设施要求
我们是否想在faror上启动
在亚马逊上启动两个实例
faris是无服务器计算
所以我们会保持启用
如果我们启用这个
我们可以将我们的任务
这个服务在亚马逊的EC two实例上启动
但对于简单起见
现在
我只会使用ais目标 并将我们的容器发送到无服务器计算模式
然后我们需要选择操作系统和架构
我们拥有的
所以Linux是可以的 我们的far容器的任务大小是多少
我们可以说
例如我们有0.5或1个vcpu
在这个例子中你可以达到16个vcpu
然后你也可以调整内存
所以你可以说
我想要
例如
最多120GB的内存 所以这一切都将由far在无服务器模式下提供
以保持非常便宜和简单
我将选择0.5个vcpu和1GB的内存
我们拥有任务滚动
任务滚动是一个 我们可以分配给我们的任务的IAM角色
如果你需要对aws服务进行api请求
但因为我们现在不做这个
我们不会指定任务滚动
但这是至关重要的
如果你的容器需要使用aws
现在将任务执行角色留空
保持默认
如果这个ECS任务执行角色没有被创建
然而，它将被ECS服务自动创建
所以我们下次见
我们的集装箱 所以名字将是nginx演示减去hello
并且图片链接将是nginx demo
你好你好
并且这将会自动从docker hub这里拉取这个镜像
这非常实用
它是必要的容器
现在我们有很多不同的选择
例如端口映射
所以我们想把端口八十映射到容器的端口八十
这正是我们所希望的，我们就这样保持不变。
然后你可以添加更多的端口映射，如果你想要的话你可以
例如 设置资源分配
限制 从文件或手动获取环境变量
并且记录
但我会把这些都设为默认值
因为它们对我们来说已经足够
Fargate存储有一些临时存储
所以我们再次保持不变
默认是2个1GB
这里就是默认值
这已经足够 保持你现在的值
让我们创建这个，这是我们创建的第一个任务定义
现在你看到我的版本2
因为我刚刚创建了两次
但是对于你来说，你应该看到版本1，所以接下来
让我们将这个任务定义作为一个服务发布
所以我们进入集群，然后进入demo集群，在服务下
我将创建一个服务
所以对于计算选项
我将进入启动类型并选择faras
我的午餐时间和平台版本将是最新的
然后应用类型是一个服务，这意味着它将是一个长期运行的服务
例如，一个Web应用程序
但如果你想进行一项调查
一项会终止的任务
例如批处理任务
那么你可以使用任务
但我们将使用服务
然后我们选择nginx演示
大家好，选择对你来说最新的版本
对你来说应该是一号
那么我们给这个服务起个名字
所以nginx演示hello
与我们的测试定义相同
这相当标准
然后服务类型将是部署选项中一个任务的副本
保持不变，部署为检测
保持不变
接下来为网络
所以我们想在这三个子网中部署到我们的vpc
我们将创建一个新的安全组
我将其命名为nginx demo hello
我将其命名为nginx的安全组
现在想法是我们想在http上允许端口八十
并且来源目前将是任何地方和公共ip
我保持这个打开并且用于负载均衡
是的 我们希望负载均衡器将是一个应用均衡器
你将会创建它
所以demo alb for ecs是名称
我们将留下零
作为健康检查的宽限期
我们应该留下足够的时间来让我们的容器启动
然后我们的容器将在80号端口运行
使用hdp协议
目标组将是tg nginx demo hello
协议是http
健康检查协议是http
健康检查路径是/
现在我们已经定义了我们的服务，我们有一个应用平衡器，一个目标组，一个安全组等等
所以我们应该可以正常运行
我们不会触碰任何正在服务的东西
自动扩展和任务放置
但这些东西是可用的
这意味着 当然你可以在ecs上自动扩展
所以现在我的部署正在进行中，我们需要等几分钟
所以我们的服务没有成功部署
所以我们点击服务，看看它
所以我们可以看到现在，我们一个期望的任务和一个正在运行的，状态是活跃的
所以这真的很好，我们可以看到服务链接到一个目标组
所以我点击目标组，在这个目标组本身，我们可以看到它链接到我们的演示ALB
这是作为此服务的一部分创建的应用程序负载均衡器
看起来有一个IP地址已注册为目标
而这个是我的容器的IP地址，非常好
所以现在如果我们看一下这个负载均衡器本身
它是活动的
我可以复制DNS名称并打开一个新标签并粘贴
然后我得到Nginx欢迎页面
这非常好 这意味着一切都在正常工作
服务器地址与这里注册的IP完全相同
这是私有IP
这很好 嗯，还有呢
如果我们在服务本身下查看
现在我们可以查看任务
如你所见，现在只有一个容器正在运行
这就是这个任务
我可以点击它并查看此任务本身
它告诉我配置
任务版本
它被启动在哪里 私有IP
我们可以查看容器
查看日志
了解我们的Nginx容器日志
这很好 如果我们查看服务本身
我们现在在服务上
然后转到事件
我们可以查看这些事件
这意味着我们有一个任务被启动
它在Atari Group中注册
然后完成部署
现在我们处于稳定状态
正如我们所见在这里
我可以转到斜杠测试
例如 URL在这里会改变
所以Nginx按预期工作
现在我们可以在ECS下做
我们可以查看我们的任务
我们有一个
但我们可以启动更多
我不会向你展示如何容易地使用Fargate启动更多任务
所以让我们更新此服务
现在所需的任务数量将变为三个
例如，每个AZ一个
例如
其余的我将保持不变
如故 我将保持测试定义不变
将保持计算配置不变
负载均衡在健康检查等方面没有变化
所以点击更新
现在将要发生什么
我们已要求ECS服务运行两个更多任务
如果我刷新并稍等片刻
现在我们有两个更多任务正在配置
他们被配置在fargate引擎上
这意味着aus将自动为启动这些任务配置所需的资源
让我们等一会儿
它们正在等待
现在正在激活
现在正在运行
所以这非常快
实际上
如果我在这里 查看
现在刷新此页面
如你所见，IP地址每次刷新都会改变
刷新 所以负载均衡器的应用正在分发负载
在我的ecs容器之间
这很好 所以这相当强大
我们刚刚演示了ecs的扩展
在扩展的同时，我们可以进行回退演示，以节省成本
我们可以在这里更新服务，使所需任务数为零
服务仍然存在
但我们没有运行的容器
在我的应用程序负载均衡器和自动扩展组下
对不起 然后我将点击这个并确保所需的容量也为零
这样 我们确定在ecs的r e c two集群上不会运行任何实例
好的 所以现在你可以验证这一点，任务已经消失，你可以继续
你可以查看事件来查看ecs做了什么
当我们要求它更新服务时
好的 就是这样
我们已经看到了如何创建ecs集群
我们已经看到了如何在fari上创建ecs服务 希望你们喜欢 我会在下一节课见到你们
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/021_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p21 06. Amazon ECR.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 让我们对亚马逊ECR做一个简短的介绍
亚马逊ECR代表弹性容器注册表
它用于在AWS上存储和管理Docker镜像
到目前为止，我们一直在使用像Docker Hub这样的在线仓库
但我们也可以将自己的镜像存储在亚马逊R中
实际上，您可以在ACR中选择两种选项
您可以私有地存储图像
仅为您的账户或使用S进行自己的账户
或者您可以使用公共仓库并将图像发布到亚马逊CR公共画廊
ECR完全与Amazon ECS集成
这很好 您的图像由Amazon S3在后台存储
因此，您的R仓库可能包含不同的Docker图像
然后您的ECS集群和
例如 一个 E C
2实例在您的ECS集群中可能需要拉取这些图像
因此，我们将为我们的EC two实例分配一个IAM角色
这个角色将允许我们的实例拉取Docker镜像
当然，所有对ECR的访问都受到IAM的保护，包括这一点 如果您在ECR上遇到权限错误
请检查您的策略
然后，您的容器将在您的E
C
2实例上启动 在拉取完成后由您的E
C
2实例拉取 E C
2实例
现在，亚马逊这里很好
因为它不仅支持仓库
还支持图像漏洞 扫描
版本控制
图像标签和图像生命周期 总的来说
每当您看到存储Docker图像时
请考虑ECR
好了 我希望你喜欢它 我会在下次讲座见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/022_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p22 07. Amazon EKS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


那么让我们来谈谈另一种在aws上运行容器的方式
这是通过使用亚马逊eks
所以amazonicase代表亚马逊弹性kubernetes服务
所以这是一条途径 正如其名所示，在aws上启动和管理kubernetes集群
那么kubernetes是什么，嗯
这是屏幕上右上角的蓝色标志
kubernetes是一个开源系统，用于自动部署
容器的规模和管理
通常是docker应用程序
所以它是ecs的一个替代品
它有着相似的目标
那就是运行你的容器
但是有着非常不同的api
想法是ecs肯定不是开源的
而kubernetes是开源的，被许多不同的云提供商使用
这给你带来了一些标准化
eks支持两种启动模式，再次
e C 两种启动模式，如果你想部署工作模式
嗯 像e C 两种实例
或者farmode，如果你想在eks集群中部署无服务器容器
所以使用eks的用例是，你的公司已经在本地使用kubernetes
或者已经在其他云中使用kubernetes
或者他们只是想使用kubernetes api
并且他们想使用kubernetes集群
然后他们会使用亚马逊eks，所以再次
kubernetes从考试角度来看是云中立的
它可以用于任何云
谷歌云等 这意味着如果你试图迁移到云中
使用amazon eks可能是一个更简单的解决方案
所以在术语上
这就是它看起来的样子
所以我们有一个vpc，分为三个可用区，分为公共子网和私有子网 你将创建eks工作节点
他们将是e
C 两种实例 例如
并且每个节点都将运行eks pod 所以它们与ecs任务非常相似
但从命名角度来看
任何时间你看到pod
它与亚马逊kubernetes有关
所以这就是它看起来的样子
好的 所以我们有eks pods，它们在eks节点上运行
这些节点现在可以被一个自动扫描组管理
这与eccs非常相似
如果我们想将eks服务暴露在kubernetes服务上
我们可以设置一个私有负载均衡器或公共负载均衡器来与web通信
让我们总结一下amazon eks中存在的不同节点类型
所以你有管理节点组
这是aws将创建和管理节点
所以e C 两个实例为您准备
这些节点属于由eks服务管理的自动扩展组
您支持按需实例和竞价实例
您还可以选择是否自己管理节点
如果您想要更多的自定义和更多的控制
在这种情况下，您需要自己创建节点
然后您需要将它们注册到eks集群中
然后您需要管理自己的节点作为asg的一部分
您仍然可以使用预构建的亚马逊eks优化ami
这可以为您节省一些时间
或者你可以自己搭建
我 这更复杂
这也支持
按需和按spot实例
最后，如果你不想看到任何节点
然后亚马逊eks
如我所述，支持fart模式
在这种模式下，不需要进行维护，没有任何节点需要管理
你可以直接在亚马逊eks上运行容器
现在您可以将数据卷附加到您的亚马逊EKS集群上，以便于此
要在您的EKS集群上指定存储类清单
并且这利用了称为容器存储接口的东西
CSI兼容驱动程序
考试中需要留意的关键词
您支持亚马逊EBS
您支持亚马逊ES
这是唯一与Fargate兼容的存储类
您有亚马逊FSx for Lustre
您有亚马逊FSx for NetApp ONTAP
这就是亚马逊EKS
我希望你喜欢它 我会在下次讲座见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/023_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p23 08. Amazon EKS - Hands On.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以我想快速向你展示如何在亚马逊eks上手
但这完全超出了免费试用的范围
所以如果你决定这样做
这将花费你不少钱
所以我建议你只看我做的
以便更好地理解
aws的kubernetes服务
我们将在aws上创建一个新集群
我将其命名为demo case
我可以选择一个kubernetes版本
我将使用默认的
然后我们需要一个服务角色来管理一切
所以创建这个角色
我需要遵循用户指南的指示
所以我去我是
然后我需要创建一个用于eks的角色
然后我需要添加eks集群角色
让我们这样做 我们将去角色
创建一个角色
这是为服务创建的
这是为eks服务创建的
好了
我们需要eks集群角色以便我们的集群可以访问其他服务
好了
我们完成了
权限已经选择，真实名称是eks集群角色
让我们创建这个角色
嗯 无效名称
让我们删除开始的第一个空格
现在角色正在创建
好的 让我们刷新一下
我们可以在这里找到这个角色
我们是否想用kms加密我们的秘密
我现在不想这样做
但这是一种安全措施
那么我们想在哪里部署我们的集群
我们有vpc和子网
这样我们就是高可用的 然后我们需要安全组
所以我们可以选择
例如默认的安全组
然后我们选择i
p
v 四种服务类型
然后集群端点访问将是公开的
我们需要任何网络附加组件吗
我们将再次使用默认设置
默认代理和DNS
正如你所看到的 这些都是大量的配置
老实说，我们应该有自己的课程
我只想向你展示
选项 这样你就可以理解正在发生的事情
然后我们可以配置控制平面的日志记录
我不会这样做
然后我们审查设置
所以我们设置了安全组
网络集群
公共访问权限
我们准备就绪
让我们创建这个，这将要做的是
它将创建集群本身
然后我们必须为集群创建节点
所以我的集群现在已经创建好了
下一步是为集群添加管理节点组以配置计算能力
或者创建一个farprofile
我们在概览中已经看到过这个
这就是我想向你展示的
所以我们进入资源
这里将管理你所有的kubernetes资源
这是一些kubernetes特定的知识
好的 但这里是kubernetes专家的地方
然后在计算中
这里是我们可以添加节点组的地方
所以如果我进入节点组
我可以在这里添加一个节点组
我将其命名为演示节点组
您需要为此节点组创建一个i am角色
所以我们进入i am控制台
然后创建一个新的角色
这个角色是为我的管理组中的部分e c two实例
所以我将使用e c two
然后我会搜索政策
我会输入ek
你想要在亚马逊Eworkernode政策中添加一个
所以我们点击下一步
为此我会输入，它在文档中某处
所以我会输入亚马逊eks节点卷
实际上这里有添加权限
然后我们还需要添加亚马逊EC two容器注册表
只读政策在这里
所以我们回到这里
我们将编辑权限并添加一项内容
亚马逊EC two容器存储库
让我们找到这里，好的
在这里
我们准备好了
让我们创建这个角色
让我们确保它不包含空格
好的
角色已创建
现在我可以进入这里
刷新这个
然后我会找到亚马逊EKS节点滚动更新
那么我们是否想为我们的EC two实例创建一个启动模板
我们可以指定一个 但我会留下这个
嗯，保持未选中
然后点击下一步
我们想要哪种类型的节点组
所以亚马逊Linux2很好
我们是想要按需实例还是spot实例
我们想要什么类型的实例
所以我们想要三个中等的三个微型
你想要什么 硬盘大小是多少
节点的缩放配置是什么
所以你希望在你的主机组中有多少个节点
这是自动缩放组的设置
然后是节点组的更新
当你进行更新时 你可以告诉我有多少个节点需要更新
所以点击下一步 你想要访问哪些子网
然后我们准备好后
我们创建了这个管理组
所以为了向你展示
这是部署亚马逊EKS集群的EC two实例的方式
但它们完全由AWS管理
这使得它非常容易
创建节点的另一种方式
所以我们向上一级
我们回到计算
创建节点的另一种方式
除了从这个节点组之外，实际上可以创建fargate，允许你不必为e
C2实例进行配置
所以我们将设置一个目标配置文件
我们现在不会这样做
我只想向你展示fargate节点组的选项
实际上我不需要这个
所以我会在它完成创建后删除它
我想向你展示的最后一个选项是关于附加组件
所以如果我们想实际使用ebs卷
我们可以安装附加组件
其中一个是亚马逊ebs csi驱动程序
这将使我们能够利用ebs为我们的亚马逊eks集群服务
它们将是csi驱动程序
同样适用于fsx esfs等
好的 这就是我想向你们展示的所有选项
说实话，kubernetes需要自己的知识
这非常困难，需要一整门课程
所以现在我将删除这个集群
为此，我将输入demo e
这将是所有
我想向你展示的是如何创建k8s集群并删除它
首先我需要删除节点组
我将跳过这部分视频
这就是这节课的内容 我希望你喜欢它 我将在下节课见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/024_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p24 01. Intro Analytics.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


接下来的部分是一个大块
AWS在数据分析领域拥有广泛的技术
事实上，有数据分析专业的认证考试
你也可以参加，你可能会期待
考试与这个有很多重叠
你可能想要考虑准备AWS认证数据分析专业考试
在你完成数据工程考试后
这里有很多要覆盖的
我们将在这部分之间切换我和Stefan
以便全部完成
AWS Glue是您数据工程管道中非常重要的组成部分
特别是在处理数据湖时，作为一个整体
围绕Glue的技术生态系统
我们将深入研究AWS
湖形成也是你应该了解的东西
以及Amazon Athena
我也会和你一起讨论Amazon EMR以及Amazon Open Search
这是Amazon的Elasticsearch版本
最后，我会谈到使用Amazon QuickSight来可视化数据
出于某种原因 考试指南将Amazon Kinesis归类为分析
Stefan将在这里与你讨论这一点
以及Amazon管理的Apache Kafka
它执行类似的角色
Kinesis更多是关于数据摄取
但它也有能力转换和执行一些分析
它在数据需要前往的地方流式传输数据
我将谈论Amazon管理的Apache Flink服务
这是另一个您可以用于流数据进行分析的工具
像往常一样 我们将以测验结束
这将挑战您将这些技术应用于现实世界中的问题
这里有很多要覆盖的 让我们开始吧
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/025_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p25 02. AWS Glue.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的
我们来谈谈aws glue
这是一个构建您表定义和执行etl（抽取、转换、加载）的系统
用于自动加载您的数据
现在，glue在考试中非常重要
所以，当他们从aws大数据考试中移除所有机器学习内容时
那时，它被称为自己机器学习的考试
他们填补了这个空白
主要是通过更多的内容关于glue和redshift
所以，glue在考试中扮演着重要角色
因此，非常重视这个部分是非常重要的
关于胶水你真的不需要知道太多
但你真的需要内化它，并理解它是如何将这些不同的服务连接在一起的
并且它如何被用作更大应用程序的组件
所以线索都是关于什么的呢
它是一个无服务器系统
所以再次 你知道
那里不需要你保持任何东西
这将自动处理表定义和模式的发现和定义。
所以它的主要用途是作为数据湖的中央元数据仓库
你将从无结构的数据中发现那些模式
这些数据存放在s3或其他地方
并为分析工具发布表定义
例如athena、redshift或emr
再次 glue本身的目的是从你的无结构数据中提取结构
如果你有数据存放在s3的数据湖中
它可以为这些数据提供一个模式
这样你就可以使用sql或类似sql的工具来查询它
包括RDS和Redshift和Athena和Amazon EMR
以及几乎任何可以使用那种模式的SQL数据库
Glue的另一件事是自定义ETL作业
一旦它发现那个模式
它可以为你处理那些数据
这可能是由触发器驱动的
你知道 基于数据接收的时间或按计划或按需
它是完全管理的
再次 没有需要您维护的服务
没有硬件供您维护ETL作业
在底层使用Apache Spark
我们还没有讨论过这一点
但Spark是一种非常流行的分布式数据处理系统
但你不需要管理Spark集群，这真是太好了
因此，您拥有Apache Spark的全部力量，用于Glue ETL
而无需管理Spark集群，这真是太好了
这真的是一个很好的事情
所以这是一个部分是Glue爬虫和数据目录
它填充了
所以 亚马逊胶水的目的是扫描s3中的数据
通常它会自动推断模式
仅仅基于它看到的数据
但有时你需要给它一些提示
您可以按需定期安排
它会填充胶水数据目录
我们有数据存储在s3中
胶水可以定期或按需扫描数据并构建其模式
这就是表格定义
你知道 数据列的信息
您以非结构化方式存储在那里
他们的数据类型
列名
它也可以推断
它存储在哪里 那也是重要的信息
它使用这些信息允许像红移
雅典娜
或运行在emr的系统上
在s3中查询未结构化数据
就像它是结构化数据一样
数据本身仍然保留在原始位置
数据仍然存储在s3中
您没有复制数据
我们没有将其复制到实际的数据库表
其他地方 我们只是使用胶水
基本上提供这些关系数据库接口之间的粘合剂
和后面的s3中未结构化数据湖一旦目录
一旦目录，数据立即可用进行分析
我们将进一步扩展并详细说明这些系统
然后你知道
一旦您实际上将数据存储在雅典娜中
您可以使用亚马逊快速站点来可视化数据
胶水再次是未结构化数据和结构化数据分析工具之间的粘合剂
工具
让我们快速谈谈胶水和s3分区
它不是完全魔法
你知道 您需要考虑如何组织您的非结构化数据
我知道这听起来很荒谬
您组织数据的方式会影响胶水如何高效工作
以及您的系统如何高效查询此未结构化数据
胶水爬虫将提取您数据的分区
基于您s3数据如何组织
您需要提前考虑如何查询
您在s3中的数据湖并那些分区应代表以获得最佳性能
所以，例如
让我们假设你有设备将传感器数据发送到你的系统
每小时将数据发送到你的数据湖
只是直接将原始数据发送到S3
想想你将如何使用这些数据
你是主要通过时间范围查询吗
如果是这样 那么你想根据这些时间范围进行分区，对吧
所以你希望将S3桶组织为年份
然后是月份
然后是日期 然后是设备
好的 这样你就知道
我们可以首先通过年份筛选数据，然后通过月份进一步筛选
然后是日期
最后是生成该数据的设备
这将允许分区使我们能够高效地访问给定日期的数据
而不必通过其他我们不关心的日期筛选信息
另一方面 如果你主要通过设备查询
而不是通过时间
你可能希望将设备放在第一位
这样你的主要分区将是设备ID
然后是年份
月份和日期
如果你经常查询特定设备的数据
并且这是你想要分区的主要东西
你将希望将其作为顶级桶
你组织数据在S3下的
这可能会在考试中出现
组织你存储在S3上的数据的最佳方式是什么
以便Glue可以提取它 并以你对其进行操作的方式使其成为有意义的分区
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/026_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p26 03. Glue, Hive, and ETL.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以我们还没有讨论Apache Hive
但基本上，Hive是一个你可能在弹性MapReduce上运行的服务
Reduce 它允许你在可以访问EMR集群的数据上发出类似于SQL的查询
我说类似于SQL，是因为从技术上讲，它被称为HiveQL
但HiveQL和SQL之间的区别在考试的目的上并不是很重要
重要的是你知道Glue可以与Hive集成
尽管在考试的目的上不是很重要
重要的是你知道Glue可以与Hive集成 尽管在考试的目的上不是很重要
所以你可以使用aws glue数据目录
这是由你的glue爬虫提取的
作为你的hive和hive的元数据存储，这叫做元存储
这是有点特殊的hive术语
反之，你也可以将hive元存储导入到glue中
所以，如果你有一个现有的hive元存储或hive数据库位于emr中
你可以转身将该hive元存储转换为glue数据目录
你可以双向操作
这将在我们更深入地讨论hive时更让人理解
但请记住，glue数据目录可以为运行在emr上的hive提供元数据信息
所以 就像胶水可以提供您在S3和亚马逊服务如红移和雅典之间的胶水
它也可以提供在您的emr集群上运行的服务之间的胶水
您知道
更传统的开源应用程序，如Apache Hive
让我们更深入地谈谈Glue ETL
这对考试非常重要
您不需要知道很多
不多
但你需要知道这一点
所以确保你内化了这些东西
朋友们
Glue ETL将自动或可以自动为你生成代码以转换您的数据
您可以前往亚马逊控制台
并说明您希望以图形方式转换您的数据
它将自动为您生成代码，该代码将在Scala或Python下运行
并利用Apache Spark引擎的优势
这是Glue ETL在幕后运行的
因此您也可以编写自己的代码
如果你想使用Scala或Python
它会处理加密，所以你可以在存储中实现服务器端加密
或者处理在传输中的数据的SSL加密
这些都由Glue处理
Glue ETL可以事件驱动
所以你可以自动启动ETL过程
只要Glue看到新数据
你可能还需要做的是为额外的dpus(数据处理单元)进行配置
这将增加底层Spark作业的性能
尽管Glue被设计为完全托管，但在底层它仍然需要这些
仍然有需要运行的这些spark作业的服务器
它们正在运行为你处理的etl过程
如果你发现这需要太长时间
你可以为那个spark集群分配额外的gpu，以便在底层运行
给那个spark集群提供更多的处理能力
你如何知道你需要多少gpu呢
你可以在你的glue etl作业中启用作业指标
以帮助你了解你需要的最大处理能力
确定你需要多少gpu的过程是
启用作业指标
研究它们 然后在作业开始时设置dpus的最大容量
这就是你为调整glue etl作业性能所遵循的过程
如果在glue etl管道中遇到错误
你可以自动将这些错误报告给cloudwatch
如果你想要收到这些错误通知
你可以将cloudwatch与s连接起来，以确保你会收到通知
或者你会收到短信
或者任何你想要的方式 如果你的etl过程遇到问题
再次
我想确保你理解这个glue
etl是一个系统，它可以自动处理和转换你的数据
你可以通过图形界面来做到这一点
该界面可以让你定义你想要的转换方式
它会使用apache spark在底层进行处理
使用scala或python代码
完全加密 无论是静止的还是在传输中的
可以事件驱动，如果你希望如此
或者可以按计划运行
关于glue etl的更多信息
再次，我想强调这一点
你可以使用它来转换你的数据
清理你的数据
或者在进行分析之前丰富你的数据
它会生成python或scala的etl代码
并且你可以修改那个代码，如果你需要的话
所以如果你生成的代码没有完全符合你的需求
你可以直接进入并修改那个代码以调整它并做到你想要的
所以你可以使用自动生成代码作为起点并构建在上面
这是用glue etl进行更复杂操作的一个非常好的方式
你也可以提供你自己的代码
如果你想要手动编写的话，你自己的spark或pi spark脚本
一个spark脚本可能是用scala编写的
但如果你更喜欢python
这在数据分析领域中有很多人使用
你可以提供一个python脚本用于spark
称为pi spark的glue etl来处理
它会直接将那个交给底层的apache spark引擎进行数据转换
清理它 丰富它
在你实际分析数据之前，你需要对数据进行处理的任何事情
使用后续系统
Glue ETL的目标可以是S3
或者可以通过JDBC接口与另一个数据库进行通信
包括RDS或Redshift
或者可以将数据输出到Glue数据目录
因此，您可以有一个仅生成数据目录的ETL作业
它是完全管理的 尽管你对底层资源有一定的控制权
你拥有的dpus数量就像我们讨论的那样，非常经济实惠
再次强调，大多数这些无服务器管理工具
你只支付实际消耗的资源费用
工作在无服务器的Spark平台上运行
当我们说无服务器时
是的 实际上，服务器在幕后运行那个Spark代码
但你看不到它们
你不必管理它们 这就是为什么我们把它叫做无服务器
这完全是一种魔法
有一个胶水调度器，你可以使用它来安排那些工作
或者你可以使用胶水触发器，根据你想要的事件自动运行工作
所以请花一点时间来内化这个幻灯片和上面的信息
这对考试来说非常重要
那么你是如何应用这些etl转换和胶水的呢
你将要处理的主要数据结构叫做动态框架
如果你熟悉apache spark
这将会让你感到非常熟悉
它非常像一个spark数据框对象
唯一的区别是它提供了更多的etl功能
它提供了scala和python的api
无论你喜欢哪个
就像数据框是行和列的集合一样
动态框是动态记录的集合
动态记录是带有某种模式的自描述记录
所以你可以在这个图片的右边看到
你可以把动态框看作是动态记录的集合
每个动态记录包含字段
列 在那个记录中你可以称它们为任何你想要的名字
在这个例子中id year month
但是它们可以是任何东西对吧
你可以有任意多的动态记录
这些所有的记录都会被分布式存储
可能在s3的底层
对吧 所以为了让它更真实
这里有一些代码片段
记住在考试本身不会被要求写代码
但如果你是一名程序员
这可能会帮助你更好地理解
例如 如果你想创建一个动态帧叫做push down events
我们可能会说glue context
获取目录源
并从某个数据库表加载
然后应用该动态帧的映射
我们可以调用push down events
点 应用映射
这将对动态帧中的每个记录应用该映射
当然，以分布式方式进行
这就是重点，对吧
以一个具体的例子来说
我们定义了一个将ID从字符串转换为长整型的映射
重命名演员点登录字段为仅演员
仓库名称为仅仓库
等等 细节并不重要
但它大致说明了
这里的概念
你可以创建一个动态框架
在这里只需要几行代码
我们可以对动态框架中的每个记录进行复杂的映射，分布式进行
让我们再多谈谈你在glue etl中可以做的转换
有一些内置的转换可以立即使用
其中包括删除字段或删除空字段
如果你只是想删除空数据
这是非常常见的预处理操作
它可以为你做这件事
你知道的 只需点击几下
所以它可以自动删除字段或删除空字段
为你删除输入数据中的空白数据
你也可以进行过滤，指定自己的过滤记录函数
所以如果你需要指定一些标准
你想要丢弃异常值或可能
你知道的，不在你预期的范围内
你可以设置这些过滤器，根据那些标准过滤记录
或者 也许你只想提取你的数据的子集，以便现在分析
这也是过滤器的一个合法用途
是的 你可能想说
我只想为这份工作过滤来自美国的数据
或者我只想过滤来自英国的数据
无论什么，一个过滤器转换
可以让你这样做
这就是一个捆绑操作，使用胶水etl非常容易完成
你也可以做连接
这也是一个捆绑转换，作为glue etl的一部分提供
如果你需要通过连接其他表来丰富你的数据
你可以这样做
这是一个常见的操作
有 你知道一些代码
或者你知道一个国家代码之类的
并且通过完整的国家名称来丰富它
我的意思是，我不想深入数据库理论太多
但连接通常用于丰富数据
例如，我在我的课程中经常使用movie lens数据集
你可能有一个电影id
如果你想在输出数据中实际放入电影名称
你可能需要与一个单独的表进行连接，该表将电影id映射到电影名称
作为示例
你也可以做映射
这可以让你做诸如
向数据添加字段
删除字段
或者手动进行外部查找
如果你只需要一行一行地转换你的数据
也许那只包括
选择你想传递到最后分析的字段
这是一个非常常见的操作
你可以使用映射操作来完成
例如 如果有一堆数据列进入你的数据
你可以使用映射
一个映射操作
以及glue etl来提取你实际上关心的字段
并删除你不关心的字段
还有一些内置的机器学习转换，非常酷
你可能会在考试中看到的一个例子是find matches ml转换
它的目的是在你数据集中识别重复或匹配的记录
即使那些记录没有共同的唯一标识符，并且没有字段完全匹配
所以它实际上可以学习一个重复记录看起来像什么
即使它不是真正的重复
你知道的 例如
如果你有重复的地址，它们非常
你知道通过某些微妙的事情
例如某人 嗯
拼写了街道而不是使用缩写
你可能使用find matches ml来处理这个问题
所以这是一个你可以使用的工具
只记得它存在
并且它是做什么的
你也可以使用胶水ETL进行格式转换
并且它可以自动转换的一些支持的格式，比如我们的CSV
逗号分隔值
JSON Avro
Parquet Orc和XML
所以很多时候，数据会以一种格式进入
而你需要将其转换为下游系统的另一种格式
胶水ETL可以很容易地为你做到这一点
而且由于它使用Apache Spark作为核心技术
Apache Spark能做的任何事情
您都可以通过Glue ETL来实现，包括一些高级功能，如
K均值聚类
这来自Spark ML的Lid包
我们将在课程后面更多地讨论Spark
但请记住，Spark能做的任何事情
您都可以通过ETL来实现
请花点时间关注这个幻灯片，伙计们
了解Glue ETL的深度是非常重要的
并且它能做什么，胶水的另一种方法
我想讨论的动态框架是解决选择
我听说这实际上现在出现在考试中
所以我们应该谈谈它
它在生活中的目的是处理你动态框架中的模糊性
并返回一个新的没有模糊性的动态框架
我什么意思叫模糊性
例如 如果我有两个字段在我的数据框中有相同的名称
这将是一个我们需要以某种方式解决的模糊性
目前有四种方法可以处理这些模糊性
一种方法是将make_call作为resolve_choice_api的参数
这会为每种类型的名称创建新的列
看下面
这里有一个例子
我们有一个动态框架my_list，其中包含两个名为price的字段
一个是数值型双精度位置数字
另一个是包含美元符号的字符串
如果我们调用
make_calls
我们得到两列新列
一列名为price_underscore_double，一列名为price_underscore_string
为了解决这种歧义
另一种选择就是类型转换
这会迫使每个实例转换为一个特定的类型
我们也可以说make_struct
这将在你的记录中创建一个结构
其中包含每种数据类型
在price中，每种数据类型单独存在于该结构中
所以我们可能有一个新的结构
那里称为价格在内部价格
结构将是该字段名字的十进制版本和字符串版本
最后我们有项目，它将每个类型投影到给定类型
所以例如 我可以只说项目：字符串
这将迫使一切成为字符串
数据类型 这将迫使一百点零零十进制类型成为字符串而不是
这就是解决选择所涉及的
它只是关于解决歧义 当你在你的动态框架中有同名不同列时
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/027_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p27 04. Modifying the Glue Data Catalog from ETL Scripts.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


有时候
你的数据ETL任务的结果是你想要修改数据目录本身
作为那部分的 也许你添加了一些字段或表
或者作为结果的移动东西
那么怎么做 当你有了这个爬虫运行它定义了你的方案
当你想要更改那个方案之后
在事后
有几种方法可以做
你也许需要更改你的划分作为ETL的一部分
你也许选择存储数据不同的方式在不同的划分
也是你需要处理的事情，结果有几种方法
所以，你可以重新运行爬虫
但是另一种方式是通过你的ETL脚本实际更改划分
通过启用更新目录和划分键选项
在它设置事情的时候
那是可能的做
你也可以更新这个表方案
再次 选项A只是重新运行爬虫
但是你也可以通过使用启用
更新 目录和更新行为从脚本做
一旦你处理完成
你也许创建新的表从你的脚本，再次
你将使用启用 更新
目录和更新行为
一起与设置目录信息创建新的表从你的脚本如果你需要
并且有一些限制做所有这个东西
首先 从ETL修改数据目录只能当
S三就是你的底层数据存储
它也仅限于json
CSV Avro 和parquet格式
并且如果你使用parquet
有一些特殊的代码你需要知道当你做它
也不支持嵌套方案
所以那些是修改你数据目录的陷阱
但是重要的是知道Glue ETL允许你做这个
你不必在每种情况下重新运行爬虫
有时候你的脚本实际上可以添加新的划分
更新表方案 或者甚至从脚本创建新的表
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/028_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p28 05. Running ETL Jobs with Bookmarks.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


运行胶水作业
有几种方法可以再次做到这一点
一种是使用基于时间的cron风格时间表
所以如果您需要根据某种固定日程运行它
您可以使用与kcron相同的格式
您可以使用称为作业书签的东西再次
这对考试很重要
因此，作业书签用于持久化作业运行状态
所以如果您需要设置东西
这样您就不会再次处理相同的数据
书签允许您跟踪您离开的位置
这样您可以防止处理旧数据
并在重新安排时仅处理新数据
作业书签跟踪您在调度运行Glue作业时离开的位置
它与多种格式的S3源一起工作
并且还可以通过JDBC与关系数据库一起工作
如果您的primary keys按顺序排列
该功能仅处理新行
不处理更新行
因此，您只能以关系型数据库使用作业书签
如果您正在注入新数据为新行
如果它正在回退并更新现有行
作业书签不会按您期望的方式工作
这可能是重要的事情要知道
此外，CloudWatch事件
因此，您可以触发lambda函数或短信通知
当Glue中的ETL作业成功或失败时，您想做的任何事情
因此，Glue与CloudWatch集成以跟踪发生了什么
并基于链式事件通过S或lambda通知您
一旦那发生，您可以调用EC two运行以进行进一步处理
将事件发送到Kinesis
激活Step Function
您想做的任何事情
因此
请记住 您知道
使用CloudWatch 您可以将Glue与其他系统（如E、C、2、Kinesis或AWS Step Functions）连接在一起 E C Kinesis
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/029_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p29 06. Glue Costs and Anti-Patterns.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们来谈谈胶水
成本模型 它做了很多
但它不免费 嗯
通常不是 基本上，你会按秒计费你的爬虫和ETL工作
所以你会被收费 基于
你的数据爬虫花费多少时间来从你的数据中提取那些模式
以及你的数据处理和转换所需的时间
使用Apache Spark进行存储
在Glue数据目录中存储的前100万个对象是免费的
所以非常慷慨的配额
但是 对于大数据应用
你可能很快就会消耗掉100万个对象
如果你使用开发终端来实际编辑你的ETL脚本
使用笔记本 这些按分钟计费
所以当你完成时确保关闭它们
你使用笔记本和开发终端的每一分钟
Glue 你会按每分钟计费
一旦进行开发，Glue就会开始计时
不要只是让它运行然后去吃午饭一周
是的
Glue反模式
你不应该用什么来代替Glue
实际上，他们投入了大量努力使其尽可能通用和灵活
他们告诉你不要使用的唯一东西
如果你试图使用多个ETL引擎
再次 记住，Glue ETL基于Apache Spark
如果你真的需要使用其他引擎
例如Hive或Pig
或者类似的东西，你可能想要使用数据管道
或者使用EMR来处理数据
这将是一个更好的选择
尽管Apache Spark可以完成你几乎想做的所有事情
我真的很难想象一个场景
你需要使用Hive或Pig或类似的东西而不是Spark
除非你有一些需要集成的遗留代码或系统
以前是一个反模式，使用Glue ETL处理流数据
但这不再是真的
自2020年4月起
Glue ETL现在支持无服务器流式ETL过程
因此，它可以连接到Kinesis或Kafka以摄取流数据
并在飞行中转换和清理那个数据
这真的很酷
然后可以将结果存储在s3或其他数据存储中
它是如何工作的
它使用Apache的Structured Streaming和Apache Spark
这不是什么新技术
但是对Apache Spark来说是一个相对较新的功能
Structured Streaming允许在底层对流数据进行真正的实时处理
Apache Spark有一种叫做数据集的东西
它用于处理正在处理的数据批次
但是使用Structured Streaming
你基本上有一个不断增长的数据集
随着流数据被添加到其中
并且可以转换该数据
使用与批处理相同的代码
如果你有一个Spark脚本在Glue ETL中
它是用于处理批处理数据的
通常也很容易将其适应为流处理
这算是一个不错的特性
稍后我们会更深入地讨论Apache Spark
所以流数据现在也OK与Glue ETL 这真的很酷
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/030_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p30 07. AWS Glue Studio.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在aws glue中有一些新功能
他们似乎大量投资于这个功能
所以我不确定你会在考试中看到这些
但你很快可能会看到
所以这些新功能之一是称为aws glue studio
它基本上是一个用于定义和创建etl工作流的可视化界面
因此它允许您创建这些更复杂的工作流
在这些工作流中，您可能可以从同一来源同时进行多个操作
或者从不同来源以不同方式并行处理
但它允许您以视觉方式设置这些复杂任务
并且通常不需要编写任何代码
因此它提供了这个视觉任务编辑器
我将稍后向您展示，允许您创建复杂的工作流
的定向无环图
该来源目前可以包括s three或kinesis或kafka
或任何jdbc源
您可以在glue studio的ui中图形设置数据的转换
您可以进行转换
您可以采样数据
您可以将表连接在一起
以及几项其他事情
然后可以将该转换的输出
发送到s three或glue数据目录
它还支持内置分区
如果您有分区数据
它将自动利用这一点并加速自己
相应地 它还提供了一个视觉任务仪表板
允许您可视化glue etl任务的状态
以及它们是如何运行的
它们花了多长时间
这可以用于快速故障排除并确保您的任务按预期运行
如预期
让我们深入研究并展示它如何工作
所以为了使glue studio再次真实
您在考试中需要了解的唯一一件事
就是它是什么以及它是做什么的
您知道
我们将在这里快速浏览一下
你不必跟着走
如果您在aws的glue控制台中
您会看到有一个aws
Glue studio链接在这里
仍然有一个新标签
我假设那最终会消失
或者glue studio可能会成为默认视图实际上在某个时候
但现在我们必须点击它
让我们说，我们希望创建一个新任务
我们将点击创建
管理任务 你可以看到我们可以轻松地从一个给定的源和目标开始
所以如果我的数据源存储在s3中
我可以选择s3
或者它可能来自rds
红移 Kinesis或Kafka
然后需要将结果放在某处，要么
S3或到Glue数据目录
让我们从一个s3存储桶开始并将其放入另一个s3存储桶
点击创建
你可以看到这是S3数据源的图形化表示
某种形式的转换
我们将应用到该数据源并设置目标以得到转换后的数据
我们不局限于这些顺序的事件
我们可以在这里设置更复杂的树形图，使用有向无环图
这与Spark的工作方式类似
这不是巧合
Glue ETL使用Spark本身
这不令人惊讶
他们采用了这种结构
所以我们可以点击s3桶
并选择数据来自哪里
我将使用我为本课程练习创建的现有一个
这部分课程
然后我们可以对其进行转换
我们可以点击现有一个
嗯 让我们说，我们要删除描述字段
并且我们要确保年份是整数，日期是整数
你知道我只是在这里向你展示它是如何工作的
我们可以点击输出模式来查看那个转换输出什么
你可以看到年份已经被转换为整数
天数已经被转换为整数，并且那个描述字段现在已经不见了
然后我们从s三存储桶这里为目的地
我们可以通过浏览s三并选择那个地方来指定它去哪里
这里有其他你可以做的事情
让我们点击回到这里
如果你在这里拉下转换菜单
让我们点击回到这里
我们可以看到，这里有很多不同的事情可以做
图形上我们可以应用映射
删除字段 重命名字段
将两个表连接在一起
将它们分开 让我们做一个过滤
你也可以在这里做自定义代码，使用自定义转换
如果你仍然想写代码
但让我们引入一个过滤转换
我们可以说添加条件
让我们假设我们只想获取在2021年之前的年份
或者类似这样
通过添加这个条件
我们实际上添加了一个过滤器，过滤掉2021年之前的任何数据
我们可能还想为这个过滤器指定一个目标
让我们将这个目标指定为S3的某个地方
我们可能不需要在这里删除这个中间目标
因此我们可以删除它
我们又回到了这个线性序列的事件链这里
但是再次强调这不一定非得是线性的
你可以将这个拆分成多个目标，并在并行中进行各种复杂的处理
这基本上是glue studio的强大之处
你可以拥有这种源数据
将其拆分为多个不同的转换
并将这些转换的输出放入不同的地方
这相当酷
如果你回到主glue studio页面
这里可以监控你已经运行的工作
我现在没有运行任何工作
但这会给你一个想法，你可以在这里看到什么
它给你一个关于正在运行的工作的概述
有多少成功了和失败了
成功率 资源使用情况
正在运行的工作类型，时间线
关于你ETL工作的所有信息
这里有一个图形仪表板
这可以使维护和查看ETL工作的运行情况变得更加容易
以及故障排除 这就是AWS Glue Studio的概述
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/031_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p31 08. AWS Glue Data Quality.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


胶水工作室的一个更近期的功能是AWS数据质量
这基本上是一个步骤
您可以在Glue作业中注入它
以自动评估传入数据的质量
如果它违反了您设置的某些参数或规则
那么它可以自动失败作业
或者为您在CloudWatch中记录某些内容
您可以手动创建这些规则
或者它们可以自动为您创建
所以您可以告诉Glue数据质量 嘿
去看看我的数据源
我可以从这些数据中推断出一些规则
你知道 这是一些数据的预期范围
数据的标准差是多少
我应该设置一些警报
如果我超出了这些范围
一旦您设置了这些数据质量规则
您可以将它们直接集成到Glue作业中
就像您在右侧的图表中看到的那样
就像另一个转换阶段一样
在某个阶段可以使用Glue数据质量评估数据质量
如果您打算手动创建这些规则
它使用称为数据质量定义语言或DQ DL的东西
我们将在下一页看一下一些示例
再次，您可能会有几种不同的方式可以做到这一点
您可以设置它
以便如果数据质量规则失败
它将失败整个作业
这可能很烦人
因为如果您有一个阈值设置得太紧
您可能会得到很多假阳性
或者您可以将结果报告到CloudWatch中
您可以根据需要解释并采取行动
这里有一个小例子，它在Glue UI中看起来如何
所以他们创建了一个名为规则集1 RS1的规则
您可以看到在下面
有一些DQ DL语言的示例规则
这是通过自动推荐创建的
因此，您可以看到可以在数据集上运行Glue数据质量
它会自动为您创建这些DQ DL规则
您可以进入并编辑它们，如果您想要的话
如果您不同意我所提出的东西
这里有一些示例
它说行数应该在这些值之间
如果我看到少于预期的行数
可能有些地方出错了
您可以确保某些列中有完整数据
您可以确保某些列的长度在特定范围内
像这样的事情
标准差 同样
那是一个有趣的一个
所以你可以说
如果违规日期或票号超出预期范围
我们也应该采取措施
好的，你可以在这里看到结果
结果会像这样
所以在这个例子中
我们制定的规则失败了
因为数据违规的标准差
数据违规列中的值与预期不符
这是阈值设置得过紧的例子
可能导致假阳性
但可能确实是一个问题
也许数据中有一些坏数据
或者需要考虑的异常值
这会影响你后来的分析
所以这是aws glue
数据质量 相对较新的功能
从2024年开始，考试应该是公平的
说2024年之后 尽管
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/032_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p32 09. AWS Glue DataBrew.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们来谈谈胶水数据 brew
这在数据工程的领域里肯定是你需要知道的事情
因为这就是一个可视化的数据准备工具
所以它是另一个用户界面，你可以用它仅仅用于转换数据
做你的ETL的T部分
所以这对考试来说非常非常相关
这是大数据集的预处理用户界面
这些数据集可以来自S3、数据仓库或任何数据库
你想要的输出
它将被数据 brew 转换后上传到S3
并且有超过250种已经准备好的转换，你可以直观地将它们应用到数据上
所以你可以在数据酿造中拖放这些东西，让它自动以你能想象的任何方式转换你的数据
我不会一一介绍这250种转换
如果你想在asa的网站上查看文档，至少浏览一下它们
这将有所帮助
了解这些功能是好的
你知道的
至少浏览一下它们
这将有所帮助 了解这些功能是好的
但简短的说，几乎你所能想象到的任何东西都在里面
可能都在那里
你可以创建这些转换的食谱
这些食谱可以在一个更大的项目中保存为工作
如果你有多个转换
你想应用到你的数据上
你可以创建这些转换的食谱
然后将其保存并应用到不同的地方
不同的项目 不同的数据集
那些食谱是由食谱操作组成的
如果你去看那些转换的文档
你会看到这些食谱操作的定义
例如
在右边 这是一个叫做nest to map的食谱操作
这个操作用于选择列并将它们转换为键值对
而不是单独的列，它可以将它们转换为json文档
这个文档将这些列和它们的值之间建立了键值映射
我不知道你为什么想要这样做
但是你可以做到这一点 你也可以选择性地移除你合并到该映射源列
以及其中的一部分
这就是在这里发生的事情
在参数中 我们指定我们要嵌套的源列
所以我们将取年龄
重量 公斤和高度厘米列
我们将它们放入一个新的单列称为列名
它将被存储为基于json的映射
我将说 例如
年龄等于53体重公斤等于100
身高厘米 啊等于
你知道500 无论它是什么
嗯 你可以看出我不真正处理公制系统
因为我确信那些数字毫无意义
然后我们也说删除源列
原始的年龄
重量和高度列将在那个映射转换的一部分中被删除
你也可以在数据酿造中定义数据质量规则
酿造中也可以，以确保你的数据在过程中有意义
你也可以使用自定义sql从红移和雪片创建数据集
从安全角度来看
数据酿造与kms集成，使用客户主密钥
当然使用ssl传输
你可以使用iam来限制谁可以做什么，就像大多数服务一样
并且就像大多数服务一样
它与云观察和云轨迹集成以进行报警和治理 以及所有好东西
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/033_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p33 10. AWS Glue DataBrew Demo.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


那么我们就来探索一下数据粘合剂的数据烘焙这里一点
我会给你一个快速的UI之旅
只是为了让它更直观地了解它都是关于什么
所以我在这里的数据粘合剂的数据烘焙控制台页面
让我们创建一个样本项目
使用他们提供的一些样本数据
探索2020年流行的婴儿名字
我们将为这创建一个新的数据烘焙项目
嗯 我们需要给它一个IAM角色以进行安全目的
嗯 我们用我已经创建的这个吧
它会出去获取一些计算资源 用于执行这个数据酿造中的这个交互式部分
在这个数据集上
嗯 正如你们从定价中记得的那样
每个交互式会话一美元
然后我们
如果你真的要把这个作为持续运行的任务
那么这就是你用的资源构建的
但现在它会很快启动一些计算资源
它将在不久后启动我的数据酿造会话
实际上这确实很快
这不像启动一个e
C 两个节点或任何类似的东西
我们开始了 它正在准备一个数据框从那个样本
从s三加载数据
我们准备好在这里摇滚了
你们可以看到这里有一个数据框
显示了样本数据，包括每个婴儿名字的出现次数，性别，与之相关的一些id，实际的名字本身，以及年份字段
这里我们拥有每个婴儿名字的出现次数，性别，与之相关的一些id，实际的名字本身，以及年份字段
这是1880年
我以为这是一个2020年的数据集
但我现在不会深入探讨这个问题
总之 这里的想法是，你可以应用这种食谱并构建它们
将这些数据转换为你想要的任何方式
你可以交互运行并下载结果
或者你可以创建一个任务，允许你重复运行它，甚至按计划运行
例如
假设我只想获取以字母L开头的婴儿名字
出于某种原因 作为一个非常简单的例子
让我们在这里创建一个新的食谱
所以我们有这个 嗯
我们可以在食谱中添加一步
这是所有那些不同转换的菜单
你可以进行二百五十多种转换
超过二百五十种 有列操作
你可以重命名列
或者更改它们的类型 或者移动它们
或者复制它们 或者删除它们
你可以过滤掉一些东西，我们将会做
你可以更改文本字段的格式
这里有内置的数据清理操作
所以如果你想要删除特殊字符或者删除数字
无论你想要做什么 如果你需要先进行预处理
然后 嗯
在进一步的处理之前
你可以很容易地使用清理过滤器来做到这一点
我们也可以提取信息之间的分隔符
如果你有一个字段包含受限制的数据
并且你只想提取这些分隔符之间的数据，很容易做到
你可以指定分隔符在哪里
并且将那个列放在哪里
一个非常容易的方法
你也可以
嗯
自动填充缺失值
这是一项很酷的功能
如果你有数据缺失，你想要做什么
你想要用空值填充吗
或者你想要删除整行
或者你可能只想用最频繁的值作为占位符来填充
有时候这样做是正确的，数据酿造中很容易做到
我们还可以做什么无效值
你可以处理这个问题 你可以处理重复值
无论你想要怎样处理，你可以分割列
合并它们 根据某种函数创建新的列，展开，旋转，分组，连接，联合
现在我们开始进入类似于SQL操作的阶段了
所以几乎你可以在数据酿造中做任何数据库能做的事情
我们可以缩放数值，这可能非常有用
如果你正在准备深度学习数据
很多时候你需要在将数据喂给神经网络之前对数据进行归一化
这些缩放操作使这一点变得非常容易
我们还有什么
聚合 文本函数
这些都是基本 SQL 函数
嗯 您可以使用
这列出了其中的大约 250 个选项
您可以看到，几乎所有您可能需要的转换都在这里
这真的很棒
这允许您使用非常灵活和强大的引擎来转换和预处理数据
这通常不需要编写任何代码
这真的很酷
假设我们想要过滤数据
只获取名字
以字母 L 开头
我将过滤条件
我们将选择名称列
我们将说它必须以 L 开头
我可以在这里使用预览按钮预览
您可以看到，只有以 L 开头的婴儿名字被保留
这真的很酷 然后我可以将此应用于整个数据集
现在我们有一个以字母 L 开头的婴儿名字数据集，这真的很酷
您可以在这里看到许多用于此的快捷方式
在这个仪表板中 我能做什么
让我们转到操作
我可以将过滤后的数据作为 CSV 文件下载
这很酷 或者我可以创建一个新的工作
如果我想重复运行它
我将其下拉到工作选项卡
我可以创建一个工作名称
嗯 婴儿名字 L 随便您想叫什么
我们可以在这里设置输出设置
您可以输出的文件类型目前很有限
但它可以是 CSV Glue
Parquet
Avro Orc xml 或 json
这些是您可以选择的当前输出路径 S3 存储桶
以及您选择的压缩
Snappy Gzip
Lz B zip 或 deflate
大致上这些是两种新的
嗯 您想要的任何分隔符
逗号分隔 冒号分隔制表符
分隔任何你可以想象的东西，你可能想要
是的 一旦你有了这个
你可以实际上作为按需东西运行那个工作
或者你可以为它创建一个日程
如果你回到数据酿造控制台这里，转到工作
你可以点击日程表
这里是你可以实际创建一个日程以重复运行一个工作的地方
在某种形式的 cron 风格日程上
所以你也可以这样做
回到主要的定价选项这里，目前那些交互式会话只是每会话一美元
但如果你想要创建一个工作并重复运行它
它会以每节点小时四八美分收费，用于处理成本
如果你想要计算这个，有一个方便的成本计算器
这就是数据酿造概览
如你所见 它的存在全部目的是为你提供一个非常容易的方式来预处理数据
在一个非常可扩展的和一个非常快的方式，而不需要写任何代码 这是一个相对新的功能，可能是使用 glue etl 的替代品
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/034_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p34 11. Handling PII in DataBrew Transformations.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


考试中提到的处理个人身份信息
在你的数据搅拌转换中
所以我们来稍微偏离一下进入那个世界
我们在课程早期讨论过一些这些技术
但只是为了加强它
并且它与数据搅拌的关系
处理个人身份信息的一种方式是替换
所以我可以只替换为某种随机数字
然后替换为不是个人身份的信息
我可以打乱
我可以把我的行中的所有个人身份信息打乱
这样A的人的信用卡号码可能与B的人的信用卡号码关联
所以可能不是好主意让原始的信用卡号码到处飘荡
确定性加密是加密个人身份信息的一种方式
以使得给定值总是产生相同的加密值
它是以这种方式确定性的
你也可以有概率加密
这可能会有多个结果
从一个加密的字段
你也可以解密一个被加密的东西
使用解密命令
处理个人身份信息的最好方式
可能就是不要有它
如果你想要消除它或者只是删除它
删除转换可以用来完全删除那个信息
你也可以遮蔽个人身份信息
所以很多时候
你会看到信用卡的最后四位 或者社会安全号的显示
其余部分被遮蔽或者以某种方式打乱
遮蔽是一种实现方式
最后有哈希
你在值上应用一种哈希函数
以一种加密的方式
哈希有点不同
因为多个值可以哈希到相同的结果
有时候这是可以的
你知道的 但这也提供了你做的事情的一些匿名性
一些处理个人身份信息的技术在数据搅拌转换中 这些是每个技术的具体转换名称
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/035_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p35 12. AWS Glue Workflows.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们花点时间来谈谈glue工作流
考试中会提到这一点很多
我认为这是因为它是一个编排工具
所以它是组织更大工作流的一种方式
这是数据工程师做的一大部分
但这是aws提供的许多编排工具之一
因此，了解不仅glue工作流能做什么，而且它们不能做什么也很重要
因为很可能你会被要求在glue工作流和aws step functions以及mwa等其他替代编排工具之间做出选择，以适应给定的用例
So it's important to know not only what glue workflows can do
But also what they can't do
Glue工作流实际上旨在在Glue中执行任务
它是用于设计多任务
多爬虫ETL过程的
但本质上，它只是在Glue中编排任务，没有其他
它是用于在Glue中进行更复杂工作流的工具，用于ETL
具体来说
它没有其他更大的功能
您可以从AWS创建这些工作流
Glue蓝图，从控制台图形化这里或通过API
所以，在这个文档中给出的示例
你可以看到他们定义了一个粘合剂工作流来去重数据
并行修复电话号码
一旦这些成功
一旦这些都完成
然后它继续进行更新模式
所以这是一个工作流，会在并行中进行几个转换
当它完成时 嗯
最后把这些信息汇总到一个更新的模式中
所以没有什么太复杂的
现在再次 我想要强调
这只是为了编排复杂的etl操作
使用glue
它不会超出这个范围
有一些 然而各种触发器可以启动你的glue工作流
这就是它如何与你更大环境中的其他服务集成的方式
它们基于我们所谓的触发器
在一个工作流中，触发器将启动一个作业或爬虫
触发器可以自动触发
当一个作业或爬虫完成时
您可以基于定时器来设置触发器
就像 cron 表达式一样
如果您想让某事每天运行
每小时运行 每月运行一次
每第三个 周二
无论它是什么
您可以设置一个固定的时间表，让您的glue工作流在
您也可以按需进行
当然，无论是通过控制台还是通过api
您也可以通过事件桥事件触发它
好的 所以记住
Glue工作流与事件桥事件集成
但没别的了
好的 它可以由单个事件或一批事件启动，这有点酷
例如
如果我有新对象进入s3
我可以立即启动工作流
基于单个对象
或者我可以设置批量条件，这样说
好的 我将等待直到我收到这个数量的新对象在这个时间窗口内
然后只有我才会启动我的新工作流，正确
那样我就可以批量处理了
说 我有足够的新数据
我应该现在去处理它
默认情况下，这是15分钟的批量窗口
但这里或那里并不重要
这就是glue工作流的精髓
这很简单，对吧
所以请记住，这不会让您在考试中出错
因为它并不难理解
只是协调 Glue
ETL作业 您可以在预定时间启动
按需 或通过事件 桥事件 就是这样
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/036_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p36 13. AWS Lake Formation.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们来谈谈aws湖形成
这是一个相对较新的功能
它于2018年宣布，并在2020年看到了很多改进
我们现在才开始看到它在考试中出现
湖形成建立在glue之上
它实际上是作为一个系统来使设置一个安全的数据湖变得更容易
他们的口号是
它使在几天内设置一个安全的数据湖变得容易
我想这相当快
它为你管理的事情是从外部来源加载您的数据并监控这些数据流
它还允许您连接到您现有的数据源
无论它们是否已经在s3中，还是在一些本地数据库中
它可以为您设置分区
它可以帮助您管理加密并管理与该加密相关的密钥
并且它可以让您从开始就设置与数据相关的转换
它建立在glue之上
所以几乎任何
glue etl或任何glue功能可以做到的都是您可用的
如果spark可以做到
那么湖形成也可以做到
它只是在使用glue在后台应用这些转换
包括将其转换为新格式
例如列格式
例如parquet或orc
这是可用的
清理您的数据
您知道 删除缺失的行
无论您想做什么，我们都谈论过的glue
您可以使用湖形成来做
它还可以帮助您管理数据湖中数据的访问控制
它可以帮助审计
实际上 它的目的是引导您通过构建您的数据湖的过程
以安全的方式
让我们从更概念化的角度来看谈它
基本上，您有一个大的s3数据湖
当您完成时，它就在那里
因此，湖形成将位于
该s3数据湖旁边
帮助您创建它 帮助您在未来维护它
因此，它可以作为一个数据源
从s3中的任何内容，从您可能拥有的任何关系数据库
甚至nosql数据源
这些也可以在本地
不一定必须在aws中
湖形成的目的是帮助您设置该数据
湖，该数据可能来自其他地方在湖形成中
您可以设置所有数据爬虫和etl
工作和数据目录、安全和访问控制
任何数据清理或转换
包括转换为parquet或orc
基本上，Glue能做的任何事情
您可以将其设置为湖形成部分
一旦您设置好了湖形成
它可以轻松地与Athena或Redshift集成，或Redshift Spectrum
因此，Athena和Redshift可以自动查询该数据
如果您将其全部注册在湖形成中
2020年一个相对新的功能是EMR也可以直接与湖形成交谈
定价湖的形成本身不花费任何费用
但底层服务会花费
当然 这包括粘合剂
S3 EMR Athena和红移
正常收费将适用于您使用它构建的任何内容
构建数据湖的过程
使用湖形成
这比你想的要复杂一些
它并不能完全为你做所有事
这就是为什么他们说这需要几天时间
而不仅仅是几分钟
基本上，你需要先为数据分析师角色创建一个iam用户
你知道，这将使用这个数据湖并构建它的人
然后你需要为输入数据源或源创建一个glue连接
为湖本身创建一个s three桶
然后你做完这些之后
你可以在湖形成中注册那个s three路径并为其授予权限
所以这就是湖泊形成开始有用的地方
然后你在湖泊形成中创建一个数据库
用于底层数据目录
并授予它必要的权限
到那时，你可以开始使用蓝图来构建工作流
例如 创建数据库快照
无论你想做什么来转换数据并将其放入数据湖中
然后你运行工作流并授予
选择权限给需要访问结果数据湖的人
雅典娜或红移光谱
或者现在是emr
这就是概念上的东西
这是你需要知道的考试中的大部分内容
嗯 他们有时会喜欢在这些小故障排除事情上深入挖掘
以及可能出错的场景
aws数据湖形成中的一个细节问题是跨账户权限问题
所以如果你需要让不同账户的人
访问您的数据湖和访问湖形成
你需要确保接收方被设置为数据湖管理员
还要注意aws资源访问管理器
A m可以是一个有用的工具，用于管理外部组织账户以及它们可以访问的账户资源
此外，IAM权限对于跨账户访问也很相关
记住，湖形成
不支持在athena或redshift查询中使用manifest
这可能会导致错误
如果你在查询中使用manifest
一件事要记住
也要记住
IAM权限对于kms
加密密钥是必要的，用于加密的数据目录在湖形成中
IAM权限是创建蓝图和工作流在湖形成中必要的
这些都是潜在的故障排除事项，你可能想要知道
你知道
如果你遇到创建蓝图或工作流的错误 这可能是IAM问题
如果你有跨账户权限问题
你可能需要处理ram
一些新特性值得讨论与湖形成
是治理表
以及一些新的安全特性
治理表是新的，支持跨多张表的acid事务
现在我们可以安全地并发地进行行级访问、更新和删除
而不用担心多个用户互相干扰
acid意味着我们有严格的事务保证
这使用了一种新的s3表底层
所以仍然在s3上
一旦你选择了治理或未治理
你不能再改变主意
但它确实给你数据湖提供了acid支持
这有点酷
对吧
我的意思是，使用关系型数据库管理系统而不是数据湖，整个目的 就是acid支持 现在你不再需要作出这种权衡
它也适用于流数据
它不一定是流数据
但你可以从kinesis管道数据到aws湖形成到一个治理表
并且实际上使用athena查询流数据
并且自动获得acid支持
通过湖形成的新治理表功能
非常酷
甚至可以自动使用athena查询
并通过湖形成的新治理表功能，自动获得acid支持
现在acid支持在数据湖上的一个缺点
是随着时间的推移，支持这需要一定的开销
我不会深入细节
但这很重要，定期压缩和压缩数据
以减少开销
为了避免访问带来的额外负担
存储随着时间的推移不断增加，aws湖形成与治理表具有自动合并功能
这将自动优化您存储随着时间的推移
因此您不必担心
尽管随着您继续前进，存在一种增加的额外负担
它将自动通过自动合并为您处理
在aws湖形成中，新的粒度访问控制与行和单元格级别安全
因此，您无论是治理表还是老式的s3表
您现在可以控制行和单元格级别的访问特定用户
这是某些应用程序的重要功能，目前支持
现在 以上我们所讨论的所有功能都会产生额外费用
但它们仅基于使用情况
所以你只会为你使用的付费
按需付费，听起来很公平
但这是aws lake formation的最新和最新技术
使用受管理的表进行资产支持，这些表会自动压缩和优化
现在我们还拥有行和单元格级别的安全控制
我想花更多的时间来谈谈数据权限和湖形成
因为据说这在考试中会出现几个地方
那么我们来深入探讨一下
湖的形成确实使得在s3中对底层数据设置权限变得更加容易
这通常是一种强制实施非常精细的权限控制的方式
我们在之前的幻灯片中讨论了新的精细化访问控制功能
即使没有那个
我们也有能力将权限限制到列级别
只需使用右侧你看到的这个UI
它非常容易使用 仅仅通过研究这个UI屏幕
你就能理解很多能力，从上到下
你可以看到，你可以将权限绑定到iam用户或角色
例如用户和组
或者外部aws账户
所有这些都是可能的
你可以构建一个不同原则的列表
你可以将特定权限绑定到它们
允许你使用策略标签
如果你想要，这可以绑定到数据库
表或列
你可以基于这些标签来设置安全
总是一个好做法
或者你可以明确设置权限
基于表格或表格中的特定列
你可以看到有一个非常方便的 you i 在那里，用于说
你想让他们做
选择、插入或删除
或者任何你可以想象到的事情
对于特定表格
甚至对于表格中的特定列
所以如果你看到关于在你的数据中实现基于列的安全性问题 湖泊的形成可能是实现这一目标的方法
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/037_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p37 14. AWS Lake Formation Data Filters.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们讨论湖泊形成中的数据过滤器
这是另一种安全限制的方式
我们可以在列、行或单元格级别进行安全限制
使用数据过滤器
当你在控制台设置湖泊形成时
这就是它看起来的样子
在这里你会看到创建一个数据过滤器的屏幕
你可以给数据过滤器起一个名字
告诉你应用这个过滤器的数据库和表
然后关键的部分在这里
在列级别访问和行过滤表达式这里
通过选择不同的东西
你可以实现列、行甚至单元格级别的安全
这是在我们授予表选择权限时应用的
那是我们得到这个选项的时候
假设我想有行级别的安全
我会通过选择列级别下的所有列来实现
但会实施一个行过滤表达式来实现行级别的安全
如果我想要列级别的安全
我会留下行过滤为空，所有行
但我可以说包含列
或排除列来在列级别访问下提供列级别的安全
对于单元格级别的安全
我可以做两者 我可以说包含特定列或排除特定列
也可以应用一个行过滤表达式
如果我选择了允许的列和行
现在我就有了单元格级别的安全
你可以通过控制台做这件事
就像我们在右边看到的那样
或者你也可以通过API做
通过CLI
这是创建数据单元格过滤器
是API的名字 调用这个
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/038_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p38 15. Amazon Athena.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们深入探索亚马逊Athena
这是一个无服务器交互式查询引擎，用于S3数据
非常酷的东西 你可以将潜在的庞大的数据湖放在S3上
只需在顶部添加Athena并进行查询
就像它是SQL数据库一样
让我们深入了解它都是关于什么的，Athena是什么
技术上再次描述
S3的交互式查询服务
再次 这基本上是您在S3中数据的一个SQL接口
只需不需要将数据加载到某些中间数据库中
数据只是停留在s三中
雅典娜知道如何解释并查询它
就好像它实际上是在底层使用presto的数据库一样
但你真的不需要知道那个
这就是他们开始构建雅典娜的基础
从那时起，它已经进化了很多。
尽管如此，它的根可能已经迅速生长
但现在它更多是自己的事情
雅典娜的大事
虽然使用set它是完全无服务器的
所以你不需要考虑底层服务器或容量或类似的任何事情
你只需访问雅典娜
指向你s3中的数据并开始编写查询
嗯 你可以做更多的事情
但你可以做那个
那就是全部 你只需访问UI
嗯 打开雅典娜的控制台，然后开始操作
你不需要考虑设置所有底层的容量
一切都被抽象化了
它支持多种不同的数据格式
你在S3中的数据可以是任何东西
你可以想象 无论是逗号分隔值还是制表符分隔值
这些都是合法的
所以任何可读的数据
你可以把它放在雅典娜上面
它可以理解它并查询它
JSON格式也支持ORC
列格式也支持
ORC和PARQUET的好地方是他们都是分割格式
这可以提高性能和以有意义的方式分段您的数据，以满足查询
为您的查询
Avro也分割
至于压缩
您可以使用Snappy存储您的数据
Zlib Lseo 或 gzip 压缩
现在，这里有一些要点
你应该记住这些要点以备考试
记住，parquet 和 orin 是列式和可分割的格式
Avro 仅支持分割
CSV、TSV 和 JSON 都是人可读的格式
好的 因此，CSV、TSV、JSON
你可以在文本编辑器中打开该文件并理解其内容
兽人木地板avro并不是那么多avro会被分割
所以你可以并行处理avro数据
因为athena可以分割那部分并在不同的块中并行处理
对了，orc和parquet
此外是列式格式
这可以加速只针对特定列的查询
并且几乎任何压缩方式都可以支持
所以为了考试的目的
这些都是你想要记住的东西
数据本身可以是无结构的
半结构化或结构化
雅典娜不在乎
所以无论你有多少的模式
它可以利用它
或者它可以推断一个新的模式
只是通过检查数据或接受你所提供的
亚马逊提供的使用示例包括对网络日志的即席查询
所以 雅典娜被更好地用作处理查询网络日志数据的方法
S三 而且他们更希望你在elasticsearch中使用这个
例如
你也可以用它来查询在加载到redshift之前的staging数据
也许你有一堆数据被导入到s3中
你想要对它进行转换并加载到一个大的红shift数据仓库中
但你可能想能够玩转这些数据并调查它
并在此之前看看它长什么样
Athena可能是一种获取更大图景的好方法
在你实际上将其提交到数据仓库之前
这在查看其他日志方面也很有用
除了存储在三个网络日志中
包括云轨迹日志
云前日志
Vpc日志 弹性负载均衡器日志
无论你有什么 如果它在三个存储中
Athena可以查询它
它还提供了与Jupyter和Zeppelin等工具的集成，以及我们的工作室笔记本
因为你可以简单地将其视为一个数据库
它具有ODBC和JDBC接口
所以你可以将athena像任何其他关系型数据库一样对待，你可能与之交互。
或者集成 这也包括快速站点
你也可以将亚马逊的快速站点可视化工具集成到雅典娜中
实际上将其用作某种粘合剂
嗯，胶水是一个不恰当的词汇选择
因为我们实际上正在使用这个胶水服务
但连接你非结构化数据的是
S三个到一个更结构化的可视化或分析工具 例如QuickSight
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/039_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p39 16. Athena and Glue, Costs, and Security.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以谈到胶水
让我们谈谈athena如何与aws集成
胶水实际上为s3中的无结构数据提供结构
以便实际上可以像数据库一样查询它
再次 让我们对胶水如何工作做一个简短的回顾
你可能有一个胶水爬虫
为你的s3数据填充胶水数据目录
这正在查看存储在s3中的内容
并尝试从中提取列和表定义供你使用
你可以使用胶水控制台根据需要精炼该定义
一旦你有了为s3数据发布的胶水数据目录
athena会自动看到它并且也可以自动构建表
所以任何时候athena看到你账户中的胶水数据目录中的任何内容
它会为你创建一个表
所以你可以像查询任何其他sql数据库一样查询它
不仅仅是athena可以使用那个胶水数据目录
或者 它将允许任何其他分析工具可视化或分析该数据
例如 rds
红移光谱emr
任何与apache hive元数据存储兼容的应用程序
因为记住胶水数据目录也可以用作hive元数据存储
但在这个例子中我们使用它来暴露给amazon athena那个表定义
并且与aws s的athena集成
允许您创建各种服务中统一的元数据仓库
爬取数据以发现模式
向您的目录添加新的和修改的表和分区定义
并在幕后维护模式版本
而athena只是坐在上面
并提供一个sql接口到那个底层的胶水结构
athena的另一个特性是工作组
您确实需要了解这些
基本上它就是听起来的样子
您可以组织用户和团队
甚至应用程序和工作负载到不同的工作组中并分组它们
您可以使用这个来控制给定工作组的查询访问权限
或者按给定工作组跟踪成本
也许 您有一个特定的团队只能访问特定的查询类型
或者他们可以运行的查询的限制
您想按个人为组织跟踪他们的成本
以了解他们为组织产生的账单
工作组与iam cloudwatch和s集成
因此您可以做诸如当限制达到通知您的事情
您使用它的方式是
您可以为每个工作组设置自己的数据限制
因此您可以限制工作组查询可以扫描的数据量
所以 如果你想确保某个团队中的人
不会构建一些效率极低的查询，导致结果集非常大
你可以在工作组级别限制这一点
每个工作组都有自己的查询历史
所以你知道 如果你想确保某个特定团队看不到其他团队创建的查询
你可以将他们分成工作组
以确保每个工作组只能看到自己的查询历史
工作组
当然 也可以有自己的IAM策略
因为这是你管理他们权限的方式
以及他们能看到什么
你也可以在每个工作组设置自己的加密设置
所以这是有效地组织组织中很多人的一种方式
并限制他们可以做的事情
并通过使用Athena中的工作组给他们特定的访问权限
所以记住Athena
工作组是你可以使用的东西
你可以用它来限制特定群体的访问权限
嗯 或者测量他们的成本
并设置他们对查询可以扫描的数据量的限制
成本模型非常简单
因为这是无服务器应用
你只需为你实际消费的活动支付费用
目前每TB扫描收费5美元
这相当慷慨
重要的是记住，成功或取消的查询都会计入扫描量
你会为这些付费
但任何失败的查询都不会收费
所以成功或取消的查询
你会为失败的查询付费
然而，失败的查询是免费的 此外，对于DDL操作，如创建、修改或删除，没有成本
这些也是免费的 一个非常重要的观点是，如果你想使用Athena节省成本
它可以通过将你的数据转换为列式格式来节省大量成本
就像我们讨论的orc和parquet一样
所以
这不仅能给你带来更好的性能，通常在查询时只访问少量列的应用 还能节省30%到90%的费用
这是因为它允许Athena在查询和处理数据时仅选择性地读取所需的列
如果你在查询中只访问数据的某些列，通过使用列式格式
你将减少实际需要扫描的数据量
从而节省成本
记住，你是按TB扫描量收费的
如果你只访问查询中的特定列，你将减少扫描的数据量
记住，你是按TB扫描量收费的
通过减少扫描，你赢了
所以，记住，雅典娜与列格式配合最好
这可以节省你很多钱，并提供更好的性能
列格式的例子包括ORC和Parquet
除了雅典娜之外
当然 Glue和S3也有各自的费用
雅典娜只是坐在Glue之上，以获取您S3数据的表定义
您的数据仍然存储在S3中
因此，Glue和S3各有各自的费用
除了使用athena之外
此外 我还应该指出，分区您的数据也可以帮助athena减少成本
因此，如果您的数据在s3中以不同的分区结构化
例如按日期或小时等
限制于给定分区的查询也会扫描更少的数据
除了使用列式格式之外
使用s3分区您的数据也可以通过athena为您节省金钱
让我们谈谈安全性
这在athena中总是很重要
它提供了多种保护你雅典娜流量的方式
其中之一是通过访问控制
谁实际上能够接触到雅典娜并查询你的信息
你可以使用iam访问控制列表和s3存储桶策略来限制访问
以限制对信息的访问
iam策略
嗯 在这里相关的是亚马逊雅典娜全权访问和aws
快速侧雅典娜访问策略
你也可以加密你的结果，如果你对你的查询结果敏感
你可以在s3的暂存目录中静态加密它们
并且有多种加密方式
你可以再次
这些很重要，伙计们
你可以加密你的s3数据，结果是服务器端加密
使用s3管理密钥
被称为s sc s3
你也可以对结果进行服务器端加密
使用kms密钥被称为ssc kms
或者你也可以在客户端使用kms密钥加密
这叫做csse kms
根据你的安全要求
这些中的一种或多种可能有意义
想想数据是如何流动的
它存储在哪里 以及你想要如何保护它
你也可以在s three桶策略中定义跨账户访问
因此，你可以访问athena到一个s three桶
这个桶不属于你的账户
如果另一个s three桶有一个策略授予了你账户的访问权限
这是可能的，一个账户中的雅典娜控制台可以访问另一个账户中的数据湖
您可以设置它，至少在传输安全方面
您可以使用TLS
所有在雅典娜和S3之间传输的流量
也可以设置为雅典娜的传输层安全
这可以用于雅典娜的传输安全设置，总是重要的是记住安全方面
伙计们 所以这是大考试的一个巨大部分
最后，让我们再次谈谈反模式
这些直接来自AWS
大数据 他们不想让你用的白皮书
雅典娜适合高度格式化的报告
或者最终可视化
雅典娜只是一个SQL查询引擎
如果你想要做 嗯
漂亮格式化的东西或者用图表和图形可视化东西
那么Quick Side就是用来干这个的
我们很快就会讲到
如果你想进行ETL提取操作
转换和加载操作
使用Athena 通常这不是最好的工具使用Glue是一个替代方案
在那里使用Glue ETL
你也可以使用Apache Spark这样做
或者更大规模的任务
这就是Athena的概述
让我们 去玩它 我们该走了
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/040_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p40 17. Athena Performance.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


关于优化athena性能的一些快速笔记
考试特别喜欢关注这类主题
基本上，我们可以讨论的三个方面
一是通常情况下，使用列式数据格式可以获得更好的性能
比如orc或parquet
嗯 如果您能够预处理数据，使athena能够更好地工作
某种方式将数据转换为列式格式
您可以使用glue或ETL转换来实现
您知道 有许多工具可以用来转换你的数据
将其转换为更适合athena使用的格式
如果你担心性能
确保你使用的是列式数据格式
athena在orc和parquet中使用列式数据格式
如果它访问s3数据
如果数据格式已经是这样
这将节省你很多工作
也要记住
少量大文件通常比大量小文件性能更好
使用雅典娜 所以，如果你能拥有几个大型的Gish列状数据文件
这可能是你最佳性能案例
同时也利用分区
所以，如果你能够组织你的数据在三中以分区格式
这与你查询数据的方式一致
所以 例如 如果你最常访问的数据是特定的日期范围或类似的
那么按日期分区你的数据就很有意义
如果你需要在事后添加分区
假设你有一个存储在三中的未分区数据集
你想通过添加分区来改善其性能
你不必从头开始
在athena中，你可以这样做
使用一个名为msc的命令
K修复表以回溯并添加分区的元数据到athena中
因此，如果你需要在现有的athena数据库中后期添加分区
你可以使用mk修复表来完成
这些是athena性能的主要要点
记住这三件事 你应该处于良好的状态
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/041_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p41 18. Athena ACID Transactions.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


雅典娜中的新特性是酸事务支持
这意味着我们对交易有严格的保证
因此你可以实际允许并发用户同时访问同一行数据
让它始终一致地工作
引擎之下由Apache Iceberg驱动
你需要做的就是添加 table type equals iceberg
当你在athena中创建一个新表时
使用创建表命令
正如我所说，这可以让你安全地让多个用户同时进行角色级别的修改或删除。
无论它是什么，都不要担心它们
互相踩踏
这实际上与弹性MapReduce兼容
Apache Spark和任何支持Apache Iceberg表格式的东西
好在它消除了自定义记录锁定的需要
如果没有资产支持
你需要有一种锁定记录和解锁它的解决方案
以围绕对它的任何修改
这样如果两个人同时访问同一记录并尝试写入它
他们不会互相争斗
Iceberg和酸交易会自动这样做
现在也有一个小小的附加好处就是它会给你提供所谓的时间旅行操作
所以你实际上有能力恢复最近删除的数据
只需使用普通的选择语句
所以为了酸交易目的保留的一些历史数据可以被使用
用于那个目的
请记住当我们谈论湖形成中的管理表时
嗯，这也是做同一件事情的另一种方式
所以管理表在s3表上给了我们酸支持，而在湖形成中
但这是另一种在athena中获得酸特性的方式
所以你可以在湖形成中设置一个管理表
然后点击雅典娜的桌子
你将获得资产支持
这只是另一种做法
所以你可以直接从雅典娜创建一个表格
直接使用表格类型iceberg
你也会得到资产支持
所以这是有两种解决同一问题的方法
你可能也会记得在湖形成中，管理表
有一个功能可以自动压缩这些表格随时间
因为随着时间的推移，会有这种开销累积
随着时间的推移，这可能会降低你的性能
现在，使用雅典娜酸
至少目前，你需要自己定期这样做
所以，如果你在雅典娜中使用酸事务
你可能想这样做
偶尔 优化表
使用bin pack重写数据
where catalog equals whatever
像这样的命令会回滚并压缩你的数据
这就是随着时间的推移，酸性支持逐渐膨胀的情况
使它再次变得高性能
这就是我可能在考试中看到的那种东西
所以那个人 有点琐碎的事情
你该怎么做
嗯 答案就是定期压缩它们
使用这里你看到的bin pack命令
这就是athena acid的精髓
2022年一个令人兴奋的新特性 随着越来越多的aws数据分析世界转向酸性支持
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/042_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p42 19. Athena Fine-Grained Access to AWS Glue Data Catalog.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们来谈谈雅典娜对aws glue数据目录的细粒度访问
这在传统意义上并不是细粒度的
想法是我们基于iam的数据库和表级安全，适用于特定的操作
这与数据过滤和湖形成相比更广泛
我们不在这里应用单元格级安全或行级安全或列级安全
这更多地是关于我们在glue数据目录中的数据库和表上可以执行的操作
你也不能限制这到特定的表版本
这适用于一切
至少 您需要具有雅典娜的iam策略
这提供了对数据库和每个地区的胶水数据目录的访问权限
但我们可以做得更多
我们也可以锁定东西
所以你可能有设置在
我可能有设置限制对修改或创建数据库操作的访问
用于创建新表
用于删除数据库
或删除表
用于修复表
甚至查看所有数据库
正在显示所有表格
这些是你可以锁定的东西
使用iam和athena细粒度访问aws glue目录
你所要做的就是找出这些操作的映射到它们的底层iam操作
这不总是直截了当的
这里是一个控制访问drop table的示例
我们说我们将允许删除表格所需的操作
到一个特定的目录
到一个特定的数据库
并且也到这个数据库中的特定表格
而且事实证明，要删除一张表
你需要访问获取分区和分区
表中的表
你需要在数据库中获取表
它在 你需要删除表的权限以及删除分区
它可能在
所以你不得不查看一下那些操作是什么，给定操作是文档记录的
然而那里有一个小链接
如果你需要参考它
无法想象他们会期望你记住那些东西
为了考试 然而，你可以根据常识来推测不同的数据库操作可能对应的操作
你可能想使用athena精细访问来锁定aws glue 数据 目录
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/043_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p43 20. Apache Spark.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以我们谈了很多关于Apache Spark的事情
在Glue ETL转换的背景下，它是间接提到的
它使用Prock作为核心
在EMR的更大背景下
它可以在你的EMR集群上运行一个Spark应用程序
所以让我们更深入地了解它是如何工作的
Apache Spark是一个开源的分布式处理框架，常用于大数据工作负载
正如你在这个图表中看到的
它与MapReduce并排
它是这个堆栈分析部分的一种替代品
所以多亏了yarn
我们可以将mapreduce替换为更好的东西
mapreduce在十年前很好
但现在spark在很大程度上超过了它
这是因为spark有内存缓存
它大部分操作在内存中进行，而不是访问硬盘或跨越网络
它还有一个查询执行优化器
所以我们可以查看复杂的sql查询并优化它
以确保您有这真的是最优的，无环
有向无环图，以最有效的方式运行您的查询
所以它不限于地图和减少的范式
它可以更高效地执行更复杂的操作
因此，它比mapreduce快得多，实际上可以快几个数量级。
因此 你真的看到火花被用在这些日子里
并不是那么多MapReduce和Spark可以用Java和Scala编写
Python 或 r
我会说Python和Scala是那里更受欢迎的选择
Spark本身是用Scala编写的，顺便说一下
Spark的一个优点是你可以将代码在不同应用之间重用
因此，因为它的软件架构
所有这些不同模块都有一个共同的接口
所以，很容易将同一段代码
用于不同类型的应用程序
这可以节省大量工作
你可以用它进行批量处理
这基本上是它开始的地方
使用Spark也可以进行交互式查询
它提供了一个JDBC接口，如果你需要的话
所以你可以连接到它，并通过Spark SQL直接查询它
它有实时分析能力
通过ml库实现机器学习能力
它也可以进行图处理 通过图形
通过图处理 我们不是指图表
我们是指信息图
你知道
就像社交网络
这种图表
它还有spark streaming和structured streaming
这允许我们在实时处理数据
当数据流接收到
并且它可以与kinesis或kafka集成在emr上
现在，spark不是为oltp设计的
它不是为了每秒处理数千个事务而构建的
从某些你知道的前方
你知道 外部面向的网站
或者类似这样 这是一个OLAP的事情
它用于运行可能需要几秒或几分钟的长寿命查询
或者甚至更长时间才能完成
所以它真的是用于分析
用于OLAP应用程序
而不是OLTP
记住它是如何工作的
这里是Spark架构看起来像什么
Spark应用程序被分解为在集群上运行的单个过程
所以，这一切的核心是一个名为spark context的火花
称为你的主程序
这个主程序通过你的分布式资源管理器协调所有其他进程
这可能是sparkzone
内置的分布式资源管理器
在emr或hadoop集群yarn的情况下
另一个资源谈判者
内置于hadoop的分布式资源管理器
但你也可以运行一个独立的spark集群
这不依赖于它
然后我们有执行者，他们负责所有的繁琐工作
所以执行者运行所有数据计算并保存与您的工作相关的数据
记住，EMR可能在具有HDFS能力的核心节点上运行，也可能在没有此能力的任务节点上运行
Spark上下文负责将应用程序代码发送到这些执行者
并将单个任务发送到这些执行者以完成
Spark由各种组件组成
所以，所有一切的基础是Spark Core
这是平台的基础
它负责内存管理
故障恢复 调度
分配和监控任务
与存储系统交互
这可能是hdfs或erfs或其他东西
它还支持与java scala进行交流的所有核心功能
python 或我们底层的脚本
在这个低级别 它使用称为弹性分布式数据集或rdd的东西
但是考试可能不需要知道这一点
在sparkor的顶部有四个主要系统
一个是spark sequel
它已经演变成spark的一个非常基础的部分
Spark sequel是一个分布式查询引擎，提供低延迟交互式查询
比map reduce快100倍
并且可以支持各种数据源和格式，如jdbc
odbc json
hdfs 兽人
壁板 如果你要查询hive表，可以使用hive ql
但是 重要的是spark sql引入了一个叫做dataset对象的东西
在python的世界里，这叫做数据框
基本上，这使得你可以将spark中的数据视为一个巨大的数据库表
并像对待一个表一样查询它
这就是spark sql的真正力量
它为我们提供了一种类似于SQL风格的接口，以便于我们访问EMR集群中的基础数据。
我们也有spark streaming
Spark streaming 是一个实时解决方案
利用 sparkcore
快速调度能力用于进行流式分析
因此，过去数据是以小批次进行摄取的
但现在有了结构化流
我们可以实际进行实时流处理和实时数据处理，使用数据集
再次 我们真酷，因为我们可以用我们为批处理编写的代码
并且很容易使用spark架构将其适应于流处理
这提高了你的生产力
因为你可以很容易地重用那个代码
并且它支持来自kafka、Flume或hdfs的数据源
以及零MQ用于支持Twitter
但那更多是一个附加功能
这些天他们基本上已经废弃了ml lib
一个非常重要的特性
那
这是大规模数据做机器学习的算法库
所以他们找到了一种方法
将各种流行的机器学习算法的分布式处理
基本上你所需要在实践中使用的所有算法都可以为你提供
这真是令人惊叹
嗯 回归
聚类分类
协同过滤
模式挖掘 它什么都能做到
它可以从任何数据中读取
任何 Spark 可以从 HDFS 读取的数据源
HBase或任何Hadoop都可以进行交流
所以 就像火花本身一样
你可以使用Scala编写你的机器学习库应用程序
Java Python 或者 是我们
但我们真的很酷，因为我们有一种方法
最终在大型数据集上执行复杂的机器学习算法
你不再受单一机器能力的限制
他们已经破解了
如何分配处理能力的密码
我们终于又有图形处理能力了
这是一个基于Spark构建的分布式图形处理框架
它提供了ETL
探索性分析和迭代图形计算
以便让你能够在大规模上交互式地构建和转换图形数据结构
它非常灵活
但说实话，它并不广泛使用
最近在Spark中，它已经逐渐被忽视
因此，你很可能不会看到任何关于图形的地方
它大部分已经被第三方解决方案所取代
让我们再多谈谈Spark Streaming和结构化Streaming
特别是 因为这在当今的Spark中是一个非常激动人心且广泛使用的部分
因此，Spark应用程序通常在其代码中使用我们称之为数据集的东西
数据集只是数据中的一张大型表格
你可以将你的数据集群视为一个大型数据库表
我们在数据分析领域有一种趋势
SQL已经成为所有这些系统的通用语言
现在几乎所有的东西都使用SQL进行交流
在结构化流式处理中，情况仍然如此
你知道 你可以基本上将你的数据视为一个无限制的表
新的行总是在从你的数据流中添加
就是这么简单
你用与查询批处理数据集相同的方式查询它
但在结构化流式处理中，它随着时间的推移不断增长
现在添加了一些额外的功能，允许您窗口那些查询
所以如果您想将其限制为说过去的一小时
或者无论它是什么
您可以这样做 您知道在这里这个例子中
这里 我们从s3的日志目录中读取流式json数据
我们在这里按它们的操作字段进行分组
我们在过去的一小时内建立了一个窗口并只是计数
在这个例子中 然后我们将结果输出通过jdbc接口
到某个mysql数据库
所以你可以大致理解结构化流式的强大之处
仅凭这两行代码
我们说
去监控流入这个s3桶的日志，格式为json
在过去一小时内累加它们
并将结果每小时通过jdbc写入外部mysql数据库
这多么酷啊，这是一个非常复杂的操作 仅凭两行代码
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/044_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p44 21. Athena, Glue, and S3 Data Lakes - Hands On.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的
让我们呃 创建一个小的假数据湖
如果我们会 我们将实际上重用用于亚马逊杂志评论的同一些数据
来自数据库部分
所以如果你还没有做过那个练习
你可能想回到数据库部分并做使用红移的练习
至少到我们设置我们的s三存储桶的地方
或者你可以直接去到太阳狗教育网站的课程材料网站
Slash aws dash data
Dash engineer 这里有一个链接到课程材料
简而言之，您只需创建一个具有自己独特名称的S3桶
该桶包含元数据和reviews文件夹
这些文件夹位于课程材料中的杂志评论数据下，为本次活动服务
我们将仅使用评论本身
这是一个JSON文件，包含来自亚马逊的杂志订阅评论
com 回到之前的练习
我们研究了那个结构，里面是什么，所以再次
我鼓励你们回去做那个红移的练习
如果你们还没有做，好的
所以如果你们已经在s3中拥有数据
我们在这个练习中将要做的是
使用glue从s3中的那个半结构化数据中提取一些结构
创建一个我们可以用来查询数据的模式
就好像它是亚马逊athena中的数据库一样
所以不同之处在于，当我们使用这在红移中时
我们将这个数据复制到红移中
这次我们只是就地查询它
而它在S3上作为一个数据湖
所以你可以想象一个世界，这里有不止一个JSON文件
也许我们有很多JSON文件，分门别类
也许我们有很多JSON文件，用于在不同时间制作的评论
所以你可以想象这是一个更大的评论信息的仓库，而不仅仅是一个单文件
所以，让我们去Glue，尝试从这半结构化的数据中提取一些结构
那么，让我们开始
你可以在这里搜索Glue
A w s glue
And let's go ahead and create a new database
This is basically a virtual database
It's going to look like a database to athena and other tools
But it's not copying data at all
It's just giving us a schema on top of our data
Lake in s three
So let's add a database
Let's give it a name Let's call it
杂志
你可以给它一个描述，如果你想要的话
但我很懒 所以我不打算打出来
创建数据库
现在我们可以做的是转到表
并为这些评论创建一个新表，使用爬虫
所以我们将设置一个Glue爬虫
以查看该JSON数据并尝试推断其模式
这个爬虫需要一个名字
让我们叫它 我不知道
杂志评论爬虫
再次，你可以给它一个描述
如果你想要 但我很懒
我们的数据没有映射到Glue表
这就是我们想要创建一个新表的点
所以我们将保持不在
并添加一个数据源来自s3
我们不需要一个高级的网络连接
它在这个账户中
让我们继续并浏览s3以获取此文件夹
所以小心不要选择文件本身
那将不会起作用
它看起来会起作用
但当你尝试查询它时，它不会起作用
所以那里有一个很微妙的边缘情况
所以我想做的是选择这里的评论文件夹
而不是评论文件夹内的实际文件
选择那个
确保其末尾有一个斜杠
并且一切都很好 即使一开始它是红色的
如果我只点击该文件夹之外
它将接受它 并且现在是一个有效的路径
如果我们有子文件夹
我们可以说如何处理它
在大多数数据湖环境中
数据可能已被正确分区
你可能有一个每个年份的文件夹
每个年份可能有每个日的子文件夹
每个日可能有每个小时的子文件夹
以此类推
在这种情况下我们只有一级
因此这不相关 但在一个真实的世界场景中，一个大型的数据湖中
那是需要考虑的事情
让我们继续添加s3数据源并点击下一步
我们需要给它一个IAM角色
所以我们在这里创建一个新的
这将使它非常容易
只需给它一个后缀
嗯 我不知道杂志
你想要什么
这将自动给它所需的权限
嗯 我们在这里没有使用湖形成
所以那里没什么可做的
我们不会对其做任何特殊处理
嗯 云观察日志的静止加密
所以我们将进入下一个目标数据库
让我们选择我们刚刚创建的杂志
我们不需要前缀
但如果你想有一个表名前缀以便更好地排序
你也可以这样做
如果你遇到这种情况
船员爬行者正在生成大量的表格
有时它会对分区感到困惑
这可能是一种调试的方式，设置一个上限
表格创建的上限
如果它不能正确推断分区结构
它会为每个分区创建不同的表格
而不是把所有内容放在一个表格下
这就是它的作用
我现在要请求按需运行
在现实世界中 你可能定期运行这个来自动获取新数据
或者自动获取方案的更改
顺便说一下，让我们在高级选项下谈谈这个问题
这里有一些关于方案更改时的操作选项
你可能记得，在数据工程基础知识部分
我们谈到了方案演化的概念
这就是关于这一点的
你告诉它如何处理
如果方案更改了 你可以说任何更改
继续上传表格定义以匹配它
或者只添加你看到的新列
不要实际删除旧的列
这可能会使现有查询更好地兼容
但我们将保持不变
因为我的方案不会改变
我只会在需要时运行它
因为我不想承担任何意外费用
如果我忘记在完成时删除这个爬虫
让我们点击下一步并查看所有设置并继续创建爬虫
我还没有完成 尽管
我需要运行它
运行爬虫
它正在尝试启动
那需要约一分钟才能完成
好的 我们现在处于运行状态
我可以等到它完成再回来
但我认为只需要再20秒左右
我只花了比预期稍长的时间
但大约一分钟后它完成了
所以现在处于完成状态
然而 有时它会撒谎
有时它并没有真正完成写入所有内容
所以让我们回到爬虫的选择这里
你可以看到它实际上还没有完成
它仍然处于停止状态
所以让我们等它显示准备好和最后一次运行成功
因为它实际上还没有完全写出那个模式
嗯 再几秒钟
它就会完成
好的 这实际上花了几分钟
你永远不知道你会得到什么
在aws上 不是吗 但它现在显示准备好了
所以让我们去看看它做了什么
让我们回到数据库并选择我们创建的杂志数据库
现在有一个评论表
耶 让我们去看看
让我们看看它提取了什么
这是它生成的模式
现在 这个模式是有效的
这是基于我找到的json数据
然而 像结构
嵌套结构和数组
这些在关系数据库的列中并不很好地映射
所以，在一个真实的etl管道中
如果我关心这些信息
我需要以某种方式转换它
并将它转换为适合sql查询的东西
如果我的目标是实际使用sql查询这些字段
以查询这些字段
然而 我在这里的任务根本不需要这些信息
我所要做的就是找到顶级评论员
这就是我们的目标
为了做到这一点 我不需要这个项目相关的图片或者与评论相关的任何图片
我也不需要评论风格的复杂结构
所以我会删除它们
让我们编辑模式
选择这两个结构数组数据类型，它们在athena中不会映射得很好
然后删除它们
如果我尝试以这种方式查询
会得到错误 让我们提前避免这个问题
现在我可以保存这个模式为一个新的表版本
嗨 我的模式已经进化了
这些看起来都是可在关系数据库中工作的有效数据类型
类似于SQL环境
所以让我们回到表格
查看这里
我需要刷新它
这是我的评论表，你可以在这里看到
有一个有用的链接，关于表格数据
这会带你去athena
我可以点击这个链接
或者从控制台去athena查询数据
但我很懒 所以让我们点击链接
这警告我离开glue进入athena
athena有自己的成本
但别担心 这不会花费太多
它很方便地给了我一个预生成的查询
只是为了采样那个数据的前10行，看起来
我们有数据，酷
哇，现在
如果你是第一次使用athena
你可能会得到一个关于输出目的地未设置的错误
没问题 只需去设置选项卡这里
从那里你可以管理查询结果位置
你只需告诉它结果放在哪个s3桶
点击管理
然后你可以导航到你拥有的任何s3桶，或者创建一个新的
如果你需要的话，然后点击保存
这就是你需要做的
然后你可以回到查询编辑器再次运行查询
棒极了 总的来说，评分是1到5星
我们有独特的评审员ID和评审员姓名
以及识别出的asin
他们实际评论的杂志，所以非常酷
让我们在这里做点什么
让我们执行一个查询
所以这里有一个小挑战，让你伸展你的sql肌肉
编写一个查询来识别顶级评审员
我会让你尝试那个
如果你想要自己解决，可以暂停
当你完成时再回来
你可以与我的解决方案进行比较
如果你想要自己解决，现在暂停
否则我将只是粘贴我的方法
好的 这里发生了什么
让我们首先运行这个
确保它确实起作用
检查一下
我们的顶级评审员是布莱恩·凯里，有五个五条评论
紧随其后的是戴尔·DK，有二六条评论
嘿 DK是不是一个出版商
他们最好不要评论自己的杂志
嗯
不管怎样 这就是查询看起来的样子
我们将所有内容按评审员ID和评审员姓名分组
现在我们这样做
因为你如果只按评审员姓名分组
有些评审员有相同的名字
你知道的 例如，简
例如 可能会有多个不同的评审员ID
所以知道
迈克或马克
很多人只使用自己的名字
但这并不是评审员的唯一标识符
但是评审员ID是，所以通过嵌套这些 按嵌套的评审员ID分组
然后按评审员姓名
我可以看到当这种情况发生
在我的结果中，没有这样的例子
如果我只按评审员姓名分组，而不是评审员ID
我将得到完全不同的结果，因为这个问题
如果我只按评审员ID分组，而不是姓名
那么我将没有姓名
我不知道这些人是谁 并且我不会知道这些人是谁
这就是为什么我们这样做
我们这里有嵌套分组
我们将按审查员ID和查看者名称分组
按评论数量排序
因为我对此感兴趣
谁是按降序排列的最活跃的评论员
对于我要显示的内容
我们将选择审查员ID
审查员名称和评论数量作为评论数量
这就是全部
雅典娜在行动
在S3数据湖中就地查询数据
使用我使用数据爬虫构建的方案
实际上在雅典娜中直接从S3加载数据是可能的
你可以说创建并直接从S3数据桶创建
这是一个小捷径
如果你只想使用雅典娜
但是通过在数据爬虫中拥有这个方案
这允许我使用其他工具查询数据
这可能是一个好事
这就是全部，雅典娜，S3，数据爬虫
都在行动 让我们清理我们的混乱
我们不需要清理雅典娜
但我们确实想删除那些数据爬虫资源
所以让我们回到数据爬虫控制台并删除那个杂志数据库
是的 我确定，让我们也删除那个爬虫
以防万一
操作删除爬虫
确保这成功了，是的
还剩下什么
IAM角色并不花费什么
我就留着吧 但是
让我们也删除那个S3桶
如果你已经完成
这不会花费你太多
但这会花费一些
那是我的mine.dacoo1.sun
狗 你的叫其他名字
让我们删除它
它必须为空
这不那么简单，首先
我们必须删除它里面的所有内容
删除
永久删除
也在元数据中
我们也会删除那个文件
使用第三方工具这会容易得多
现在我们应该能够删除整个桶
因为它现在是空的
删除
我需要输入桶的名称来确认，是的
我想清除那些数据
好的 我们已经成功地清理了我们的混乱
并且成功地展示了数据湖的使用，以及使用athena和glue进行查询 非常酷
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/045_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p45 22. Athena and CREATE TABLE AS SELECT (CTAS).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们来谈谈创建表为选择语句或简称CTAS任务
你可能会在亚马逊雅典娜的背景下看到这一点
尽管这在其他数据库中也是一个东西
语法可能略有不同
尽管与材料化视图和红移非常相似
它从查询结果创建新表
所以你可以做任何选择语句
你可以想象并创建一个新表从查询结果
也许你可以使用它来创建一个新表
那是另一个表的子集
所以，我可以说 我要去做一个选择自旧表
为那个表中的一部分
并基于查询结果创建一个新表
然而，它也是将数据转换为新底层格式的一个小技巧
这尤其有趣
记住雅典娜可以位于数据湖之上S3
所以你可以使用此方法将S3中的数据转换为完全不同的格式
这可能对雅典娜更有效
例如
让我们看看下面这两个例子
左边的那个我们说创建表
新表宽度格式等于parquet
右压缩等于snappy选择星号自旧表
这里发生了什么
我们将从旧表中选择一切
并将它重写为一个名为新表的表
但在途中我们将其更改为parquet
这是一种列格式
这将更好地优化雅典娜进行查询
并且使用snappy压缩进一步优化
所以这是一个简单的方法来
快速压缩或更改您在S3中的原始数据
并将其更改为与雅典娜工作得更有效的东西
右边的另一个例子这里
同样的精神这里
我们选择一切自旧表并将其放入一个新表
称为我orc c as table
在这种情况下 我们不仅在途中将格式更改为orc
我们还指定了一个新的外部位置
所以我说 好的
我不仅想要为此表更改格式
我将明确地将其存储在此S3路径中
在那个我雅典娜结果桶中正确
所以这有点有趣
因为我们不仅改变了雅典娜本身如何暴露数据
我们还改变了并存储了S3中的原始底层数据
所以这可能是一个快速而肮脏的数据转换方式
或者转换底层数据的实际格式
将原始数据转换为其他形式
只需使用athena
然后我可以去那个s三号桶并取回它
如果我想要的话创建表
选择语句是一个非常实用的技巧，用于将数据转换为更高效的形式
也许还能将其存储在某个地方以便以后获取 别忘了
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/046_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p46 23. Spark Integration with Kinesis and Redshift.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们来深入探讨一下Spark Streaming
所以Spark应用程序
正如我们所说，通常使用称为数据集的东西在代码中引用您的数据
在Spark Streaming和结构化Streaming中，数据集被处理得非常像数据库表
特别是 你可以基本上想象你的流数据像一个数据库表，它就永远增长
当流接收到新数据时
它只是不断向那个虚拟数据库表添加更多的行
以数据集的形式
您可以使用时间窗口查询此数据
如果我们看看这个例子
这个代码正在做的是
让我们监控正在发送到S3的日志
在S3中 这个操作以一个小时的时间窗口进行
这意味着我们将继续计算那个桶在过去一小时内接收到的文件数
然后我们将使用JDBC将这些计数写入到外部MySQL数据库
为此类事情编写的代码实际上非常简单
这就是Spark Streaming和结构化Streaming的力量
特别是
是的，您需要编写代码
但通常这是非常简单的代码 您可以依赖Spark
来管理所有确保您可靠地接收流数据 以及将该流数据处理分布在整个集群上
可靠地
您只需关注您想要执行的逻辑
而Spark Streaming处理所有不愉快的部分
确保以可靠方式进行
您可以将Spark Streaming与Amazon Kinesis集成 我的意思是，没有什么神奇的事情可以将Spark Streaming与任何其他系统集成
因为那是代码，您可以做任何事情
结果，有人写了一个库
基于Kinesis客户端库
允许Spark Streaming从Amazon Kinesis数据流导入数据
您只需插入那个库并编写代码，就可以将其作为任何其他数据流处理
进入Spark结构化Streaming的数据集
例如，您可能有一些Kinesis生产者
一堆EC two主机生成日志，将数据推送到Kinesis数据流
然后您可以使用Spark数据集集成代码将其集成为任何其他数据集
流入Spark Streaming
然后在整个EMR集群上使用Apache Spark处理它
所以 例如，您可能有一些Kinesis生产者
一些EC two主机生成日志 将数据推送到Kinesis数据流 您可以使用Spark数据集集成代码将其集成为任何其他数据集
流入Spark Streaming
然后在整个EMR集群上使用Apache Spark处理它
所以
你也可以将spark与红移集成
我们还没有讨论过红移
但基本上，它是aws提供的大规模分布式数据仓库
就像那里有一个spark kinesis包
也有一个spark红移包
这使得spark能够将红移的数据集视为任何其他SQL数据库
所以它只是将红移伪装成任何其他SQL数据源，以供apache spark使用
再次，您可以使用apache spark的强大功能来处理整个emr集群中的数据
以任何方式进行分布式和可扩展的处理 这是用apache spark在红移上进行etl的好方法
例如，想象一下
如果我们有大量航空公司航班数据存储在大型数据湖中
这些数据存储在亚马逊s3中
例如，我可以在s3中的数据上部署亚马逊红移光谱
这将为我提供对存储在s3中的数据的sql接口
使用spark红移包
我可以在apache spark上启动亚马逊emr集群
并使用该集群对存储在s3中的数据进行etl处理
通过红移
因为红移对apache spark来说就像任何其他SQL数据源
通过将红移与spark集成
我可以将存储在s3中的巨大数据集的处理分布
并将处理后的数据放回另一个红移表
以供进一步处理
也许我正在做一些准备数据以供将来使用的机器学习算法
该机器学习算法可以使用我从spark作业中生成的新红移表中的预处理和转换数据
该数据是从我spark作业中生成的
也许我正在做一些准备数据以供将来使用的机器学习算法
该机器学习算法可以使用我从spark作业中生成的新红移表中的预处理和转换数据
该数据是从我spark作业中生成的
也许我正在做一些准备数据以供将来使用的机器学习算法 该机器学习算法可以使用我从spark作业中生成的新红移表中的预处理和转换数据
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/047_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p47 24. Spark Integration with Athena.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


除了将Spark与Redshift集成，
您还可以将其与Amazon Athena集成
现在，这是2023年的新功能
因此，您可能不会期望在2022年考试中看到它
直到2024年和以后 但如果您正在观看此视频
那么请留意，即使它不在考试中
但这也是好事，因为这有助于您了解在实际工作中可以使用的工具
了解您所拥有的工具是一个好主意
这里的想法是，您可以运行Jupyter笔记本
在雅典娜控制台中已经启用了Spark
所以在顶部的图表中你可以概念性地看到这是如何工作的
在这里右边看起来像亚马逊雅典娜
但基本上你有一个在S3中的数据湖
这可能有一个AWS Glue
数据目录为S3中的数据提供了某种结构和模式
然后你可以使用Amazon Athena for Apache Spark
以交互式方式探索和准备你的数据
只需使用雅典娜控制台中构建的笔记本
然后你可以在笔记本中构建你的应用程序来分析和可视化你的数据
并与其他机器学习工具集成，以探索数据
或者将其转移到你管道的其他部分
是的 顺便说一句，那些笔记本可以自动加密
或者使用您的kms密钥，就像athena本身一样
这是一个完全无服务器的解决方案
因此，您不需要考虑运行spark或jupyter notebook的底层服务器
或者你的jupiter笔记本
嗯 好吧，我们会稍后再谈
我们需要考虑容量问题
仍然 基本上，这个工作的方式是当你在亚马逊雅典娜时
这通常是一个用于处理非结构化数据的SQL查询引擎
而不是选择雅典娜SQL作为你的分析引擎
你可以选择Apache Spark代替
他们还提供了一个示例笔记本选项，如图所示
如果你开启它 这样我们就可以
你将获得一些有用的示例代码，用于查询雅典娜中的数据
并且利用其进行后台操作
它使用称为firecracker的东西来快速启动底层spark资源
而不是坐在那里
等待为新的spark服务器进行配置
这基本上立即发生
因为它使用称为firecracker的东西来加速这个过程
还有api和命令行界面访问athena for spark
你可能知道 你可能使用像创建工作区这样的命令
创建笔记本
开始会话 或者开始计算执行，以便从API或命令行执行所有这些
它工作的方式是
您只需将笔记本的代码作为参数传递给说
创建笔记本
现在您可以调整dpus
雅典娜的数据处理单元（dpus）
对于协调器和执行人的底层Spark集群大小
尽管我们说它是无服务器
您仍然想要考虑您想要支付的dpu容量
所以您可以设置一些上限，用于您想要使用的容量
定价将基于您的总计算使用
以及每小时的数据处理单元（dpus）数量 当我们更深入地讨论雅典娜时，我们将详细讨论这一点
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/048_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p48 25. Amazon EMR.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


下一个
让我们谈谈emr或弹性地图减少
在aws的数据分析世界中，另一个非常重要的服务
什么是emr再次
它代表弹性地图减少
实际上它是一个在e上运行的管理Hadoop框架
C 两个实例
所以基本上亚马逊采用了hadoop，并将其与e集成
C two意味着你可以通过aws控制台启动一个运行hadoop的集群
这是一个管理框架
但你知道 你必须指定你想要在哪些服务器上运行它
嗯 你想要在你的集群中有多少个实例
你想要安装什么软件在上面
作为回报 你将能够登录到这些实例并进行低级别的操作
例如 你可以说你想在emr集群中安装apache spark
所以，而不是有一个完全不透明的系统
就像你用胶水etl做的那样，你看不到服务器
如果你想深入下去，实际上对apache spark有低级别的访问权限
并且想要能够对资源有明确的控制
那么，emr可能适合你
你也可以包括诸如h base或presto或flink或hive等应用程序
稍后我们会更深入地讨论这些
如果你想开发利用你集群上的代码
它提供了一种称为emr笔记本的东西，你可以开发和迭代你的脚本
在你的集群上 只是在浏览器中使用笔记本环境
他们已经集成了emr或集成了hadoop
而是在aws的几个关键点进行集成
我们也会谈得更多
那么构成一个emr集群的是什么
嗯 emr集群本质上只是运行hadoop的e和c实例的集合
每个实例被称为一个节点 现在每一个这些实例被称为一个节点
每个节点在集群中都有一个角色
我们称之为节点类型
因此，一个在emr集群中的实例可能会执行三种不同类型的节点
一种是主节点
你需要至少一个
主节点将通过运行软件组件来管理集群
以协调数据的分布和任务的分配
在其他节点上
处理任务是跟踪这些任务的状态并监控整个集群的健康状况
所以每个集群都有一个主节点
甚至可以创建一个只有一个主节点的单节点集群
因此，一个基本的最小单节点emr集群将只有一个主节点，负责所有任务
主节点也被称为领导者节点
有时我们称之为核心节点
核心节点运行软件组件，用于运行任务并在hadoop分布式文件系统中存储数据
在你的集群或emr上使用hdfs
可能是emrfs
它允许你将数据写入s3
但核心节点负责实际的工作，并将工作分布在整个集群中
因此，你的工作将被分配到多个核心节点
每个节点可能负责存储你数据的一部分，并处理这部分数据
因此，多节点集群将至少有一个核心节点
如果你需要分布处理，至少需要一个核心节点
现在，emr的第三种类型是任务节点
这是一个相对较新的概念
任务节点运行软件组件，只运行任务，但不在hdfs或emrfs中存储数据
它们是可选的
因此，如果你需要增加集群的处理能力，但不需要更多的存储能力
任务节点可能是一个好选择
如果你这么想
emr经常使用s3作为其存储底层
因此，你并不需要在emr集群中拥有更多的存储
因为最终数据都会流入s3
在emr中，任务节点非常有用
你不需要在emr集群中拥有大量存储
这可以节省你的钱
因为你不必为未使用的存储付费 移除任务节点不会造成数据丢失
因为它不能写入数据
这意味着它可以用于spot实例
这是一个你可以在考试中遇到的概念
spot实例对于任务节点非常有用
如果你希望在emr集群中降低成本或以成本高效的方式增加容量
spot实例是一个很好的选择
因为你并不关心它的可用性
它只是用于额外的处理能力
集群可以处理这种情况
如果你在处理作业时spot实例出现或消失，这不会造成数据丢失 因为任务节点不存储数据
在emr中，spot实例对于任务节点非常有用 如果你希望在emr集群中降低成本或以成本高效的方式增加容量
spot实例是一个很好的选择
因为你并不关心它的可用性
它只是用于额外的处理能力
集群可以处理这种情况
如果你在处理作业时spot实例出现或消失，这不会造成数据丢失
因为任务节点不存储数据
因此，spot实例对于任务节点非常有用
因为任务节点不存储数据
因此，spot实例对于任务节点非常有用
记住一个扩大你集群动态的方法是使用spot实例在任务节点上
使用emr的几种方式
所以，一个方法是使用瞬态集群
瞬态集群将在所有步骤完成后自动终止，这些步骤已被分配
当你设置瞬态集群时
你可以说 好的 我想让你启动这个emr集群，它具有这种硬件
它将运行我提前定义的这些处理步骤
然后，当它完成时，关闭自己
所以你可以提前定义
在emr中的操作步骤，用于加载你的数据
处理你的数据并保存结果
然后当它完成后，自动
关闭集群
这就是临时集群的全部内容
显然，这节省了金钱
因为你不需要为集群支付时间
超出你所需的部分
所以，如果你有这种类型的一次性工作，你需要偶尔运行一次
瞬时集群是实现这一目标的好方法
它会启动那些集群资源并运行你的工作
然后关闭，这是节省资金的好方法
然而 有时你想要建立一个长期存在的数据仓库
你不想总是启动并关闭这个东西并失去所有数据
对吧
所以如果你想建立一个数据仓库 并在某些大型数据集上进行周期性处理
这就是你想要的
你可以随时访问那个数据，或者持续在上面运行任务
一个长期运行的集群可能更合适
所以这种情况下，你会启动一个集群
你知道 用你事先指定的参数
并且让它永远运行下去
嗯 不是真的永远 但直到你手动关闭它
如果你需要后期增加容量
你可以随时启动更多的任务节点
使用spot实例来满足临时的容量需求，就像我们之前讨论的那样
如果你想要节省更多的钱
你可以在这些长期运行的集群上使用预留实例来节省更多的钱
如果你知道你将会运行这个集群很长时间
那么显然，预留实例是支持这个集群硬件的好选择
因此，默认情况下，终止保护将处于开启状态
在长期运行的集群上，自动终止将处于关闭状态
它将尽可能多地保护自己
所以这里有两个用例
临时集群，如果你只是想按需启动一个集群
运行你预先定义的东西
然后关闭它
或者一个长期运行的集群，你可以让它一直运行
并且随时访问它
并以更持久的方式保留它
你指定框架和应用程序时启动集群
所以我们讨论了 例如
运行Spark，这将是你选择的东西
在你启动集群之前
它将自动安装你想要的框架和应用程序
一旦集群被启动
如果是长期运行的而不是临时运行的
你可以直接连接到主节点
并从那个主节点运行你的工作
如果你是一个命令行类型的人
你将 基本上例如
设置一个Spark启用的emr集群
连接到主节点
启动你的Spark驱动脚本
它将开始做它的事情，使用整个集群的力量
这就是使用emr的一种方式
或者你可以通过aws控制台提交步骤到emr
这些步骤你可以严格地在控制台中图形化定义
你可以做基本的事情，例如在s3中处理数据或从hdfs
这是hadoop底层的文件系统
你可以将数据输出到s3或其他地方
但是一旦定义
你可以通过aws控制台启动那些步骤
而不是通过命令行提示符，而是通过直接连接到主节点
所以使用emr的两种方式，你可以登录到主节点并启动东西
你知道 老方法
或者你可以完全通过控制台定义步骤，并通过控制台启动那些步骤 在集群上运行
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/049_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p49 26. EMR, AWS integration, and Storage.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以emr不仅仅是在e上运行的hadoop
C two 他们实际上已经付出了很多努力
以在多个点将emr与aws生态系统集成
所以让我们谈谈这些，因为它们很重要要知道
所以它确实使用了亚马逊的e
C 在构成你emr集群节点的实例的内部
所以在内部你有e
C 两个实例正在运行集群中的这些节点
它们可以做e
C Two可以做
它还使用亚马逊VPCs来配置您正在启动的虚拟网络
您正在启动实例
启动您的实例
因此，您是在亚马逊VPC中为该集群启动实例
它与亚马逊S3集成，因此，通常在Hadoop中
您只能访问Hadoop分布式文件系统
但是他们已经通过emr扩展了这一点
因此你可以实际将你的数据输入和输出存储到亚马逊的s3中而不是
我们将讨论更多关于这是如何工作的
它也集成了亚马逊云监控
正如你集群性能的监控几乎一切一样
为了配置您可能需要为集群设置的任何警报
你可以使用aws iam来配置emr的权限
我们将很快深入探讨安全问题
云轨迹可以用于审计对emr服务的请求
And aws data pipeline integrates with the emr
So that you can schedule and start your clusters as part of a data pipeline
Remember you can set up these transient clusters that will automatically kick off steps
And with the data pipeline you can schedule when that happens as part of a larger process
So let's talk more about storage with emr
So there are several options available to you
One is hdfs
So remember this is fundamentally a hadoop cluster running on e
C Two 如果你想使用hdfs文件系统
你可以 这将使用每个实例的本地存储
以将存储分布到整个集群
hdfs的工作方式是它将每个文件块的多个副本存储在集群实例中
这样我们就有了冗余
以防某个节点崩溃
它可以从那里恢复
因为它总是确保你的数据被复制到多个地方
如果单个实例失败
没有丢失数据
它可以继续重新路由到另一个备份副本
并开始构建一个新的备份副本
所以每个hdfs文件都以块的形式存储
那些块就是分布在集群中的数据
不是文件 现在是构成每个文件的数据块
默认情况下 hdfs中块的大小是128兆字节
这可能很重要
然而 问题是，使用emr时，hdfs存储将是临时的
一旦你关闭你的集群
你将失去所有存储
这就是emr的
问题所在
这并不是说你会永远运行它
就像你会传统hadoop集群一样
尽管你可以
但你会被按分钟计费
你必须记住，一旦你终止那个集群
如果你有节省一些钱的想法，停止累积你的账单
这对于一个大型集群来说可以很快地累积起来
这些数据将会消失
而这是一个问题
所以，如果你将你所有输出写入hdfs并且关闭你的集群
因为某人想要节省几块钱
所有这些数据都消失了
你将会有一个非常糟糕的一天
没错 然而
你可以仍然用它来缓存
中间结果或具有显著随机
I/O HDFS的好地方是
Hadoop会尝试处理存储在HDFS上的数据
所以它会尝试分布处理工作
这样，一个块的处理发生在该块存储的节点上
这是HDFS实现的一个非常重要的优化
但这确实是一个致命的问题，一旦你终止你的集群
一切都没了 那你能做什么
这就是emr fs的作用
emr fs或emr文件系统的想法是访问它像hdfs一样
就像hdfs一样
使用emrfs可以像hdfs一样写出数据
你只需使用不同的前缀来写它
你知道 s3
这允许即使在你的集群终止后仍然有持久性存储
以前存在的问题之一是一致性问题
如果你有多个节点同时写入s3的同一位置会发生什么
同时 你知道这可能会有竞争问题
就像谁实际上得到
谁先走
以前有一个叫做erfs一致性视图的东西
现在，我不确定考试是否仍然需要了解这一点
所以您可能需要了解关于这一点的信息
有时考试在技术上落后几个月
所以我仍然在这里提到它
但这是S3一致性的一种选择
它使用DynamoDB来跟踪文件访问
确保S3数据存储的一致性
确保如果你有多个节点从你的EMR集群
同时访问S3上的同一文件，仍然能正常工作
这实际上会变得非常复杂
因为你可能有大量的I/O流量
你可能需要调整底层DynamoDB实例的读写能力
要让它正常工作 可靠地
有点麻烦
幸运的是，亚马逊意识到这是一个麻烦，并在2021年
S3本身现在自身也是高度一致的
所以现在S3保证一致性
我们不需要担心
Errfs的一致视图
所以它不再是一个问题
它神奇地工作
现在，现在
我们还在努力优化在同一台机器上处理数据的过程
数据存储在哪里 显然，S3将把这份数据存储在网络另一端的某个地方
理论上，这不如使用HDFS快
但在实践中，它仍然非常快
S3可以非常快速
你知道，这一切都在同一个数据中心
因此，在实践中，仍然可以构建大规模的高效应用
使用EMRFS，也就是使用S3
而不是内置在hadoop中的hdfs文件系统
这样你就可以使用s3作为一个数据持久存储来运行hadoop应用
这样当你关闭你的集群时
你的数据不会消失
其他选项是使用本地文件系统
显然这也将是临时的，也不会在任何地方备份
所以 尽管它会很快
但它并不多
它只适合临时数据
所以如果你想要有临时的缓冲区或缓存，没问题
你可以在本地写入它们
只需确保你不依赖于这一点
它不会持续很长时间
还有弹性块存储用于hdfs
你可以使用emr在m四或c四类型的ebs上，利用这个特性
但是再次 就像本地存储一样
你的弹性块存储在你集群被终止时也会被删除
所以这不会给你带来太多除了hdfs以外的东西
但是如果你没有本地文件系统
因为你使用的是ebs实例类型
它还可以使用ebs作为本地存储和hdfs的后端
ebs卷只能在启动集群时附加
记住 所以你无法在事后扩展存储容量
这也是另一个原因
也许避免它
顺便说一下
如果你在运行时手动断开ebs卷
emr会将其视为失败
它会自动替换它
所以它对这种失败模式是有弹性的
至少 但是再次
如果你想要持续存储，即使你的集群被终止也会持续存在 你基本上必须使用rfs与s3
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/050_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p50 27. EMR Promises; Intro to Hadoop.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


那么我们来谈谈emr向你承诺的事情
首先 让我们谈谈定价
emr按小时向您收费
您集群运行的时间越长
他们向您收取的费用就越多
如果您使用昂贵的实例类型，费用会迅速累积
例如GPU实例
以及类似的东西
请记住
你知道 如果您可以将任务作为自动启动和停止集群的一系列步骤运行
当任务完成后 那样您可以节省很多钱
如果您需要24/7运行集群
尽管可能会很昂贵
并且您需要关注失败的情况
如果核心节点失败
emr将自动为您创建一个新节点
至少您不需要担心这个问题
并且它将自动
嗯 在离开地方继续
如果您需要调整大小
如果您需要增加容量
尽管最好的做法是在飞行中添加或删除任务节点
记住，任务节点和核心节点类似
但它们没有自己存储容量
任务节点没有与HDFS相关的存储容量
通常您不会担心
因为您可能正在使用emr fs与S3进行通信
这样您的数据就更加持久
但是
即使您使用HDFS
您仍然可以使用任务节点来增加处理能力
如果您不需要同时增加存储容量
一个典型的例子是您
假设您正在运行一个电子商务网站
比如您在亚马逊工作
在圣诞节期间，您需要处理大量数据
假设您的数据存储没有问题 但是您需要更多的处理能力
处理这些数据的CPU时间更多
您可以暂时向emr集群添加大量任务节点
以处理这些处理需求
并在处理高峰期过后删除这些任务节点
因此，在飞行中添加或删除任务节点
是处理临时处理需求高峰的好方法
添加和删除任务节点是处理临时处理需求高峰的好方法
在飞行中添加或删除任务节点
记住，你可以在不遇到太多麻烦的情况下调整正在运行的集群的核心节点
这可以是增加你的处理能力和hdfs容量的一种方式
所以你知道，如果你需同时增加你的处理时间和存储容量
并且你没有使用像emrfs这样你的数据存储在其他地方的东西
不管怎样 你也可以调整核心节点
在线添加和删除核心节点也是可能的
这是新近添加的功能
但是如果你删除一个核心节点，可能不总是一个好主意
记住，你也在删除底层存储
所以，如果你在使用hdfs的容量
你冒着数据丢失的风险，通过移除一个核心节点
所以，即使你可以这样做
并不意味着你应该这样做
一般而言 如果你需要临时增加处理能力
增加任务节点是一个很好的方法
如果你确实需要在hdfs上增加存储容量
你可以扩大核心节点
你也可以在飞行中添加和删除角落节点
就像你可以与任务节点一样
但是数据丢失的风险稍微大一些
在核心节点上与任务节点上进行那个操作时
那么，让我们谈谈更多关于在emr中如何管理缩放的内容
所以，管理缩放实际上是在2020年引入的，在那之前他们称之为emr
自动缩放
那就是过去的做法
并且它创建了基于云观察指标的自定义缩放规则
所以你知道 在二零二零年之前，甚至二零二零年上半年
这就是自动扩展和emr的工作方式
你将自动监控云观察指标
你可以自动为emr集群添加或移除容量
但它仅限于实例组
因此，你不能真正混合匹配不同实例类型的自动扩展
在2020年，emr管理扩展改变了这一点
所以，现在 除了支持实例组
它还可以支持实例舰队
因此，它可以扩展您的spot
您在节省计划内的按需实例和常规实例在同一集群中
这在Spark中实际上可用
特别是Hive或Yarn工作负载在emr中
因此，当您在emr中扩展时
管理扩展 它的工作方式是先尝试添加核心节点
然后如果达到这个限制
它会添加任务节点
并且它将这样做，直到您指定的最大单位
因此，在管理扩展中提供的配置是
我想添加的最大核心节点数是多少
我想添加的最大任务节点数是多少
如果需要缩放
如果看到某种云指标表明您
嗯 您拥有过多的容量
它将开始通过移除任务节点
然后通过移除核心节点
不要低于您设置的最小约束
因此您可以说，我想添加的最小任务节点数不少于这个数量
我想添加的最小核心节点数不少于这个数量
您可以说，我想添加的最小任务节点数不少于这个数量
随着需要，管理缩放将尊重这一点
请注意，spot节点总是将在需求缩放之前移除
好的 这就是管理缩放
以及它如何工作 基本上
您指定最大单元数
并设置核心节点和任务节点的最小约束
核心节点和任务节点都受到管理
您可以将此应用于整个集群，而不仅仅是实例组
让我们开始谈论更多关于您将在emr集群上运行的应用程序
所以本质上它是一个hadoop集群
所以让我们谈谈hadoop
好吧，hadoop是什么
hadoop架构由多个模块组成
这些模块组成
我们称之为hadoop common或hadoop core
这是有点像hadoop本身的基础
您在这里看到的这三个组件
这些是库和实用程序，其他hadoop模块需要它们来构建
提供我们在集群上所需的所有文件系统和操作系统级抽象
以及启动hadoop所需的jar文件和脚本
所以在底部，我们有hadoop分布式文件系统，hdfs
这是一个分布式可扩展的文件系统，用于在集群中的实例上存储数据块的副本
hdfs
它在不同的实例上存储这些块的多个副本，以确保不会丢失数据
如果单个实例失败，现在，在emr中
hdfs中的数据将在终止集群时丢失
但是，它对于在mapreduce处理期间缓存中间结果或对于具有显著随机I/O的工作负载仍然有用
或者您打算永远运行emr集群
好吧
去吧
你可以将其用于持久化存储
但这不会便宜
它建立在hdfs的yarn之上，yarn代表另一个资源谈判者
这基本上是在hadoop中添加的抽象层
在mapreduce和hdfs之间，2.0版本
正如我们将看到的 这允许我们在hdfs上运行mapreduce的更好替代品
嗯
它允许您集中管理集群资源
这样我们就可以有多个数据处理框架
我们不再直接依赖于map reduce
这在hadoop 1中是常见的
现在hadoop本身核心的数据处理框架是mapreduce
它至今仍在使用
它是一个软件框架，易于编写处理大量数据的应用程序
在大型商用硬件的集群上以可靠的容错方式并行处理
你可以猜到，这就是官方定义
但基本上
为什么它被称为mapreduce
嗯 因为它包含mapper和reducer
mapreduce中的map函数会将数据映射到一组键值对
这些基本上是处理过程中的中间结果
这基本上是解析你的数据
提取你需要处理的数据
并以键值对的形式组织它们
reducer函数
合并这些中间结果
对它们应用额外的算法
并产生你的最终输出
通常你有mapper和reducer来提取数据
减少到你想要的最终答案并输出
所以最初你是如何在hadoop集群上进行分布式文件处理的
但现在它被apache spark广泛取代
因为它更快，更灵活，能做更多的事情
我们接下来会谈到spark
因为它更快，更灵活，能做更多的事情 我们接下来会谈到spark
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/051_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p51 28. EMR Serverless; EMR on EKS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们来谈谈emr emr serverless的新功能
它基本上与emr的工作方式相同
不同的是，定义我们需要多少容量的中间步骤从我们手中移除
因此，我们可以让emr自己决定
它需要多少个节点在集群中实际完成我们所要做的事情
因此，要使用emr serverless
我们首先选择我们的emr发布和运行时spark hive for presto
就像我们以前做的那样
然后我们只需将我们的emr serverless实例提交查询和脚本
通过工作运行请求
所以，而不是你知道的
直接登录到你的主节点
它是无服务器的 所以你知道，你想间接与之交互
你给它一个路径指向某个s3的东西 某个地方说这是我的脚本在哪里
你会从s3引用数据
然后你就把那个脚本
或者那个hive查询喂给它
从s3到你的无服务器服务器
理论上它会自动计算出你需要多少容量才能成功运行那个任务
emr会管理你底层的容量
但如果你想考虑底层的容量
你还是可以做到的 在很多情况下这可能是个不错的主意
如果你知道你的任务需要少量资源
你可以提前指定默认的工作者大小
你也可以提前初始化你的容量
而不是从零开始
你可以告诉他们
无服务器 嘿 我认为我需要的基本容量
是这些大小的节点这么多，然后从那里开始，这将帮助你
否则它会自动为你计算
即使你开始时以一个预初始化容量的基本设置开始
它会分析你的工作实际需要的资源
并根据调整它，按需安排工人
并在你不需要时不安排他们
作为权利 现在，所有内容都在同一地区工作
跨多个可用区
当然，所以你的无服务器集群将位于同一地区
那么为什么这是如此重要的事情
嗯 你不再需要弄清楚你需要多少工人
在像Apache Spark这样的事情中，一个非常大的痛点是
如果你猜得太小
如果你没有足够的资源来完成一份工作
你将会耗尽内存
你的工作将在中途失败
这可能是一个耗时且昂贵的过程
来确定我需要多少容量
因为这并不是一个很好的前瞻性方法来确定这一点
因此，emr无服务器正在解决这个问题
这实际上是相当令人印象深刻的
如果你问我，你将会自动为工作分配所需的容量
所以你不必担心
这真是一个大问题
你可以专注于你的脚本和你正在做的事情
而不是考虑你需要多少容量来完成它
就像我说的，它并不是真正的无服务器
与其他无服务器服务器一样
你根本不会考虑服务器
在底层
你仍然使用spark、hive或presto
以及这些技术
它们假设你对工作节点的水平有一定的了解
或者驱动器水平
所以，如果你仍然想配置spark到那个级别
你仍然需要考虑工作节点的容量
不同之处在于，你不知道你有多少个节点
也不知道它们位于何处
我如何使用emr无服务器
嗯 你首先需要一个iam用户，你将通过它来与它交互
这将与aws cli一起工作
因此，你的用户将通过cli创建这个无服务器集群
目前，只支持cli
我肯定很快会有变化
我无法想象他们不会提供控制台支持和sdk支持
在不远的将来 但目前，只支持命令行界面
然后你需要设置一个工作执行角色
以便于工作本身
确保你的工作有权限访问aws emr无服务器
以及访问s3脚本的权限
以及访问s3数据的权限
如果你使用spark sql
你可能需要访问glue或glue元数据
或者任何你设置的kms密钥
有了这些
你可以创建一个emr无服务器应用，使用spark、hive等
然后只需通过emr工作请求将其发送给它 所以你只需将你的spark脚本链接发送给它
你的hive查询
无论它是什么 这可能是一个命令行示例
所以这是一个示例命令来调用emr无服务器工作
我们只是在说aw cmr 无服务器启动工作，传递我们的应用ID
我们之前提到的执行角色，带有路径
然后在工作驱动器下
这是有趣的地方
我们在说在spark提交下，入口点
是到这个脚本的路径
我们也可以向这个入口点传递参数
如果我们需要一个参数
告诉它应该把输出放在哪里
例如 我们也可以覆盖Spark提交的参数
所以，如果你想向Spark提交参数
在Spark提交的命令行上
你也可以在这里指定
再次 你知道
即使我们在谈论无服务器
我仍然在想
我的执行器核有多大
我的驱动器核看起来像什么，诸如此类
所以你仍然可以控制到这一层
即使你正在将实际的容量移交出去
并将节点数量移交给emr无服务器
你也可以发送特定的emr无服务器配置覆盖
例如 这里我们说我们要输出云观察日志
到s3的这个路径
当我们完成
我们得到了我们的输出在我们的日志中
无论我们把它们放在哪里，就这样
当你完成时，只需关闭它
它就像emr
让我们谈谈emr无服务器应用的生命周期
这里很快 所以你通过命令行客户端明确创建你的应用程序
如果成功
那太好了 你创建了你的应用程序
如果不成功 它会自动从那一点终止
你有一个新的应用程序在那里，是你创建的
你可能选择立即终止它
这是一个选项 不确定你为什么要这样做
但通常你会继续发出启动命令来启动你的应用程序
这将启动它，一旦启动，它将实际运行
它将成功运行
并在完成后进入停止阶段
或者失败并自动进入停止阶段
一旦成功停止
你可以再次运行它
返回开始或转到终止阶段并关闭它
当你完成时
我想强调的是这不是自动发生的
我需要通过API调用命令
例如创建应用程序
启动应用程序
停止应用程序
在我完成时
删除应用程序以使这一切发生
所以我已经收到了学生的联系
他们说 你知道你
你在许多课程中使用emr
并且你总是警告我们要记得在你完成时关闭你的集群
以便你不被意外地收取费用
意外地
我应该使用emr serverless吗
这样我就不用担心了
不 这不解决问题
它有帮助，因为你会自动
你知道你不会再用比所需更多的容量
但你仍然需要明确删除你的应用程序，当你完成时
如果你忘记这样做
您仍将为该无服务器应用程序付费
即使它坐在那里什么也不做
好的 所以
不要认为 Emr无服务器是一个借口让你在管理你的资源时变得懒惰
你仍然需要考虑明确地启动、停止和终止那个集群
当你完成时 Emr无服务器只意味着它考虑容量而不是你
但你仍然需要管理那个容量
在某种程度上你自己
让我们更深入地谈谈Emr无服务器的预初始化容量
所以这里是一个例子在右边
这里那是从命令行可能看起来的样子
你看这里 我们有一个初始容量块，这是我们之前没有的
我们说我们要从五个司机开始
特定的资源配置类型
以及每个司机可用的CPU和内存
对于执行节点
我们说我们要从50个开始
每个有四个vCPU和8GB内存
所以你知道
让我们说，我想启动多少个进程来运行我的应用程序
它还是emr服务器无服务器
S的任务是找出这些进程在哪里运行
以及实际使其发生的所需容量
但你可以给它一个提示，像这样说，嘿
我知道这是一个要求很高的工作
我想从50个执行者开始
找出如何使其发生
但只是为了让事情顺利进行
并确保我有足够的初始容量
我想你从这个点开始
这里还传递了一个最大容量的参数
我想说我不想使用超过400个虚拟CPU
以及不超过1TB的内存
这是一个很好的安全措施
以防你做了傻事
想要使用无限的资源
这将成为你的一个安全网
那里 文档中的一点说明
这可能是我在考试中看到的那种东西
在某个时刻 记住，Spark会在任何请求给驱动程序和执行者的内存上增加百分之十的额外开销。
所以确保你的最初的容量请求考虑到这一点
确保你请求的内存至少比你的工作实际需要的多10%
一些关于emr服务器无服务器安全的笔记
它基本上与存储的emr相同
如果你在使用emf
嗯 在引擎盖下写着三
所以你知道
服务器端还是客户端
S3存储中的加密将确保您的数据在传输过程中的安全
EMR节点和S3之间的TLS加密将确保数据安全
如果您直接写入S3
显然，S3提供的所有安全功能都将为您所用
包括S3 A C KMS
您可以使用任何临时存储在本地磁盘上的节点
这些数据也将在传输过程中加密
驱动节点和执行进行通信的数据也将自动加密
如果您使用Hive和Glue进行通信
Meta store和emr在传输中也是加密的
使用TLS，这里有一份来自文档的快速笔记
我想传达
记住，如果你使用S3
你可以强制使用HTTPS在传输中
流量到S3 使用AWS安全传输策略
另一种无服务器EMR方法是EMR on EKS
那就是Elastic Kubernetes Service
所以如果你的世界建立在Kubernetes上
对许多人来说这是真的
这是一个在亚马逊EKS中使用EMR和Spark的特定方式
因此，这可以让你在Elastic Kubernetes Service上提交一个Spark作业
而不必担心为其配置集群
这给你带来了所有Kubernetes的好处
而不是仅仅拥有一个独立的EMR集群
这也给你带来了EMR所有完全管理的好处
因此，你不必在EKS中设置自己的Spark应用程序
这将这一切自动化
这是一个完全管理的解决方案
MapReduce和Spark，特别是您在EK世界的顶部
这样做的优势是您可以在Spark和其他应用程序之间共享资源
这些可能在Kubernetes中运行
所以，右边的图表基本上说明了这一点
我们在这里从顶部开始，通过EKS进行EMR
您可以看到Spark应用程序
只是您在EKS集群中可能正在执行的众多事物之一
正确 因此，您可以有其他应用程序执行完全不同的事情
可能是一个分析应用程序
你可能甚至在所有东西旁边运行不同版本的Spark
通过Kubernetes的魔法
我们可以更有效地共享这些资源
所以这可能是更好地利用您拥有的硬件的一种方式，最终节省金钱
您也知道，我们还看到它具有所有这些集成
与底部的亚马逊服务
分散 可能在多个可用性区域之间
你不必太担心这一点
只需在控制台中点击几下
你可以选择你想要的Apache Spark版本
在亚马逊上部署一个emr工作负载
Eks Emr会自动将该工作负载打包成容器
并提供这些预构建的连接器，以集成这些其他AWS服务
Emr然后将容器部署在Eks集群上 它将为您管理该工作负载的所有缩放、日志和监控
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/052_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p52 29. Amazon Kinesis Data Streams.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以第一个服务你需要了解的是
你可以看到数据流并且你可以看到数据流
这是你在系统中流式传输大数据的一种方式
一个Kinesis数据流由多个分片组成，分片按顺序编号，从1到n
从1到2 一直到编号n
这是你需要在事先配置好的
所以当你开始使用Kinesis数据流时
你说，嘿 我想要一个包含6个分片的流
因此数据将被分布在所有碎片中
好的 碎片将定义您的流容量
在摄入和消费速率方面
所以目前
让我们先从这里开始 然后我们有生产者
生产者将数据发送到Kinesis数据流
生产者可以是多方面的
它们可以是应用程序
它们可能是客户
例如桌面和移动客户端
他们可能在一个非常低的层面上利用aws sdk
或者更高层次的kinesis生产库kpl
我们将有一个深深的感觉
在接下来的讲座中，我们将更深入地探讨生产者。
或者可能是服务器内部的kinesis代理来流式传输
例如 应用程序登录到Kinesis数据流
所以所有这些生产商都做完全相同的事情，他们依赖于sdk在非常
非常低级
他们将记录发送到我们的kinesis流
所以记录本质上由两件事组成
它由分区键组成
它由最大1MB的数据块或值组成
分区键将帮助确定记录将发送到哪个扇区
数据块是值本身
当生产者将数据发送到
你能看到数据流吗 他们可以以每秒1MB的速度发送数据
或者每秒千条消息每片
所以如果你有六片
你可以得到每秒六兆字节或者每秒六千条消息
好的 一旦数据不再与流连接
它可以被许多消费者消费
而这些消费者又可以有多种形式
我们将在本节中详细探讨
所以我们有应用程序
它们可能依赖于SDK
或者从高层次来看，kinesis客户端库
所以kcl它们可以是lambda函数
如果你想在kinesis数据流上实现无服务器处理
它也可以是 你能看到数据火炬车
正如我们在本节中看到的
或者你可以帮助数据分析
所以当消费者接收到一个记录
它再次接收到分区键
还有序列号
它代表了记录在分片中的位置
以及数据块
因此数据本身
现在我们有不同的消费模式来处理kinesis数据流
我们有每个分片共享的2兆字节/秒的吞吐量
好的 或者你每条消费者得到2兆字节/秒
如果你启用了增强型消费者模式
增强型扇出 所以我们将在本节中详细查看
所以再次，生产者发送数据
你可以看到数据流
它保持在那里一段时间
然后由许多不同消费者读取
好的 一些属性
你可以看到数据流 第一个是保留时间可以设置为一天到三百六十五
五天 这意味着默认情况下你有重新处理或重放数据的能力
一旦数据被插入到kinesis中
它无法被删除
这就是不可变性
另外，当你将消息发送到kinesis数据流时
你添加一个分区键
具有相同分区键的消息将发送到同一个分片
并且这给你带来了基于键的排序
对于生产者，你可以使用SDK发送数据
你可以使用kpl或kag库
对于消费者，你可以编写自己的
所以kinesis客户端库kcl或SDK
或者你可以使用aws管理的消费者
例如aos lambda
你可以是数据火炬车
或者你可以帮助数据分析
对于容量模式，你有两个选项
第一个是kinesis数据流的历史容量模式
它被称为预定容量模式
所以你选择一个预定的分片数量
然后你可以手动或使用API进行扩展
每一片碎片
你能看到流将获得每秒1兆字节
或者每秒1000条记录
然后对于输出
每一片碎片将获得每秒2兆字节
这适用于经典或扇出消费者
您还需要按片碎片预配置每小时支付费用
因此您需要在提前大量思考
这就是为什么它被称为预配置模式
但是第二种模式是一种新的模式
按需模式
在这点上，你不需要配置或管理容量
这意味着容量将随时间进行调整
根据需求 你将获得默认的容量配置
这是每秒四兆字节或每秒四千条记录
然后这将会自动扩展
根据过去三十天的观测吞吐量峰值
在这种模式下，你还是需要按小时、按流量和按数据量付费。
每千兆字节 所以另一种定价模式
如果你不知道你的容量事件，选择按需
但如果你能规划容量事件
你应该选择预留模式，以肯尼的数据流为安全
它部署在一个区域
所以你有你的图表片
你可以控制对片进行写入和读取的访问
使用iam策略
有传输加密
使用https和存储加密
使用 kms
您可以在客户端侧实现自己的数据加密和解密
这被称为客户端加密
这更难实现
因为你需要自己加密和解密数据
但这提高了安全性
Kinesis 提供了 VPC 终点
这允许您直接从 E 实例访问 Kinesis
C 实例在私有子网中，而不通过互联网 没有通过互联网
最后，所有API调用都可以使用云追踪进行监控
这就是Kinesis数据流的概述
我希望你喜欢它 我将在下一节课中深入探讨所有移动部件
在...中的所有移动部件 你可以看到数据流
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/053_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p53 30. Amazon Kinesis Data Streams - Producers.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以我们需要知道我们如何将数据输入到kinesis中
这就是我们的亚马逊kinesis流
考试将期望您在高层次上了解这些如何工作
第一个是SDK
SDK允许您编写代码
或者使用CLI直接将数据发送到亚马逊kstreams
第二个是使用kinesis生产库或kpl
记住这个缩写
kinesis库将更先进
你将写更好的代码
它有一些非常好的特性，我会在本次讲座中描述这些特性，
这些特性可以让您在Kinesis流中获得更高的吞吐量。
第三个方法是使用Kinesis代理。
Kinesis代理是一个在服务器上运行的Linux程序。
记住，这是运行在服务器上的代理。
它基本上允许您获取日志文件，
例如，并将其可靠地发送到Amazon Kinesis流。
最后，您可以使用第三方库，这些库通常基于SDK构建，
例如Apache。
Spark 卡夫卡
连接 尼菲
等等 所有这些都将允许您可靠地发送数据到流中
看看这张图表
记住，Kinesis 流可以从各种来源获取数据有多种方式
现在让我们深入研究左侧的所有方法
首先，使用生产者SDK
它使用put record或put records with an s api
所以每当你看到 put record 这意味着 sdk
正如名称所暗示的那样，设置记录为放记录
你发送了一条记录
如果你放有s的唱片
你将发送许多记录
记录将使用批处理，因此会增加你的处理能力
那是因为你将许多记录作为一次HTTP请求发送了。
因此你在HTTP请求上节省了，并且现在获得了更高的吞吐量
如果你确实超过了你的处理量
尽管你可能会得到一个资源通过量超出的例外
知道如何处理这一点很重要
我们将在下一个幻灯片中看到
这个生产者SDK可以用于各种不同的方式
你可以在你的
在你的应用程序中 也可以在你的移动设备上
例如安卓
iOS 等
那么你会选择在何时使用生产者SDK呢
任何时间我们有一些低吞吐量的用例
或者我们不介意更高的延迟
我们希望有一个非常简单的API
或者我们正直接从AWS Lambda工作
这就是你会使用生产者SDK的地方
你也需要知道，有一些管理的AWS源用于Kenny使用数据流
所以它们背后的场景使用了生产者SDK
但你看不到
因此这些管理的源将是CloudWatch日志
你可以直接将日志从CloudWatch发送到Kinesis
你有aws iot
我们在这个部分会看到
最后，kinesis数据分析能够将结果输出到kinesis数据流
记住这一点，现在我们如何处理kinesis api的异常呢
如果你收到了一个配额超限异常
这种情况发生在你发送的数据量超过了你的承受能力
例如 你超过了每秒兆字节的数量
或者你超过了每秒记录的数量
你可以发送任何扇区
所以你需要确保当你得到这个时，你不应该有一个热图
例如 如果你有设备id
你的密钥并且你有90%的设备是iphone
那么你将得到一个热滑雪
一个热分区
因为你的所有设备或iphone
它将全部指向同一个分区
所以请确保你尽可能多地分配
你选择的密钥
为了不产生热点分区
所以解决方案是，如果你遇到提供吞吐量异常
尝试使用重试和备份
这意味着你将在可能两秒后重试
如果这不起作用 你将尝试四秒，然后八秒，这些都是随意的数字
或者你可以增加你在kinesis中的分片数量
基本上以增加你可以做的扩展量
你需要确保你的分区键是一个很好的
一个非常均匀分布的
所以例如 对于移动设备
而不是选择苹果与安卓
你可能选择设备ID
这肯定对于每个用户都是不同的
仅作为例子
所以现在让我们谈谈kinesis生产库或kpl
你需要确切地知道这是如何工作的
因为这是非常重要的 去考试
这是一个易于使用且高度可配置的
C++或Java库和个人经验
我看到当您使用KPL时，Java被使用得更多
当您想要构建高性能时，它会被使用
长时间运行的生产者
并且具有重试机制的自动化
所以，我刚刚描述的异常
当我们使用API时，我们必须处理它
Kinesis库会自动知道如何处理这个问题
库自动知道如何处理
现在Kinesis库有两种API
有一种同步API
它与SDK相同或异步API
它会为您提供更好的性能
但你需要处理异步性
显然每次在考试中你看到
好的 我们需要异步将数据发送到Kinesis数据流
通常使用Kinesis生产库或KPI来实现
这是一个非常好的库
因为它也能将指标发送到云观察进行监控
所以，每当你编写一个带有kpl的应用程序时
您可以直接在云观察中监控它
并且它支持一个非常重要的机制，称为批处理，批处理有两个子部分。
默认情况下，它们都已开启
他们将帮助你提高吞吐量和降低成本
你需要知道那些绝对地
第一个是在同一次写入记录中收集记录并将它们写入多个分片。
API调用
并且第二个是聚合
这将增加延迟，但也会增加效率
这意味着可以在一个记录中存储多个记录
您可以使用此方法超过每记录1000个记录的限制
我会在下一张幻灯片中详细说明
这将允许您增加负载大小并因此增加吞吐量
这将使您能够更加一致地达到每秒1兆字节的限制
如果您想使用压缩
这意味着使您的记录更小
但这并不是Kinesis数据流库出盒支持的内容
不幸的是 当你我们在使用kinesis生产库设置记录时，你必须自己实现这一点
尽管这是一个非常特殊的记录
你不能仅仅使用CLI来读取它
你需要使用KL或一个特殊的辅助库
在下一节课中，我们将学习KCL
让我们来谈谈这个批处理
因为在kinesis生产库中，这是理解一个如此重要的概念
而且考试会问你关于这一点
例如，这里是我向kinesis发送记录的示例
发送记录到kinesis
而且它只有2KB，我正在使用Kinesis生产库发送它
结果它不会立即发送
它会等一段时间，看看是否有更多的数据流入
也许我正在设置下一个记录为40KB
也许我正在发送下一个记录为500KB
而Kinesis会做的就是在某个时候我说等等
我会把所有这些记录聚合成一个记录
所以而不是设置3个记录
我们现在发送一个记录
而这个记录仍然小于1MB
所以我们要这样做多次
也许我们会有一个三十八十和二百千字节的
它将会把这个也聚合成一个记录
所以现在我们有两个记录
我们已经看到了聚合是什么
然后它会说
等一下 现在我们必须发送两个大记录
但我们不会采用put记录api
我们会选择其中一个 我们将进行收集
我们将使用put records api
在这里你可以看到
我想要向kinesis发送7条记录
我最终只做了一次api调用
因为我们有聚合和收集
那么kinesis如何知道应该等待多长时间来批量这些记录呢
你可以控制它
使用这条记录
最大缓冲时间
默认为一百毫秒
基本上你是在说
好的 我愿意等待一百毫秒
这是在牺牲一些延迟的情况下增加了一点效率
所以如果你想要更少的延迟
你可以降低这个设置
如果你想要更多的批量处理 你可以增加这个设置
这就是kpl的全部内容
请记住批处理机制
这真的很重要
有一种情况你可能不想使用canis生产库
这可能会出现在考试中
当你使用kpl时
你可能会产生额外的处理延迟，高达记录最大缓冲时间
这是一个用户可配置的库设置
想法是缓冲时间越长
那个记录最大缓冲时间的值越大
这意味着您的打包效率将更高，性能更好
因为更多的记录将累积在那个缓冲区
它们将被作为一条更大的记录发送
这将更有效率
潜在地压缩
等等
但如果您有一款不能容忍额外延迟的应用程序
那么您可能需要直接使用aws sdk
这意味着您想立即发送数据
假设您有一个iot传感器
它带有air sdk并且您想产生两条数据流
但该iot传感器有时离线
所以如果您使用kpl并且有离线事件
将会发生的是kpl将保留数据并累积它
这意味着当我的设备重新上线时
它可能需要一些时间来接收最新的数据
也许我们只想要求对最新数据采取行动
在这种情况下
而不是使用kpl
直接实现我们的应用程序更有意义
使用sdk api调用put records
因为当我们使用put records时
我们可以选择丢弃所有数据
并且我们可以选择在设备在线时只发送最新、最相关数据
现在你知道这种用例
这可能在考试中出现
生产两个kinesis的方法之一是使用kinesis agent
该代理将基本安装
并且它将监控日志文件并直接发送到kinesis数据流
只需进行配置
这是一个基于java的代理
实际上它是基于kpl库构建的，这使得它非常可靠和高效
您将在Linux服务器环境中安装它
功能包括从多个目录写入和多个流写入
它具有基于目录或日志文件的路由功能
并且可以在发送数据到kinesis数据流之前预处理数据
它可以进行单行分割
csv到json
日志到json
此外
kinesis的编写非常出色
它将处理任何日志文件轮换
以及失败时的检查点和重试
因为它使用kpl库
它将向云观察器发送指标以进行监控
如果您需要对日志进行聚合并几乎实时地进行大规模处理
那么kinesis agent是您的选择
我们已经看到了所有生产kinesis的方法
我知道这很多 但请记住，在高层次上
你需要理解它们是如何工作的
我希望我做到了
正是如此 下次课见
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/054_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p54 31. Amazon Kinesis Data Streams - Consumers.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈我们如何从kinesis数据流中消费数据
我们先来谈谈经典消费者
所以第一个是kinesis sdk
所以我们可以用cli或编程语言将数据推入
你可以看到数据流 我们可以使用sdk或cli从kinesis数据流中读取数据
我们也可以使用kinesis客户端库
我在之前的讲座中暗示过
它被称为kcl
我们用kpl生产，用kcl读取
还有Kinesis连接器库
可以简称为KCL
但实际上不是 所以它与消费者库有点不同
嗯 客户端库
然后我们有第三方库，如Apache Spark Log4J Flume Connect所有这些东西
但考试期望你知道
Apache Spark能够从Kinesis数据流中读取数据
作为一个消费者
我们也可以使用kinesis数据火烈鸟
以及AWS Lambda
如果我们需要的话，这里有一个消费机制
称为kinesis消费者增强扇出
我将在下次讲座中讨论它
所以目前让我们只考虑经典消费者在kinesis中的工作方式
首先，SDK获取记录
这就是经典kinesis
这意味着记录将由消费者从分片拉取或拉取
每个碎片将获得最多2兆字节的总体吞吐量
所以每个碎片记得1兆字节的生产者
2兆字节的消费者
所以这里是一个例子，我们的生产者生产到我们的can
您使用数据流，它的形式可能是
让我们假设是三个碎片
因此，如果我们有三个碎片
那么我们对下游的总体吞吐量就有6兆字节
但每个碎片本身将获得2兆字节
所以现在我们有一个消费者应用程序，它想要从
例如 碎片一号
它会执行获取记录的操作
通过API调用，碎片将返回一些数据
如果消费者需要更多的数据
他需要执行另一次获取记录的操作
API调用 这就是为什么它被称为轮询机制
每次运行时都会获取记录
它将返回不超过10兆字节的数据
然后因为10MB的数据需要经过2MB/秒的总带宽
你需要等待5秒才能进行下一次获取记录的操作
或者它最多返回10,000条记录
这意味着你还需要了解另一个限制
首先，每次获取记录的API调用最多只有5次
每个分片每秒的API调用次数
这意味着你的客户端应用
它不能仅仅每秒进行20次获取记录的操作
它只能每秒进行5次获取记录的操作
这意味着您的数据延迟将达到200毫秒
所以记住这个数字，因为它真的很重要
但是现在这意味着什么
如果我们看到这些限制，并且开始添加更多的消费者
一个五消费者的应用程序将从同一分区中消费
它们是不同的应用程序
它们都需要读取相同的数据
那么每个消费者基本上可以每秒拉取一次
并且可以接收每秒少于400KB
这意味着您拥有的消费者越多
你每名消费者通过的流量会减少
如果我们有消费者b和消费者c
他们都会共享每秒2兆字节的限制
并且他们都会共享每秒5个Get记录API调用的限制
所以理解这一点非常重要
我们将看到如何通过为消费者增强扇出来解决这个问题
接下来我们将探索Kinesis客户端库
这是一个以Java为主的库
但也适用于其他语言
例如Go语言 Python
Ruby Node和net
想法是使用KCL
您可以从Kinesis中读取KPL生成的记录
因为它在库中有去重机制
它还有可能让多个消费者共享多个分片，使用组的概念
这意味着有分片发现过程
包括在KCL中
还有检查点功能
如果你的应用程序崩溃或者记录处理器崩溃
那么它们可以从最后一次处理的地方继续
它还利用dynamodb进行协调和检查点
这意味着每个分区将有一行新dynamodb
这意味着因为dynamodb参与了过程
从kinesis数据流中读取
这意味着你需要管理你的dynamodb表的配置
这意味着你需要足够的权利容量单位w cu
以及读取容量单位rcu
或者如果你不想担心它
你可以使用请求模式来处理数据，而不会出现任何限速异常
如果你在dynamodb表中有任何异常
那么mob可能会限速，从而减慢你的客户端
你可以从肯尼亚用户那里读取数据
现在记录处理器将处理数据
如果你得到
如我所说 过期迭代器异常
这意味着在你的qa库中发生了一些事情
这意味着你需要增加cu
因为你的dynamodb表速度不够快，无法跟上权利的更新
这是一个考试问题
让我们看看它是如何工作的
我们有一个数据流
它可能有四个分区
我们有一个dynamodb表
它将用于检查点和协调我们的kcl记录处理器
在这个例子中，我有两个相同的kcl应用程序在同一组中运行在两个不同的ec实例上
实例
因此，多亏了这个碎片发现机制
他们可以共享碎片
所以第一个会从碎片1和2读取
第二个会从碎片3和4读取
然后kcl应用会将进度检查点写入动态数据库表
这经常发生
让我解释一下，你需要为你的动态数据库表提供足够的wcu
再次如果你得到一个过期迭代器异常
你需要增加wcu
你需要为你的数据库表增加足够的wcu
还有一个完全令人困惑的连接器库
也被称为KCL
但它是Kinesis连接器库
它是2016年的老Java库
它利用KCL库在底层
它用于将数据写入亚马逊
S3或DynamoDB
Redshift或OpenSearch
连接器库必须运行在EC two上
E 两个实例 例如
为了它发生 所以它是一个应用程序，其唯一目的是从ky的数据流中获取数据
并将其发送到所有这些目的地
你可能在想
哦 我们已经可以用这个做了
可以这个火炬
那是真的 我们已经可以做到这一点
我们可以使用火候 我们将看到这些目标中的一些
我们可以使用kinesis火候
例如，对于s三和红移
但对于其他人，我们可以使用aws lambda，所以总的来说
我想说，kinesis connect库可以在考试中出现
但它有点过时，被kenny取代
使用火候和lambda一起
让我们谈谈lambda
现在，lambda可以从kenny的数据流中读取记录
而lambda消费者也有一个小的库
这真的很好
用于从kpl解聚合记录
因此，您可以使用kpl生产出数据，并使用小库从lambda消费者读取数据
现在，lambda可以用于轻量级etl
因此，我们可以将数据发送到亚马逊s三dynamodb红移搜索或任何地方
只要您能编程它
lambda还可以用于实时读取
从kinesis数据流中读取并触发通知
或者 例如 实时发送电子邮件或您想要的任何东西
最后，Frank会详细描述这一点
但存在一个可配置的批处理大小
在lambda sanction中我们会看到这一点
但基本上您可以说，kinesis中的时间段内应读取多少数据
帮助您调节吞吐量
所以总的来说，我们已经看到了从canis数据流中读取的所有方法
有很多不同的方法
但希望这可以为您选择哪种适合哪种用例提供一些上下文 我会在下次讲座见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/055_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p55 32. Amazon Kinesis Data Streams - Hands On.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


那么我们开始做一些练习
你能协助数据流吗
我将打开罐装服务并创建我们的第一个
你能看到数据流吗 正如我们所见，这里有三个选项
我们可以使用数据流
数据火喉或数据分析
但我们目前只知道数据流
那么我们开始 我们得到一些关于价格的信息
每片我们支付零点
零一五美元每小时
好的 然后，做空期权或向Kinesis数据流发送数据的成本
所以我们将创建一个数据流
让我们给我们的流起名为演示流
然后我们必须定义数据流容量
正如你所看到的，我们有两种模式
我们有 按需模式和预配置模式
在按需模式下
你不必考虑容量
它将自动为您扩展。
因此，最大吞吐量为每秒二百兆字节
每秒两万条记录
并且每位消费者的最大容量为每秒四百兆字节
如果你在使用增强型扇出选项
因此，按需作为纸张通过量定价模型
但是没有免费层
好的 并且供应模式这里也没有冰箱
在供应模式下，您需要分配分区
因此，有一个分区估算工具
如果您想理解根据您发送每秒多少记录
以及您记录的大小
以及您有多少消费者
来确定您需要多少分区
好的 但在我们的示例中
我们将只设置一个分区
一片碎片给我们每秒1兆字节的写入速度
以及每秒2兆字节的读取速度
显然如果你放置10片碎片
那么一切都会乘以10
好的，所以我们需要一个碎片来做演示
这也是我们能得到的最便宜的选项
如果你不想在这个课程中支付任何金钱
那么你就不要做这个实践，因为你会为你的碎片支付一些金钱
尽管我们会处理它并且足够快地删除它
所以当你准备好
你只需点击创建数据流
然后你等待流被创建
我们的流现在已成功创建
在应用方面我们可以看到
我们有生产者 我们推荐了三个选项
kinesis代理
SDK或kinesis生产库
这些都可以在github上找到
这是应用服务器将数据流式传输到的方式
你可以使用这个数据流
SDK免费开发生产者到很低的级别
kpl是为了你开发生产者到一个很高的级别有一个更好的API
在消费者方面我们得到
你可以看到数据分析
你可以看到数据火枪
或kis客户端库或lambda
这不是这里显示的选项
好的 我们可以看到我们的
你可以看到数据流 你可以看到我们的数据流 你可以看到我们发送的记录数量
我们可以查看配置
如果你想扩展流
我们可以说我们想要多少
我们可以从一到说五扩展我们的canis在流
我们可以添加一些标签
然后我们可以使用增强的扇出并配置它
如果我们想要有一些消费者应用程序利用增强的扇出能力
但现在让我们保持简单
我们只想写和读我们的流
所以我们想要使用SDK生产消费
因此
我们要打开一个cli并让我们使用云壳
因为它很有趣 所以我要点击云壳在这里
这是铃铛图标旁边的图标
这将为我打开
一个aws的命令行界面作为替代
你可以使用你自己的终端或cli
如果你预先配置 但我喜欢切换东西
这个我真的很喜欢因为没有多少配置
至少创建环境
第一次可能需要一些时间
云壳在aws上是免费的
所以没有在这里
与此同时
在kinesis
打开kinesis流文件，我们将使用那个整体
所以，向流中写入命令有两种类型
根据你的cli版本，通常你已经安装了版本二
但你可能无意中安装了版本一
然后你获取版本
你只需输入aws version并粘贴
然后你会得到一些关于aws cli版本的信息，所以，在云壳中
将要安装的cli版本是版本二
如你所见
cli版本二点一六
我们将使用cli命令版本二
但如果你想使用版本一
那么你将使用这些命令
好的 现在我们可以开始了
我们想做的第一件事是将记录发送到我们的kinesis数据流
为此有一个名为put record和put records的api
我们需要指定一个流名
在这个例子中，我没有将我的流命名为test
我命名为demo stream
所以我们需要在cli命令中更改这一点
但你明白了
然后你指定数据的分区键
你设置 所以user one并且记住，具有相同分区键的数据将发送到同一扇区
但我们只有一块扇区
所以这不重要
然后数据本身
用户注册
最后，因为我们正在写一些文本数据
我们需要说这个选项
cli二进制格式
raw n base sixty four out
好的 所以让我们粘贴这个命令
所以复制并粘贴
但让我先编辑流名以确保它是demo stream
云壳会自动配置为你自己的i凭据
所以它将继承你的i凭据
并且我们也将使用默认的区域
它在哪个区域启动 所以us east one我按回车
现在我得到一个成功的消息
所以消息发送到了扇区id零零零零零零
这是他们的第一扇区
消息的序列号在这里
如果我再做一次
我将得到一个第二条消息
带有成功的
所以，我们可以做用户注册
我们可以混合消息
然后用户登录
然后可能用户注销
所以我们只是 我们只是向kinesis数据流发送一些消息，完美
所以我要清除这个
如果你等一会儿，然后进入监控并查看流指标
我会有一个小时的
你会在这里看到一个写入记录
但是需要一些时间才能让云监控指标更新
但是你在这里可以看到
好的 接下来我们希望能够从我们的kinesis数据流中消费
为了做到这一点 我们将首先创建
描述流以获取有关此流构成的一些信息
因为我们需要从特定的分片消费
因为它正在演示
我将按下回车键
正如你所看到的
我们有流描述
我们有一个名为 shard id zero zero zero zero zero 的片段
所以我们需要把这一点记在心里
以便能够从流中读取
当你使用 CLI、SDK 时，在最低层次上
你需要指定你从哪个片段读取
但如果你使用 Kinesis 客户库
这一切都由库本身处理
但我们使用 CLI
所以我们必须指定片段 ID
所以我按q退出这个
我将会消耗一些数据
所以我将运行这里的命令，让我清除这个
这里有两件事需要注意
第一 我需要更改流名
我从中消费 所以demo stream是一个流
然后shard iterator类型是trim horizon
这意味着你将从流的开始读取
所以，它会读取从开始发送的所有记录
另一个选项就是确保只接收从那个时刻起发送的记录
从新发布开始，无论怎样
所以我要按回车
然后这会给我一个分片迭代器 这个分片迭代器可以用来
我们消费记录
所以下一个API是 你可以获取记录吗
分片迭代器
然后我们只需指定这里整个字符串
所以，我现在通过使用低级API描述流
获取分片迭代器并获取记录使用的是共享消费模型
这不是使用增强型扇出
在我看来，应该使用Kinesis客户端库
消费者库，以便您能够充分利用它
并且有一个很好的API来实现这一点
但这是低级的
所以让我们点击并按Enter键获取Kinesis记录
然后我们从中获得一批记录
所以我们有记录一在这里
这是patchi用户一
我们这里有一些数据
但它是以基六十四编码的
我们又有另一组以基六十四编码的数据
我们获得一些时间戳信息
另一组编码数据
然后我按回车
它会往下走一点
一些更多的以基六十四编码的数据，只是为了确保我们可以读取数据
我可以访问网站
在线解码基六十四
我将只是将这个数据粘贴到这里
粘贴到这解码基六十四中并点击解码
这给了我们用户注册
如果我将第二种类型的数据粘贴在这里
这将给我们用户登录
这就是我们发送的数据
所以这完美 一切正常
然后，正如你所看到的
这里有一个下一个分片迭代器参数
所以下一次我们消费
我们需要指定这个下一个分片迭代器参数来消费
我们从哪里停止消费
所以这是你必须在代码中迭代的东西
但在低层次上
我们已经产生了数据，可以看到数据流并从kenny到流消费数据
这太棒了
我们在同时使用了云外壳
我认为这非常方便
这就是这次演示的全部内容
保持水流畅通
正如我们将其用于kinesis数据一样
在几秒钟内形成水柱 我会在下次讲座见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/056_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p56 33. Amazon Kinesis Data Streams - Enhanced Fan Out.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以我想确保我教你关于现实世界中的大数据，而不仅仅是考试中的数据
即使这还不算是考试，但我认为它很快就会出现
我认为它会很快出现在考试中
在我看来，kinesis增强的扇出功能是我所谓的改变游戏规则的功能
它在2018年8月出现
所以我确定它很快就会出现在考试中
那么我们如何利用这一点呢
如果你使用KCL 2.0或Ada是Lambda从2018年11月开始
所以它由这两者支持，增强的扇出是什么意思
这意味着每个消费者将获得一个分区每秒2兆字节
所以看起来与以前一样
但它并不完全相同
所以我们有一个生产者生产两个Kinesis数据流，并且有
例如，一个分区 例如
我们将有一个消费者应用程序a
它将调用API称为订阅分区
并将自动进行
Kinesis数据流将以每秒两兆字节的速度推送数据
所以我们不再进行轮询
我们只是订阅分片
分片将向我们发送每秒两兆字节的数据
这意味着如果我们有20个消费者
我们将获得每分片每秒40兆字节
因此我们可以开始添加消费者应用程序
再次进行订阅调用并获取另外的每秒两兆字节
所以之前我们每个分片每秒有两兆字节的限制
但现在我们有每秒两兆字节的限制，每个分片每个消费者
原因在于增强型扇出使得Kinesis将数据通过HTTP2推送给消费者
这种方案的好处首先就是我们可以扩展更多的消费者应用
另一个好处是我们可以获得更低的延迟
之前记得如果我们有一个消费者，延迟是200毫秒
因为消费者可以每秒拉取5次
实际上如果你有更多的消费者
他们的延迟会是1秒
而使用增强型扇出
现在我们可以获得更低的延迟，因为我们接收到的数据是通过推送的
平均延迟会降到70毫秒
这比两毫秒或一秒少得多
因此，对我来说这是一个改变游戏的特性
我只是很高兴告诉你它存在
显然你得为此多付一点钱
在定价页面上，这将帮助你理解你需要为它支付多少费用。
现在 增强数据和消费者标准之间的区别是什么
那么您将如何使用这些标准消费者
当你的消耗应用数量较少时，你会使用它
说一、二或三
你可以容忍一些延迟
你可以容忍200毫秒或更多的延迟
并且你想要最小化成本
因为中央消费者的成本是包含在kis中的
并且你会使用增强型扇出消费者
如果你想要多个应用程序消费同一流
我们谈论的是像5或10个应用程序同时
并且你想要非常低的延迟要求
所以你可能只能容忍17毫秒的延迟要求
显然这会带来更高的成本
如我所说 请查看kinesis定价页面
并且默认情况下
虽然 当你使用扇出消费者时
你有20个消费者可以使用扇出的限额
但你可以增加这个数量
通过在aws支持提交服务请求
我希望这说得通
我希望你对这个新功能感到兴奋
老实说 这太棒了
我肯定考试会很快问你这个问题
所以了解它是有好处的
对我来说 这是一场革命
因为我们现在可以在一个扇区中有20个消费者而不会影响到每一个
这对我来说是真正的革命
好的 我已经说了足够多了 我会在下次讲座再见
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/057_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p57 34. Amazon Kinesis Data Streams - Scaling.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


从考试和现实世界的角度来看
理解如何扩展Kinesis确实非常重要
所以，我们可以进行的第一个操作是添加碎片，而在kinesis中添加碎片。
焊接被称为裂纹扩展
我们将会看到为什么在短时间内它可以被用来基本上增加流容量
所以记得你每有数据每秒可以得到一兆字节
嗯，每份数据有
那么如果你有十片碎片
你有十兆字节是正确的
那么，如果你想要增加流量的容量
你需要将一个碎片分割
也可以通过使用和划分热点图表来帮助
所以如果你有一个热点碎片
一个比其他图表使用更多的图表
我们就会分割它
这可能会增加我们的吞吐量
当你分割一个碎片时会发生什么
当所有碎片关闭并且一旦其中的数据过期就会被删除
所以让我们看看图表
因为我认为这会更清晰
所以在这个例子中，我们有碎片一、二和三
现在他们占据了相同的空间
碎片二非常热
我们希望将其分割以增加这个分片二的键空间的吞吐量
所以我们要进行一个拆分操作
而且将要发生什么
那就是将要被创造的东西是四个
并且有五个正在被创建
正如你所看到的，它们占据了碎片二的相同空间
但现在因为我们有两个碎片
我们有每秒2兆字节
在这个空间里，而不是每秒1兆字节
然后其他碎片一和三保持不变
所以，在我们碎片看起来像这样之前
它有三个碎片或流
看起来它有三个碎片
然后分裂后我们有四个碎片
我们可以看到四和五占据了碎片二的空间，所以碎片短
碎片二将可用，只要其中的数据没有到期
但当它过期 它将消失
所以现在这是我们的新kinesis流状态
当生产者将其数据写入时
他们有四个分区可以写入
所以这是一个非常重要的概念，你必须理解
因为基本上，你可以随时将分区分成多少个
并以此提高吞吐量
现在 添加或分割分区的反操作是什么？
是减少分区或合并分区
当你合并碎片时
你将使用它来降低流容量并节省一些成本
它可以被使用
例如 将两个流量低的碎片分组
你想将它们合并以节省成本
所以所有碎片将根据数据过期而被关闭和删除
所以现在 例如
假设我们有这个碎片和流
它有一个 四和五
三就像以前一样
我们希望将一和四合并
所以我们将它们合并
它变成碎片六
因为可能碎片一和四没有太多流量
所以我们可以将它们合并以节省一些成本
所以我们将它们合并然后五和三将保持不变
他们在同一
嗯 同一
所以现在我们从四个碎片变为三个碎片在新的流中
因为我们合并了
所以你可以看到我们可以合并和分割合并和分割
我们可以基本模块化我们的整个can流以增加和减少通过时间
这就是它工作的方式
考试中一个常见的可怕问题是
将询问你关于生产者将数据发送到一致流
使用正确的分区键
然而，在某些时候，您的消费者将接收到数据乱序
其原因可能是重新分区
所以让我们探索这个案例
当你进行kinesis重新分区时
你可以直接从重新分区后的子碎片读取
然而，如果你还没有从父碎片读取所有数据
那么你可能会以特定哈希键的顺序读取数据
让我们在图表中看看
我们有父碎片和一个使用kpl产生两个数据流的设备
例如
我们有一个在e上运行的消费者应用程序 使用SDK
所以我们将进行获取记录
API调用
所以假设在重新分区操作之前，物联网设备发送了一些数据一和二
在消费率方面
我们的消费者应用程序一直在消费父碎片中的所有数据
直到数据点一和二
我们的消费者应用程序一直在消费父碎片中的所有数据，直到数据点一和二
然后我们开始进行碎片分割操作
因此，从这个父碎片我们将有两个新的子碎片
接下来会发生的是，一旦分割完成
物联网设备将开始向新的子碎片发送数据
因为父碎片将关闭写入权限
这意味着数据点
三和四将出现在子碎片上
如果你的消费者应用程序不够聪明
你将会直接从子碎片请求数据
在你读取父碎片末尾之前
这意味着你会先看到数据点三和四
然后可能是一和二
这可能会导致记录顺序混乱
重新碎片化后记录顺序混乱
抱歉 重新碎片化后
你需要确保你有逻辑来完全从父碎片读取
这意味着你必须读取父碎片的所有记录
这意味着你到达了父碎片的末尾
然后你可以开始从子碎片读取
这将保证记录的顺序
现在 这可能比较复杂实现
所以请注意
kinesis客户端
la rae kcl
已经内置了这一逻辑
即使重新碎片化后
所以你可以继续
但希望你已经理解了这个案例
这可能会出现在考试中
那么关于自动扩展
你可能会说，我们是否必须手动进行这一操作
有自动扩展功能
但这不是kinesis原生功能
你可以使用api调用更新碎片数量
称为update chart count
你可以使用lambda实现某种自动扩展
有一篇相关的博客
如果你感兴趣 这是直接从那篇博客中复制的架构图
阅读那篇博客
如果你对实现自动扩展感兴趣
但你需要手动配置才能使其工作
那么关于kinesis扩展
它的局限性是什么
你不能并行进行重新碎片化
因此你需要提前规划容量
这意味着你不能同时进行一千次重新分区
一千次重新分区同时进行是不可能的
所以你需要在提前规划好容量
以防你预期有大量的吞吐量
你只能同时进行一次重新分区操作
这需要几秒钟
例如 如果你有一千个分区
这将花费你大约三十千秒
或者8.3小时将流分割块翻倍到2000个
所以你可以想象，kinesis的扩展并不是即时的
需要时间
如你所见 从1000个流分割块翻倍到2000个需要超过8小时
我们需要在上面规划很多事件
现在有一些限制
它们有点复杂，你不需要记住它们
但我在幻灯片中添加了
以防你需要在现实世界中使用它们
你需要了解这些
所以这里它们全都在这里，基本上这意味着你不能太快地放大
你也不能太快地缩小
Eos对你有一些限制
我不会读它们给你听
你不需要在考试中知道它们
好的 我只需要知道分片不能并行进行
你需要知道重新分片每个分片需要几秒钟
所以记住，对于一千个分片
大约需要8点
三个小时将shard的数量翻倍到2000
这是我希望你记住的
显然不需要记住所有的限制
但它们就在这里作为参考
在你需要它们用于你的实际应用时
这就是kinesis的扩展 我希望你喜欢 我会在下次讲座再见
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/058_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p58 35. Amazon Kinesis Data Streams - Handling Duplicates.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈你如何处理流中的重复记录
这可能发生在两种情况下
一种是在生产者侧，另一种是在消费者侧
因此，生产者的重试可能导致重复
这主要是由于网络超时
这是一个考试问题
那么这是如何工作的呢
给你的生产者 这可能是应用程序
也可能是sdk或kpl
他们将数据发送到kinesis数据流
当我们执行put记录并发送一些数据时
我们知道会发生什么
数据将被写入kinesis数据流
数据将获得一个唯一的顺序号
然后您可以看到流说，我已正确写入您的数据
但在网络超时的情况下
此确认可能永远不会到达我们的生产者应用程序
可能是因为网络超时，生产者无法知道
也许它说 好的
我不认为这个数据已被写入
因为我没有收到任何确认
现在发生了超时
那么会发生什么
是生产者记录将由我们的应用程序重试
通过再次发出API调用
所以我们将再次对相同数据执行put记录
这将第二次将我的数据写入到我的数据流中
如您所见 顺序号从一到二增加到一到二十四
现在确认将返回到我们的生产者，我们就能继续
但从流的角度来看
我们创建了重复记录
因为相同数据被写入了两次，并且获得了两个不同顺序号
尽管两个记录具有相同数据
它们也具有唯一的顺序号
因此，我们的kinesis数据流中将有两个数据点
解决方法是在数据中嵌入一个唯一的记录ID
以便能够根据该唯一记录ID去重
因此在消费者侧
所以 尽管看起来吓人
我会说这很少发生
但这是一个用例
这是一个你需要了解的情况，考试也会测试你
让我们谈谈消费者侧可能发生的重复情况
消费者重试可能导致你的应用程序读取相同数据两次
消费者重试可能发生在四种不同类型的用例中
但它们都对应于记录处理器重启的情况
所以，当工人意外终止时，可能是这种情况
当工人实例被添加或删除时，也可能是这种情况
当分区合并或分裂时，也可能是这种情况
或者，当应用程序部署时，也可能是这种情况
所以，请记住这四种情况，因为它们也可能出现在考试中
解决这个问题相当困难，文档对此的解释也很模糊
但基本思路是，使你的消费应用程序具有幂等性
这意味着如果你两次读取相同的数据
那么它不会产生两次相同的副作用
此外，文档还建议，如果最终目的地可以处理重复项
那么建议在那里处理
例如
如果你可以依赖唯一的记录ID，并将其插入数据库 那么数据库将不允许你插入两次相同的唯一记录ID
这可能对你来说是一个好方法
如果你需要更多信息
你可以在这里阅读文档
但从考试角度来看
你需要知道网络超时可能会引入生产者侧的重复项
而从消费者的角度来看，这四种消费者重试可能会引入重复项
因为数据将被读取两次
所以，这就是本讲座的内容
我希望你喜欢它 我将在下次讲座见到你 再见
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/059_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p59 36. Amazon Kinesis Data Streams - Security.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 我们来谈谈kinesis的安全性
所以安全将是考试中的一个重要部分
它有一个专门的章节
因此我们需要了解每种技术的安全措施
那么我们来谈谈kinesis
基本上，我们通过iam策略来控制对kinesis的访问和授权
我们可以在飞行中使用加密
使用http端点
这意味着我们发送到kinesis的数据将被加密
并且它无法被拦截
我们还可以在kinesis流中使用kms来保护静态加密
如果你想在客户端对消息进行加密
然后在客户端进行解密
那么你必须手动完成
这是可能的
但这会更困难，没有客户端来帮助你完成
这取决于你实现这种加密技术
最后 如果你想在你的vpc中访问kinesis
在一个私有网络中
您可以使用称为vpc端点的东西
它们对kinesis是可访问的，基本上允许您访问kinesis
不在公共互联网上
但在您的私有vpc互联网上
所以让我们看看安全
总的来说，不要担心
课程结束时
我会有一个关于安全的整节
详细描述每种技术的安全
但是至少它能给你一些关于Kinesis安全如何工作的想法，好的 就是这样 下次课再见
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/060_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p60 37. Amazon Kinesis Data Firehose.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以我们已经看到了
你能看到数据流吗
Firehose 所以Firehose实际上相当容易
将数据存储到目标目的地
所以我们有生产者
又可以是 一堆之前可以直接送数据到Kinesis Data Firehose的应用程序
你能看到Firehose正在读取吗
你能看到从Amazon CloudWatch或AWS IoT的数据流吗
但你最常看到的将是Firehose正在读取
你能看到数据流
它将逐个读取记录
每次最多1MB
好的 如果你想要转换记录
我们可以创建一个Lambda函数来转换记录进行一些小修改
然后你能看到Firehose会尝试填充一大批数据
将数据写入目标数据库或目标目的地
好的 这就是为什么它被称为批处理写入
它不会即时写入
它会尝试批量写入以提高效率
因此Kinesis Data Firehose是一个接近实时的服务
在目标方面我们有AWS
所以Amazon S3非常重要
Amazon Redshift和写入到Redshift
你能看到Firehose会首先将数据写入Amazon S3
然后发出命令将数据复制到Redshift或Amazon OpenSearch
你必须记住这三种目的地非常重要
其他目的地包括来自合作伙伴的第三方目的地，如Datadog
Splunk New Relic和MongoDB
但可能会有其他合作伙伴将来
你不需要了解他们
这就是我为什么不会更新这个幻灯片
如果有新的合作伙伴 但你可以看到随着时间的推移会有新的合作伙伴
或者也可能有一个自定义目的地
只要有一个有效的HTTP端点和API
你可以让Firehose将数据发送到那个目的地
最后，如果你在处理过程中遇到失败
或者你想将所有数据发送到Firehose进行存档
将所有数据或仅失败的数据发送到备份S3桶是完全可能的
Kinesis Data Firehose
嗯 你能看到Data Firehose是一个完全管理的服务
它不需要任何管理
并且被认为是接近实时的
因为你有一个缓冲区
基于在缓冲区中花费的时间以及缓冲区的大小
然后数据将发送到目的地，现在可以选择禁用缓冲区
那么你使用firehose做什么呢
你将数据加载到红移中
亚马逊是三个开放搜索或splunk
你必须记住这四个目的地
所以记住红移s三开搜索和splunk
数据firehose的酷之处在于，不同于你可以看到的流
有自动扩展
这意味着如果你需要更多的吞吐量
你能看到数据流会为你扩展吗
如果你需要更少 它也会为你缩小
它还支持多种数据格式
也能进行数据转换
例如从json到parking或rc
如果你使用s3
其他数据转换可以通过aws lambda进行
例如 如果你想将csv转换为adjacent文件
我们可以使用aws lambda来完成那个转换
结合kinesis数据
Firehose还支持压缩，当目标为亚马逊s3时
所以你能够压缩发送到s3的数据，使用gzip
Zip或snappy直接发送到s3
如果你将其加载到red shift的更远地方
那么gzip将是唯一的支持的压缩机制
最后，你将只支付通过firehose的数据量
所以真的很酷的是
如果你不需要提前配置firehose
你并不会因为预留容量而被建设
你只会因为使用的容量而被收费
最后，考试会骗你
认为spark或kl可以从kdf数据中读取
消防栓 这不是事实
spark streaming和kinesis客户端库不从firehose中读取
它们只从kinesis数据流中读取
所以我只是想现在立刻非常清楚地说明这一点
好的 那么交付图景看起来怎么样，嗯
我们有消防水带在中间
它会有一个来源
例如 在这个例子中，我的来源是一个kinesis数据流
所以它会将该流发送到消防水带
也许我们可以做一些数据转换
所以我说过，csv到json
实际上，有许多可用的aws lambda蓝图
以帮助您在消防水带中转换数据
在你想要的格式中
一旦数据被格式化和转换
我们可以发送输出
例如发送到亚马逊
S3存储桶 如果你的目的地是S3
或者如果你的目的地是红移
实际上它会通过S3
然后那里会发出一个复制命令将数据放入红移
那么源记录呢
我们可以将源记录放入这个免费存储桶吗
答案是肯定的
任何通过Kinesis数据
流入的数据都可以配置将其放入你选择的另一个存储桶
如果考试问你
好的 我们如何将所有数据源放入亚马逊的免费存储桶中
你能看到数据流入吗
这是直接从流入中获取的功能
此外，如果在转换过程中出现错误
我们可以将转换失败存档到亚马逊S3存储桶中
甚至如果出现交付失败
我们也可以将交付失败放入亚马逊S3存储桶中
你记得这个图吗
从这个图中你记得的是，Kinesis数据流入不会丢失数据
要么数据到达目标
要么你会有转换失败的记录
交付失败的记录 甚至源记录
如果你想将其放入另一个S3存储桶
记住这个图
这真的很重要
这基本上就是你对数据流入的理解
现在 我们将看看Kinesis数据流入多久将输出数据发送到目标
为了理解这一点，我们需要了解流入的缓冲大小
所以流入是如何工作的呢
流入将从源接收大量记录
并将这些记录累积到一个缓冲区中
这个缓冲区
因此得名，不会随时刷新
它会根据一些规则刷新
它基于时间和大小规则
我们需要定义一个缓冲区大小
例如32MB
这意味着如果你从源接收足够的数据来填充
一个32MB的缓冲区
那么它会自动刷新
另一个设置是缓冲时间
例如2分钟
这意味着，如果两分钟后你的数据缓冲区仍未满
尽管如此，它将被刷新
所以你可以看到火
哦对不起 可以自动增加缓冲区大小以提高性能
所以它可以很好地扩展，以适应任何吞吐量
所以如果我们有高吞吐量
有大量数据通过
可以自动增加缓冲区大小以提高性能
那么缓冲区大小限制将被触发并基于缓冲区大小刷新
但如果我们有低吞吐量
通常这意味着缓冲区大小限制将不会被触发
但取而代之的是缓冲时间限制将被触发
但时间为最小值你可以设置一分钟
但为尺寸你可以设置几兆字节
好的
所以你可以看到数据流与火
它们看起来相似 但它们实际上是不同的
流是你必须为你的生产者和消费者编写自定义代码
并且它将是实时的
我们将获得200毫秒的延迟
嗯 消费者
或者对于增强型扇出消费者将获得70毫秒的延迟
你必须自己管理扩展
所以你需要执行分片分裂和分片合并
以控制你的成本和吞吐量
嗯 随着时间的推移，你的数据存储将移动
你可以存储数据1到365天
重放功能和多个消费者
你也可以使用lambda将数据实时插入到open search中
例如fios
另一方面 将完全管理
自动扩展 它将将数据发送到s3 splunk redshift和open search
并且它有一些无服务器数据转换
它可以使用aws lambda并且是接近实时的
它是自动扩展的
火hose没有数据存储
所以火hose不会重放数据或类似的功能
所以你需要真正理解两者之间的区别
它们很清楚它们是什么
但你最终需要记住，无论是生产者还是kpl
你可以将数据发送到流或火hose，这都可以
所以记住你的用例
如果你的用例是让应用程序读取数据
那可能会是流
但是如果你所有的用例都是关于将数据发送到S3或Red的
转换或splunk 那么也许消防水炮将是你的用例
记住真实时间与接近真实时间的限制
所以这是为火主机设计的 我希望你喜欢它 我会在下次讲座见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/061_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p61 38. Kinesis Data Stream Troubleshooting and Performance Tuning.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


这些考试喜欢给你一些事情出错的场景以及如何修复它们的步骤
所以我们花一点时间来讨论一些故障排除的步骤
对于你可能遇到的kinesis数据流问题
让我们从生产者端的性能开始
假设我们的数据流生产者写入速度太慢
这可能有几个原因之一，你可能遇到了服务限制
你可能只是想检查一下吞吐量异常
看看是否发生了什么
如果是这样，异常可能会指向问题的所在
不同的调用有不同的限制
你可能已经超过了那些限制
在读取和写入方面也存在碎片级别限制，你可能已经超过了这些限制。
我们之前已经讨论过那些了
一些操作也有流级别限制
因此，对于创建流列表，需要列出流和描述流在流级别上
在那边的速率限制在每秒钟五到二十次呼叫之间
你也许在数热碎片
你知道 数据偏斜
我们在第一节就讨论过那个方法了
也许这就是正在发生的事
也许你需要一个更好的分区键来更好地
均匀地将你的写入操作分配到分片上
也许你有一个热点分片，当您生产时它会放慢速度
如果您有大的生产者
可以将事情批量处理
因此，您可以使用Kinesis生产库与put records API
将多个记录批量在一起并发送
这可以更有效率
或者将记录聚合到大文件中
然后发送这些更大的文件，而不是单个记录
小型生产者喜欢一个应用程序
将put记录的建议也适用
你也可以使用AWS中称为kinesis recorder的东西
使用移动SDK自动完成
你可能还会看到一些生产者的其他问题
如果你收到500或500错误
这通常表示你正在看到亚马逊Kinesis异常
超过1%的时间
处理这个问题的方法是实现重试机制
这样你就不会失败
你会尝试直到它成功
你可能看到Flink到Kinesis的连接错误
如果是这样，这可能指向网络问题或Flink环境中资源不足
也可能是VPC配置错误
也许你无法从Flink到Kinesis
因为 它们位于不同的VPC子网中
或者你没有使用VPC对等连接
如果你看到超时错误，Flink到Kinesis的连接是好的
但是超时发生得很好
你可以通过请求增加超时时间
超时设置 你也可以在Flink Kinesis生产者中设置更高的q限制
你可能看到限速错误
如果是这样，请再次检查热点分区
使用增强监控查看分区级情况
你也可以检查日志中称为微尖峰的情况
你可能会看到突然爆发的流量
或者你可能会看到一些模糊的指标超过了其限制
这可能指向这些微尖峰
你也可以尝试不同的分区策略
尝试随机分区键
或者再次尝试改善键的分布
这可能是热点分区
如果你有数据偏斜
使用不同的分区键是解决这个问题的一种方法
你也可以实现指数后退机制
如果你正在遇到限速问题
你可以有一个系统自动在生产者一侧后退
直到事情得到解决
你也可以进行老式的速率限制
确保你永远不会发送超过给定速率
超过你的处理能力
让我们也看看消费者端
你可能会在消费者端遇到一些问题
如果我看到Kinesis客户端库中记录被跳过
这可能意味着我需要检查处理记录时未处理的异常
如果我看到在同一分区中记录被多个处理器处理
这可能表明记录处理器工人发生了故障
我可能需要调整故障转移时间
处理方法之一也是处理带有原因的关闭方法
僵尸
这就是你看到多个东西处理的原因
如果我的读取速度太慢
我可以增加分区数量
这是最基本的事情
或者是我的每调用记录数太少
也可能是你代码太慢
为了查看消费者侧代码是否是瓶颈
只需测试一个空处理器与你自己的
并查看其性能
Get Records可能为你返回空结果
在消费者侧 这是完全正常的
只需继续调用Get Records
在某些情况下，数据可能尚未准备好
所以不必担心
如果你的分片迭代器意外过期
这可能表明你需要更多的权利能力
在Dynamo DB的碎片表底层
最后，如果记录处理落后了
那么 一种买时间的方法是增加保留期
在你试图找出真正问题期间
但通常这指向资源不足的问题
所以使用获取记录迭代器年龄和最新记录滞后毫秒来监控
当你试图找出资源限制可能是什么时 一些其他问题可能在消费者侧
也许你的Lambda函数没有触发
这通常与执行角色的权限问题有关
所以检查你的IAM权限在Lambda函数上
以确保Kinesis可以与之通信
你的功能也可能超时
所以检查Lambda函数的最大执行时间
确保足够大以处理你需要做的事情
你可能正在突破你的Lambda函数的并发限制
并留意迭代器年龄指标
以查看Lambda是否被阻塞
你也可能遇到读取
已提供
通过率超出
异常 这表示发生了阻塞
你可能需要根据这种情况重新划分你的分流
或者减少获取记录请求的大小
这也是一个增强扇出可以帮忙的情况
所以
这可能会成为一个好的考试问题
我正在遇到重新提供 通过率超出异常
这可能是处理它的一种方式
那些答案之一可能是使用
增强扇出
你也可以使用重试和指数后退
以尝试找到一个你能处理的更好的通过率
在你试图找出发生了什么并调试它时
但首先尝试增加你的分数数量，你可能不够
还有一些更多的这里 如果你在消费者侧看到高延迟
你可以使用获取记录延迟和迭代器h来监控
当你试图找出发生了什么并调试它时
但首先尝试增加你的分数数量，你可能不够
并且增加保留期以确保不会丢失记录
检查CPU和内存使用情况
你可能需要更多的内存来更快地处理事情
如果你看到500个错误
这在生产者侧是同样的事情
这表明你的高异常率超过1%
它指示你获取了高异常率超过1%
你所能做的就是确保你有一个重试机制
这样如果你遇到异常
你会再次尝试 而不是得到一个500错误
你可以在消费者侧看到被阻塞或卡住的kcl应用
如果确实如此 你可能需要优化你的过程记录方法，使其更快
你可能需要将最大租赁工作文件数增加
或者你可能需要打开kcl调试日志，以查看正在发生的事情 以更深入的方式找出根本问题
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/062_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p62 39. Kinesis Data Analytics  Amazon Managed Service for Apache Flink (MSAF).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们来谈谈kinesis数据分析
它正在被称为管理Apache Flink服务的东西所取代
这仅仅是一种查询数据流并在途中处理这些数据流的方式
历史上我们有一个叫做Amazon Kinesis数据分析的东西
它会从Kinesis数据流或Kinesis数据火炬中摄取数据
在接收到的数据流上进行某种形式的转换
然后将转换后的数据发送到一些分析工具或下游目的地
这基本上是一种流式ETL方式
现在，所有这些 现在只剩下一个叫做Kinesis数据分析SQL应用的东西
这使得我们能够在流经时对该流应用sql操作
我可能不会对这很快被废弃感到惊讶
所以我不太确定我在考试中会看到这一点
因为我认为它快要不行了
但就概念而言，它这样运作
因此你可以从数据流或数据喷泉中接收数据
你也可以拥有被称为参考表，它们是托管在
S三个 那就是一种进行非常便宜连接的方式
我会在几秒钟内详细讨论这一点
但是不管怎样 您可以在Kinesis Data Analytics for SQL中应用SQL转换
应用于输入流
您只需提供一个SQL命令
它会在接收数据时应用该命令并将结果输出到输出流
还会输出到错误流
如果您需要的话
输出流可以是Kinesis Data Streams或数据管道
然后可以存储在Amazon S3中
或由Amazon Redshift查询
现在，这个参考表东西有点重要
这基本上是一种快速查找的方式，直接正确
因此，例如
假设我想要查找与一个邮政编码相关的城市
我可能会将那个映射存储在s三中
这是一个非常简单和经济的方式
向我的sql查询提供数据
我将在那查询中使用一个连接命令来引用那个外部参考表。
那就是坐在三个地方
因此，我们也可以将kinesis数据分析与lambda函数配对
它不一定非要直接发送到流中
你也可以将其发送到lambda
这将为您提供更多的灵活性以进行后处理
我可以将行聚集在一起
我可以将其转换为另一种格式
我可以进一步转换数据并丰富它
我甚至可以加密它 如果我想
我也可以与其他服务和目的地进行通信
这样不仅允许我与s3和红移等事物进行交流
我也可以去Dynamo
DB Aurora S SQS
Cloudwatch 无论Lambda能说什么
这些天几乎什么都能说
然而，AWS的热门服务Apache Flink
这在AWS中有一种较长的历史
最初这被称为Kinesis数据分析Java
然后他们将其重命名为Kinesis数据分析Apache Flink
现在称为Kinesis数据分析Apache Flink
尽管Kinesis数据分析总是使用Apache Flink
因此他们正在更加透明地展示其工作原理
而不是将其呈现为一种独特的工具
他们表示
我们所提供的只是一个在AWS中集成的开源工具的管理环境
这是与AWS集成的开放源代码工具
这是AWS近年来的趋势
它不再仅限于Java
现在支持Python和Scala
也 Flink 顺便说一句，只是一个用于处理数据流的框架
这就是为什么它一直是Kinesis数据分析的底层技术
它们只是不再隐瞒这一点
基本上，AWS管理的Apache Flink服务将Flink与AWS集成
而不是像之前看到的那样使用SQL
你可以从头开始开发你的整个
你自己的整个Flink应用程序，并将其加载到
管理的Apache Flink服务中从S3
所以基本上，你开发了一个Flink应用程序，它可以对你的流数据做任何你想要的事情
你将该应用程序加载到S3
然后你说嘿
托管服务为Apache Flink
这是我的Flink应用程序在S3中
加载并托管它
你处理那个服务的管理
所以我不必
除了用于处理流数据的数据流API之外
还有表API用于SQL访问
这就是我为什么认为啊
数据分析SQL可能即将消失
因为你可以用Apache Flink做同样的事情
使用那个表格API
而不是使用数据流API
这个概念看起来如何
哦，是的 它是无服务器的
显然嗯 因为它被正确管理
那无需多说
所以 我们有一个叫做flink sources的东西
在aws的背景下
这可能是一个kinesis数据流进来
或者它也可能是亚马逊管理的apache kafka流进来
如果你有kafka作为来源
作为源被管道到apache flink的管理服务中
我们的实际flink应用正在做一些事情
在这种情况下使用数据流api操作kinesis或msk
再次 实际应用是通过s3加载的
一旦你的应用程序完成
它需要做的任何事情 它可以继续与亚马逊的其他服务交谈
这些是flink同步
这可能是s3 kinesis数据流
或者kinesis数据火烈鸟用于输出转换的流数据
一些常见的用例
流etl
你知道我们已经讨论过了
如果你想做那个提取
转换和加载实时数据
一个flink应用可以做到这一点
你也可以使用它进行连续的度量生成
所以如果我想要监控一个数据流
并保持实时的统计数据
这可能是它的另一个应用
响应式分析
基本上是同样的想法
所以如果你要知道 如果我想要实时分析谁访问我的网站 或者这可能是实现它的一种方式
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/063_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p63 41. Kinesis Analytics Costs; RANDOM_CUT_FOREST.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们来谈谈Kinesis Analytics的成本模型
它是无服务器的
所以你只需为你消费的资源付费，没有其他
它会自动为你扩展
你不必担心 运行你的Kinesis Analytics应用程序所需的底层资源
然而它并不廉价
不像AWS提供的其他某些无服务器服务在我制作这门课程的时候
开发这门课程的过程中，Kinesis Analytics实际上占据了我产生的大部分费用
产生的费用
所以请谨慎使用，并确保在完成使用后关闭任何分析任务
一旦你完成使用
为了安全起见
就安全而言
你可以使用iam权限来访问您正在使用的流源和目标服务
因此您可以配置
iam允许您的kinesis分析应用程序与它需要通信的上游和下游服务进行通信
还有一个酷功能叫做kinesis分析中的schema discovery
这就是如何在sql中找到列名
它可以实际分析您在设置过程中从您的数据流中输入的数据。
所以你可以检查它是否起作用了
随着我们进入我们的实践练习，我们将看到这一点。
但是它很酷的是，它可以实际分析一个流入的流
在AWS控制台中
你可以看到它如何试图给那个数据赋予意义
并在其之上施加一个模式
你可以根据需要编辑和更正那个数据
值得讨论的还有被称为随机切割森林的东西
这是Kinesis数据分析提供的一个SQL函数
你可以在你的数据分析应用中使用它
用于在流中的任何数值列上检测异常
你可以看出亚马逊对此感到非常自豪，因为他们为此写了一整篇论文
如果你想的话，你可以去查看一下
这是用于识别数据集中异常值的一种新颖方法
这样你就可以处理它们 然而
你需要一个例子，他们在论文中给出了一个例子
那就是在新加坡马拉松期间检测异常的地铁乘客流量
他们有一个地铁闸机的人流数据流
和 他们正在使用随机切割力
来自动识别地铁系统的使用率的异常
这很重要因为
如果你曾经看到过关于尝试检测异常或离群值
在一串数据中
随机切割森林与kinesis分析是很有可能成为一个好答案
那么让我们深入实践例子 因为这样你会看到所有的东西在行动中会更有意义
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/064_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p64 42. Amazon MSK.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈msk
因为msk开始在数据和分析专业考试中出现
你不需要知道如何使用kafka
因为它相当复杂
实际上我不使用kafka
所以这里不会有实际的操作
但你需要了解msk是如何工作的
了解其架构以及与kinesis的区别
kafka是kinesis的替代品
下一节课会有关于kafka与kinesis的对比讲座
因为考试主要会尝试让你在卡夫卡和卡因西斯之间选择一个场景
基于某些场景
所以mk中的卡夫卡多到底是什么
所以msk是aws上完全管理的apache kafka服务
这允许你创建
更新和删除kafka集群
在那种情况下
亚马逊 msk会为你创建和管理kafka broker节点和zookeeper节点
你将在你的vpc中部署你的msc集群
它支持多实例，最多可达三份，以实现高可用性
有自动恢复机制来应对常见的Apache Kafka故障
数据存储在EBS卷上
想法是msk和a kafka是Kinesis的替代品
你在同样的地方
您的数据生产者和消费者
但是，kafka的力量在于你可以为你的集群创建一些自定义配置
例如，在考试中可能会出现的一个。
在Apache Kafka中，默认的消息大小是一兆字节吗？
但是你可以配置achi kafka能够发送和接收大消息
例如，通过进行一些自定义配置，可以将多达10MB的数据放入您的集群中
这与Kinesis有很大的不同
因为Kinesis消息的大小有严格的限制，最大1MB
从宏观角度来看
您会发现Kafka集群架构与Kinesis非常相似
现在我们来看看数据如何进入您的集群
您有代理，被称为Apache Kafka
代理和数据将在它们之间进行复制
例如， 我在此示例中展示了您在Caf集群中的三个代理
现在生产者可以是你代码的一部分
并且可以是你想要的任何内容
你可以创建生产者从kinesis获取数据
并将其放入kafka集群、物联网或rds中
等等 所以你只需要编写一些应用程序来截取一些数据
并将其放入称为kafka主题的地方
现在kafka主题是复制的
这就是你的数据如何在多个可用区中保存的方式
一旦数据写入kafka主题并进行了复制
然后消费者
这将是你的代码 将从你的kafka主题中拉取
然后消费者可以做他们想做的任何事情
他们可以做一些流处理逻辑
或者将数据发送到目标数据源
例如emr s three
sagemaker kinesis
rds 等等等等
所以你可以看到它与kinesis非常相似
API的差异不同
工具不同
Kafka将更加可配置
这将允许您绕过一些Kinesis的限制
在配置方面
您选择多个可用区
推荐三个或两个
然后您选择VPC和子网
这是一个私有集群
您选择代理实例类型
例如 在m五大型e中
C 两个实例 每个az的代理数量
您可以随时添加代理
最终给您
每个可用区一个zookeeper和一个az一个kafka代理
所以在这个例子中 嗯或者一、二每az
例如 在这个例子中 所以我们有三个zookeeper节点和六个kafka代理
并且有三个az在这个例子中
最后，您选择EBS卷的大小在1GB到16TB之间
这允许您根据时间要求保留数据多长时间
这可能给您带来更多的灵活性
嗯 你能看到数据流吗
好的现在
在Apache Kafka中，安全性非常重要
考试中可能会问你这个问题
让我们以一个有三个broker的集群为例
我们可以在每个broker之间使用TLS进行飞行加密
使用TLS
默认情况下已启用
但你可以禁用它
如果你为了性能改进而想要
您还可以获得客户端与代理之间的可选TLS加密传输
并且默认情况下，这个功能是启用的
但你也可以禁用它
出于性能原因
您将获得使用KMS对EBS卷进行静态加密
这是为了网络安全而启用的
您可以将安全组附加到您的客户端
以确保您代理与ECS之间的网络安全
二实例 最重要的是，它将围绕身份验证和授权进行
这是非常重要的
因此，您可以定义谁可以读取
以及谁可以向Kafka集群中的主题写入
因此，今天在Apache Kafka中有三种机制
第一种称为互 TLS
这意味着使用TLS证书进行加密
但也用于身份验证
这就是为什么我在身份验证中写了互 TLS
因此，如果您使用此机制
您必须使用Kafka ACL
因此，访问控制列表用于在主题级别进行授权
Kafka ACL是您Kafka集群内的一种机制
您还可以使用称为SASL SCRAM的东西
这很简单，用户名和密码用于身份验证
再次，您需要使用Kafka ACL进行授权
或者您可以使用访问控制 我是
访问控制允许您使用AM策略进行身份验证和授权
因此，这就是您在Kinesis数据流中获得的东西
因此，这也是您在Kafka安全中考虑的事情 好的
因此，您需要了解的一件事
虽然
是Kafka对于互 TLS和SCRAM无法使用IAM策略进行管理
他们必须在您的Kafka集群内定义 好的
接下来，我们有监控 因此，我们有CloudWatch指标
您将获得三个级别
我们获得基本监控以获取集群和代理级别的指标 增强型监控以获取更多
代理级别的指标或主题级别的监控
因此，对于您集群中的每个数据流
这将允许您获得更高级的主题级别的指标 您将获得Prometheus的开放源码监控
这允许您直接使用Gen X导出器或新导出器提取数据
您将获得CloudWatch Logs监控
这允许您获取Kafka集群的日志
您将获得CloudTrail监控
这允许您获取Kafka集群的API调用日志
使用Prometheus协议和经纪人日志交付可以实现向云服务器的数据传输
亚马逊提供免费服务，也可以帮助数据流
这就是关于MSK的全部内容
只需将其记住为Kinesis的替代品，且更具可配置性
您需要记住MSK
嗯，安全
因为在考试中会问到 下次讲座我们将讨论Kinesis数据流与亚马逊MSK
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/065_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p65 43. Amazon MSK - Connect.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以这里有一个关于msk的特性叫做msk connect
想法是在你使用kafka的世界里，你会有一个叫做kafka connect的东西
并且kafka connect是一个框架，用于从kafka获取数据
并将其放在其他地方
或者反之，从其他地方获取数据并将其放入kafka
当你在msk中使用kafka connect框架时
你可以使用msk connect
这将给你提供一个在aws上管理的kafka connect工人
具有自动扩展能力
这意味着你可以在msk connect上部署kafka connect的基础设施
然后你可以在msk connect上部署任何kafka connect连接器
作为一个插件
并且这些插件可以是
例如 将数据发送到亚马逊s3，亚马逊红shift
亚马逊open search Dbesium
等等 所以这里有一个例子，你有一个正在运行的msk集群
然后你将创建你的msk connect工人
感谢msk connect服务
并且你将部署亚马逊s3 kafka连接器插件
然后connect工人将从你的msk集群中拉取主题数据
然后数据将被写入到亚马逊s3中
我们没有管理任何基础设施，一切都是由msk connect完成的
并且只需要进行配置
并且这里有一个定价
这里有一个例子
你将支付每工人每小时11美分 当然，你拥有的数据越多
你就需要更多的工人
这就是msk connect的全部内容
仅作介绍，如果你在考试中看到它
我希望你喜欢它 我将在下次讲座见到你 谢谢
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/066_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p66 44. Amazon MSK - Serverless.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以这里是另一个非常短的讲座，称为msk无服务器
在这个情况下我们知道我们为kafka集群预留了资源并按小时付费
但是使用msk无服务器
我们在msk上运行apache kafka而不管理容量
msk将自动分配资源并扩展计算和存储
因此这对我们理解发生了什么变得非常容易
因此从用户的角度来看
你所需要做的就是定义你想要的主题以及每个主题有多少分区
然后你就可以在安全方面得到
所有集群的访问控制
这是价格示例
每小时每簇支付75美分
每簇每月支付558美元
每小时每分区支付价格
每个分区每月约1美元
每存储每进每出支付价格
这是msk的定价方式
也许更适合你的模式
好的 就是这样 我希望你喜欢 我会在下次讲座见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/067_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p67 45. Amazon Kinesis vs. Amazon MSK.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的
所以在考试中
你最有可能选择kinesis数据流而不是亚马逊和sk
因为ais希望你使用他们的产品
但重要的是，也许在一两个边缘情况下
你可能不得不说亚马逊和k是正确答案
而为了你知道这一点
你需要知道kenny see数据流和亚马逊msk之间的区别
主要的 在kenny is数据流中
每条消息的大小有1兆字节的限制
你不能超过它
在亚马逊k中
虽然默认是1兆字节
但你可以配置更高
例如10兆字节
这将是一个巨大的差异
如果在考试中你看到大消息
这将排除任何数据流答案或firehose答案
并直接进入亚马逊msi i
而不是流
在kinesis数据流中称为分区
在亚马逊kis中称为kafka主题
但它们在数据流中很相似
尽管你可以做分区
拆分和合并
因此可以增加或减少你的数据流的吞吐量
而在亚马逊msk中扩展稍微困难
你只能向主题添加分区
你不能从主题中删除分区
在加密方面
你可以看到流在飞行加密默认启用
而在亚马逊mk中
你可以选择发送数据为明文
因此没有加密或飞行加密
你可以获得kis数据加密
在安全性方面
你有授权和认证策略
在kinesis数据流方面
在亚马逊k中
你可以获得互认证书
和kafka授权
这是组合一
或csec认证
和kafka授权
这是组合二
或在mk中，你可以进行双重认证 所以，你必须选择其中一个选项
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/068_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p68 46. Amazon OpenSearch Service.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


那么我们来深入了解一下亚马逊的开放搜索服务
它以前被称为亚马逊的Elasticsearch服务
我们很快就会讲到它的历史
但这是一项非常有趣的技术，用于在大规模上进行分析和报告
在PB级别
有趣的是，尽管开放搜索
以前称为Elasticsearch最初是作为搜索引擎开始的
它本质上就是用于搜索的
最初它不再仅仅是用于搜索
实际上，现在它主要用于分析和报告
对于某些应用，它们实际上可以分析大量的数据集，比如那个
比Apache Spark快得多
对于正确的查询类型
Open Search可以在短时间内快速获取答案，是一个非常好的选择
在一个可能跨越整个集群的PB级大数据集上
让我们深入了解它的内容
那么Open Search是什么
让我们来了解一下
Open Search是Elasticsearch和Kibana的一个分支
这里有一点戏剧性
基本上，弹性和亚马逊之间存在许可纠纷
最初，亚马逊提供亚马逊Elasticsearch服务
这仅仅是在他们的服务器上运行的Elasticsearch产品
基本上，弹性公司并不喜欢他们以这种方式侵蚀自己的收入
因此，他们改变了Elasticsearch的许可协议
这迫使亚马逊开发自己的分支
他们自行维护并称之为Open Search
这一切发生在2021年9月
因此在2021年底
你可能会继续看到这种情况
在考试和其他地方，亚马逊Elasticsearch服务被称为亚马逊Elasticsearch服务
现在，他们称Elasticsearch为OpenSearch
Cabana实际上是一个可视化工具
它是我们称为Elastic Stack的一部分
所以Elasticsearch是Elastic公司更大生态系统的一部分
包括Elasticsearch
一个名为Cabana的可视化工具
以及用于数据摄取的工具称为Beats
但对于OpenSearch
它是Elasticsearch和Cabana的一个分支
他们现在称这个为开放搜索和仪表板
但主要信息是开放搜索本质上是一个搜索引擎
所以你可以发送一个json风格的请求并说
去索引这个文档，另一个json请求说
去搜索包含这些关键词或这些属性的文档
或者你可以做模糊匹配等事情
所以核心 开放搜索本身是一个非常好、可扩展且非常快的搜索引擎
它实际上是建立在一个开源解决方案lucene之上
所以我们有一种层次结构，elasticsearch是建立在lucene之上的
然后从elasticsearch分支出搜索功能
这真的很复杂
但这就是我们如何来到这里的
但本质上，open search 是 leucine 的一个可扩展版本
并且可以水平分布到集群中的多个节点上
然而，随着时间的推移，它已经包括了越来越多的工具
并成为了一个分析和可视化工具
open search 包的另一部分被称为仪表板
这是 elasticsearch 中称为 kibana 的一个分支，明白了吗，没错
基本上，它是一个用于查询和分析的可视化工具
可视化存储在开放搜索中的数据
这样你不必限制自己将文档信息存储在开放搜索中
我的意思是你可以用它来构建一个维基百科搜索引擎或类似的东西
但你也可以用它来存储半结构化数据
例如来自服务器日志的数据或类似的东西
如果你这样做
你可以使用仪表板来可视化这种数据
并创建你自己的小谷歌分析仪表板
从零开始
它还充当数据管道
所以他们意识到需要将数据大规模地喂入 elasticsearch 或 open search 中
并将数据输入到 elasticsearch 或 open search 中
你可以使用如 kinesis 和 kafka 等技术来实现这一点
在其他系统中使用 open search
在其他系统中使用 kinesis 来处理数据的摄入
从服务器将数据摄入到 open search 中
在 elasticsearch 世界中
他们有称为 beats 和 logstash 的技术来做到这一点
在考试中，当然
你可以期待他们谈论 kinesis 与 open search 的集成
与beats和logstash的集成相反
因为他们试图摆脱整个那个世界
但这就是开放式搜索的全部内容
基本上它是lucene
随着您向集群添加更多服务器，它可以无限扩展
所以我承诺会给你一个更详细的看板查看
以前称为cabana
这是一个看板的例子
只是为了给你一种直观的感觉
它能力的感觉
正如我们所说，您可以轻松地将日志数据导入到Kinesis中，然后导入到Open Search集群中
然后使用仪表板来可视化这些数据
您也可以使用仪表板作为某种好的
前端 或者UI来实际交互式地查询这些数据
并尝试对数据集进行特定请求
而不必在终端中使用curl等命令行
所以这就是仪表板的样子
只需将其想象为存储在Open Search中的数据的谷歌分析仪表板
再次
这是一个非常可扩展的解决方案
有些公司可能不愿意将他们的所有数据暴露给谷歌
或者他们的数据量太大，谷歌分析无法处理
对于他们来说 仪表板和开放搜索实际上是一个很好的选择
那么，开放搜索用于什么
如我所说，不再仅仅是搜索
尽管这仍然是一个非常好的用途
如果你需要构建一个网站搜索引擎什么的
Elasticsearch是一个很好的工具
它也是非常可扩展的
所以构建像维基百科这样的搜索是完全可行的
使用Elasticsearch或开放搜索
实际上 我现在还在说这两个词是同一个东西
但它也用于其他事情
比如日志分析
我们之前讲过 它非常适合这一点
也是基于传入日志数据的应用程序监控
这是非常快速和简单的方式来实时可视化正在发生的事情
作为数据从监控服务器流入
安全分析也是点击流分析
真的 这些都属于日志数据分析的范畴
尽管这确实是开放搜索开始填补的领域
那么，让我们进入一些具体的例子
尽管 全文搜索
Mirror web使用亚马逊Elasticsearch或开放搜索服务
以便使英国政府和英国议会的网页归档可搜索
并且使用亚马逊开放搜索服务
Mirror web索引140亿个文档
一个十亿文档的b，仅需337美元
并且每小时可以索引1.46亿个文档
这比之前使用的技术快14倍
正如你可能猜到的
我正在谈论案例研究
AWS会提供给你
我正在谈论的 他们谈论的如何令人惊叹
但实际上确实令人印象深刻 性能如此出色且价格如此便宜
对于日志分析
Adobe也使用亚马逊的开放搜索服务
并且它们为其开发者平台可视化大量日志数据
在峰值时，他们每秒接收超过20万次API调用
我的意思是 那里有相当惊人的流量
即使是对我来说 但是使用亚马逊的开放搜索
Adobe可以轻松查看流量模式和错误率
并迅速识别和解决任何潜在的问题
这一切都伴随着减少的操作成本
因为亚马逊为他们进行服务器维护和实时应用程序监控
Expedia是亚马逊的一个客户
他们使用亚马逊的开放搜索服务
用于应用程序监控和根因分析以及价格优化
亚马逊开放搜索使Expedia能够成本效益高地监控大量Docker日志
并实时识别和解决问题，易于扩展以适应额外的日志源
同时减轻运营负担
因为再次 这是一个安全分析的管理解决方案
亚马逊开放搜索允许您集中并分析来自您整个组织的事件
实时
您可以在收到来自多个来源的数据后立即索引和分析
立即并更快地发现和防止威胁，最后对于点击流分析
他们给出的例子是赫斯特公司
他们构建了一个点击流分析平台
使用亚马逊的开放搜索服务和亚马逊Kinesis流
亚马逊Kinesis Firehose
我们将很快讨论所有这些如何协同工作的
但是，它被设计用来每天传输和处理30太字节的数据
来自全球三百多家赫斯特网站
那里有很多信息
并且使用这个平台
赫斯特能够从网站点击中获取整个数据流
编辑者可以在几分钟内访问汇总的数据
因此它是大规模的接近实时
而且这是非常棒的东西，价格又便宜
我们来谈谈开放搜索
这里主要概念
所以基本上你需要考虑的有三种不同实体
在开放搜索的背景下
一个是文档
所以基本上开放搜索是一个文档存储和检索引擎
文档是你正在寻找的东西
这不仅仅是限于文本
你也可以将结构化的json数据放在那里
每个文档都会有一些唯一的文档id，你可以通过它进行搜索
但不会太久
也有一个与之相关的类型
类型将定义文档的架构和映射，这些文档代表同一种类型的事物
例如日志条目类型或百科全书文章类型
基本上，它定义了你期望与该类型文档相关的架构
然而 类型正在消失，我们正在转向每个索引只有一个类型
在稍后的elasticsearch版本中
并且在预期的后续open search版本中
它们将完全被消除
事实上 我们已经在新的UI上进行了开放搜索
你会发现类型字段被灰化
因为他们甚至不想让你再考虑类型
所以今天我们更多地谈论索引而不是类型
一个索引将使搜索能够遍历集合中所有类型的文档
索引还包含一个逆索引，可以让你在该索引中搜索所有内容
一次性完成
在开放搜索中，每个索引只有一个类型
所以预期的结构是，对于任何给定的文档类型
你将为那种类型有一个特定的单独索引
最终类型的整个概念将消失
你可以将你想要的任何数据放入你的json数据中
再次 类型很快将成为过去
你真的只想考虑文档和索引
但如果你在考试中看到关于elasticsearch六的东西，仍然类型
在那个上下文中仍然可能是一个东西
但我肯定他们会很快更新考试
如果他们还没有谈论类型
一切都怎么样
一个索引 再次是一个与某些事物相关的文档集合被分成片
所以这基本上是直接的水平扩展的东西
对 基本上每个文档都被哈希到一个特定的片
并且每个片可能生活在集群中某个地方的不同节点上
有趣的是，OpenSearch中的每个片实际上都是自己的
包含的莱文索引
所以每个片都是自己的小迷你搜索引擎
现在挺酷的
冗余工作方式与你预期的一样
所以在这个例子中
我们有两个主分区和每个主分区两个副本分区
它工作的方式是写请求将路由到您的主分区
然后复制到
然而，您指定的多少个副本请求
无论可以来自主分区或副本分区
这取决于您的应用程序
实际上进行轮询
尽量分配那些读请求的负载
这样你就不会把所有的负载都加到主分片上用于读操作 你也可以利用这些副本节点来扩展你的读吞吐量
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/069_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p69 47. Amazon OpenSearch Service, Pt. 2.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以我们已经讨论了elasticsearch的架构组件
它与亚马逊开放搜索的共同之处
让我们深入到实际的开放搜索服务本身
并且讨论这实际上与elasticsearch有何不同
因此，开放搜索有几种不同的风味
这是管理的版本
这就是我们在接下来的幻灯片中将要讨论的内容。
也有无服务器版本
现在完全无服务器的选项
我们将在几分钟内讨论这一点
但是现在我们要讨论的是完全自动管理的解决方案
有两个不同且并行的开放搜索宇宙
如果你选择的是管理的解决方案
就像RDS或EMR那样，它是管理的
然后有一个完全无服务器的解决方案
在管理的解决方案中，你不需要考虑底层服务器
然而，你可以在不停机的情况下进行扩展和缩减
这意味着你可以更改你的开放搜索集群中的主机数量
但你仍然需要考虑节点的数量
在管理的解决方案中，这不会自动为你完成
按需付费
所以你只需根据实例运行时间付费
以及你使用的存储量
以及数据传输量
即使你什么也没做，也会产生费用
如果你有一个开放的搜索集群
它处于闲置状态 你也会为这些实例运行时间付费
所以记得关闭它
记住关闭它
如果你不用它 或者你会产生一笔不小的费用
是的
它提供了很高的网络隔离
所以使用亚马逊VPC
你可以通过在休息和传输期间使用密钥加密数据来确保你的数据安全
你可以通过亚马逊Cognito或AWS IAM策略来管理身份验证和访问控制
这里有很多集成点
它们可以通过Lambda与S3桶连接
与Kinesis连接
它们可以与Kinesis数据流集成
它可以与DynamoDB流集成
当然，它也可以与CloudWatch和CloudTrail集成
这些都是对Elasticsearch的扩展，显然在AWS中并不存在
这不是AWS特定的Elasticsearch版本
这就是他们添加的内容，以便将其集成到AWS生态系统中
它还提供了区域感知功能
通过在相同地区的多个可用区之间分配区域
您可以通过使用区域感知功能来提高高可用性
然而，这可能会导致延迟
这里有一些你可以期待的问题
这些问题比较具体于亚马逊的实现
一个是你想要多少个专用主节点
你必须选择多少个以及你想要使用哪种实例类型
现在 主节点是用于管理你正在创建的open search域的
它不存储或处理任何数据
所以通常你不需要太多
除非你的集群非常大
什么是域，再次
这基本上是一个亚马逊特定的事情
这是一个亚马逊开放搜索服务领域
这是一个开放搜索集群所需的所有资源的集合
所以它包含整个集群的配置
所以基本上在亚马逊开放搜索的术语中，集群是一个领域
它还允许您启用自动快照到S3以进行数据备份目的
所以如果您无意中关闭了您的集群
您不会丢失那个数据
就像我们谈论过的那样
区域意识也是一个选项
如果你想以更高的延迟为代价提高可用性
安全性在考试中总是一个重要的点
所以让我们谈谈亚马逊的安全系统如何与亚马逊开放搜索集成
我们将深入探讨这一点 当我们更多地进入练习时
它允许基于资源的策略
因此，您可以将这些附加到服务域中，该域确定原则可以在开放搜索API上采取哪些操作
您还可以使用IAM策略进行基于身份的策略，或基于IP的策略，将特定操作与特定IP范围绑定
基于身份的策略
使用IAM策略或基于IP的策略
你也可以对你的请求进行签名，以便发送到亚马逊开放搜索
实际上，所有发送到亚马逊开放搜索的请求都必须签名
当你使用AWS SDK发送请求到开放搜索时
这将为你提供所需的数字签名请求的方法
否则，所有流量都将是未加密的JSON数据
这显然不是一个非常安全的方法
当你通过互联网发送数据时，为了进一步安全
你也可以将集群放在一个VPC中，而不是使其公开
这将为你的开放搜索集群提供额外的安全
而不是使其对外界可访问
总之 当然
这会使得你连接到你的集群并使用像仪表板这样的工具变得更加困难
但我们将在下一张幻灯片上讨论这一点
请记住，你不能先在一个VPC中设置你的集群
然后再将集群移出VPC
或者反过来 你必须提前决定
你的集群是生活在VPC中还是公开可访问
你不能在后期更改这一点
最后，它集成了Cognito服务
主要这对于谈论如何访问Dashboards非常有用，Dashboards以前被称为Cabban
所以，事情是 如果你在你的Open Search集群中托管在一个VPC中
你如何通过互联网访问它
通过网络浏览器
你通过Web界面访问Dashboards
所以它需要打开一个HTTP连接来访问你的集群
以便能够查看和与Dashboards进行交互
处理这个问题最简单的方式是使用Cognito
AWS提供了一个Cognito服务集成
允许用户通过企业身份提供者登录Dashboards
例如Microsoft的Active Directory
也可以通过社交媒体身份提供者如Google或Facebook 使用Samuel2.0
所以你可以设置，Cognito允许人们使用他们的Facebook账户登录
即使你的Dashboards隐藏在VPC后面
你还可以设置安全的，扩展的，简化的登录体验
使用Amazon Cognito用户池
这使得管理这些更加容易
即使你的Dashboards隐藏在VPC后面，Cognito也能让你登录
你也可以设置安全的，扩展的，简化的登录体验
使用Amazon Cognito用户池
这使得管理这些更加容易
但是要从外部访问VPC仍然很困难
有一种方法是使用反向代理服务器
这就是我们在这里的图表中展示的
在右边这里
Ninx是一个反向代理服务器的例子
你可以在某个EC two主机上运行它
然后它将请求转发到你的Open Search域
在VPC中
你也可以在端口50601上打开SSH隧道
这是Dashboards监听的端口
其他选项包括VPC Direct Connect 有一些反模式和应该避免使用Amazon Open Search服务的方式
这些直接来自AWS大数据白皮书
我再次强烈建议你阅读 一种就是OLTP
尽管它非常快
但它并不是为了像Web服务那样被访问设计的
所以它没有像真正的数据库那样的事务支持 如果你打算这样做，RDS或Dynamo会更合适
有一些反模式和应该避免使用Amazon Open Search服务的方式
尽管它非常快
但它并不是为了像Web服务那样被访问设计的
所以它没有像真正的数据库那样的事务支持
如果你打算这样做，RDS或Dynamo会更合适
有一些反模式和应该避免使用Amazon Open Search服务的方式
Db可能是一个更好的选择
Ad hoc数据查询
嗯 我不太确定这就是为什么这是一个反模式
因为你肯定可以使用仪表板来实现这一点
因为aws更希望你使用athena来进行那种操作
因为嗯，那就是那个服务的真正用途
但请记住，亚马逊开放搜索主要是用于搜索和分析
那就是亚马逊开放搜索主要适合的桶
所以如果你在寻找一个搜索或分析解决方案的问题 亚马逊开放搜索可能是那个问题的潜在解决方案
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/070_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p70 48. OpenSearch Index Management and Designing for Stability.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


亚马逊通过添加冷数据的不同存储类型扩展了elasticsearch的一种方式。
温暖和热的存储
让我们谈谈那件事
因为他可能会被问到这个问题
因此，你的标准数据节点默认使用所谓的热存储
这就是你的数据节点上的存储
那就是在节点里 要么通过实例存储，要么通过EBS弹性块存储卷
这将给您带来最快的性能
它就像 你知道 在EC two实例上的正常存储
C 两个实例 所以这是默认的，你所期望得到的
但这也是最昂贵的
所以作为替代方案
他们提供了一种新的称为超冷存储的东西，超冷存储
在文档中，他们几乎交替使用温暖的术语
这实际上使用S3来备份数据
再加上一个缓存层，让它比温暖还要温暖，非常温暖，非常
也许超温暖 我猜他们那里叫它
超温暖存储对于索引没有太多读写的情况是最佳的
所以对于不可变的东西
像日志数据
超温暖存储可能是一个很好的选择
它的性能较低
但是它的成本要低得多，因为你使用的是S3而不是本地存储
为了使用超温暖存储
你需要有一个专门的主节点
在实际操作中，你通常会有三个主节点
但我们稍后会详细讨论这一点
你不能没有一个主节点的独立集群
如果你想使用不同类型的存储
如果你想节省更多的钱
现在也有冷存储可用
它也使用s3进行备份
而且更便宜
这是为了像定期研究或对旧数据进行法医分析的用途而设计的
例如 如果你有非常旧的日志数据，你对它不再那么关心
冷存储可能是它再次的好去处
你必须有一个专门的主节点
你也必须启用超温
以便使用冷存储
冷存储的一个陷阱是它目前与两台t不兼容
两或三个数据节点实例类型
如果你在使用细粒度访问控制
你需要在open search仪表板中将用户映射到冷管理角色以便正确使用它
所以这里有一些实现的细节
你也可以根据需要迁移你的数据到不同的存储类型
正如我们将看到的 有方法可以自动迁移数据从热存储到温存储再到冷存储随时间推移
如果你愿意 这样你不必提前决定你的数据想要哪种存储类型
你可以根据需要，在你索引之间移动数据
这引导我们到索引状态管理
这更多是一个Elasticsearch的特性
你知道 我们可以花几个小时甚至几个小时
谈论Elasticsearch的细节和深度
我有一个关于它的课程，持续永远
但我们只会谈论足够的内容让你通过考试
索引状态管理或ISSM的想法是自动化索引管理策略
所以这是什么意思
例如 如果你有一些旧的索引你想要在特定时间后删除
也许你有按月分割的索引
你只关心过去三年内的数据
你可能有一个索引状态管理策略自动删除这些旧的索引
在特定时间间隔后
你也可能用它将索引移动到一个只读状态
在特定时间后
你可能需要为了合规原因
确保你不在事后修改数据
就像我们之前说的
你也可以用这个自动将索引从热移动到超温
再到冷存储随时间推移
如果你知道你访问数据频率会越来越少
它变得越老
可能明智的将索引自动移动到越来越冷的存储中
以节省一些钱
你也可以用索引状态管理减少你的复制账户随时间推移
如果你有一个索引，随着时间推移你关心越来越少
你可以减少它的复制账户
太 如果你不在乎随着时间的推移失去这些数据
你也可以用它自动索引快照
你知道 备份你的索引并放在某处
ISSM策略每30到48分钟运行一次
这是一个奇怪的数字 不是吗
之所以有这个范围是因为有一个随机抖动引入到它
以确保你不在同一时间运行索引数据
这可能会影响你集群的性能
你也可以设置索引状态管理发送通知一旦它们完成
而且这有点特定的亚马逊是
你可以通过亚马逊奇米发送通知到一个房间
让系统知道政策已经生效
更多索引管理内容
嗯 索引汇总
所以你可以定期将旧数据汇总成摘要索引
这样可以节省大量存储成本
如果在一定时间后
你不再需要详细信息
但只需要摘要信息
这是一个节省资金的好主意
因此新汇总的索引可能包含较少字段
也许你会丢弃一些你不关心的字段
在一定时间后或更粗的时间间隔
这是另一种方式 你可以合并东西
索引转换也是可用的
与汇总类似
但这里的目的不是汇总数据
而是以不同方式进行分析
以创建不同视图
你可能在那里进行分组和聚合
例如 我可以按某些标识符分组我的所有数据
并创建一个按我关心的单独字段分组的独立视图
或者我可以进行聚合
也许我对该组的总和感兴趣
或该时间段内的总和
或平均值
无论它是什么
索引转换可以自动为你创建那些新视图
我们还要谈谈跨集群复制在索引管理中的应用
你可以在域之间复制你的索引及其映射和元数据
这在几个方面很重要
首先 这确保了在故障情况下的高可用性
如果你将所有数据复制到另一个集群
位于另一个数据中心
或者甚至另一个地区
如果这个地区或数据中心出现故障
你将会在其他地方拥有那个备份副本
因为你正在复制它
这也是降低延迟的一种方法，如果你有一个全球应用
你可以将你的数据地理上复制到世界各地
使用复制以获得不同地理位置的更低延迟
如果你有人试图从欧洲访问你的open search集群
最好在欧洲有一个副本
而不是让他们跨越大洋
去访问位于美国的集群
或者它是这样工作的，
你设置一个称为追随者索引
这是从他们所谓的领导者索引中提取数据
所以领导者索引基本上是你的主副本
跟随者将是复制品
他们使用的术语
要使用跨集群复制
你需要在你的open search集群上启用细粒度的访问控制
以及节点到节点的加密，以确保我们可以安全地这样做
还有一个类似的功能
叫做远程重新索引，它允许您从一个集群到另一个集群按需复制特定索引
所以如果您不需要复制整个集群本身
连同其所有映射和元数据
远程重新索引允许您在任何需要时将索引从一个集群复制到另一个集群
这可能是您需要了解的Open Search稳定性的一些注意事项
首先
重要的是要有至少三个专用的主节点
这是最佳实践
如果您有一个 它可能会崩溃
然后你就完了 你处于停工状态
正确 你不能只有一个主节点
因为那是一个单点故障
为什么不能有两个呢
两个主节点的问题
是你可能会进入所谓的分裂大脑状态
并且可能会出现两个主节点同时运行的故障模式
基本上没有人知道哪个是真正的权威主节点
所以有三个主节点
第三个节点将决定谁是真正的主节点
所以，至少拥有三个主节点
可以避免脑裂故障模式
可以避免只有一个主节点的单点故障
这被认为是最佳实践
也要确保不会出现磁盘空间不足
这实际上是
开放式搜索和elasticsearch集群通常见的一个问题
所以你得算一下
确保你为存储分配足够的存储容量
我还没有听到关于考试进入这个细节级别的报告
但这是他们喜欢询问的一类问题
所以，如果你需要为一组数据估算存储需求
一种粗略的方法是
你需要考虑源数据的大小
即你试图存储的数据量
加上副本的数量
所以，显然你需要考虑所有数据存储在主集群上
以及你可能复制到的任何副本
用于备份或降低延迟
就像我们讨论过的，乘以1.45
这就是存储数据的所有额外开销的估计
你还需要选择正确的分片数量，这也是一门科学
而且 不幸的是
这是你必须通过实验来找到正确平衡的事情
但他们也提供了一种方程式
至少可以让你大致了解
这里的想法是将源数据的大小
加上你想要预留的增长空间
你知道我想要假设多少额外的容量
为了随着时间的推移数据增长，乘以索引开销
那么索引开销是什么
嗯 它不仅仅是存储你的数据
它还需要存储关于如何索引数据的信息
这会占用额外的空间
通常这会增加大约百分之十的空间
所以大致你可以用这个作为估计
尽管你可以实际查询你的索引来得到确切的数字
有几条命令你可以发送给它
给我这个索引的精确大小
然后你可以拿这个数据与数据本身的大小进行比较
然后计算出你特定索引的实际索引开销
如果你需要，但十美分是一个合理的猜测
然后除以你希望的分片大小
你放入每个分片的数据量
这将给你一个估计你需要的总分片数量
现在保留你的数据
在某些罕见情况下
你可能也需要限制每个节点的分片数量
然而 在实践中 这通常不会成问题
我通常在节点每片空间耗尽之前耗尽磁盘空间
同样，你需要明智地选择实例类型
你需要至少三个数据节点，原因与以前一样
一个是单点故障
两个你可能会进入奇怪的状态
至少三个是最佳实践
你只需要考虑你的需求
所以再次 打开搜索功能非常占用存储空间
所以你会思考
主要是如何选择有足够存储的节点
一些例子包括m6g.large.search
这些都是点搜索实例类型
如果你需要更大的
i3.4xlarge.search
如果你实际存储的信息达到PB级别 你可能选择使用i3.16xlarge.search实例类型
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/071_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p71 49. Amazon OpenSearch Service Performance.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


关于优化亚马逊开放式搜索集群性能的几点快速笔记
需要考虑的一点是JVM的内存压力
有几种情况可能导致内存压力不平衡
那就是在你的节点上，索引的分片分配不均匀
如果你有一些分片承担了大部分工作
而另一些分片则几乎无所事事
这可能会导致工作量较大的分片内存压力过大
当你的内存耗尽时
就会出现问题
你会开始将内存交换到磁盘，性能会急剧下降
因此这可能会导致性能问题
集群中索引太多也会造成麻烦
分布式计算的一个基本真理是，少即是多
分发处理有时比想象的要多
特别是需要更多的内存来管理
所以从这些中得出的结论是
如果你在日志中看到JVM内存压力错误
通常可以通过减少索引来获得更好的性能
减少索引的方法是通过删除旧的未使用的索引
如果你能做到
如果有一种方法可以将您集群中的数据卸载到某些归档中
您不再需要这些数据
也许您可以将数据从冰川卸载或做一些其他事情
并删除与那些旧无用数据相关的索引
这可能是解决JVM内存压力错误的好方法
或者您可能有些从不使用的索引
删除它们也会有所帮助
是的 这是避免JVM内存压力错误的快速说明 特别是在亚马逊开放搜索中
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/072_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p72 50. Amazon OpenSearch Serverless.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


关于更近的无服务器版本开放搜索的几句话，请记住
这仅在2023年1月被引入
新科技通常需要至少一年的时间才能在考试中出现
如果你在2024年之前看到这段视频
你可能不需要太关注它
但无论如何你应该知道它的存在
你知道 如果你真的要将你所学的应用到现实世界中
希望这是你的目标 Which hopefully is the goal
你应该知道无服务器搜索服务器是一个现实
它可以让你的生活变得更容易
它提供按需自动扩展
所以与无服务器搜索的管理版本不同
你不必考虑底层服务器的数量
它会为你管理
在使用无服务器版本与管理版本时，主要区别是
你不再需要考虑域名
你考虑集合
这只是你如何组织你的索引的另一种方式
你可以创建两种不同类型的集合
搜索集合或时间序列
集合类型 这两种不同类型的集合
开源的无服务器总是加密的
你可以给它一个kms密钥
它会处理其余的
你可以强制执行自己的数据访问策略，并且始终需要加密的存储
它总是与无服务器版本一起存在
你可以一次性配置多个集合的安全策略
与需要针对每个单独的域名设置单独策略的管理版本不同，服务器less版本稍微容易一些。
在更大规模上使用时，稍微容易一些。
无服务器版本的OpenSearch的容量以OpenSearch计算单位（OCU）来衡量。
你可以设置上限来控制成本。
如果你想要设置下限，
但是，索引和搜索服务器的数量总是为2。
超出这个设置，
设置和使用它和管理版本几乎相同。
你只需访问OpenSearch控制台。
现在你会看到两个不同的部分之间
在左边的菜单上
一个是为管理版本准备的，另一个是为无服务器版本准备的
我设想随着时间的推移，他们可能会更多地推广无服务器版本
但是至少直到录制为止
它 嗯 它还是一个新事物 所以我们会看看它会怎样
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/073_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p73 51. Amazon QuickSight.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们深入亚马逊快速站点
这是AWS用于大数据的可视化工具
什么是快速站点
嗯 来自亚马逊的官方定义是
这是一个快速、容易、云驱动的业务分析服务
让我们分解一下
所以是的 它本质上是一个用于可视化数据的服务
而快速和容易云驱动的部分意味着它可扩展
它不仅仅是为开发者设计的
它实际上是一个网络应用程序
组织中的任何人都可以使用它来查看仪表板
或创建自己的仪表板
创建自己的图表和图形
他们需要做的任何事情
解释数据并从商业角度寻找趋势
它是为那些对数据熟悉的人设计的
但不一定是开发者
它允许组织中的所有员工
不仅仅是开发者，在数据上构建可视化
你也可以为管理层创建分页报告
这是快速站点的新功能
相信吧 你可能会认为它应该存在
但他们最近才意识到这一点
你可以用它进行即席分析
它可以运行在你数据上
湖 你的数据仓库
无论你的数据在哪里
我们会稍后讨论那些来源，并实时可视化数据
随着它的变化 如果你想深入研究最新数据并了解其行为
趋势是什么
这对业务意味着什么
你可以快速设置新的可视化
查看数据 解释它
试图理解它 试图将其应用于业务
你也可以收到检测到异常的警报
它有一个功能可以自动监控你的数据
并说 嘿 这里有些不寻常的东西
也许你应该亲自处理一下
你也可以更广泛地使用它来快速从数据中获得业务洞察
这就是快速站点的全部意义
这就是快速部分 我猜测
所以这应该是一个相对简单的方法来可视化你的数据
你不一定需要知道
SQL 尽管它有帮助
但是有一些特性会自动尝试创建一个合适的可视化
仅仅通过检查数据并试图自己找出答案
所以了解SQL可以帮助你不需要
尽管 并且它适用于任何设备
所以即使你是CEO
如果你想在你的手机上查看最新的商业仪表板
Quick Site可以成为前端
并且它是完全无服务器的
当然你知道我们不想让你的CEO
担心驱动Quick Site的服务器数量
它都会为你自动扩展和缩小
有很多数据源
Quick
我可以从 所以您想要可视化的数据在哪里
这可能是您作为开发者需要担心的事情
显而易见的选择是Redshift
你可能有一个庞大的数据仓库
当然
并且你只是想在数据仓库之上制作一些图表、图形和报告 Quick Site可以做到这一点
当然，你可以在Aurora数据库或RDS数据库上运行 所以如果你运行一些
你知道MySQL实例或其他类型的SQL数据库
你可以在Quick Site之上运行它
Athena也是一个受欢迎的选择
所以如果你有一个大的数据湖在某个地方
你可以在Athena之上运行它
并且在你的数据湖中通过Athena可视化数据
这是一项很酷的能力
没有真正的数据库
你可以通过Athena查询S3中的无结构数据
并且使用Quick Site可视化它
没有真正的数据库
你不仅可以通过Athena查询S3中的无结构数据
也可以使用Quick Site可视化它
它也可以与OpenSearch交谈
所以这是一个稍微有点冷门的用例
但你可以在OpenSearch数据库之上运行Quick Site
如果你愿意
人们通常会把他们的
嗯 服务器数据
将他们的服务器日志数据导入到开放搜索中
因此，如果您想可视化这些数据并查看诸如服务器延迟之类的信息
或者你知道
响应代码随时间的变化
你也可以使用quickside进行此操作
它还与iot analytics集成
互联网事物 尽管aws近年来并没有过多强调这一点
如果您有一个e
托管数据库
也许你可以在自己的EC two实例上运行MySQL实例
也可以在旁边快速访问
也可以直接使用原始文件
你可以使用CSV或TSV文件
使用逗号分隔或制表符分隔的文件
甚至Excel文件
甚至常见的日志格式
它们可能存储在S3上或本地
你知道，我可以快速访问的地方
我以某种方式可以访问
并且可以直接可视化
在你分析或可视化之前，甚至可以准备数据
如果你想在短时间内有限地做到这一点
所以，如果你需要对文件中的列名进行更改
你可以这样做 更改字段名
添加计算字段
你可以使用SQL查询
你可以更改数据类型
如果你需要在可视化之前对数据进行一些处理
你可以在quickside内完成
也
quick site的一个重要部分是香料
而这就是它用来加速查询的诀窍
香料代表一个超级快的并行内存计算引擎
所以它是将你的数据从athena复制到一个更优化的格式
以便从quick site获得更快的查询
所以，它的想法是在内存访问机器代码生成中使用列存储
目的是加速在大数据集上进行交互式查询
现在，这会占用一些空间
当然，存储容量
它基本上需要将您的数据从athena复制到
这个更紧凑和优化的表示形式，该表示形式由spice使用
每个用户默认获得10GB的spice
它被承诺是高度可用和高度耐用的
就像s3一样
它可以扩展到数万个用户
他们已经使其变得耐用和可扩展
所以spice的想法又是加速您的查询
但它是额外的一层
所以你必须考虑这一点
有时候是如此
尽管它可以加速通常超时的大型查询
如果你使用直接查询模式
直接查询模式是另一个词，直接访问athena
嗯 你可能会遇到查询太大
即使是对香料来说 所以您可能会遇到的一种情况是
如果从athena导入数据到香料需要超过30分钟
你还是会超时
所以甚至有那些设计来加速大查询的香料，通过快速查看
那里仍然有一个上限
你可能达到的，所以那里有些东西要意识到
但请记住
香料是超级快的
并行内存计算引擎，存在是为了给你一个更优化的数据表示
并通过快速查看加速你的交互式查询
尤其是在大数据集上
但你知道，有可能太大，甚至香料也无法处理
以下是亚马逊推荐的快速站点的一些用例
一个是对数据的交互式即兴探索和可视化
所以你可能记得，这实际上是明确地其他服务的反模式
然而，快速站点从底层设计就是为了即兴探索和可视化您的数据
你也可以用快速视图创建仪表板
你可以制作这些漂亮的仪表板
以定期的方式可视化数据
并确保您的高级管理人员可以看到这些并了解业务状况
分析多种来源的数据进行可视化也非常有用
包括S3日志
任何您可能在本地拥有的数据库
我们之前列出的其他aws服务
Rds Redshift Athena n s three
您还可以将quick site与像salesforce这样的东西集成
这可能对您了解任何软件有用
作为服务应用程序可能可以与quick site集成
以及任何jdbc或odbc数据源
所以几乎任何您想象的数据源
快速站点可以导入它并让您非常快速地可视化它
并且大规模
然而 但有些事情quickside并不适合
尽管它有有限的etl能力
如果您想在数据上进行更强大或更大规模的etl转换
quick sight真的不是那个工具您想使用glue etl相反
或者也许apache spark
但是再次 quick site确实有一些有限的转换
你可以做任何事情，只是为了让你的数据从安全角度来看更好一点
它为访问快速站点的帐户提供多因素身份验证
记住 这更像是一个终端用户工具
所以你不会使用低级别的安全功能
它确实提供了VPC连接
然而 因此，您可以将快速站点的IP地址范围添加到您的数据库安全组中
您将不得不担心快速站点如何与您的数据库通信
并通过他们上的安全措施
它还提供了行级安全或RLS
这使快速站点数据集所有者能够根据与数据交互的用户相关的权限控制对数据的访问
基于权限
并且与RLS一起
快速站点用户只需管理一个数据集
并在2020年底为其应用适当的角色级数据集规则
他们还添加了列级安全或CLS
这仅在快速站点的企业版中出现
但它确实允许您在行级和列级管理安全
您还具有私有VPC访问权限，这是通过弹性网络接口或ENI实现的
或E I
以创建与数据源的安全私有通信
在VPC中
这也使用AWS Direct Connect以创建与您本地资源之间的安全私有链接
因此，如果您想在本地数据库或文件上使用快速站点
您可以使用Direct Connect来实现
再谈谈快速站点的安全
那么您如何管理资源访问呢
这很简单 真的，从快速站点控制台
有一个管理快速站点菜单
并从那里您可以转到安全与权限
并从那里您可以确保快速站点本身被授权使用Athena S3
特别是您的S3桶
因此在到达用户级别之前
您必须确保快速站点本身被授权访问您想要使用的数据
无论是Athena还是S3
并且再次，这可以从快速站点控制台管理
如果您需要限制特定用户可以访问的特定用户
您可以创建IAM策略以限制给定快速站点用户可以访问的S3中的数据
所以请记住，您需要确保快速站点本身被授权使用您的数据
并且您还可以需要通过IAM策略控制用户级别上的更细粒度权限
因此，快速站点本身您可以通过快速站点控制台管理
但如果您需要更细粒度的用户权限
IAM可以用于此
所以还有快速站点和Redshift之间的特定问题
从安全角度来看
通常而言
并且一般而言
Quicksite just tries to handle all this stuff under the hood for you
Because it's really aimed more at end users and not aws express
But this is one area that can trip you up
So the thing is by default
Quick side can only access data stored within the same region
As the one that you're actually running quick site within
So you could have a situation where you have an existing redshift cluster running in some region
Like you know japan or something
But you actually spun up quick side in virginia
And in that case you're going to get errors
因为那两个区域无法相互通信
你可能认为你可以创建一个VPC并配置它以跨区域工作
但这实际上不能与Quick Sight一起工作
如果你有这种情况
修复的方法是创建一个新的安全域，添加一个入站规则
允许来自该区域Quick Sight服务器的IP范围
我正在强调这一点，原因在于
伙计们 这些范围幸运的是有文档记录
你必须查找它们 但这是你必须采取的一步
如果你想要快速访问站点
位于不同区域的红移集群
设置一个新的安全组，添加一个入站规则
包含快速站点服务器的IP范围
你必须查找这些IP范围的详细信息
如果你使用的是快速站点的企业版
有处理跨区域问题的其他方法
从位于不同区域的快速站点访问红移或RDS数据
问题
而且这些技术在某些情况下也适用于跨账户访问，所以再次
如果您有企业版
那么您就有在VPC内创建私有子网的能力
然后使用弹性网络接口将QuickSight放入该子网
在您的VPC内
现在从私有子网中访问QuickSight是需使用企业版才能完成的部分
但是一旦您这样做了
所有连接子网之间的技术，无论是跨区域还是跨账户，都会对您可用
所以让我们从那里继续深入
所以这是步骤一 无论你如何做
你需要将快速站点放在一个私有子网中，在一个VPC中
使用弹性网络接口
或者e 我 简称e
进行跨区域访问的一种方法是，将RDS或Redshift
或者你的数据放在一个另一个私有子网中
然后将它们通过私有连接连接起来
快速网络使用对等连接
好的 基本上，你需要将quickside放入私有子网中
你有RDS或Redshift在内部私有子网中，可能位于不同账户
可能位于不同地区
但你仍然可以使用跨区域连接来获取跨地区的访问权限
这是考试喜欢提问的内容
所以请尽量理解这一点
这不仅对跨区域访问有用
在某些情况下，也可以用于跨账户访问
如果你的目标是跨账户访问
还有其他几种方法
基本上，如果你用AWS传输网关替换跨区域连接
这也是连接两个不同子网的另一种方式
但是传输网关本身仅限于在同一个组织区域中
连接你要连接的东西
但是，你可以将传输网关配对
这可能是连接东西的实际方法
如果你想要跨账户访问，并且那些账户在同一个组织，同一地区
否则，你最好直接使用跨区域连接
另一种方法是使用AWS私有链接
并且基本上，这将在顶部图表中用AWS私有链接替换私有链接
同样的想法
另一种技术是使用VPC共享
你可以在这里的右下角图表中看到
这是两个私有子网
一个是quicksite 一个是你的数据源
然后使用VPC共享将它们连接起来
这是为两个不同账户提供访问权限的一种方式
你可以在这里的图表中看到关于账户A和账户B的内容
这就是这些方法的目标
记住 这些都是处理跨区域和跨账户访问的方法
如果你的目标是跨区域访问
跨区域连接可能是一个好方法
但如果你想要跨账户访问
传输 网关
私有链接 或者VPC共享可能根据你的要求是可行的
谈谈quicksite的用户管理
你实际上如何管理 谁对它有访问权限
你知道，AWS试图使quicksite和用户友好
你可以在IAM中定义用户，并以这种方式进行管理
这给你更多的严格控制，关于个用户可以访问什么数据
底层S3桶
Redshift数据库
等等
或者你可以配置它，让人们通过电子邮件注册并得到批准
所以，如果你对此感到舒适，这是一个更简单的设置方式
重要的是要理解，在quickside中也有活动目录集成
有一个叫做活动目录连接器的东西
这仅在quick side企业版中提供，再次
在活动目录连接器的帮助下，尽可能简化设置
所有密钥都由aws管理
在quick sight中，你不能使用客户提供的密钥与活动目录一起使用
请记住，活动目录支持现在仅在企业版中可用
你可以设置不同的用户组，这些组对应于活动目录用户组
并通过iam调整他们的安全访问权限
如果你需要 所以你仍然可以通过活动目录设置用户组
并通过iam调整他们可以访问或不能访问的内容
所以，这里的要点是，安全体系的基础起点
quick site是基于iam的，你可以使用它来定义用户可以做什么或不能做什么
电子邮件注册是一个选项
如果你使用的是企业版
你可以使用活动目录连接器
但请记住，所有密钥都由aws管理
你不能使用客户提供的密钥 并且它仅在企业版中支持
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/074_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p74 52. QuickSight Pricing and Dashboards; ML Insights.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们来谈谈quick sight的定价模型，再次强调
它针对的是普通终端用户而不是开发者
他们不会根据查询数量或存储空间等来收取费用
这是更高层次的
就像服务的其余部分一样 所以有两种选择
你可以订阅年费订阅
通过全年订阅节省一些钱
他们会按月按用户收费
好的
所以对于你的quick site计划中的每个用户
如果你使用标准版 那将是每用户每月9美元
而企业版则是每用户每月18美元
还有另一个功能叫quick side q
这是quick site的自然语言处理接口
我还没有提到它
但我们很快就会
那更贵
每用户每月2.8美元
如果你需要额外的spice容量 这就有点技术性了
如果你需要超过10GB的spice容量
用于加速你的查询
他们将收取你2.5美分
对于标准计划
或3.8美分
对于企业计划每GB每月
但再次，那只会是一个问题
如果你有大量数据集
你可以按月订阅
他们将收取你更多的费用
每用户每月12美元
对于标准计划
2.4美元 对于企业计划
如果你需要在quick site上使用自然语言处理功能
加上quick side q 那将是惊人的3.4美元
每用户每月
提醒一下，企业版给你什么
这给你提供数据加密
以及quick site导出结果的加密
它还提供与微软活动目录的集成
quick side的一个重要功能是仪表板
如果你做过任何数据分析
你知道仪表板是什么
就像它听起来的那样
这是一个只读的分析快照 你可以以图形形式向人们展示
你知道 让他们浏览各种重要图表和图形
以及与你关心的数字相关的数字
并以紧凑的方式向你的业务所有者展示这一切
这样他们可以实际利用这个信息
仪表板 当然
可以与其他快速用户共享
他们确实需要快速访问
尽管 并且设置成quick site的用户
因为quick site按使用情况收费，现在是谁在使用它
除了通过quickside本身查看您的仪表板
他们还有一个叫做嵌入式仪表板的东西
所以你实际上可以在自己的应用程序中更广泛地共享这些内容
所以，有了嵌入式仪表板
您将获得一个javascript
SDK和一个API，可以让您将由quick site驱动的仪表板嵌入到您提供的应用程序中
可以是一个移动应用，也可以是一个网页应用
我不知道 但这是将快速幻灯片仪表板嵌入其他应用程序的一种方式
那么这是如何工作的
你可能想到的主要问题
确实 快速洞察如何为我构建这一点
如果快速洞察基于使用和按使用量计费
A 它如何知道谁在使用它
并且b 我如何通过某种应用防止全世界的人都访问我的快速仪表板
而我要为全世界的每个人买单
所以有很多身份验证正在发生
为了使嵌入式仪表板现在发挥作用
为了做到无缝衔接
你可以验证嵌入了你的仪表板的
活动目录或认知服务
或者基本上任何您可能正在使用的单一登录技术
因此，经常如此 有可能使用您正在使用的同一认证机制
为了验证对应用程序的访问权限
你嵌入仪表板所在的地方
并将其用于向quickside发送授权
你需要将允许嵌入的域名列入白名单
在quicksite中
你需要给它一个明确的列表说
我将允许来自这些域名的嵌入
来自这些应用程序
这样
互联网上的随机人不能访问你的仪表板
和 让你为那些你从实施角度上没有预见到的东西付费
它有一个javascript sck，用于轻松地将这些仪表板嵌入其中
在任何类型的基于Web的应用程序中
它还提供了一个API
如果你需要更低层次的访问
或者如果你想将此嵌入到更低层次的应用程序中
那就是你知道的 向下到C++或类似的东西
有javascript sck用于在Web应用程序中快速嵌入
但也有API可以让你在想象中几乎任何上下文中这样做
所以这是仪表板
关于仪表板本身没有什么好说的
它就是你所想的那样
但嵌入仪表板有一些值得了解的微妙之处
你需要了解快速站点
机器学习洞察功能
这是一个相对新的能力
并且它即将出现在考试中
越来越多
快速可以使用机器学习做几件事
所以机器学习洞察 你知道的
这只是一个概括性术语
用于快速洞察中的一些不同功能 一个是机器学习驱动的异常检测
所以它可以使用称为随机切割森林的东西
这是一项亚马逊发布的专有算法，他们非常自豪
它可以自动识别对您指标产生重大变化的顶级贡献者
所以它可以自动找到数据集中的异常值
它试图在数据集中找出异常值
并可以识别出这些异常值
如果您需要了解数据集中的异常值
并尝试理解它们产生的原因
您可以使用机器学习驱动的异常检测来尝试识别异常值
自动基于机器学习
它也可以做机器学习预测
记住这一点非常重要，机器学习预测背后的算法
这也使用了他们的随机切割森林算法
但它用于检测时间序列中的季节性和趋势
它通过自动使用随机切割森林排除异常值来工作
它还可以填充缺失值
因此通过使用机器学习
它可以将不完整或杂乱无章的数据集
仍然可以进行预测
同时保持季节性和长期趋势
自动
这就是机器学习预测为您做的事情
另一个机器学习洞察功能是自动叙事
基本上，它添加了他们称之为数据的故事到你的仪表板和快速侧边栏中
这是一个示例，您可以在这里看到
例如，在这个例子中
从他们的文档中
11月的总收入
172018年减少了多少百分比，从blah到blah
过去四天的复合增长率为blah
这比预期更糟
它允许你将数据翻译成普通英语
你知道 有时这对人们来说比图表或视觉更容易消费
它允许你解释这个数据
并自动使用机器学习告诉你这个数据是好是坏
快速侧边栏在其UI中也有一个洞察力选项卡
这提供了所谓的建议洞察力
如果您单击洞察力选项卡
它可以告诉您哪些机器学习功能可能对您的数据集相关 并给您准备好的建议洞察力，您可以查看并纳入分析中
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/075_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p75 01. Intro Application Integration.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


应用集成是构建数据工程管道的一个重要部分
这里不需要学习大量的信息
但理解亚马逊事件桥，亚马逊应用流和AWS步进函数如何融入您的数据管道非常重要
以及自动化您的ETL流程
斯特凡还将涵盖亚马逊简单队列服务
以及亚马逊简单通知服务作为这部分的内容
我会插进来讨论一个新的系统
亚马逊管理的Apache Airflow或简称亚马逊MWA
这似乎是亚马逊提供完全管理的开源工具环境的趋势的一部分
在您完成时添加一些额外的AWS集成
您将通过简短的测验将这些工具应用于实际问题中进行练习
所以让我们深入探讨
构建数据工程管道的一个重要部分 这里不需要学习大量的信息
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/076_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p76 02. Amazon SQS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以SQS是一个相对较小的话题
大数据考试的复习仍然在进行中
我只想给你一个概述
现在，你对SQS有一个新鲜的复习，你不需要记住所有小细节
在下一节课中，我们将看到SQS和Kinesis的区别
但你只需要对SQS的工作原理有一个整体的理解
让我们来看看
SQS是一个队列
生产者或生产者会向SQS发送消息
一个或多个消费者会从队列中拉取消息
这就是队列现在的工作方式
这看起来与kinesis非常相似
但实际上它们非常不同
Sqs是AWS十年前推出的服务
它是完全管理的
Sqs会自动从每秒一条消息扩展到每秒15000条消息
你不需要做任何事情
消息的默认保留期为4天
但你可以设置最长14天
如果你想要
队列中可以无限制地存储消息
它的延迟极低
每个发布和接收API的延迟小于10毫秒
消费者数量可以水平扩展
你可以创建任意数量的消费者
可以支持高并发
高吞吐量
至少会交付一次
偶尔也会出现消息顺序混乱的情况
它的顺序处理能力不如kinesis
并且正在进行最佳努力排序
最后，消息必须很小
低于kinesis
每条消息的发送限制为256KB
你需要记住这些限制
现在 我们如何生产消息
消息和kinesis
它们有一个主体 正如我所说
他们已经达到256千字节
它们是由字符串组成的
所以它们是由文本组成的
你可以为它添加元数据属性
你可以添加键值对
基本上可以在消息上添加
你可以提供延迟交付
那是可选的 然后将消息发送到sqs
你得到的回复是一个机制
搜索ID 以及身体部分的MD5哈希
以便记住你发送到SQS的内容
这与Kinesis非常不同
你能记得我们在发送字节吗？我们在发送字符串
你能看到总最大尺寸是一兆字节吗
我们这里有2.56KB
好的 现在
消费消息 看起来怎么样，嗯
消费者 他们会从SQS获取消息
并且他们可以一次接收多达10条消息
他们必须在所谓的可见性超时内处理这些消息
当他们完成处理该消息后
他们会使用收到的消息ID和一个收据处理来删除该消息从队列
这意味着当你使用SQS时
你的客户端会拉取消息
消费者会从SQS获取消息
并且您的客户会处理这些消息
然后将它们从 sqsq 中删除
因此，消息不能被多个不同的客户端应用程序处理
这与kinesis有很大的不同
所以请记住，处理消费者从 sqs 中拉取消息
处理它们 然后删除它们
现在有一种新的 sqsq 类型
之前有一种标准 q
而现在有一种 fifo 或 fifo q
这意味着先进先出
这不是所有地区都有的
但我认为它几乎已经完成
基本上，你的队列名称是cust.in.dot.fifo
以表示它是一个FIFO队列
你将获得更低的吞吐量
现在你有限制 你可以达到每秒3000条消息，使用批量处理
或者没有批量处理的话，每秒300条消息，所以吞吐量肯定更低
但你得到的是，你的消息由你的客户端按顺序处理
所以你会得到第一和第一输出
类型的提供排序消息将恰好发送一次
并且你有一个5分钟的复制间隔
如果你发送一个称为复制ID的东西
所以它是不同的 用例较少每put
但是对排序有更多的保证
如果你需要这个，那么FIFO队列就能工作
如果你的消息被发送出一、二、三、四、五
那么它们也会以一、二、三、四、五的顺序被你的消费者读取
那么关于那个256KB的限制
我们在SQS中如何发送大消息
这并不是特别推荐
但是存在一个叫做SQS扩展客户端的东西
它是一个Java库
它使用了一个技巧
基本上它会使用亚马逊S3桶作为伴侣
我们说 好的
我将发送非常大的负载
也许像 你知道的10GB或者
你知道的5MB或者10MB上传到S3
它会发送一个元数据消息到SQS
然后客户端在消费者侧会接收到一个小的元数据消息
告诉你文件在S3的位置
消费者能够直接从S3读取大的消息
这就是你如何绕过250
6KB的消息限制
如果你想使用SQS扩展客户端
这对于大数据非常有帮助
Sqs的使用场景可以是解耦合应用程序
例如 如果你想异步处理支付或缓冲数据库的权利
例如你有一个投票应用程序
你预计会有通量的峰值
你需要能够快速扩展或处理大量传入的消息
例如 如果你有一个邮件发送器
Sqs也可以与Cloudwatch通过Auto Scaling进行集成
如果你有e C 两个实例从你的sqs立方体读取
现在 关于限制
嗯 你同时处理的待处理消息最多为十二万条
批处理请求只能拉取最多10条消息
每条消息最大256KB
消息内容是文本
所以它必须是xml json
或者只是文本本身
以及标准队列本身
尽管它们有无限的
每秒无限次的事务
所以你可以在吞吐量上每秒有尽可能多的消息
进入sqs
的fifo q
尽管支持使用批处理高达三千条消息每秒
所以记住这一点 以及最大消息的大小
我认为最大是10英尺
大约是256KB
如果你需要更多 你可以使用扩展的客户端
数据保留期可以从一分钟到十四天
但是记住，一旦消息被读取
它们将从SQS中删除
在定价方面
它与Kinesis非常不同
你将按API请求付费
并且你将为网络使用付费
在SQS安全性方面
我们使用SSL加密
如果我们使用SQS的HTTP端点
这是默认的
我们可以使用服务器端加密
使用KMS 例如
我们可以设置我们要使用的主密钥
然后SQS
KMS将直接加密我们发送到SQS的消息正文
但不会加密元数据本身
所以消息ID
时间戳 和属性不会被服务器端加密
策略必须允许使用SQS
你可以定义哪个用户可以向哪个队列发送数据
并且我们可以在此基础上设置SQS Q访问策略
如果我们想要进一步限制安全性
它可以获得更精细的粒度控制
例如通过IP
或者我们可以控制请求的时间
这就是SQS
只是一个高层次的概述
你不需要记住所有这些细节
你只需要记住大致的想法
限制
所有这些东西 我希望这有所帮助
在下一讲中，我们将比较
你可以保存数据流和SQS 所以下次见
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/077_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p77 03. Amazon Kinesis Data Streams vs. Amazon SQS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以进入考试
对你来说真正的挑战是能够告诉你
当你将要使用kenny时
看到流量和Sqs
所以流是一个大数据工具
当数据在流中
它可以被不同的应用程序多次消费
数据只有在保留期后才会被删除
这是一个允许数据多次被消费的特性
这就是允许数据多次被消费的特性
在记录的排序方面
它在分片级别得到保留
即使在重放期间
这非常方便，允许你在Kinesis数据流上分布你的读操作
你也可以从同一个流独立运行多个应用程序
因此实现了一种发布/订阅类型的架构
如果你使用 例如
像Spark或Flink这样的大数据框架
你可以在Kinesis数据流上执行流式MapReduce
此外，如果你想知道你在肯尼数据流中阅读了多远，你可以通过检查点来跟踪进度
这就是k cl所做的
肯尼的客户端库通过使用dynamodb作为检查点设施做得很好
最后，数据流有两种模式
提供模式，你在事先选择容量
或者按需模式，它会随着时间的推移适应sqs的容量
它非常不同 它是队列，正如名字所表明的
它用于解耦应用程序
你每条队列只能设置一个读取应用程序
然后，一旦记录被读取
它们会在消费后被删除
所以，一旦它们被确认就会被删除
或者，如果它们多次失败
它们可以 例如进入死信队列
Q
标准队列中的消息独立处理
这意味着你可以处理
例如 在同一时间在sqs中处理一千条消息
如果你需要排序约束，这是完全可能的
你需要使用FIFO队列
这会降低你的处理能力
但这会给你带来排序约束
你还有能力延迟消息，sqs可以动态扩展
你不需要操作来扩展sqs 所以，我认为我们可以从这里看到它们在使用场景上非常不同
总的来说，它们在使用场景上非常不同
一个是用于流数据和应用，另一个则是用于解耦应用并创建应用队列
现在有一个方便的
嗯 小表格
我为你整理好了
它比较的是数据流
Kinesis 数据流，SQS 标准和 FIFO
这些都是由 AWS 管理的
排序发生在 Kinesis 数据流的分区和键级别
在那里这仅适用于特定群体
ID在SQS FIFO中
而对于其他两个，交付中并未发生
至少在前三个服务中至少一次
而在亚马逊上则是恰好一次
SQS FIFO
Kinesis数据流具有重放能力
而其他则没有重放能力
Kinesis数据流的最大数据保留时间为三百六十
五天
零天给肯尼迪
火主机 所以数据只是发送到目的地然后忘记
对于sqs标准和sqs5o
在扩展和吞吐量方面，它指的是14天
我们有两个规定
流数据片断在规定模式下
所以你每秒得到1兆字节的生产者每片
或每消费者每秒2兆字节每片，以及在按需模式下
然后你想要的扩展量有多少就有多少
你不需要在事先选择分片的数量
没有数据的规模限制
火烈鸟和sql标准
对于sqs fifo
你有每秒三千条消息的批处理
或者每批三十千条消息的批处理
如果你使用sqs fifo的高吞吐模式，关于对象大小
最大对象大小是一兆字节
你可以看到流
一百二十千兆字节的目的地
256千字节用于sqs
但如果您使用扩展库，可以将消息存储在sqs之外，这样可以扩展它
并且只将一些元数据发送到sqs
我想现在我们知道它们在sqs的使用场景上非常不同
你有订单处理
图像处理
或自动扩展队列
根据消息 或为将来的处理缓冲和批量消息
或从一个服务到另一个服务的请求流
在您能看到不同用例的数据流
例如 快速日志和事件数据收集
和处理 实时指标和报告
移动数据捕获
实时数据分析
游戏数据馈送
从物联网复杂的数据处理
好的 所以希望
现在你理解了两者之间的区别
你对此非常自信
我希望你喜欢这节课 下次课再见
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/078_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p78 04. Amazon SQS - Dead Letter Queues.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈SQS中的死信队列
设想一个场景
消费者在可见性超时期间未能处理消息
我们知道消息会自动回到队列中
消费者读取了一条消息
可能出现了故障
可能时间不够
消息回到队列中
如果这种情况经常发生
这可能是个问题
例如 我们再次读取消息
可能消息有问题
可能消费者无法理解消息
或者无法处理消息
那么消息将回到队列中
又会重复发生
我们再次从SQS读取消息
消息又会回到队列中
我们可以设置一个阈值，限制这种情况发生的次数
但这种失败循环可能是个大问题
但我们可以设置一个最大接收阈值
如果超过了这个阈值
我们可以告诉SQS
这条消息看起来有点奇怪
看起来被处理了太多次，仍未成功
因此将其发送到死信队列中，队列将包含该消息
以便稍后处理
因此消息将从第一个队列移除并发送到第二个队列
我们为何要有死信队列
嗯 死信队列对于调试非常有用
如果消息进入队列
因为它是SQS，你必须处理它
但至少它给你时间理解发生了什么 有几点需要注意，死信队列必须是标准队列
以及死信队列的标准队列也必须是标准队列
最后，由于我们有死信队列
你需要确保消息在从队列过期之前被处理
因此，设置一个较长的时间，例如14天的保留期，是很好的主意
在死信队列中
接下来，管理您的死信队列的下一个特性是重定向到源特性 这是一个特性，帮助您消费死信队列中的消息
以了解它们出了什么问题
因此，您现在拥有消息
您知道它们没有在源队列中被处理
因此它们处于死信队列中
您将进行手动检查和调试这些消息
然后你会修复你的消费者代码
理解为什么消息没有被处理，即使消息是正确的
然后你可以做的就是从死信队列中重新提取消息，并将其发送到源Sqs队列
这将发生什么
是消费者可以重新处理这个消息
而不知道消息已经进入了队列
然后消息处理已经完成
这是一个很棒的功能
所以现在让我们进入控制台 这样我可以向你展示死信队列的功能
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/079_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p79 05. Amazon SQS - Dead Letter Queues - Hands On.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


那么让我们开始创建一个死信队列，用于我的演示队列
所以我称之为演示队列
Dlq
我会滚动下来
因为它是一个死信队列
我想给自己足够的时间来保留和分析消息
所以我会有一个14天的消息保留期
好的，完美
那么让我们滚动下来
我们将有一个默认的加密启用
一切都看起来不错
我们将创建这个队列
所以现在这个队列已成功创建
让我们打开一个新标签并进入我的演示队列的配置
在这里我将编辑队列本身
可见性超时需要设置为非常慢
非常低 对不起，我要在演示中稍微快一点
所以我们将其设置为5秒读取消息3次非常快
然后如果我滚动到死信队列下面
我可以启用它
我可以选择死
演示
Q Dq
Sq sq
作为我的死信
现在我们需要指定最大接收
这是消息应该在成为死信队列之前应该被接收的次数
为了更快
我们说3 所以消息在第四次被读取并放回队列时
那么它将最终进入我的sqs死信队列
让我们保存这个
我们很好
所以现在我将去我的dlq
我将开始接收消息并开始拉取消息
目前 当然我的dlq中没有任何消息
但我现在去我的常规队列
我将发送和接收消息
这将是hello world毒丸
因为它实际上会让我的消费应用失败
所以这就是它被称为毒丸的原因
但我们没有消费应用
这只是一个消费应用本身
那么让我们发送消息
消息已发送
现在让我们拉取消息
所以这里我们正在接收消息，第一次接收
然后五秒后我们将第二次接收它
然后五秒后我们将第三次接收它
正如你看到的
然后消息被接收三次后
因为它总是被放回队列中
它不会被删除
然后消息将被发送到dlq
所以让我们现在验证这一点
如果我现在停止拉取
然后我尝试再次拉取新消息
我将看到不再接收消息
所以消息去哪里了，嗯
如果我们进入dlq本身
这是我的dq，正如你所见，我正在拉取消息
我现在在我的dq中看到了消息
这条消息 如果我点击它作为dlq
我可以说 我可以查看这条消息
这条消息是我的主应用程序崩溃的原因
最后 让我看看如何从dlq将消息重新路由到第一个队列，以防
例如 我们修复了消费应用程序
在我的dlq中
我将在这里的右上角
有一个开始 Dl q q redrive
所以我们说，嘿
这是一个dlq并且你收到了消息
所以我们想将这些消息重新路由到源队列
我们只需点击此选项进行速度控制
我们可以系统优化
然后我们可以检查消息，如果我们想要的话
然后我们只需点击dlq redrive
所以它被称为dlq redrive任务
如果我在这里查看
我已经看到它已成功完成
这意味着如果我回到我的演示
队列源队列，发送和接收消息并拉取消息，正如你所见，消息又出现了这里
因此重定向工作
希望这给你们提供了一个好的理解，了解死信队列是如何工作的
我希望你喜欢它 我将在下次讲座见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/081_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p81 07. Amazon SNS - with SQS Fan Out.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈SS+SQS扇出模式
想法是你想让消息发送给多个SQS
但如果你单独发送给每个SQS
它们可能会出现一些问题
例如 如果你的应用程序在中间崩溃
或者它们的交付失败
或者你添加了更多的SQS在后续
因此我们想使用最终模式
想法是你将向S主题推送一次
然后你将订阅你想要的任意多个SQS
这些队列是订阅者
他们将接收发送到S的主题中的消息
例如
我们有一个购买服务想要向两个SQS发送消息 它将会这样做
它将向S主题发送一条消息
而SQS是该主题的订阅者
这样欺诈服务和物流服务可以从他们自己的SQS读取所有消息
这种模式的想法是完全解耦的
并且没有资料丢失
SQS将提供资料持久性
以及处理延迟和作业重试
并且使用这种模式
我们可以随着时间的推移添加更多的SQS作为NS主题的订阅者
为此
我们需要确保你的SQS访问策略
如我们所见
允许你的主题向你的SQS写入
再次
这是使用队列访问策略的另一个用例
并且我们有跨区域交付
因此一个区域中的主题可以向其他区域的SQS发送消息
如果安全允许
接下来
我们如何使用这种模式用于其他目的
例如
S3事件发送到多个队列 因此S3事件的规则限制是
对于事件类型组合
例如
一个对象正在被创建和前缀 例如images
你只能拥有一个S3事件规则
但如果你想要发送相同的S3事件通知
到多个SQS
在这种情况下你将使用整个模式
例如
我们有一个S3对象创建的事件出现在S3桶中
我们将这个事件发送到S主题
我们将为主题订阅许多sqs队列作为有限模式
但我们也可以订阅其他类型的应用程序
电子邮件 Lambda函数
等等
然后，我们从中得到的是，亚马逊三中发生的事件的消息将发送到多个不同的目的地
多播模式使得这一点成为可能
另一种架构是您可以直接从s将数据发送到亚马逊三
通过kinesis数据火烈鸟
因此，因为S直接与KDF集成
那么，您的购买服务可以将数据发送到S主题
然后您可以看到数据流
KDF将接收该信息
然后您可以将数据从KDF发送到您的Amazon S3存储桶
或者，更重要的是
任何特定于KDF的目的地
这允许您以您想要的方式进行扩展
您可能希望从主题中持久化您的消息
因此，我们可以将有限模式应用于FIFO主题
所以亚马逊有一个FIFO（先进先出）的能力
这是先进先出的缩写
这确保了主题中消息的顺序
所以生产者发送消息一二
三四 目前订阅者只能是SQS FIFO队列
这将接收消息一二
三四按顺序
所以FIFO的想法是我们可以获得与SQSVO相同的功能
我们可以通过消息组ID获得排序
我们使用去重ID或基于内容的去重来消除重复
而SQS标准和FIFO队列都可以作为订阅者
在吞吐量方面
你受到限制 你获得的吞吐量与SQSv4q相同
因为目前只有4q可以读取和SV4主题
那么我们为什么需要这个呢？如果你需要使用SQSvO进行广播
那么你需要广播、排序和去重
所以银行服务会将数据发送到SVO主题
然后它会广播到两个SQSFIFO队列
可以同时从FIFO队列中读取欺诈服务和物流服务
最后一个ns的特性可以非常实用
关于扇出模式，你可以在snes中进行消息过滤
那么消息筛选是什么好呢
用于过滤发送到us主题的消息的相邻策略
订阅
因此，如果一个订阅没有筛选策略
它将接收到每一条消息
那就是默认行为
但是让我们举一个例子，说明当我们设置消息过滤策略时发生了什么
所以我们有一个购买服务
它将交易发送到s主题
例如 交易看起来有一个订单号作为产品
例如 铅笔
数量
和当前状态
我们想要创建一个sq sq
仅限已放置订单
不是所有订单 但仅限已放置订单
因此我们将订阅sqsq到sms主题
我们将在json中应用过滤策略
并在策略中指定我们希望状态等于已放置
因此只有匹配的消息
策略将进入x qs two
但我们可以有一个sqsq用于取消订单
因此我们可以创建我们自己的过滤策略
用于取消订单并将它们从同一主题发送到sqs cube
因此已放置订单和取消订单
s q不会具有相同的消息
我们也可以使用相同的过滤策略
取消的一个是用于取消订单的电子邮件订阅
我们可以有一个过滤策略用于拒绝订单
例如 并且作为另一个sqsq
或者我们可以创建一个sqsq没有过滤策略
以从该主题获取所有消息
因此使用所有这些有限模式和消息过滤
Fifo队列和Fifo主题
我们得到许多不同的可能性
考试将尝试测试所有这些
这就是本讲座的全部内容 我希望你喜欢它 我将在下次讲座见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/082_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p82 08. Amazon SNS - Hands On.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


那么我们来练习使用s
我们将进入简单通知服务并创建我们的第一个主题
我将其命名为我的第一个主题
然后点击下一步
创建主题的两种方式
它可以是标准主题
你将创建这个
这是尽力而为
消息排序至少一次消息交付
最高吞吐量以秒为单位发布
And sqs lambda http ms
电子邮件和移动应用程序端点可以从此主题读取
或者我们看到我们可以创建一个s fifo主题
这样可以严格保证的消息顺序和恰好一次的消息传递
高吞吐量高达每秒300个发布者
并且唯一可以订阅fifo的主题是sqsq
在这种情况下
如果我们有fifo 我们可以看到猪的名字必须以点fifo结束
We can see that the name of the pig has to end with dot fifo
所以我们将使用标准
然后我们将使用我的第一个主题作为名称
我们可以在主题中加密消息
我们可以设置一个访问策略
访问策略将定义谁和什么可以写入主题
这与S3桶策略相似
这与SQS访问策略相似
好的 想法是，使用此访问策略
例如 我们可以设置一个免费的桶，将事件写入到短信主题中
然后主题可以将数据发送到SQS等
因此，访问策略需要允许STHREE桶写入我们的主题
所以，我现在将使用基本
因为我们不需要做任何复杂的事情
然后我们不会设置任何这些
然后我们点击创建主题
所以我的第一个主题已经创建
正如我们所见，目前我们没有任何订阅
所以我们需要创建我们的第一个订阅
所以我要创建一个
我们选择协议
所以它可以是kinesis数据
火炬流 SQS
Lambda邮件
JSON HTTP http s和ms
好的 你必须记住那些参加考试的人
但要让它非常简单
首先我们将使用邮件，实际上在这个实际操作中我们只使用邮件
现在我们需要使用邮件地址的端点
我将使用stefan the teacher @ melena or dot com
这只是一个服务，供我获得一个临时的邮件地址
所以如果我输入stefan the teacher
嗯，然后点击go
那么我将获得一个公共的收件箱，它将从这个地址接收邮件
好的 所以我们要创建这个
正如我们看到的这里
我们可以设置一个订阅过滤器策略
这是可选的
但通过此策略我们可以过滤将要发送到订阅的消息
这可能非常有帮助
如果你有很多订阅者
他们只需要接收发送到你SQ主题中的一部分消息
但我们现在不会设置它
所以我们会创建一个订阅
现在我们需要
嗯 验证订阅
正如你所看到的
它目前正在等待确认，所以要确认它
我将进入我的邮件
我收到了一封电子邮件
通过点击此确认订阅
我将确认我的订阅
好吧，我们继续 现在如果我刷新这一页
我应该看到的是我的订阅在这里被确认
所以这完美
所以再次 如果我点击我的订阅
我可以查看它
看到它将向这个端点发送电子邮件
如果我想这样做
我可以有一个订阅过滤策略
但现在我们没有任何消息
因为我们希望所有发送到我的主题的消息都能直接进入这个描述
所以现在让我们做一个测试
我们将发布一条消息
我将输入一个结构
我说你好
焊接 这是非常常见的
当我们测试一些服务时
然后发布消息
所以消息已经发布到我的第一个主题中
所以我应该看到的是
如果我回到我的电子邮件并去我的 uh
公开消息 所以回到收件箱
我应该看到是的
有一个通知消息直接来自
S 说
你好，世界 他们很容易
非常酷 它只表明 s 在为我们工作
所以这完美 我们准备好了
如果我们想要实现 sqs 扇出模式
我们需要选择 sqs 并设置许多
许多不同的队列作为我们主题的订阅接收者
是的 这就是 s
当你完成时 删除订阅
然后回到左边的主题
然后点击删除
删除我并离开
这就是 ea s 我希望你喜欢它 我会在下一节课见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/083_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p83 09. AWS Step Functions.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们来做一个AWS Step Functions的高级概述
这与数据管道的概念非常相似
它的主要目的是让你在AWS中设计工作流
当你构建一个分析数据的系统时
Step Functions可能将所有这些步骤连接在一起，构建一个更大的系统
它也可以让你非常容易地可视化这些工作流
正如我们很快就会看到的 它会生成这些漂亮的小图表
显示事情实际执行的顺序
它还有一个高级的错误处理机制和重试机制，独立于代码之外
你实际上在你的aws step function中运行
这样你就可以管理
嗯 你工作流的一部分
你知道 你不必依赖每个步骤的代码
实际上能够监控自己
aws step function可以从外部查看并处理错误
当事情没有按照预期进行时
它还维护了一个所有运行过的工作流的审计历史
所以你可以回去看看历史上发生了什么
如果你需要调试某事
它允许你在步骤之间等待任意长的时间
所以如果你只是想
嗯 在这些步骤之间放一些时间
你知道 与你拥有的一些业务流程同步
你可以这样做
给定状态机或给用工作流的最大执行时间
这是一年的时间 所以你可以定义一个跨越一整年的工作流程
如果你需要，想象不出需要这样做的情况
但如果你需要
它可以处理非常长的生命周期过程和工作流程
所有这些工作的语法实际上并不重要
嗯 它们使用基于JSON的AWS状态语言定义
或者ASL
考试不会期望你能够解析或编写这种代码
或者不会说话 你只需要知道它做什么
只要你知道步骤函数是用来做什么的
和一些你可能用它的例子
这就是考试重要的
所以 我们将会 看几个例子，你可能在现实生活中使用步骤函数
这是一个使用步骤函数来训练机器学习模型的例子
所以你可以看到当我们开始时
我们在这个例子中生成了数据集
通过启动一个lambda函数来完成
然后使用sagemaxg boost算法训练模型
当它完成后 它会从sagemaker保存那个训练好的模型，并用批处理转换那个模型
从我们已有的数据集中
到这一步就完成了，所以你可以看到这里
我们自动化了整个生成我们数据训练
机器学习模型的过程 使用那些数据并保存训练好的模型
然后应用这个训练好的数据模型
所有的步骤都在这里通过step functions管理
我们也可以使用它来调整机器学习模型
在这个例子中我们生成了一个训练数据集
然后我们在sagemaker中启动一个超参数
调优工作，尝试在机器学习模型上使用不同的参数
以找出给定数据集正确的参数集
当它完成时 它提取模型路径
然后保存它选择的调优模型
我们提取了旋律模型的模型名称
然后使用这个来对一批数据的变换进行应用
自动调整到那个调优模型
所以再次 只是一个例子，说明你可能使用步骤函数做什么
对于考试来说，理解机器学习是如何工作的并不重要
但是，重要的是要知道，步进函数可以自动化生成你的数据集的过程
并将其喂入一个机器学习模型中
你也可以使用它 例如
管理一个批次作业
所以在这个例子中 我们提交一个批次作业
然后我们将使用步骤函数来通知我们
如果它成功或失败非常简单
所以步骤函数不一定要复杂
它们可以用于监控单个作业
并使用其功能来监控和通知我们
当某个步骤成功或失败时
有时步骤函数只有一个步骤
这就是你需要了解的aws s step functions的深度
你需要知道的是他们管理工作流
它们会给你一个工作流的良好图形表示 它们可以监控单个步骤是否成功或失败，并根据那里发生的事情通知你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/084_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p84 10. AWS Step Functions State Machines and States.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们对AWS Step Functions进行更深入的了解
因为这是一个非常强大的编排工具
它对数据工程来说非常重要
所以你可以在考试中期待很多关于这方面的问题，包括一些专业术语
你在Step Functions中定义的工作流称为状态机
这只是一种优雅的说法
我们有一个系统，它定义了你的数据管道可能处于的一系列状态
并且它之间移动
基于某些标准
工作流中的每个步骤都称为状态
因为这是状态机的一部分
对 这就是一些术语
就是一些词汇 然而，在aws step functions中，你可以使用许多不同类型的状态
因此，了解这些是很重要的
最有用的一种是任务状态
这就是你的工作流状态机中的状态，它会做一些事情
也许它会启动一个lambda函数
也许它会启动其他aws服务来转换你的数据
或者移动它，或者加载它
或者它需要做的任何事情
也许它还可以与第三方API进行通信，以便在AWS之外执行某些操作
无论如何，这都是一个任务节点
它在处理数据时执行某种实际的操作
随着你工作的进行 此外，还有选择状态，通过选择规则添加条件逻辑
这些可以是比较之类的事情
你知道的 如果我的数据大于这个量
也许我会对它做一些特别的事情
如果这个数据包含π或者其他什么
也许我会在它正确的地方做一些特别的事情
所以选择状态只是应用条件逻辑的机会
也有等待状态
它可以用来延迟状态机一定时间
如果你出于某种原因想要给予一些东西
你知道的，给一些时间让事情赶上
这算是一种hacky的方式
但你也可以给你的步骤函数加入时间延迟
带有权重的状态
还有并行状态
这使得你可以在并行和执行分支中做不同的事情
并行状态不是很好
状态概念有点模糊
它实际上是在说 我将在此处创建许多并行运行的状态
我可以同时进行不同的事情
一旦我遇到并行状态
现在也有映射状态
所以这可以让人困惑
这是应该记住的事情
所以地图也在并行运行
但它说我将运行这些步骤的集合
在每个数据集中的并行
所以这对数据工程尤其相关
因为它专门用于处理数据
它将能够处理json文件作为三个对象
CSV文件
你可能有的任何东西
但是，这将需要一系列步骤，并将这些步骤并行应用到那个数据集上
所以，记住，映射操作会将数据分成多个部分，以便在数据上进行并行处理
就像Apache Spark一样
或者任何其他工具
它将在无需在数据前面设置单独的并行状态的情况下并行运行
它本身就是并行的
好的 所以，这里有一点关于并行和映射状态的混淆
并行用于在您的状态机中启动单独的分支，它可以做任何事情
但是映射是专门用于在并行方式下操作您的数据
最后有通过
成功 和失败
只是让你的数据传递到下一步
它通常只用于调试
成功和失败 或者你将要结束的地方
所以你完成了你的状态机
你将要么成功使用那个状态机，要么失败
这就是你如何把事情整理好的方式
这样你就能更详细地理解步骤函数
你还记得有效的状态类型是什么以及它们做了什么吗 这可能会派上用场
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/085_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p85 11. Amazon AppFlow.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈亚马逊 airflow
所以 airflow 是完全管理的集成服务，它使你能够将数据传输到软件
作为服务应用程序和 aws
这些集成很难编写
并且与上传它们非常容易
数据的来源可以是
例如 Salesforce sap zendesk
Slack service now
Salesforce 可能会出现在考试中
你可以将数据发送到的目的地
可以是 亚马逊 is free
亚马逊 redshift
甚至非 aws 目标，如 snowflake 和 salesforce
你可以定义集成在日程安排上运行
或在特定事件或按需响应中
App flow 的数据转换功能，如过滤和验证
数据在公共互联网上加密
或您可以使用私有链接传输它
想法是，使用 app flow
你不必花费一些时间编写集成
您立即可以利用这些 api
您会在自己的帐户中享受数据
在 app flow 界面中
您将拥有一个来源 您可以选择许多不同的来源
但您需要记住的一些重要的来源是我之前提到的
包括 salesforce
然后使用 app flow
您可以将此数据发送到各种目的地
如您所见 red shift s three 等等
这就是 app flow
只是一个介绍 我希望你喜欢它 我会在下次讲座见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/086_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p86 12. Amazon EventBridge.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈亚马逊事件桥
亚马逊事件桥以前被称为云观察事件
所以你会在考试中看到事件桥
但只是你要知道 如果你来自旧的aws经验
那么它以前被称为云观察事件
所以通过桥你可以做很多事情
例如我们可以在云中安排cron作业
所以我们可以安排脚本
例如我们说
每小时
请触发一个lambda函数
并且该lambda函数将运行一个脚本
因此事件每小时生成
因此得名亚马逊事件桥
但不仅是安排
例如每小时
你也可以对事件模式做出反应
这些是事件规则可以对服务做某事做出反应
例如你可以对事件做出反应
我是root用户在控制台登录
所以当发生这种情况 也许你想将消息发送到s主题并接收电子邮件通知
所以如果有人使用root账户
那么你会收到电子邮件
这可能是您账户的良好安全功能
例如你有不同的目的地
你可以触发和函数
发送和消息等等我会在几秒钟内显示所有这些
所以桥在中间
我们有所有可能将事件发送到亚马逊事件桥的来源
例如 EC two实例启动
当他们停止 当他们被终止
等等可以构建
例如如果你有一个构建失败
或s3每当有事件
例如当一个对象被上传
或可信顾问
当你账户中有新的安全性发现
或作为好搭档
你可以结合事件桥和clap轨迹
并且实际上可以拦截任何api调用
这在很大程度上
此外 如我所说
你可以有安排或cron
所以你可以说每四个小时或每个星期一上午8点
每月第一个星期一
这是你也能做到的事情
然后这些事件会被发送到亚马逊事件
你可以设置一个过滤器
例如 你说嘿
我只想为特定的亚马逊s三八获取这些事件
例如
然后eventbridge将生成一个json文档，该文档代表事件的详细信息
例如启动实例
例如get started with its id等等，有很多信息
时间，ip等等
一旦完成
然后这个json文档
这个事件可以被发送到许多不同类型的目的地
允许你做
实际上可以进行一些集成
例如 你可以安排并触发一个lambda函数
你可以安排一个batch批处理
你可以启动一个amazon ecs任务
你可以将消息发送到sqs或甚至数据流
你可以 例如启动一个stamp step function
你可以启动一个code pipeline的ccd管道
或者启动一个code build构建
你不需要 你知道所有这些东西
当然这些都是不同的服务
但我只是想给你一个概述
你可以做的事情 你也可以
例如启动一个ssm自动化
或一个特定的e
C 两个操作 例如启动或停止或重启一个e
C 两个实例 你可以看到可能性是无限的
这真地取决于你的使用案例
所以亚马逊事件桥是我们称之为默认事件总线
这就是我们刚刚看到的
它代表aws服务将事件发送到默认事件总线
但亚马逊有更多的能力
有一个叫做合作伙伴事件总线
这是aws与合作伙伴集成
他们很可能是软件即服务合作伙伴
他们将直接将他们的事件发送到您的合作伙伴事件总线
如果你使用 例如zendesk
Datadog 或者零个或多个你需要检查合作伙伴列表
然后它们有可能直接将事件发送到指定的合作伙伴事件总线
因此您可以直接在账户中响应 AWS 外部发生的更改
好的 最后有一个自定义事件总线
您可以创建自己的事件总线
然后您自己的应用程序可以向自定义事件总线发送自己的事件
因此您具有相同的能力将这些事件发送到不同的目的地
多亏了 EventBridge 规则
您也可以访问事件总线
跨账户使用资源基于策略
正如我们将很快看到的
您也可以存档事件
所有或仅限子集
感谢您的过滤器
通过存档事件
您可以将其设置为无限期保留或保留期
好的 您可以对这些事件进行重放
例如 假设您的 Lambda 函数存在错误
并且您想要修复它
您已经修复了它 然后您想要重新测试事件
重放 您可以重放这些存档事件
这对于调试非常有用
对于故障排除和生产修复也非常有用
现在 EventBridge 从许多不同的地方接收大量事件
因此您需要了解事件将如何看起来
记住这些事件以 JSON 格式存在
我们刚刚看到了
因此存在一个 schema 仓库
EventBridge 的能力是分析您的 bus 中的事件
然后它会推断出 schema
schema 树中的 schema 允许您为应用程序生成代码
这将让您提前了解事件总线中数据的结构
例如 这是一个特定于 CodePipeline 操作的示例
有一个 schema 并且您可以下载代码
他们使用橙色按钮
这将直接了解如何推断 schema 并结构化您的事件总线中的数据
此外 schema 可以版本化
因此您可以随着时间的推移迭代您的应用程序 schema
当然现在我们有一个基于资源的 EventBridge 策略
那意味着什么 这意味着您可以管理
嗯 特定事件总线的权限
例如 你可以说这是一个特定的事件总线
可以允许或拒绝其他地区或账户的事件，并且用例
例如 它将是在你的aws组织内的一个中央事件总线
一组账户
然后所有这些事件都将被聚合
这是否工作良好
我们有一个特定的账户的中央事件总线
我们将添加一个特定的资源基策略
允许其他账户将其事件发送到它
因此，例如，这个其他账户
将能够执行put events并将事件直接发送到中央事件总线 所以让我们从左到右说桥梁
你知道关于它的一切
所以记住，你可以响应你账户内发生的事件 多亏了默认的事件总线
但也包括合作伙伴事件
以及你自己的事件与自定义总线
你有模式限制能力
然后你有资源基策略
允许你有跨账户的能力
例如，对于事件总线的能力
好的
就是这样 我希望你喜欢它 我会在下次讲座见到你 再见
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/087_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p87 13. Amazon EventBridge - Hands On.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 首先让我看看云监控事件
Ui 这就是事件和云监控事件
它最终会消失
所以不要惊讶
如果你找不到它 因为现在云监控事件是事件桥
这是云监控事件的增强版本
所以我可以进入亚马逊
查看事件桥的新界面
在这里我可以访问多个东西
在事件总线下
我们可以访问默认事件总线
这个事件总线是在你账户中默认创建的
你已经可以开始定义规则
但你可以创建自己的事件总线并称之为中央事件总线
我正在重新创建它 v2 你可以启用事件存档
如果你想要存档这些事件并永久保留它们
以便进行一些调试以及进行自动化模式发现
如果你需要对此事件总线进行跨账户访问
那么你可以定义一个基于资源的策略
你需要编辑一切
如果你没有基于资源的策略
那么只有事件总线所有者可以向事件总线发送事件
这对我们来说是可以的
让我们创建这个
你已经创建了一个自定义事件总线
现在我们可以谈论事件了
我们有事件源可以来自合作伙伴
例如我们可以说
我想捕获来自of zero的所有事件
所以我们会设置
这里有设置你事件总线为of zero的方式
这里有一些of zero网站上的指示
然后一旦一切设置完成
我就能在事件桥中直接捕获of zero的事件
所以现在所有事件都被捕获到桥中
你需要继续创建一些规则
你需要选择创建规则选项
我会称其为demo rule event bridge
然后你可以选择总线
总线可以是默认的
可以是中央总线
或者你可以创建任何你想要的
我会使用默认总线
因为它是默认创建的
你可以选择在特定时间运行
例如我说
我想要这个规则每小时运行一次
那么我们选择第一个
接下来，我们必须选择一个事件源
我们有几个选项
我们可以选择事件源来自AWS内部
我们选择AWS服务
或者它是一个自定义事件或合作伙伴事件
我们将选择其他，如果您想将所有事件集中到您的账户中，跨越多个账户
那么您将选择所有事件
然后将其发送到一个中央事件总线
感谢您的资源基策略
但现在我们将选择AWS服务
接下来，我们可以筛选样本事件
所以这是一个新功能，被称为沙盒
所以如果您回到Amazon Bridge并转到沙盒这里
您可以使用样本事件进行测试
然后测试与您的事件模式进行测试，而不创建规则，这些UI也是可访问的
当您创建角色时
对于样本事件
我将选择E
C Two和不要获取全部
这里有自动扩展 但您向下滚动，现在有E
C Two并且您将选择EC two状态更改通知
这是将被发送到EventBridge的事件类型
每当有状态更改通知时
所以我们可以选择不同的样本事件
如您所见 这个瞬间状态是即将发生的
而这个是正在运行
好的 让我们假设我们想要生成事件，每当我们的实例进入um时
让我们选择停止状态
因为我们想知道它们是否已被停止
好的 停止，这就对了
所以我将向下滚动并创建一个事件模式
我将选择一个服务提供商
这是E C Two并且选择一个事件类型
这是即时实例状态更改通知
我们可以选择所有状态
如果我现在测试模式
它将匹配样本事件
但我可以指定一个特定的类型
嗯，状态 例如
只有停止或终止的实例
正如你所看到的，我的事件模式变得更加精细
这很好 但如果我选择
例如，处于待命或运行状态，然后测试模式
将不会截断
所以我们可以肯定我们现在捕捉到的事件是每当一个e
C 两个实例被停止或终止
我们做这个的原因是什么
也许因为每当这种情况发生
我们希望被通知
所以目标可以是一个事件桥
事件总线 当你想集中目的地时
或者你可以选择一个以下aws服务
你可以看到有很多可能的目标或操作
但是对我而言最有趣的是主题
因为我想将我的演示主题发送到一个消息，每当一个e
C两个实例被停止或终止
我会点击下一步下一步
然后审查和创建是
我的规则看起来很好
让我们创建这个规则
所以现在你可以尝试并开始启动这些两个实例
然后终止它或停止它
然后确保你订阅到你的主题的电子邮件中
然后你将收到一封电子邮件，每当一个实例被停止或终止
我认为这相当不错
所以总结一下
我们还有归档来找到你的所有事件归档
如果你需要的话
以及重放来重放事件
并将它们放回事件总线以正确修复你的集成
最后你有这个模式注册器
这是一个你可以查看这些事件模式的方式
例如我们可以查看aws事件模式注册器
我们输入aws点
说e C two并且你可以查看e
C two实例状态更改通知
这是版本一
模式类型是open api three zero
这就是模式本身
这意味着它定义了进入这个事件的可能性
我们可以看到
id是一个字符串
来源是一个字符串
时间是一个字符串
以日期时间格式
这就是定义你的事件本身中事物如何看起来的方式
在你的事件本身中
多亏了这个相当复杂的模式
你可以实际下载代码绑定
它将为你生成代码
例如，Java
Python TypeScript
或Go 这样你就可以用更简单的方式在你的代码中操纵这些对象
而不必为你写一些手动代码
好的 这就是桥梁
正如你所看到的，这是一个功能非常全面的服务，具有许多不同的功能
但最重要的将是设置规则和设置事件总线
与资源策略
好的 就是这样 我希望你喜欢它 我会在下次讲座见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/088_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p88 14. Amazon Managed Workflows for Apache Airflow (Amazon MWAA).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


Apache airflow 是一个流行的工作流管理工具
亚马逊也支持这一点
以亚马逊管理的 Apache Airflow 的工作流形式
这基本上是一个 Apache Airflow 的主机环境
Apache Airflow 是一种批处理导向的工作流程工具
它可以开发、调度和监控你的工作流程
并控制你的数据和数据工程管道的流动
你使用 Python 代码定义它们
所以它不会使用一些拖放
图形用户界面来定义这些工作流程
你必须实际编写Python代码来设置它们
而那个Python代码需要做的是
创建一个称为有向无环图或DAG的东西
就像Apache Spark工作的方式一样，对吧
亚马逊和AA带来的是Apache Airflow的管理工具
所以你仍然需要编写代码并弄清楚如何配置它
但你不需要处理或安装服务本身
所以它只是以一种更简单的方式运行Airflow
所以如果你有一个更复杂的工作流程
或者你需要与ETL或可能准备机器学习数据进行某种复杂的协调
这可能是你想在Python和Apache中做的事情
Airflow可能是一个很好的匹配
这里有一个非常简单的例子
所以假设
例如 嗯
我们在这里创建了几个任务
一个是叫做hello
它只是调用bash操作符
在bash控制台运行
回声你好
然后我们还有一种东西叫做气流
这只是一个函数，它只是打印出单词气流
然后我们在两个任务之间设置一个依赖关系
我们说你好
胡萝卜胡萝卜气流
然后在Python中创建了这个依赖关系
然后一旦我们做了这件事，气流将确保
它将按照你想要的日程安排完成它需要做的事情
它也可以创建一个漂亮的工作流程可视化
你在那个图表中看到的
在下面看到的
我们有hello 它指向airflow是下一步
并且围绕它你可以看到关于它运行的频率
它的日程安排等信息
所以它有一个漂亮的网页界面，你可以用它来可视化它正在做什么
并且控制调度
但实际有向无环图的内容，就是它实际在做什么
现在是你需要用python代码编写的
记住，考试永远不会要求你写代码
嗯 所以不要过于纠结这里的语法
你只需要理解
你用Python使用Airflow创建DAG
那么这如何与亚马逊集成
所以你的定向无环图
创建这些工作流的Python代码上传到S3
然后加载到亚马逊管理的Apache Airflow中
Airflow会继续处理
你也可以将Python代码
与任何所需的插件或其他依赖一起打包
所以所有内容都保持在S3中
一旦NWA提取它
它会调度
并安排你需要的所有内容
这由每个定向无环图中定义的
很难说
NWA在一个VPC中运行
你应该至少部署到两个可用区以实现冗余
你也可以将其连接到通过IAM管理的私有和公共端点
这将控制对Airflow Web服务器的访问
记得我们之前看到的那个场景
你有一个漂亮的UI来可视化这些工作流
并安排它们
如果你想在VPC之外访问
你需要一个公共端点
它会自动扩展
在底层，Airflow工人使其发生
NWA会自动扩展那些工人，直到你定义的任何限制
你可能会控制成本
与其他服务的集成
NWA仅依赖于其他亚马逊服务的开源集成
但还有很多
你可以将NWA与Athena和AWS Batch集成
CloudWatch Dynamo
DB 数据同步 EMR
Fargate EKS Kinesis
Glue Lambda Redshift
SQS SageMaker S3等
当然，你需要访问这些服务的所有安全服务
如AWS Secrets Manager
调度器和工人本身在aws fargate容器中实现
这是底层架构的一个小快照
基本上，有两个vpc在幕后运行，mwa
有一个客户vpc，包含所有调度器和工人
它们与您正在处理的服务进行通信，从那些dag
它可以与云watch s三sqs等通信，无论您需要什么
并通过vpc端点
它还与元数据数据库通信
恢复那些工作流
以及airflow web服务器
这是您用于可视化和调度这些工作流的用户界面
再次 如果您没有公共网络接口
无法从外部访问该web服务器
但如果您在那个服务vpc中 仍然可以从那里获取 它
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/089_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p89 15. Full Data Engineering Pipelines.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 所以，这只是一个非常大且冗长的
我们讲座的目的是总结我们所看到的所有服务
并尝试理解它们如何相互配合。现在这可能对你来说相当沉重
但我希望你现在理解我所说的每一点
这只是复习
但如果你还不理解每个服务如何相互关联
这将为你提供一些启示
那么我们来谈谈实时层
首先我们有生产者将数据写入Kinesis数据流
例如 我们可以将其与Kinesis数据分析连接起来
以进行实时分析
我们可以使用Lambda读取该流并在实时中进行响应
或者我们可以将新的目的流直接发送到Kinesis数据流
或者我们可以将其发送到
数据火炬
如果您想在某处摄取数据
因此，如果它进入Kinesis数据流
也许我们在AWS上会有一个应用程序
C两个数据流，一个用于读取数据流，另一个用于进行一些分析，以及一些机器学习
它可以肯定地与亚马逊sagemaker服务进行通信，以进行实时机器学习
如果它在 你能看到数据火炬
它可以转换为orc格式，发送到亚马逊s3
然后加载到红移中
或者它可以以json格式将数据发送到亚马逊elasticsearch服务
例如
我们也不一定非要产生两个kinesis数据流
我们也可以生产到kinesis数据火炬
从那里 例如
将其发送到pocket格式到亚马逊s3
正如我们所知道的
我们可以连接亚马逊
你可以看到数据火炬到亚马逊kinesis数据分析
正如我们在之前的讲座中看到的动手操作
所以这仅显示了我们可以有很多不同的组合
可以看到数据流 可以看到数据分析
流数据等等
但希望这能让你理解
并且你正在尝试 你开始最终理解所有这些服务之间的区别以及这个实时层
接近实时层如何一起工作
好的 接下来我们将进入kinesis视频层
所以我们有视频生产者
例如它可以是一个摄像头
但也是深层次的植物
一个设备
我们被发送到kinesis视频流
记住每个设备一个流
也许我们会有识别
这是一个机器学习服务，弗兰克会介绍你，从那个视频流中读取
并产生一个充满数据的kinesis数据流
我们从这里了解我们已经见过
我们可以有亚马逊c2
你能看到firehose或kinesis数据分析吗
读取那个流
并做我们需要做的事情，反之亦然
我们可以只有有亚马逊e
c 两个实例 读取canes
使用视频流
然后与sagemaker交谈以产生另一个
你能看到数据流，那可能被读取
例如由lambda来实时反应通知
例如当视频看起来有什么问题
好的 现在我们有了批处理层
批处理层是关于处理数据
然后 所以，假设你的数据在on premise在mysql中
我们首先使用数据库迁移服务将其发送到rds
这是一个一对一映射
没有数据转换
它只是发送并复制数据从on premise到rds
然后我们想分析这个数据
所以我们会使用数据管道从rds获取数据
并将数据从rds发送到amazon s3存储桶作为csv格式
我们可以用dynamodb做同样的事情通过数据管道
并将数据插入到json格式的amazon s3存储桶中
然后我们需要将数据转换为
嗯 与etl一起进入我们喜欢的格式
所以我们可以使用glue etl来做一些etl
并在最后将其转换为parquet格式存储到amazon s3中
例如 说，我们的工作创建了许多不同的文件
我们可以使用批处理任务清理你的书
你的书存储桶一天一次
所以这是一个完美的用例批处理
因为批处理转换数据
它将定期清理一些文件在你的amazon s3存储桶中
好的 那么我们如何协调所有这些事情
嗯 分段函数对于确保管道能够响应并跟踪所有服务执行的各个方面将是很好的选择
响应式 并跟踪整个不同服务的所有执行
现在我们在如此多的不同地方有数据
创建一个aws glue数据目录来了解数据在哪里以及其模式将是很好的
以及其模式
因此 我们将在dynamodb、rds、s three等部署爬虫以创建该数据目录并使其与模式保持最新
以保持最新
这将是批处理层
最后还有分析层
所以数据存储在亚马逊s3中，我们希望进行分析
因此我们可以使用emr，Frank会向你介绍emr
这是hadoop Sparkhive
我们可以使用红移或红移光谱进行数据仓库操作
或者我们可以创建一个数据目录，将s3中的所有数据进行索引
并且会创建此模式
因此我们可以使用amazon athena以无服务器方式进行分析
以分析s3中的数据
然后我们分析了数据
也许我们想可视化它
所以我们将使用quicksite，弗兰克也会向你介绍
在redshift或athena上进行可视化
这就是整个数据工程模块的内容
我希望你喜欢它 我希望你学到了很多
每当我提到机器学习或一个服务你没有看到
例如 emr或quick site
不要担心 弗兰克会向你介绍一切
我就把你交给他了
这就是我在这门课程上的全部内容 希望你喜欢 我会再见到你们，弗兰克也会再见到你们
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/090_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p90 01. Intro Security, Identity, and Compliance.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


安全认证的每个部分都非常重要，数据工程也不例外，尤其是这里。
也许这里更重要。
因为你的工作可能涉及处理个人身份信息和其他敏感数据。
如果你不恰当地处理这些信息，
你可能很快会失去这份工作。
安全和治理在数据工程考试中占18%的分数。
所以不要跳过这部分。
我将从权限最小化的原则开始。
当你创建iam策略时，应将其应用于此。
考试指南提到了
斯斯蒂芬将覆盖大部分内容
包括深入研究
亚马逊kms
aws密钥管理服务
aws web应用程序
防火墙 aws保护
并对安全服务如何与其他服务集成进行深入探讨
特别注意亚马逊maci及其与entbridge的集成 因为这是数据工程背景下一个特别相关的安全主题
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/091_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p91 02. Principle of Least Privilege.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们深入探讨安全问题
身份识别与合规性
这是一个简短的部分
我们将从安全领域的一个基本原则开始
但这也是一个重要的原则
最小权限原则
考试指南确实提到你需要了解这是什么
这很简单 虽然
它只意味着你只授予某人所需的权限
执行给定的任务
所以这有点常识
是的 不要赋予人们超出他们需要的权限去做他们需要做的事情
否则 他们可能会想出一种利用或滥用他们现在多余的权限的方式
在开发过程中 从更广泛的权限开始当然很有道理
因为你不一定确切地知道你需要权限做什么
当你还在构建系统并弄清楚哪些组件
编造它 一旦你完成过一次
你真的知道某人需要对那个系统做什么
你应该锁定它一次
你有那个更好的想法，关于确切的服务和操作，一个工作负载需要超过
在这里右边 我们有那个例子
所以这只是一个iam策略，它限制了s3对列出一个特定桶的访问
并且进一步在这个桶的特定前缀内
所以那里上面的一半说的是
我只允许在特定的桶内进行桶列表操作
在数据报告路径那里
实际上读取数据
我允许的唯一事情就是使用这个IAM策略来读取CSV文件。
那些在我特定范畴内
数据切片 Slash报告
以点csv后缀结束
所以如果你有 例如
一个需要人们参与的工作流程
或者一个系统或服务从特定位置读取CSV文件
没有理由给他们提供超出该位置的任何其他访问权限
而且更进一步，只给他们需要从该位置获取的特定类型的文件
是的 这就是最小权限原则的一个例子
如果你不确定进行某项操作所需的权限
可以使用一个名为iam access analyzer的工具
它会根据实际观察到的访问活动自动生成最小权限策略
所以当你在测试你的系统时
你只是做你应该做的事情
你可以用我是
访问分析器自动给你一个起点，那就是 那些权限最低的策略可能看起来像什么
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/092_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p92 03. Data Masking and Anonymization.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们之前几次提到过数据遮蔽和匿名化
在本课程中 但似乎有必要在安全部分重新回顾一下
再次 这是一个重要的概念，考试期望你知道
所以，每当你处理个人身份可识别信息
或其他敏感数据时，遮蔽是一种方法
例如
你可能在信用卡账单上看到过
他们会遮蔽掉你信用卡号的所有但最后四位
只是为了确保如果有人得到了那张账单的副本
他们不能带着你完整的信用卡号码逃跑
同样的地方，你看到你的社会安全号码被列出
通常 你会看到除了最后四位数之外的所有数字被模糊处理
你也可以用它来模糊处理密码
当你输入密码时
你通常会看到它被遮蔽或模糊处理，用星号或其他什么
并且内置了用于模糊处理的支持，无论是在glue中
还是在redshift中
下面有一个掩码政策的例子
这将完全掩蔽一个信用卡号码
创建掩码政策
这是在红移 顺便说一下
全额信用卡掩蔽是那个政策的名称
以信用卡为字段包含那个信用卡号码的类型
使用far card two five six
然后那个掩码字符串
我们将说那就是要进入它的文本
这就是创建一个遮蔽策略的一个例子
在红移下遮蔽一个信用卡号码
再次 语法错误不是你需要了解的东西
我不这样认为 但请知道你可以这样做
另外，匿名化是另一种方法
所以，而不是遮蔽某物
你可以用无法追踪到其来源的数据来替换它
你可以用一个信用卡号码或密码来替换它
或者任何与随机信息相关的内容
你可以打乱它
这样你就可以打乱给定列中的所有内容
这样人们实际的信用卡号码就不会与实际的人匹配
另一种方法是加密
这可以是确定性的
你可以总是从加密相同的输入中获得相同的结果
也可以是概率性的
每次加密时可能会有不同的结果
哈希是另一种我们之前讨论过的技术
太 这就是将称为哈希函数的东西应用到字符串上
哈希的事情的是
你可能得到一个值哈希到相同的值
但你知道最后那只会增加它的匿名化
或者更好
只是删除它 或者不要首先导入它
如果你不需要信用卡号码或密码为你正在做的
不要取它
你知道如果你看到信息进来
删除它你的etl
或者甚至不尝试导入它从一开始
那是处理个人身份信息最安全的方式 首先不要拥有它
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/093_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p93 04. Key Salting.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


另一种提高个人或敏感信息安全性的技术被称为盐值处理（key salting）
如果你要对信用卡号或密码进行哈希处理
盐值的概念就是在哈希之前，要么在数据后面添加，要么在数据前面添加随机值
这个随机值被称为盐值（salt），它被添加到要哈希的数据之前
这样可以防止人们进行所谓的彩虹表攻击（rainbow table attacks）
他们只有一张已经使用过的密码列表，或者从某些非法渠道获取的密码列表
然后他们就可以通过这张表来查找匹配的密码
所以如果你存储的密码中有一些随机信息，无论是添加在密码的前面还是后面
那么即使有人拥有所有可能的密码列表，他们也无法通过简单的匹配来找到你的密码
这就是盐值处理的作用
这将迫使他们猜测那些密码是什么
这确保了相同的数据，如两个相同的密码
在不同的实例中不会生成一个相同的哈希
由于在前面或后面添加了这个唯一的盐值
最佳实践是确保你的盐值是强大的
加密安全的随机值
所以没有人可以尝试猜测它们可能是什么
你也想定期轮换那些盐值，使其更加困难
所以如果有人破解了你的盐值列表
你不会必然陷入麻烦
你还要确保每个用户都有自己的唯一盐值
你不想对每个用户重复使用相同的盐值
如果有人发现那个盐值
你就完了 那对任何人都没有意义了
所以更好的做法是为每个用户生成一个唯一的盐值
所以你要做的是盐值
然后在存储它们之前对你的密码进行哈希处理
这样存储的密码值会更难破解
因为有了这个随机的盐值作为它的一部分，让这个更真实
以一个例子 假设我们有用户一和用户二
他们使用相同的密码
他们并不擅长创建密码
所以他们的密码都是'password one two three'
然而 他们都有独特的盐值，这些盐值是随机且加密安全的
与每个用户相关联
因此用户一有一个盐值
用户二有一个不同的盐值
如果我们在每个密码字符串后面加上每个锯齿值
你可以看到，使用sha256算法对那个进行哈希后
它们看起来彼此完全不同
尽管它们编码的是同一个确切的密码
这就是一个额外的安全小技巧
以及你是如何存储你的个人身份信息的
只要你知道每个用户的盐是什么
你还是可以为那个用户的密码创建相同的哈希值 并确保它们在你的应用程序中正确比较和匹配
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/094_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p94 05. Preventing Backups or Replication to Disallowed AWS Regions.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


另一个合规问题可能是如何确保数据不存储在禁止的区域内
你可能需要在不允许存储数据的地理区域外保持数据
因此，你通常会有关于数据可以存储在哪些地区的限制
不同国家有不同的法律关于如何存储个人身份信息
以及隐私法律等
因此，有时你需要小心你实际存储数据的管辖区
不小心或错误地将数据发送到不应存储的地方，这很容易发生
这是一个非常常见的原因
备份或复制
你可能是出于好意
将您的数据备份到不同地区以提高单一地区故障的韧性
但如果您不小心
您可能会将备份或复制操作
放置到不允许存储数据的地方
有一些方法可以强制执行这一点，有一个系统叫做aws organizations
这是我们没有详细讨论的
但它是存在的
它有一个称为服务控制策略的功能
这是用于强制执行此类地理限制的一种方式
更常见的方式是使用iam策略和s3存储桶策略
为了确保您将所有事情锁定到地区
数据允许存在
所以你只需要对你的iam策略给予更多的思考
并且想想我不仅允许哪些操作
但我为什么允许这些事情发生
所以你需要小心谨慎
特别是在配置RDS备份和复制的权限时
极光 红移
或者任何其他数据库或数据存储
并且你还想设置你的监控和报警
使用云轨迹和云监控
所以如果事情错误地进入了错误的国家
至少你会很快知道
你可以处理这种情况
只是为了一个真实的例子
这里是一个iam策略
它的目的是确保我们不会在美国东部1区域以外的区域备份任何rd
数据库
并且美国西部1区域
所以如果你仔细研究这里的声明
我们说我们将只允许创建db快照
和将db快照复制到u
S西一区及u东一区
因此我们将这些备份限制在这两个区域
然而，我们将允许所有其他操作在任何地方进行
因此该政策的第二部分说我们将允许任何其他操作
除了创建db快照或复制db快照，任何地方
这仅仅是确保我们不会将数据复制到那些区域之外的地方 但我们仍然可以从任何地方访问这些数据
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/095_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p95 06. IAM Introduction Users, Groups, Policies.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


欢迎来到第一个深度探索电子服务
第一个叫做我是
所以我是代表身份和访问管理
这是一个全球服务
因为在我是中，我们将创建我们的用户并将它们分配到组
我们已经使用过
在不知道我是的情况下 当我们创建账户时
我们创建了一个根账户
并且它是默认创建的
这是我们账户的root用户
并且你唯一应该用它做的就是设置你的账户
就像我们现在要做的那样
但之后你不应该再用这个账户
甚至不应该分享它
你应该做的事情 是创建用户
所以你将创建用户
在i am中，一个用户代表你组织中的一个人
用户可以分组
如果这有意义
那么我们来举个例子
我们有一个由六个人组成的组织
我们有alice, bob, charles
David Edward和fred
所以这些人都在你的组织里
现在alice, bob和charles
他们一起工作 他们都是开发者
所以我们要创建一个名为开发人员的组，将爱丽丝、鲍勃和查尔斯重新分组
结果发现大卫和爱德华也一起工作
所以我们要创建一个运营组
现在我们有两个组
组中只能包含用户
不能包含其他组
这是一个非常重要的事情要去理解
组只能包含用户
现在
一些用户不需要属于任何组
例如 弗雷德
他就在这里独自一人
他不属于任何组，这不是最佳实践
但这是亚马逊云服务可以做的事情
而且一个用户可以属于多个组
这意味着 例如
如果你知道查尔斯和大卫一起工作
他们是你审计团队的一部分
你可以创建一个由查尔斯和大卫组成的第三个组
正如你所看到的
在这个例子中 查尔斯和大卫属于两个不同的组
所以这就是可能的配置
因为我是
那么我们为什么创建用户，为什么创建组呢
因为我们想让他们使用我们的aws账户，并且允许他们这样做
我们必须给他们权限
这样用户就可以被分配到组里
这叫做json文档
我现在就向你展示它的意思
这叫做策略，我是策略
所以它看起来就像这样
你不必是个程序员
这不是编程 这只是在描述
我认为这是简单的英语
用户可以做什么
或者一个组 以及组中所有用户可以做什么
在这个例子中 我们可以看到，我们允许人们使用e
C 两服务并描述它
使用弹性负载均衡服务
并描述它
并使用云观察
现在我们将看看EC two弹性负载均衡和云观察意味着什么
但通过这份看起来像是这个的JSON文档
我们允许用户使用一些AWS服务
因此这些策略将帮助我们定义用户的权限
所以在AWS中
你不允许每个人做任何事情
那样将是灾难性的
因为新用户可以基本上启动许多服务
这将花费你很多钱
那将花费你很多钱
或者这将是安全的一个问题
所以在aws中，你应用一个原则叫做最小权限原则
所以你不给予用户多于其需要的权限
好的 所以如果一个用户只需要访问三个服务
只需为该用户创建权限
所以现在我们已经概述了
我是 让我们进入下一讲进行实践 创建用户和组
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/096_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p96 07. IAM Users & Groups Hands On.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


那么让我们开始练习吧
使用 i am 服务在 AWS 中创建用户在搜索栏中
我只是有我，并且我进入控制台
所以到达仪表盘上时
我们有一些安全建议，我们现在可以不予理会
我想吸引你注意的地方是
在那边，左手边，我们前往用户
这就是我们创建的东西，用于创建用户，我是
但是首先让我们注意到一些事情
如果你去右上角并点击全球
你可以看到区域选择不活跃
这意味着我作为一个整体服务是全球服务
因此没有区域可供选择
当你在其中创建一个用户时，它将到处可用
但一些其他控制台在本课程中我们将看到将是区域特定的
所以只是有些要注意
好的 现在我们有用户和为什么我们要创建用户
我们创建用户 因为目前我们正在使用被称为根用户
所以如果你点击这个
你看 你只能看到账户ID
当你有这个
这意味着你在使用根账户
使用根账户并不是最佳实践
因此我们想要创建用户，如管理员
这些用户将使我们能够更安全地使用我们的账户
所以让我们去创建一个用户
我会提供一个用户名
例如 斯特凡
所以 当然，我想为自己提供访问管理控制台的权限
所以我要做这个
我们有选择使用身份中心
这是推荐的或者创建一个eye用户
我将选择第二个选项
因为它更简单
从考试角度
这就是你需要知道的
但不用担心 这不会影响你的课程进展
好的 所以我们创建一个iam用户
现在我们需要设置密码
所以如果这个用户不是我
我会保留为自动生成密码
我会保留这个选项，使得用户在下次登录时必须更改密码
但因为这是我
我只是要输入一个自定义密码，我会把这个
因为我在下次登录时不需要更改密码
所以点击下一步
我们需要为这个用户添加权限
我们可以直接添加
或者我们可以从组开始
所以我们创建一个组，我们将创建一个组
组名称将是管理员，策略名称将是管理员访问
所以现在这已经完成
我们可以将用户添加到管理员组
所以点击下一步
现在我们可以查看一切
我们有用户名
在组中的权限
我们有标签，标签在aws中无处不在
它们是可选的 但它们允许您为许多资源添加元数据
例如 我可以说stefan的部门是工程
这不是我在课程中到处要做的事情
但我会向你展示一次如何将标签添加到资源在ok
所以现在用户已成功创建
所以现在我们可以发送登录指示或双CSV文件
然后我们可以使用此用户登录
但首先让我们返回到用户列表并查看一切
这是我的用户列表
这里有我和我们也有组
所以如果我去左边的用户组
我们有管理员 所以让我们观察管理员
所以管理员有一个用户在里面名为stefan
如果你看管理员权限
你看到管理员组中附有管理员访问权限
如果我去我的用户
stefan在这里
我们可以看权限策略并看到它也有管理员访问权限
但这一个没有直接附加
它通过组管理员附加
这意味着stefan继承了组的任何权限 它属于
这就是为什么我们把用户放在组中，
这样管理权限会更简单
所以现在让我们回到我们的仪表板
我们要用我们的用户 stefan登录
所以首先我们可以看我们的账户，它有一个账户
它有一个签名URL
现在您可以很容易地自定义此签名URL
通过创建称为账户别名的内容
所以可以是stefan v3，然后创建别名 例如v5是可用的
所以现在使用这个别名
我现在可以使用这个别名简化我的签名URL，使用我的stefan账户签名
我们可以使用相同的浏览器
或者我们可以在新的隐私模式下创建一个新的浏览器窗口
这样做的好处是我们可以使用aws在两个窗口并排显示
如果你不这样做
那也没关系 但是如果你在右边的窗口使用stefan账户登录
那么你在左边的窗口会被断开
这就是唯一的区别
所以如果你想同时使用两个账户
根账户在左边，我的账户在右边
我所做的一个技巧是我在我的网页浏览器中使用一个隐私窗口
Chrome有这个功能
Firefox将来也会有
Safari 将来也会有
所以当我粘贴签名URL时
如你所见 我得到了签名
作为am用户
为了到达这个页面
我们可以回到1
如你所见 当你在aws进行签名时
你需要根用户登录
或者iam用户登录
为了回到这个页面
我们转到iam用户
我们可以输入账户ID或账户别名，我可以在这里复制
然后我们会被带到这个页面
iam用户名将是stefan
密码将是你之前设置的
然后你登录
所以现在酷的地方是
如果我看顶部右边
我是使用我iam用户登录的
所以它说账户ID和iam用户
但如果我在这里看顶部右边
它只说账户
这表明它是根账户
所以我们在这里 我们在左边通过一个普通窗口插入了根账户
我们在右边通过一个隐私窗口插入了iam用户
请确保不要丢失你的根账户登录和你的管理员登录
你将不得不联系aws寻求支持
坦白说，我现在无法帮助你
从课程的角度来看
我推荐您使用您的用户，而不是使用root用户
但这只是一个普通的建议
有时您会看到我在使用root
有时我在使用iuser
但当您必须使用root时
或者当您必须使用iam用户时
我会在课程中告诉您
所以不用担心
现在，其余部分的内容
请保留这两个窗口打开 我会在下一节课再见您
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/097_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p97 08. IAM Policies.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 现在我们来讨论
深入的策略
假设我们有一组开发者
爱丽丝，鲍勃和查尔斯
我们在组级别添加一项策略
在这种情况下，该策略将应用于组中的每个成员
所以爱丽丝
鲍勃和查尔斯
他们都将获得访问权限并继承这项策略
如果你有一个操作组，他们有不同的政策
大卫和爱德华将有不同于开发人员的政策
如果弗雷德是用户
他可能不属于任何组
我们有创建所谓的内联政策的可能性
这是一项仅适用于特定用户的政策
因此，该用户可能或不属于任何组
您可以为任何用户创建内联政策
最后，如果查尔斯和大卫都属于审计团队
并且您为审计团队附加了政策
查尔斯和大卫也会继承审计团队的政策
所以，在这种情况下，查尔斯从开发团队继承了政策，并从其他团队继承了政策
而大卫从其他团队继承了政策，从运营团队继承了政策
当我们进入实际操作时，这将使很多事情变得清晰
在政策结构方面
您只需要在高层次上了解其工作原理以及其命名方式
所以在aws中，这将是您经常看到的东西
所以熟悉这种结构
这是相邻的文档
因此，iam政策结构包括版本号
通常它是2012年
十七 这是政策语言版本
这是一个标识符，用于识别
嗯 该政策是可选的
然后有更多的声明，声明可以是一个或多个
并且声明有一些非常重要的部分
所以声明ID是声明标识符
这是语句的标识符，同样也是可选的
所以在右边是数字一
政策的直接影响
所以它是语句是否允许或拒绝对某些api的访问
所以在右边这是被允许的
但你也可以看到拒绝
原则包括哪个账户用户或角色
这是政策
这个政策将应用于哪个
所以在这个例子中
它应用到你的aws的根账户上。
账户操作是您将根据效果被拒绝或允许的API调用列表。
并且资源是一组资源列表，这些资源将应用到操作上。
所以，在这个例子中它是一个桶
但是它可能是很多不同的事情
最后，在这里没有被表示。
但是，有一个条件，根据这个条件，这个陈述应该被应用还是不应该被应用
这里并没有表示这一点
因为它是可选的
那么进入考试时
你需要确保你真的理解了效果
原理 行动和资源
但是别担心 你会在课程中看到这些
所以你应该在课程结束时对它们充满信心
这就是这节课的内容 我希望你喜欢它 我会在下节课见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/098_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p98 09. IAM Policies - Hands On.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


那么现在让我们深入研究一下我们的IAM策略
我将详细解释IAM策略
首先，让我们从用户开始
如你所见，用户范是管理员组的一部分
因此，它具有对aws us的管理员访问权限
这意味着如果我使用我的用户Stefan进入控制台
现在，我正在使用我的用户Stefan
然后我向左侧点击用户
如你所见，我可以看到我的用户Stefan，他正在这里
因此，我的用户Stefan具有执行任何操作的权限
因为我们不是 这是一个管理员
但我要做的是，我会去管理员组
然后我会从该组移除我的用户stefan
通过移除用户
我现在已经做了
这样stefan在剩余部分失去权限
我们怎么确保这个呢，嗯
让我们刷新这一页
如你所见，我现在看到零用户并且被拒绝访问
它说我没有权限做这件事
我是列表中的用户
所以我因为我移除了stefan用户出管理员组
所以我失去了在右侧查看用户的权限
让我们尝试修复这个问题
让我们进入
我是，我们将在
用户这里找到定义
但现在你可以看到stefan没有零权限策略
但我们将添加权限
所以我们可以直接添加权限
或者创建一个内联策略
所以我们添加权限
这将会更容易
所以我们可以将用户添加到组中，这不是我们要的
或者我们可以直接将策略附加到我的用户
所以我要附加的策略是
我是只读访问
这将允许我的用户斯泰凡读取任何在
我是中的内容，这是我们想要的
那么我们添加了这个权限
现在政策已经添加
那么我们回到这里
让我们刷新这一页
正如你所看到的
我终于可以再次进行我的api调用
看看我的用户类别中的stefan用户
所以我可以查看用户
我可以查看管理员等用户组
但我能创建一个组吗
让我们尝试创建开发者组
然后创建这个组
正如你所看到的，我无法创建它
因为我没有权限创建组
我只能在IAM上进行只读访问，所以我
因为我只有只读访问权限
所以我无法创建组
这表明您只能对用户进行权限设置，让他们做他们应该做的事情
当然，如果我想给右侧的创建组权限
我们需要附加一个更大的权限集
例如IAM的全面访问
接下来 让我们做点事情，接下来
我将进入左侧的用户组
我将创建一个组
这个组将被命名为开发者
然后我将添加用户到该组中
我将附加任何我可以找到的策略
例如Alexa for Business
但这并不重要 只需附加您能找到的第一个策略
让我们创建这个组
好的 所以这已经添加了
最后，让我们进入管理员组
再次我们将添加用户并将Stefan重新添加到该组中
所以现在如果我们回到Stefan用户
所以让我们进入IAM并查看我们用户并查看Stefan
我将关闭右侧的使命
所以如果我们查看Stefan作为一个用户
如您所见，我们为用户添加了三个权限策略
我们继承了来自组的管理员访问权限
管理员我们有这个Alexa for Business
管理策略，该策略通过开发者组附加
最后，我们有IAM只读访问，该策略直接附加
因此，正如您所看到的，我根据它是如何附加的继承了不同的权限
现在让我们详细看一下策略
在左侧
让我们看一下策略
首先让我们看一下此管理员访问策略
如果我们看一下它
它是给我们提供访问所有事项的权限
如果您看一下此策略中定义的权限摘要
如您所见
这允许我们访问AWS的所有服务
这个数字可能会随时间变化
没关系 课程将保持最新
所有这些服务
例如 应用网格或商业亚历克斯
放大
他们都有完全的访问权限
那么这种权限是如何被明确定义的呢
如果你点击json
这是这项政策的相邻形式
我们可以在这里看到，我们有允许动作明星和资源明星
因此成为明星意味着
任何事情 这意味着我们允许对任何资源进行任何操作
当然，允许对任何资源的任何操作与给予某人管理员权限是相同的
这就是它被定义的方式
如果我们看一下另一项政策
例如 我们之前看到的i am只读访问
如果我们看一下
我们看到i am是授权的全列表和有限读取
如果我点击它
你可以实际查看作为此政策一部分允许的所有api调用
这非常有用
但如果我们看看这是如何被实际定义的
点击j
这里我们有json文档，显示了这是如何被定义的
效果是允许
然后我们列出被允许的api调用
我们有这个
还有这个 然后我们有获取明星
所以当你有获取明星时
它说任何以获取开头的东西
然后它有一个授权后的东西
例如获取用户或获取组，列表同样适用于此
所以我们有一个列表星
所以列表用户或列表组
所以通过使用星
我们涵盖了并将它们分组
将许多API调用结合在一起
所以所有这些都在资源星上允许
因此这就是阅读完全访问策略的总结
这就是只读I am策略的组成
所以这非常有用
你也可以创建自己的策略
让我们创建一个策略，我们有一个可视化编辑器或JSON编辑器
如果你有JSON
你可以非常简单地编辑这个并用这个构建器创建你的相邻文档
这非常有用
或者你可以使用可视化编辑器
例如 假设我是
我们希望为'我是'创建东西
我们想要授权什么操作呢
我们想要授权列表中的用户
所以我们将获取用户
只需两个API调用
正如我们所见
我们从38个列表中选择了一个，从32个读取中选择了一个
然后我们想要授权这个在什么上呢
所有资源还是特定资源
这是一个非常简单的例子
但你可以看到这个构建器非常实用
当你点击下一步时
你可以查看并说我的
这是我的权限
然后我们创建这个策略
如果我们查看创建的策略
我们可以查看对应的JSON并看到确实
通过视觉
视觉编辑器我们允许了
我是列表中的用户和我是获取用户在资源star上
然后我们可以将这个策略附加到组或用户等
这就是在AWS中管理权限的方式
所以现在来总结一下这个实践
让我们转到用户组并删除开发者组
因为我们不需要它
然后我将进入我的stefan用户
我将删除这个直接附加的
我是只读访问权限
所以现在stefan只属于管理员组
它具有管理员访问权限
当然如果我回到my
我是conant这里并查看用户正如你所见是的
一切都显示正常
所以它正常工作
好的 这就是这节课的内容
我希望你喜欢它 我将在下节课见到你
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/099_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p99 10. IAM MFA.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们已经创建了用户和组
现在是时候保护这些用户和组免受损害
为此我们有两种防御机制
第一种是定义所谓的密码策略
为什么
因为使用的密码越强
账户的安全性就越高
在aws中，您可以使用不同选项设置密码策略
首先，您可以设置最小密码长度
并且要求特定的字符类型
例如，您可能希望有一个大写字母
小写字母，数字
非字母数字字符
例如问号等等
然后您可以允许或不允许
允许用户更改自己的密码
或者您可以要求用户在一定时间后更改其密码以使密码过期
例如说每90天
用户必须更改其密码
最后，您还可以防止密码重复使用
这样用户在更改密码时
就不会更改为他们已经拥有的密码
或者更改为他们之前使用的密码
这很好 密码策略确实对账户的暴力攻击有很好的防护作用
但在考试中你需要了解的第二个防御机制是
多因素认证或MFA
你可能已经在一些网站上使用过它
但 是的
这是非常推荐的，必须使用
这样用户就可以访问你的账户
他们可以做很多事情
尤其是管理员
他们可以更改配置
删除资源等
所以你绝对想保护至少你的root账户，希望所有的iam用户也能得到保护
除了密码之外，我们是如何保护他们的呢
你使用mfa设备
那么mfa是什么
Mfa 使用的是你知道的密码和你拥有的安全设备组合
这两者结合在一起，比仅仅使用密码具有更高的安全性
例如 让我们以爱丽丝为例，爱丽丝知道她的密码
但她也有一个MFA生成令牌
通过使用这两者
在登录时 她将能够成功在MFA上进行登录
MFA的好处是即使爱丽丝失去了她的密码
因为它被盗或被黑客攻击
账户不会被泄露
因为黑客还需要拿到爱丽丝的物理设备
这可能是你的手机 例如用于登录
显然这要少得多
所以mfa设备是什么
aws中的选项
你需要在考试前了解它们
但别担心 它们很简单
第一个是虚拟MFA设备
这就是我们在实操中会使用的
所以你可以使用Google Authenticator
它只能同时使用一个手机上
或者使用iSO for a i
它支持多个令牌在同一设备上
这意味着使用虚拟MFA设备
你可以拥有根账户
你的IAM用户
另一个账户
另一个IAM用户 这取决于你
您可以在虚拟MFA设备上拥有任何数量的用户和帐户
这使得它成为一个非常容易使用的解决方案
现在我们有一个叫做通用二次因素或UTF安全密钥的东西
这是一个物理设备
例如，UB Key由UBO和UBIO提供，这是一个第三方AWS服务
这不是AWS服务
而是由第三方提供 这是一个第三方
我们使用一个物理设备
因为可能它非常容易
你把它放在你的钥匙环上
然后你就可以出发了
所以这款UV钥匙支持多条路线
我允许用户使用单个安全钥匙
所以你不需要像用户一样多的钥匙
否则将会是一场噩梦
然后你有其他选择
你有一个硬件钥匙
优秀的MFA设备
例如由Jamalta提供的这个设备
这也是AWS的第三方
最后，如果你在美国使用政府的云
地区政府云
那么你有一个特殊的密钥fab，看起来像这样
由Sure Pass ID提供
这也是第三方
就是这样 我们已经看到了理论和如何保护你的账户
但是让我们在下次讲座中实现这一点 所以下次讲座再见
```

### /content/drive/MyDrive/bilibili/Udemy–AWSCertifiedDataEngineerAssociate2025–HandsOn!part2/100_Udemy – AWS Certified Data Engineer Associate 2025 – Hands On! part2 p100 11. IAM MFA - Hands On.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以我们首先定义一个密码策略
点击账户设置
在左侧
你会找到密码策略
你可以编辑它
在这里我们可以使用默认的密码策略
它包括这些种类的要求
或者我们可以自定义密码策略并强制密码的最短长度
我们也可以要求大写字母
小写字母 一个数字
一个非字母数字字符
我们也可以启用密码过期来启用
例如 在90天后
或者要求管理员重置密码
或者我们可以允许用户自己更改密码
或者我们可以阻止密码重复使用
所以这个密码可以在项目控制台直接编辑
这就是安全的第一部分
第二部分是关于为你的根账户设置多因素认证
所以你点击账户名称然后点击安全凭证
如果你以根用户登录
你会看到我的安全控制
根用户现在有一种方法可以保护你的根用户
这是你aws账户中最重要的账户
你可以通过使用多因素认证来保护它
我只是要在你面前演示它是如何工作的
但我有学生因为丢失了他们的多因素认证设备而锁住了自己的账户
因此
如果你认为你有丢失手机的风险不要这样做
只是保持你的手机在身边 只是看我的视频
这将足够好如果你想与我一起练习 你也可以删除mfa设备在激活后
好的
但我们继续分配mfa设备 所以我会叫它我的iphone
因为这是我有的
但你可以给它起任何名字 嗯然后你可以选择方法设备类型
所以它可以是一个验证应用
这是我将要使用的
但也可以是安全密钥或硬件totp令牌
所以我会使用验证应用
因为它将是虚拟的
现在我们进入应用的设置
这里有一个兼容应用的列表你可以在这里找到
所以这就是全部
对于安卓和ios
这是我们知道与aws os配合良好的
因此我将使用Twilio Authenticator
这是我喜欢的应用
所以我需要做的是
实际上在我的手机上启动该应用
然后点击显示二维码
当你扫描二维码时
你需要直接在手机上扫描此二维码
为此您需要添加帐户
你扫描这里这个二维码，一旦扫描成功
它会添加账户并开始给它命名
所以我们就保存这个 看起来不错
然后我们就能获取到MFA码
所以这是第一个MFA码
所以是三零一九三五
所以这是我iPhone实时生成的代码
而且这个代码会随时间变化
所以为什么需要两个码
那是确保MFA设备正确设置的意思
然后确保代码准确无误
第二个代码是792
843
当然，每个设备的代码都不同
一旦这两个代码输入完毕
点击 添加MFA
如您所见，目前可以恢复至8个MFA设备
您可以向下滚动并在这里看到它们
所以多因素认证mfa一
它被称为我的iphone 这是目前创建的
所以如果你想删除它
你可以删除它等等
但是所以如何使用mfa现在
如果我从aws登出
然后我重新登录
所以我将使用我的路由器账户和密码
在成功登录后
我有进入的mfa代码
于是我打开我的app并输入我看到的代码，然后点击提交
这样我就登录成功了
这太完美了，因为我们账户上有额外的安全级别
这就是这节课的全部内容 希望你喜欢 下次课再见
```