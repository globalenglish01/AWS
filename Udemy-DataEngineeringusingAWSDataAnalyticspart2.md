### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/001_Udemy - Data Engineering using AWS Data Analytics part2 p01 13. Process data using Spark APIs.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止，我们已经看到了如何将文件读入数据框中
现在 让我们处理数据中的交叉项
以便我们能够按照我们希望的方式写入目标
我们想要的方式 我们希望最终按年份解析数据
以及月份和日期
为了实际处理这一点
我们不得不添加新的列
这些列可以是年份
月份和日期 或者你可以使用你想要使用的任何命名约定
我将使用年份 月份和日期
话虽如此 让我们从创建程序开始
程序名无非是process.py
让我说 这里的process
它将负责创建程序
使用process pi作为名称
现在我可以定义一个名为transform的函数
它将接受一个数据框作为输入
并且它将是一个数据框
但这个数据框将具有额外的列
作为此过程的一部分，我们将应用函数
称为year 月份和日期
有一个名为created date的列
作为json文件的一部分
我们将使用那个创建日期字段
我们将提取年份
月份和日期 以便我们实际上在稍后的时间将数据写入目标时解析我们的数据
让我们先看看数据
并且探索APIs
这样我们就可以理解我们如何实际上一旦我们探索了数据就可以转换数据
我们将实际上在我们的程序中处理实现
话虽如此 我们有一个名为数据的子文件夹在我们的项目目录中
在这个文件中，我们将使用这个文件来理解数据的结构，以及如何使用现有列上的功能来提取月份和日期
我们还将了解如何使用现有列上的功能来提取月份和日期
现在我可以说pi spark
它将负责启动pi sparks cli
一旦它启动
我可以实际说df等于spark点read点json
然后粘贴这个路径
在这个情况下，我正在使用相对路径
这就是为什么它以data开头
只要从这个位置这个路径是正确的
它将没有任何问题地工作
如果你发现类似路径未找到或未找到的错误
你需要确保你传递的是完整路径或正确的相对路径
现在我可以按回车
它将负责创建数据框
一旦数据框创建完成
我们应该能够运行这段代码来打印模式
你可以看到有一个名为has created yet的字段在最开始
我们可以利用它来提取年份
月份和日期 这样我们就可以解析数据
最终你可以在这里看到
你也可以通过说 df 点
选择 created yet 然后
这样它会处理打印第一个二十 created yet
现在让我们深入了解我们如何可以提取年
月和日从这个 created
为此首先我们必须导入适当的函数
函数在 pi spark dot sql dot functions 下
你可以说 import year month
日期输入
你可以说df点列名
我们在现有模式中添加新列
为了给数据框添加新列
我们可以使用这个列函数
然后我们可以实际指定列名
然后我们可以使用函数
然后我们可以将列名作为字符串像这样传递
现在您可以说显示它将处理添加额外列
然而 而不是啊
打印所有内容 我只想打印 created yet 和 here 以便我说
选择 created yet
然后这里然后显示否则模式很大
他们会看到很多信息
对我们来说获取 created yet 和 here 的汇总会很困难因此
我使用这种方法
现在我可以按回车
你可以看到日期和日期的年份部分像这样
你应该能够添加额外的列
让我删除这个并添加额外的列
额外的列什么也不是
我们可以使用month函数
我们已经导入了这个
然后传递created到这里
然后对于这一天我们可以使用day of month
这也已经导入了
我们可以传递created it到这里
然后我可以说选择created yet在这里
月份和日期
这将为我们处理违规行为
然后我们可以说显示在这个上面
我们应该能够看到日期和年份
月份和日期 这就是你如何实际提取信息的方式
因为我们得到了所需的逻辑
我们应该能够处理在这里实施它
然而将不仅选择创建
作为部分数据框
将直接使用列
并将在这里衍生
在这个现有的数据框上月份和日期
我只需要说从pi spark dot sql dot functions导入这里
然后月然后日期
我现在可以实际滚动
复制粘贴
然后中心内容
我必须稍微清理一下
现在我们可以读取带有附加列的年份的数据框
月份和日期
因为我们已经完成了处理逻辑
现在我们可以去app dot py并调用它
我可以说df underscore transformed等于first
我必须导入函数
函数是处理部分的
函数名称只不过是transform
我可以说transform的df
然后我可以实际上打印df on transformed的schema
我也应该能够选择不仅报告星
但也包括附加字段，如created yet年份和月份
然后现在让我保存这个并让我运行这个
我们已经定义了所有必要的环境变量作为早期的主题
因此我们不需要太担心更改任何东西
作为部分的配置
不更改任何东西使用现有的环境变量本身
我们应该能够运行此程序
在添加转换逻辑后
您可以在这里看到输出rapport相关字段的顶部
如id名称和URL
我们还获得了created yet年份
月份和日期 因为我们也包括这些作为调用的一部分
这就是你应该能够转换数据的方式
我们应该能够利用年份
月份和日期来配对目标
让我们进入关于如何重构代码的详细信息
以按年份和月份将数据写入目标 月份和日期
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/002_Udemy - Data Engineering using AWS Data Analytics part2 p02 15. Write data to files.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


正如我们所成功创建了一个包含额外字段的数据框。
例如那一年 月份和日期
让我们理解我们应该如何利用现有的api
将数据框写入文件
因此，在划分数据框时
让我们深入细节
克 我们已经有了一个数据框，它的名字是df
我们可以利用这一逻辑现在
让我复制这个，然后转到这些程序之一，从这里不运行
我会只是粘贴，然后清理这个
然后我会把它提升到进行转换
让我删除这个
然后让我复制并粘贴到终端这里
我们已经在这个pbox里，让我粘贴这里
我们已经将数据框分配给了需要转换的附加字段
这将包含我们原始数据框中所有字段
再加上这三个字段
如果你想 实际上可以说 df.transformed.dot.print.schema
转换后的Df类型是数据框
它现在有所有这些字段
作为数据框的一部分
它暴露了一个名为right的属性
实际上可以说 df 变换了
然后说对了
它是数据框写入器类型
使用此属性
我们应该能够探索APIs将数据框写入文件
如果你只是说 df transform dot right and then dot
然后按tab
它会实际上显示所有可用的api
你可以将数据框写入json
使用json 使用parquet写入parquet
你也可以将数据框写入spark matter
使用insert into存储表
你也可以使用format
在将数据框写入到任何目标时，你可以指定格式
在这些事物之上
也有被称为部分的一个功能
并且我们应该能够利用此人
在将数据写入目标之前，实际进行数据清洗
通常我们在这些文件中写入之前会使用它
在这种情况下，我们不得不说 df_underscore_transform.dot right partby
我们必须度过去年
月份和日期给它
然后我们应该能够调用适当的api来写入文件
当我们尝试将数据框写入文件时
如果你处理的数据量过多
可能会产生许多小文件以减轻这个问题
在数据框上方
还有一个名为cos的功能
如果我说df_transform点并按tab
你可以看到有一个名为coil_us的功能
你应该能够利用coil_us来实际减少文件数量
让我看看关于coil_us的详细信息
这就是你可以利用的
当我们想把数据写入较少文件时
无论有多少执行进行数据处理
我们都会使用coil_us
有时候我们可能需要将数据写入比实际数据更多的文件
实现这一目标的一种方法是使用parts
我们需要付出一些代价
因此我们在使用repartition之前需要三思
它可以用于写入多个文件
但kois更好
因为它在性能上更好
因为它不会涉及称为shuffle的东西
现在不要担心太多细节
记住，我将使用coil_us
它也用于将数据框写入较少文件
然后我将在其上方工作
使用coil_us后，我们将得到一个数据框对象
使用数据框对象
在quis之后
我们应该能够使用
然后在其上方
我们应该能够说通过并继续
让我们现在详细看一下
我只是说coil_us of 16
然后写入
然后按年份，月份
让我赋给它另一个变量
我将说df_parts等于并按回车
一旦按回车，返回的内容
将分配给这个
df_one_score_person 我们应该能够通过复制粘贴来检查其类型
你可以看到它是数据框类型
它仍然写了称为数据框写入器
这是数据框写入器
你应该能够利用数据框写入器提供的API
这些只是格式和传递给它
或者ah 你也可以使用直接API
例如para json等
现在利用这些知识
让我们来 起一个名为程序的名字
点 py 我是对的
点击这个 然后新建一个 python 文件
然后我会定义一个函数
函数的名字会是 to underscore files
Df 会传递数据框到这个
传入的数据框的参数就是 df
然后目标位置
然后文件格式
这三个是我们将要传递给 two files 函数的参数
现在 我可以说 df. coil 是十六
我现在是硬编码
在实际的应用开发中
我们可能需要配置
我们将要使用的零件数量
有时候我们可能会将其定义为属性的一部分
有时候我们可能需要开发逻辑来确定应该使用多少个文件
来写入文件
请记住
在实际开发中我们可能不会硬编码
在很多情况下 要么
它会是属性文件的一部分，我们会确定
每个工作流的文件数量
或者在运行时动态确定
根据我们写入目标之前的数据量
现在我可以说 right then parson by
年份月份然后日期点反斜杠
然后模式追加
我使用模式追加是因为
因为我们会在每次运行中写入一天的数据
如果目标文件夹已经存在
如果不使用模式追加
它会失败
或者或者 right 或者它会处理写入现有文件的情况
追加只会创建额外的文件夹并将文件复制到基文件夹
基文件夹就是年份的父文件夹
这将作为目标目录的一部分传递
在定义了 mod 之后
你可以实际说格式
格式就是我们将要传递的文件格式
然后保存目标目录
这就是你可以处理将数据框写入到此位置的方式
使用这种格式
你也可以使用 parker 直接
你可以负责将文件写入parquet文件格式
如果你想使用直接aps
你只需说df点
Cowrite Parby点mode
然后点 然后parker
作为part of parquet 你只需传递目标位置
它将负责将文件写入parquet文件格式
只是我们的情况我们今天想要使它动态
我们可能想要使用明天来写
我们可能想要更改为json
如果那样 我们不需要太担心代码更改
我们可以直接a在运行时配置我想要写入到某些其他文件格式
例如json
然后parquet 它将使用该文件格式对数据帧进行照明
因为我们已经完成了逻辑实际上将数据帧写入到目标位置使用指定的文件格式
现在到我们在验证之前验证时间
我们也必须更改app 让我们进入下一主题的详细信息
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/003_Udemy - Data Engineering using AWS Data Analytics part2 p03 17. Validating Writing Data to Files.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


当我们完成了使用指定文件格式将数据框写入目标位置的逻辑之后
现在是时候将我们的代码作为app.py的一部分进行封装了
首先我们需要导入所需的函数
我们可以说from right import
然后导入两个文件
然后你可以实际上删除这两行代码
我们正在打印模式
以及我们实际显示一些记录的地方
现在我们必须调用函数_to_files
它接受三个参数
让我们说两个文件
第一个参数就是df_transformed
我们需要有年份 月份和日期也很好
因为我们将使用这些进行分区
我们还需要传递目标日记和目标文件格式
让我复制并粘贴这里
我使用这个代码的原因是，我们将传递
甚至目标以及目标文件格式
作为环境变量本身
这里 我必须说 tgt_underscore the
这将是第二个参数到这个
然后第三个参数什么文件格式
变量什么 tgt_underscore 文件格式
环境变量什么 tgt_underscore 文件和分数格式
全大写 我可以实际上作为第三个参数到这个现在运行这个
我必须设置这个到额外的环境变量为那
我可以去运行
然后编辑配置
然后我实际上可以点击这个
让我们定义变量t t gt underscore the
稍后会传递值首先
让我实际上定义变量tgt underscore file underscore format
值只不过是parquet
现在当它涉及到目标
位置只不过是这个一个是着陆
我们只需要说raw
所以在实际产品目录
这就是用户站点多样性项目
内部ITV活动数据
ITV GitHub
我们之前定义它是一个源的着陆点
现在我们实际上使用原始数据
这是唯一的变化 其余的东西都是一样的
甚至在原始文件夹下的活动没有任何东西
现在文件将使用Parker文件格式在此位置创建
文件将按年份解析
月份和日期 现在我们假设好的
然后应用
然后点击确定
因为我们已经完成了作为部分aby所需的更改
并且正如我们已经完成了运行时和环境变量
所以配置 我们应该能够运行这个
你可以实际上看到文件在目标位置
让我们正确 点击这个
然后运行应用程序
它将处理运行程序
它将读取json文件
应用转换
添加额外的列
例如年份 月份和日期
然后数据框将写入目标位置
使用该信息
让我们验证 一旦执行完成现在已成功执行
你应该关注的第一个检查无非就是退出代码
你的spark drops应该返回退出代码零
如果不 这意味着有什么地方出错了
在这种情况下退出代码为零
你可以在这里看到
让我们离开这个spark控制台
然后我们应该能够说查找
然后数据并按Enter
你可以在这里看到所有数据文件夹中的文件
这是处理的一个
你可以在这里看到目标
原因是它只处理了这一天
数据是运行时配置之一实际上告诉应该处理哪些日期文件
这只是源文件模式
你可以看到它被配置为2021年1月13日
现在我们可以说好的
然后确定
对于2021年1月13日
你可以在这里看到pass文件夹中的文件
即使我们说16个cos
它只创建了一个文件
因为我们在本地运行并且数据量有限
因此我们只获得一个文件
这就是你应该能够处理初步验证的方式
以查看文件是否按预期复制
现在我们必须处理其他验证
源文件和目标文件应该有确切的计数
因为我们现在不为那个应用过滤器
我可以实际上说pi spark
它将为我们处理启动pi spark
我可以实际上说src _ df等于spark. read. json
源位置什么也不是，数据
然后itv github登陆活动
2021 一月十三
零点json.gz
你可以把它粘贴在这里
然后关闭它
一旦数据框创建
我们应该能够运行计数，说src和score df. count像这样
让我们看看有多少记录
我们有99 11作为文件一部分
2021 通常十三
现在我们有一
顶部类似代码的目标位置
目标文件什么也不是，之后spark
我可以说spark. read. spark
我可以指定目标位置
到目前为止，我们只有一个文件夹
因此，我们可以指定到文件夹
我们不需要指定文件名
然后实际上可以粘贴这里
按Enter，您可以看到它创建了数据框
您可以实际上说tgt _ df. count并按Enter
您可以看到计数
您可以运行一些其他违反，说tgt df. print schema
查看所有字段都在
作为数据框的一部分
您可以在这里看到所有字段
为什么它不显示年份
月份和日期我们实际上直接传递到日期
如果我删除这个
如果我运行它
它将考虑甚至文件夹名称作为数据框的附加列
现在我可以说print schema
我们可以在这里看到 月份和日期我们可以运行计数
计数将相同
因为我们只有一个文件
您也可以运行通过实际上从数据框打印记录
我们也可以说a select report star只需获取有关rapport属性的详细信息
作为数据框
实际上可以看到结果
这就是你应该能够运行的方式
也处理验证
验证是关键
一旦你运行 你还应该验证以确保逻辑按预期工作
在这个案例中我们得到了计数
我们打印了模式
我们已经审查了文件和文件夹结构
我们还查看了退出代码
然后我们甚至预览了数据
这些常识检查是必须的以确保逻辑按预期工作
在我们在本地进行开发和验证完成后
现在是时候将我们的代码生产化并在生产环境中部署
我们可能需要重构一些代码以实际生产化
我们还会深入探讨这些细节以实现代码的生产化 以便将其部署在高级环境中
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/004_Udemy - Data Engineering using AWS Data Analytics part2 p04 18. Productionizing the Code.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


这个主题的一部分将是对代码进行必要的更改
以便我们可以在一个使用yarn进行资源管理的多节点集群上运行这个
一旦我们做了这些更改
我们应该能够在hardtworks集群或云集群上运行此代码
甚至运行在亚马逊emr上
在多节点集群模式下运行时，我们将设置环境变量到prod
为此原因 我们只需在这里添加额外的条件
我们可以说如果f e
V W等于prod
我们可以复制这段代码并这里做必要的更改
我们必须说其余的东西是相同的
我们不需要对其余的东西做任何更改
现在我们应该能够在开发模式或生产模式下运行这段代码
当我们想在生产模式下运行时
它必须在多节点集群上部署
在那里有一个监控与Spark相关的资源
我们通常将整个代码库复制到网关节点
并在网关节点上部署
让我们理解我们如何实际构建zip文件
你也可以直接复制一个文件夹
你应该能够直接运行源代码
但这不是一个好做法
一种常见的做法是构建一个zip文件并使用它
实际运行代码
使用集群的网关节点
让我们详细探讨如何构建文件
使用这个代码库 现在
我要去终端
我在这个项目的根目录下
如果我说我
我们有这些Python脚本
我们可以通过这种方式压缩
我们应该能够复制并部署它
当我们使用spark submitter运行作业时
我们必须传递程序名称
它就是一个点py
我们不应该将点py作为zip文件的一部分
在创建压缩文件时，必须将文件放在外面的压缩文件中。
让我去集群的根目录并创建一个基础文件夹
我们将复制那个时间的文件
在我的终端中使用 set 命令并连接到一个集群的一个节点
它只是gw zero one dot i dot com
现在我已经连接到网关节点
使用培训作为当前用户，我现在应该能够通过名称创建名为itv activity的文件夹
这是我将要复制文件的文件夹
除了应用点py
现在让我从这里离开
当他们从获取方式节点上下来后，现在我应该能够压缩文件
在这种情况下，我想压缩所有内容
除了app.py
我想压缩process.py
读取.py 和write.py到一个zip文件中
文件名就是itv-hyphen-activity.zip
这就是我应该压缩那些文件的方式
我必须说python r
然后压缩文件的名称
然后处理dot.py
读取dot.py
利用util.py
就像使用dot.py 它将会创建包含这四个文件的zip文件
您可以在这里查看详细信息
让我负责复制zip文件
以及将app.py复制到itvg文件夹中
在我的网关节点上
我能说scp itv hyphen h dot zip
然后训练在gw zero one dot i dot com column tilde为家目录
Itv hyphen g活动
这是我们之前通过登录get in order now创建的文件夹
我应该能够运行这个 它会处理复制zip文件
一旦zip文件被复制
我们必须复制app dot py现在
让我处理将app dot py复制到同一文件夹
我只需说scp app dot py
在gw zero one dot dot com列表中进行ITV h活动
我现在按回车
甚至app.py现在也被复制了
让我们使用常规的训练连接到gw zero dot s dot com
并验证文件夹
Itvg activity中既有zip文件，也有app.py
而不是使用Python中提供的隧道
我将使用我的常规mac终端
让我转到我的mac终端，我说训练在gw
Zero one dot dot com以连接到集群的闸门
那么接下来让我来说cd itv 斜杠 和 jh activity
然后运行ls 斜杠 ltl
你可以在这里看到app.py和zip文件
这就是你应该能够生产化代码的方式
然后在集群的gate上部署
现在是时候在我们生产模式下验证这个应用了
这样我们就可以将数据从源位置复制到目标位置
但在这样做之前
我们需要设置数据集
然后我们运行应用程序
我们继续设置红色环境
然后我们会回来在这个网关节点上运行这个应用程序
使用生产环境
我们会实际运行几天 并实际验证以确保一切按预期工作
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/005_Udemy - Data Engineering using AWS Data Analytics part2 p05 1. Introduction - Overview of Glue Components.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为模块的一部分，你将会了解不同粘合剂组件的高级价值
我们将详细探讨每个组件
作为后续模块或部分的一部分，我们将深入了解粘合剂
你可以在这里的搜索栏中实际搜索粘合剂
或者你可以滚动浏览，你可以去分析
作为分析部分的一部分
你应该能够在这里找到粘合剂
你可以点击这个
你应该能够在左边看到所有与粘合剂相关的关键组件
我们有一个称为数据目录的东西
作为那部分，我们有数据库、表爬虫
我们将在本节中详细探讨这些细节，为你提供一个高级概述
然后，我们将在随后的部分看几个例子，以了解所有关键方面
与ETL相关的数据目录方面
我们有一个新的组件称为Glue Studio
它还有一些问题
因此也不会作为主要课程的一部分进行覆盖
但我们将涵盖ETL中的其他重要组件
它们无非是作业、触发器和工作流
我们将在本模块中以高级方式探讨所有这些三个方面
然后在随后的模块或部分中
我们将详细探讨所有这些关键组件
我们还将触及一些与数据点相关的方面
在随后的部分中，话说回来
与粘合剂相关的关键概念无非是粘合剂
目录、数据库、表和爬虫
以及作业、触发器和工作流
你也可以对Devon points有一定的理解，以简化你的开发过程
Glue Studio相对较新
而且它还不稳定
话说回来，作为这部分，我们将实际创建端到端工作流
我们将获得关于Glue每个组件的高级概述
计划是第一
我们将创建爬虫
并将创建名为航班的目录表
数据由AWS本身提供
使用它自己的数据集，我们将创建爬虫和目录表
然后，我们将实际使用Ethana进行验证
Ethana无非是一个无服务器查询引擎
它将有助于验证
但它不是Glue的一部分
但它有助于验证与Glue相关的事物是否正常工作
然后，我们将实际创建Glue作业，将文件格式从CSV转换为Parquet
我们将运行并监控作业
并将在新的Parquet文件格式上创建目录表
一旦我们在新的位置创建了目录表
我们将使用Ethana验证新表
然后，我们将清理所有内容
并将构建称为管道的工作流
作为构建的一部分 工作流程将使用触发器
你将获得关于每个Glue组件的完整音频
作为这部分本身
作为后续部分的详细信息将得到很多 这样你就可以掌握Glue并且对使用Glue实现解决方案感到舒适
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/006_Udemy - Data Engineering using AWS Data Analytics part2 p06 3. Create Crawler and Catalog Table.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们定义并运行爬虫，为航班数据集创建目录表
航班数据集由aws自身提供
如果你想获取创建爬虫的指示
以及与航班数据集相关的表
你可以向下滚动并点击添加爬虫
你可以点击爬虫
你应该能看到指示
我现在不在所有这些指示上进行详细说明
我直接创建爬虫并创建数据库以及表
使用爬虫
你可以去爬虫并点击
添加爬虫去爬虫页面
或者你也可以去表然后说使用爬虫添加表
它也会带你到添加爬虫页面
你现在可以给爬虫命名
在这种情况下我将其命名为航班数据爬虫
你可以先创建数据库
或者你可以实际在爬虫定义本身中创建数据库
我们将使用爬虫定义来创建数据库
如果它不存在
我们应该能够点击下一步
你可以关闭这个
你不需要遵循这些指示
你只需点击下一步
这里的s三位置就是这个你可以使用这个位置
你应该能够将其粘贴在这里
让我关闭这个然后粘贴在这里
我不确定为什么它显示
当我点击添加表并去爬虫页面时
通常不会 我没有通过教程来
但出于某种原因它出现了
但那没关系 让我点击下一步
我们不会添加数据存储
因此点击没有下一步
如果你说是
你将看到额外的数据存储
让我点击下一步
你可以创建一个新的iam角色
它将为您提供访问s三位置的权限
所以我会创建一个新的iam角色
我将其命名为航班
它将实际上在这个s三桶上有权限
以便爬虫可以读取数据以创建表
那是文件中的一部分
它将实际读取标题
并为我们创建表
我们将在后续时间详细讨论这些细节
我们将快速创建爬虫
使用爬虫将创建表格
现在我们可以点击下一步
我们将选择按需运行
你也可以安排
但在我们的情况下我们将使用按需
点击下一步 你可以选择现有数据库
或者你可以创建一个新的数据库
在这种情况下我们将创建一个新的数据库
数据库名称只不过是flights-hyphen-db
然后我们可以说创建
它将负责创建数据库
当爬取运行时
当我们定义爬取时
数据库将不会自动创建
但当我们运行爬虫时
它将负责创建数据库以及表格
让我们在这里点击下一步
让我们审查我们所做的所有选择
然后我们可以点击完成
它将负责为我们创建爬取
你可以看到以名称
Flights 数据爬虫
然而 如果你去数据库
你不看到flights db
或者你不看到表格在这里
表格名称将根据文件位置继承
你可以添加前缀
但你可以完全改变表格名称远离文件名
我们将再次详细讨论这些细节
作为下一部分主题之一，我们将详细讨论目录
现在我们快速创建了爬虫
我们应该能够选择爬虫
然后说一个爬虫
它将负责运行爬虫
它将实际上负责为我们创建数据库以及表格
它将花费一些时间
我们必须等到爬取成功运行
然后我们可以去数据库并验证数据库是否创建以及表格是否创建
现在爬取已成功运行
如果有任何问题
你可以点击锁定以调试
但很可能不会在flights数据集上遇到任何问题
我们可以去数据库
如果你在这里不看到数据库名称 只需点击此刷新
我们不会遇到任何问题
数据集
你应该能看到数据库
如果爬取成功运行
你可以点击这里，然后点击这里转到表
你可以在这里看到表名
它只不过是CSV
如果你对表名不满意
如果你想给它加上前缀
你可以这样做，删除这个表，点击操作
像这样删除表
然后转到爬虫
你应该能够编辑爬虫，点击操作
编辑爬虫
这里我们讨论的是保存部分的输出
我们需要指定表名的前缀
你可以在这里指定前缀
假设是航班 这样表名将会是flight.csv
后缀将由输入指定的位置决定
那就是我们的S3位置
它现在在数据存储中
点击下一步，然后关闭
然后滚动并点击完成
以使用前缀flights刷新爬虫
在创建表时
选择这个并点击运行
一旦成功运行
我们应该能看到名为flight.csv的表
让我们等待完成
它已经进入停止模式
我们应该能够刷新并预览表是否创建成功
让我们到这里查看表，你可以看到表名flight.csv
这就是你应该如何创建爬虫
并创建与问题定义的位置相关的数据库和表
你可以点击这里，你应该能够预览关于这个目录表的详细信息
你可以看到所有列以及关联的数据类型
我们有65列
这个数据集
你还可以看到党派
你可以在这里看到有关这个人的详细信息
如果你想看创建目录表时创建的所有部分
你可以点击查看部分
你应该能看到部分和详细信息
我们创建了12个部分
你可以通过看这个数字来确认
所以当创建表时有12个部分
一旦目录表创建
我们应该能够使用不同的服务在这个目录表上执行不同类型的任务
我们应该能够使用Ethena来查询这个表
我们应该能够使用Glue Jobs来处理数据
使用这个表格 我们应该能够配置Databricks来
实际上指向这个表格并在Databricks上进行处理等等
有许多服务可以操作这些Glue目录表
这是从AWS提供的一个非常强大的服务
因此你应该探索它 如果你希望使用AWS堆栈构建数据工程解决方案
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/007_Udemy - Data Engineering using AWS Data Analytics part2 p07 5. Analyze Data using Athena.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你是否已成功在Glue目录上创建了一张表，用于存储S3中的数据
让我们看看如何使用名为Etherea的无服务器查询引擎
来查询使用Glue目录表的数据
在这种情况下，数据存储在S3中
我们有一个结构化的Glue目录表形式
使用该Glue目录表
我们应该能够使用名为Etherea的无服务器查询引擎运行查询
要进入Etherea 你可以向下滚动到分析
然后你应该能看到分析中的第一个服务
你可以点击它
然后你应该能够进入Etherea
当你开始使用Etherea时
你可能需要配置S3桶以便开始存储查询结果
如果没有S3桶来存储查询结果
你可能在任何时候都无法使用Etherea运行查询
如果你想更改你的查询结果位置
你可以转到设置并应该能够使用此页面配置桶
对于本主题 我们将主要运行查询到我们的Glue目录表
我们将了解有关Etherea查询编辑器和其他Etherea页面的更多信息
作为Etherea的详细模块的一部分，稍后
默认目录是AWS数据目录
你也可以连接到其他数据源
但我们不会探索那些东西啊
此时你可以实际上扩展此数据库
你可以选择数据库
我们中有我们的航班
数据只不过是航班-hyphen-db
我们可以选择这个
你应该能够看到我们的航班数据表在数据库下
现在，你可以运行选择计数of 1 from此表
称为航班csv
一旦你输入了查询
在查询末尾使用分号是可选的
你应该能够选择此
然后说运行查询
你应该能够在这里看到结果
它说输出桶不存在
S3 itv aa 未保存所以等等
我想我已经删除了S3桶
这就是为什么它失败了
让我们验证一下，让我扩展服务
然后转到S3
使用另一个标签作为S3
让我验证我是否有一个名为itv ethera的桶
或者不是 让我说itv
Etherea这里 没有以该名称创建的桶
这就是为什么它失败了
我想我在使用ethanna时删除过一次
这就是为什么它失败了
现在 我可以说itv ea创建一个桶
让我向下滚动，然后说创建桶
现在桶已经创建
让我们看看查询是否会成功运行
现在查询正在运行
你应该能看到计数
我们在名为flights csv的表中有500万到48940条记录
如果你想知道列和数据类型
你可以展开这个 你应该能看到列以及数据类型
你也可以展开这个
它有选项如预览
表格显示属性
删除表格加载部分
等 这些都是额外的选项，你可以根据需要探索，说到这里
你也可以在保存查询中查看保存的查询
你可以实际将运行在这里的查询保存为保存的查询
你也可以通过历史记录来获取历史
历史记录的目的是实际上获取查询状态
以及结果来自我们之前配置的s三存储桶
你也可以通过这里自定义工作组
默认情况下，主工作组将在您开始使用ea时创建
你可以添加额外的工作组并现代化
取决于您的项目或组织要求在任何时间点
如果你想要更改查询结果位置
你可以去设置那里更改这个
这就是关于ea的简要说明
它可以用于验证在创建时我们的目录表
也可以用于使用像quick site这样的工具进行报告
这是从aws服务堆栈的服务无服务器报告工具
因为我们成功地创建了目录表
并且我们也通过运行一个简单的查询来验证了它
现在
是时候我们实际上创建一个glue job 将文件格式从csv转换为parquet 然后继续
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/008_Udemy - Data Engineering using AWS Data Analytics part2 p08 7. Creating S3 Bucket and Role.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止探索更高级别的胶水
我们已经创建了胶水爬虫
以及航班数据集的目录表
表格名称无非就是航班csv
假设我们要将文件格式从csv转换为parquet
我们可以使用胶水作业来实际处理这个转换
创建作业 你可以去作业
不会作为这个话题的一部分创建作业
作为下一个话题的一部分处理
但是在创建工作之前，有一些事情需要处理
那是什么？让我们尝试创建一个工作
然后我们会看到您可以点击
在这里添加工作 您可以给名称
然而，您还需要指定IAM角色
并且该角色需要具有从源读取数据的权限
以及将数据写入目标
在这个例子中，源只是指向公共S3存储桶的表
由aws提供动力 目标是否是我们的一个存储桶
因此我们必须创建一个s3存储桶
我们必须确保该存储桶的权限对角色开放
这将与这里的工作相关联
让我们详细探讨为我们的目标位置创建s3存储桶的详细信息
同时创建具有适当权限的角色，用于测试存储桶
在使用该角色创建工作期间
同时创建工作作为后续主题的一部分
目前我正从glue控制台移开
让我扩大这个并点击s三作为s三的一部分
我们将创建一个名为itv航班的新桶
我们可以实际说创建桶在这里
然后说itv-航班
这是我们的目标桶
我们可以实际说创建桶在这里
一旦桶被创建
我们需要将此桶的权限分配给角色
我们可以有一个策略
我们可以将该策略与角色关联
然后我们可以进一步深入
我们先来谈政策
然后我们会进入下一步
我们将那个政策附加到你的角色上
我们将为那个创建新的角色
我们来到服务这里
然后我们必须进入iam的一部分
我是 我们应该能够进入政策
在这种情况下 我们将在itv-hyphen-flights上创建具有读写权限的策略
实际上我们可以说
S3策略具有读写权限
我们应该能够访问此页面并可以复制并粘贴此内容
我们可以通过点击此处进行复制
然后转到此处
让我们说创建策略jason
让我们将此内容替换为此内容，bucket名称实际上就是itv-hyphen-flights
让我复制并粘贴此处
我们在此策略上为该bucket提供了完全权限
我们可以查看策略
然后为策略命名，称为itv-flights-has-3-full-policy
这是策略的名称
我们可以说创建策略
一旦策略创建完成
我们应该能够创建一个角色并将此策略附加到该角色上
让我们转到角色
角色实际上就是aws-glue-service-role
我们可以使用现有的aws服务角色并附加此角色
或者我们可以创建一个新的角色
让我们创建一个新的角色
让我们说glue
让我们说next-permissions
让我们说glue
我们希望附加与glue服务角色相关的策略
这样所有glue运行所需的权限都附加到新角色上
然后说next-to-text-next-review
给角色命名itv-flights-glue-role
然后我们可以说创建角色
现在角色将创建
我们可以在此查看现有策略
我们可以附加一个新策略
该策略实际上就是itv-flights-s3-full-policy
我可以选择此并附加策略
所以 使用创建的新角色
该角色实际上就是itv-flights-glue-role
我们应该能够为glue授予相关权限
以及运行作业时使用s3
这就是您应该能够为glue作业创建角色的方式
您可以决定命名约定并继续
因为他们创建了一个自定义角色
我使用了itv命名约定
因此名称就是itv-flights，我们将使用它
在将csv格式转换为parquet时创建glue作业 使用我们的flights数据集
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/009_Udemy - Data Engineering using AWS Data Analytics part2 p09 9. Create and Run the Glue Job.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


正如你所创建的三个存储桶和角色一样
这样胶水工作实际上可以将数据写入S3桶
现在没任何问题
让我们去胶水控制台
让我们去分析服务
在这之中，这就是粘合剂
它将带我们到胶水控制台
一旦你进入了胶水控制台
你可以去工作
然后你可以点击添加工作
让我们将名称设置为航班
Jason不是json实际上是csv到公园
说到iam角色
我们将选择itv航班角色
你看它只显示那些角色
它们有glue权限
它不显示其他角色
所以我们可以选择这个
让我们滚动下来
展开此监控选项并选择作业矩阵
连续记录以及停车
这将使我们能够监控工作
同时也能快速解决遇到的问题
一旦工作失败或完成
让我们选择这个spark事件日志的存储桶
让我展开说明 这将写入相同的s3存储桶，在那里
我们将写入数据
让我们向下滚动
这只是itv航班
我将选择这个在这个
我将创建一个名为火花-锁的文件夹
让我向下滚动
我们不需要太担心其他选项
只需点击下一步
然后继续
我们将进入
这些选项的重要细节
作为后续部分和相应主题的一部分
现在 我们正在创建一个从头到尾的管道，以便为你提供结果
出于这个原因 我不会详细说明任何细节
我只是选择快速创建工作
让我们选择这个
然后点击下一步 我们实际上选择了胶水目录表
航班csv，位于航班数据库
这早先已经创建
让我们点击下一步
在这种情况下我们只是更改了模式
我们将文件格式从CSV转换为Parquet
让我们点击下一步
我们将在目标中创建表
将写入到S3
让我们在这里选择S3
格式是Parquet
我们需要提供目标桶
在这种情况下，目标桶就是itv flights
让我们选择这个
然后让我命名为flights parquet
只是为了遵循约定
这与在创建flights表时创建的一致
数据集在glue目录中
它是flights csv对于CSV数据集
表名为flight csv
文件夹名为csv
在这种情况下，我将文件夹本身命名为flights spark
让我们点击下一步
它不会创建任何表
它只会创建以这个名字的文件夹
文件将被复制到这个文件夹中
让我们点击下一步
您可以预览此内容
它实际上显示每个列的映射
如果您想删除一些列
您可以点击此处并删除
我们不会清理任何这些内容
我们将点击保存作业并编辑脚本
它将带我们转到由于所选内容生成的glue脚本
生成的glue脚本将处理从glue目录中读取数据flights csv表
到S3桶和对应的对象flights flights parquet
我们的桶名为 flights
文件名为flights parquet
数据将被复制到该位置
使用Parquet文件格式和snappy压缩
由于脚本已生成
我们不需要太担心自定义
我们将在后续模块中详细讨论
让我们点击运行作业并查看结果
它将实际显示运行时参数选项
如果您想在运行它之前自定义任何东西
您可以自定义
然后您可以点击运行作业
它将触发作业将数据从flight csv表复制到flights
Spark文件夹在S3桶中
称为itv Flights
既然它正在运行，让我们回顾一下我所做的所有选择
让我打开标签
去aws
亚马逊.com
让我再次登录aws控制台
然后让我转到glue控制台，glue控制台在分析下
你可以点击这个
然后可以转到工作
然后如果你转到ad工作
我选择了名字
我正在滚动
然后提出了由airless glu生成的脚本
根据做出的选择
脚本现在生成了
让我们回到这里，看看工作是否出现了
它出现了
它没有开始 我们将详细讨论工作进度
一旦开始 实际启动需要一些时间
启动工作 但是一旦工作开始
你会在这里看到一些进展
选择这些选项后
我已经扩展了监控选项并选择了这些东西
然后点击了下一步
它实际上向我展示了有关源的详细信息
需要进行哪些转换
以及目标
一旦一切都准备好了
然后开始
它展示了创建脚本的选项
一旦我们点击它
脚本将为我们创建
生成的脚本像这样
如果我关闭这个
如果我去这里并选择这个
如果我点击脚本
您可以在这里看到脚本
您可以点击编辑脚本
以在编辑器中打开脚本
这就是之前发生的事情
才能达到这个阶段
现在你看到工作正在运行
工作处于零状态
需要一些时间来读取数据
同时也将数据写入s三存储桶
这些进度和所有信息将只显示
如果我们选择持续日志作为工作的一部分监控
在创建工作时
如果你没有选择
你将无法看到这条进度条，所以请确保在监控工作时进行选择
现在任务已成功完成
您可以检查s3桶并确认数据是否已复制
如果数据未复制
那么我们必须解决这个问题
这是启动任务的原始位置
这是我进入以审查所做选择的位置
让我们使用这个会话进入s3桶
让我扩展这个
让我进入s3
让我点击这个
桶名无非是itv hyphen flights
一旦控制台打开
然后我可以说itv hyphen flights
并点击这个
然后点击这个
您可以在这里看到文件
这就是您应该能够创建并运行第一个glue任务的方式
将文件格式从一种格式转换为另一种格式
由于您已成功将文件格式转换为
让我们审查输出并查看源数据与目标数据之间的差异
如果您去原始csv文件夹
csv文件在哪里
有多个与月份相关的文件夹
让我们去glue目录
我不去实际的源文件夹
但glue目录表
它将给您一个关于文件夹的想法
作为主要文件夹的一部分
csv我们对其创建了表
让我进入glue目录
展开此，然后转到分析并转到glue
您可以转到表
表名为flight csv
我们实际上只看到正确的表
因此我们可以点击此
然后点击此以查看部分
您可以在这里看到详细信息
实际上获取有关文件的详细信息
您可以点击这些链接之一
这将我们带到s3控制台
并将我们带到我们为glue目录表创建的适当位置
如果您查看flights
数据结构集
这是s3爬取
公共东一
这是aws公共数据集
Flight是该文件夹的主要文件夹，其中包含2016
然后csv
然后基于月份的文件夹
到目前为止，这个文件夹没有文件
但我们可以去这里，我们可以实际查看文件夹名称
它们等于
因此，这就是你应该能够审查你的来源文件的方式
你可以在这里看到文件
好的，此外，文件直接位于飞行下面
二一六 CSV
我不知道为什么这里有文件
还有这些文件夹
我认为文件夹内有文件，外面也有，不管怎样
现在不需要太担心解决这个问题的公共数据集
让我们回到目标文件夹
它只不过是s3 itv 飞行
如果我们点击一个飞行，spark和审查
你可以看到 这里有文件
没有以命名约定或月份等于命名的文件夹
因此，因为默认行为是
它将直接写入指定的目标文件夹
当我们创建glue工作时
因为你已成功将文件格式从CSV转换为parquet
让我们在这个parasset上创建一个glue目录表 然后使用ethana对表运行查询
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/010_Udemy - Data Engineering using AWS Data Analytics part2 p10 11. Validate using Glue CatalogTable and Athena.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


因为我们已经创建了我们的第一个蓝色工作来将文件从csu转换为parquet
现在是时候为我们验证一种有效的方式了
方法是获取这个文件夹中的glue目录表
然后使用ethera查询
让我们复制s three ua 对于这个文件夹
它什么也不是s three itv flights flights spark
然后转到glue控制台
我正在使用最近wied部分并选择glue
一旦我们进入aws glue
我们需要转到爬虫
然后说添加爬虫
或者我们也可以转到表单表单表单
使用爬虫 无论是哪种方式
它会带你到相同的页面
我们可以将爬虫命名为flights parquet数据爬虫
因为我们已经拥有flights
我们的csv数据集
让我们点击下一步
我们将选择数据存储作为此页面的一部分
它什么也不是s three 而这个路径就是我们已经通过
点击复制s earlier复制的，因此我在这里粘贴了它
让我们点击下一步，我们不会添加任何其他数据存储
因此我们需要确保我们选择否
然后点击下一步
现在我们可以选择现有的
我角色 它什么也不是aws glue服务角色flights
它可能会对我们的目标桶添加权限
让我们这样做
让我们点击下一步
现在将在需求下运行
让我们点击下一步
目标数据库是lydb
在同一个数据库中，我们有flat
Csv表将创建一个新表
因此我们在这里选择了flights db
你不需要太担心其他事情
表名为flights spark
如果它成功运行
让我们点击下一步
您可以审查此页面上所做的选择
然后点击完成，现在创建了爬虫
您只需选择爬虫
然后点击运行爬虫以运行爬虫
以查看创建的表
如果表没有创建
我们将解决问题并实际上修复爬虫
然后我们会重新运行，这样表就可以在没有任何挑战的情况下创建
现在状态正在改变，变为停止
我们应该能够去查看表格
让我们关闭这个
让我们去数据库
点击数据库
点击表格
没有显示
这意味着新的表格没有创建
它可能会遇到一些问题
最有可能的是它会被访问
我们现在去查看爬虫
一旦它停止
您将看到错误消息，直到您无法看到错误消息
但最有可能的情况是，我们选择的角色
没有适当的权限来爬取路径内的文件
该路径用于创建目录表
让我们等到它完全失败
让我们审查消息
然后通过编辑爬虫来修复它
我们可以点击这里的日志
一旦它在后续运行中出现
在停止模式下不要点击锁定
一旦它停止
然后你必须进入锁定
否则你将看到所有锁定，你可以在这里看到那里
然后错误说访问被拒绝
因此没有适当的权限角色
这是选择从sri读取这些文件的
这是现在用于创建此目录表的
让我们去这里Glue控制台
让我们点击这里编辑爬虫
问题是关于
我是角色 让我们这样做
让我们在一个我是角色的政策中更新
角色只不过是航班
所以我们应该做出的选择就是这个
这只是在一个我是角色中更新政策并选择角色
即使角色在s三文件桶中没有权限
它将负责现在授予权限
我们可以点击下一步、下一步、然后完成
角色已更新为适当的权限
可以通过这里验证
让我们选择角色名称
这就是AWS Glue服务角色，用于执行该操作
我们必须去 我是控制台
让我选择这里的i
让我们去ro
角色就是AWS Glue服务角色，如果你在这里查看
如果你看一下政策
我们有aws服务角色
还有aws胶水服务角色航班
这是在我们创建第一个爬行器早些时候创建的
如果你展开这个并转到编辑策略
你应该在上面看到对它的权限v航班s三桶
让我们转到jason
在这里你看到所需的权限通过此策略授予给角色
因为我们选择了更新现有铁人角色的策略
我们应该能够无问题创建胶水日志表
让我取消这个
让我转到这个胶水控制台
然后转到爬行者
让我选择这个爬行者并点击运行
这段时间表肯定会创建爬行器已完成
我们应该能够转到表并应该能够看到此表
这次胶水爬行者已成功运行胶水目录表已创建
使用胶水爬行者
现在是时候使用ea进行验证了
我们必须转到ethana控制台
让我转到ethana从这里
一旦你在ea控制台
你可以转到查询编辑器
你已经在正确的数据库中
你可以看到航班的parquet在这里
让我们运行查询
选择count of one from航班parquet
让我们选择此查询
然后说运行查询
你应该能够看到此处的计数
计数是五百万到四十
八千 四三十九
csv的计数也相同
如果你想获取完整的航班csv
你可以转到历史记录
我们已经在那个过去运行了那个查询
这是您可以选择的查询
你应该能够看到结果
这就是你应该能够使用ethana来验证创建的新表
由于胶水爬行者
对由我们的第一个胶水工作生成的输出
将csv文件转换为parquet文件
你可以看到csv和parquet之间的计数匹配
还有一个验证 你应该做的就是我们是否能够以预期方式看到数据
为此你可以只是复制粘贴这个改变count到星
然后说限制十
让我们选择这个并运行
看起来有问题
查询正在对默认数据库运行
因为它已刷新为默认设置
让我们在这里选择航班数据库，然后运行此查询
现在您可以看到输出
这意味着文件转换成功
这是我们的第一个glue工作
它已将文件从csv转换为par
我们能够在数据集上创建名为log的看板
我们可以使用ea进行查询 这无非是一个无服务器查询引擎
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/011_Udemy - Data Engineering using AWS Data Analytics part2 p11 13. Create and Run Glue Trigger.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


正如您已成功将文件格式从CSV转换为Parquet使用Glue作业
并且使用Ethanna进行了验证
让我们谈谈Glue触发器
您可以转到Glue控制台
我们在编辑脚本模式
我们可以点击这里返回到Glue作业这里
您可以看到创建触发器的作业
您可以转到触发器
触发器无非就是Glue作业可以运行的时间表
让我们点击一个触发器并进一步查看
我们可以给触发器起一个名字
触发器名字无非就是航班
CSV到Parquet触发器
我们将启用此按需
因此让我们选择按需
然后点击下一步
您可以选择Glue作业
我们只有一个作业
我正在选择它
让我们点击 添加这里
您可以实际自定义此作业的运行时行为
在这种情况下没有安全配置
作业书签将禁用
我将在随后的部分和相关主题中讨论作业书签
目前 我们将禁用它
如果您想传递任何运行时参数
我们可以将它们作为键和值粘贴
我们将在后续的时间内详细讨论这些细节
让我们选择禁用作业书签
然后点击下一步
然后点击完成
现在触发器已创建
每当我们选择它
然后说操作开始
触发器将在运行之前提前启动
让我们转到S3桶
无非就是itv航班
让我们清理航班
Parquet文件夹并运行此触发器
以查看我们是否能够重新处理CSV数据
将其转换为Parfl格式
让我转到S3控制台这里作为S3控制台
我将转到itv航班桶
我可以从这里搜索
让我删除这个文件夹
您必须输入永久删除
这样我们就授权了文件夹被删除
您可以看到文件已被删除
我们点击退出
现在 没有飞行火花文件夹
一旦我们运行触发器
我们应该能够再次看到平面文件夹
我们应该能够使用ethera运行查询
让我们执行这个触发器操作
启动触发器
让我们等到它完全运行
然后我们将实际验证目标位置
我们也将使用ea再次运行查询
以确认数据可以被查询
在这种情况下，我们在glue job上创建了触发器
以确认触发器是否仍在运行
实际上，您可以转到工作
选择这个工作，什么也没有
但此一扩展此并看看发生了什么
与关于此工作
您可以看到状态在这里是运行
一旦工作完成，自动
触发状态将更改为已完成
从现在开始，该触发器已启动此工作
必须完成此工作
这将花费一点时间
一旦完成 我们应该能够在s三中验证
然后我们应该能够使用查询进行验证
现在您可以看到工作的状态现在是成功的
我们应该能够转到触发器并检查触发器的状态
它不会显示实际状态
让我刷新一下
可能我们刷新一次
我们就可以看到没有
它实际上并没有说关于运行或完成的任何事情
但它肯定已经完成了
因为触发这个触发器的胶水工作状态是已完成
我们可以去s三台控制台刷新一下
我们应该能看到新的文件夹
航班spark 你可以看到现在的文件
我们应该能够去EA并运行相同的查询
这是我们之前运行以获取计数的查询
我们也能预览数据
查询在新查询一中
您可以选择此运行查询
我们应该能够获取计数
我们也应该能够预览数据
这意味着触发工作的过程
使用触发器是成功的
我们创建了触发器并触发了将CSV转换为Parker的工作 我们已经成功验证了
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/012_Udemy - Data Engineering using AWS Data Analytics part2 p12 15. Create Glue Workflow.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为前一个主题的一部分
我们创建了触发器
我们也启动了它
我们还验证了触发器下面的工作是否成功运行
我们创建的触发器完全是按需的
现在让我们进入工作流
工作流只不过是一个编排的管道
我们将使用触发器作为工作流的一部分
谈到触发器
我们只能触发一个工作
然而 如果你想构建一个编排的管道
我们必须使用工作流作为工作流
我们可以有任意多的触发器
取决于管道的复杂性
让我们定义管道
然后让我们进入工作流
如果你记得
我们在航班CSV数据集上创建了一个Glue目录表
然后我们创建了一个Glue作业将CSV文件转换为Parquet
然后在Parquet上有查询
在创建了Par目录上的Glue目录表之后
所以我们创建了在源数据集上的Glue目录表
那就是航班CSV
然后是Glue作业
然后是在我们Parquet数据集上的Glue目录表
以便他们可以被查询
我们将清理东西并验证一切从头到尾
但在那之前
让我们创建工作流
你可以通过Glue控制台创建工作流
你可以去工作流
然后说添加工作流
你可以给名字 它可以是英文
让我命名为航班CSV到Parquet工作流
我们可以使用大部分Glue会议命名
使用英文
现在我们应该能够点击
添加工作流以创建工作流和工作流已创建
你可以选择这个工作流
然后你可以实际上去工作流编辑器
工作流编辑器显示在底部
它只不过是一个图表
你必须添加触发器
并定义作为触发器的操作应该做什么
让我们点击添加触发器这里
让我们先创建一个新的触发器
我们将触发
在我们CSV数据集上的Glue目录表的创建
那么让我命名为运行胶水爬虫
全飞行CSV触发器
然后我们可以说添加
你可以看到触发器已创建
你可以选择这个
你应该能够更改详细信息
所以这个案例让我首先点击编辑让我更改为按需
这是起点
所以我是说按需
这样我们就可以运行
我们想要的时候
然后我们可以保存这个
此外，如果你展开这个
你可以添加工作爬虫触发
你必须选择这个 在这种情况下我们将在爬虫中对齐这个触发器
我们要运行的第一个爬取是我们将要运行的航班数据爬取
这样我们的胶水目录将创建航班CSV数据集
我们可以说添加
然后我们可以将爬取与触发器关联
当涉及到UI时
它有点粗糙
你可以在这里看到
它不是很友好
让我放大一点
可能它会更好
好的 然而，这不是一个很方便的编辑器
他们可能会在未来改进
但到目前为止，它按预期工作
我们已经创建了第一个触发器
并且我们已经将爬虫与CSV数据集创建目录表关联
现在你可以选择这个并添加触发器
这次触发器是触发工作
所以运行航班CSV到Parquet
工作你可以看到添加
但在去添加之前
让我们选择所有观察事件之后开始
观察事件没有什么，之前的事件
我们可以说添加
只要价格事件成功
这将被触发
现在你必须将这个与工作关联
我们刚刚创建了触发器名称
我们没有将这个与任何工作关联
你看到有额外的连接器因为我们选择了所有现在
选择这个然后转到操作然后说
添加工作或爬虫到触发器
这次我们将触发工作
因此我们必须选择工作
让我们选择这个，然后说添加
到目前为止，我们已经定义了爬虫的工作以及任务
我们已经将它们按顺序连接起来
现在您可以选择这个任务
您可以看到添加的额外触发器
但我们还没有进行任何配置，您可以选择这个
然后您可以给适当的名称
所以，在这个情况下，它只是航班
Parquet数据爬虫
让我实际上说
取消这里让我滚动
它只是运行
Glue爬虫为航班CSV
让我命名为Glue爬虫
填充航班Spark
然后点击添加之前可以说添加
让我启动在所有观看事件之后
然后说添加，现在您应该能够选择这个
这次我们将配置爬虫
我们可以选择这个爬虫
然后说添加
我们已经定义了完整的工作流
它从运行我们的第一个爬虫开始
以创建CSV数据集的目录表
然后我们运行任务
然后我们实际上触发了爬虫
以在Parquet数据集上创建表
不要担心这个额外的东西
您不需要过于担心它
它将继续运行到
确保它按预期工作
让我转到详细信息，我也转到图表
您可以看到它更好
这个不必要的东西被添加到底部
工作流已定义
让我们清理表以及目标文件
然后运行它以清理它
我正在转到表
让我放大这里
让我删除这两个表
当调用者作为工作流一部分运行时
这些表将为我们创建
因此我们可以删除这些表
我们还必须转到S3
我们必须删除目标文件夹
不要删除源文件夹
我们需要有一个源文件夹
没有那个源文件夹
将无法创建表并从源读取数据
在这个情况下，我们只有航班数据集的读取权限
这是由aws在他们的公共数据集中遵循的
因此将无法在任何地方删除，话虽如此
让我们进入桶
itv航班
这是桶
让我们选择这个，然后说删除
让我永久删除
所以我们确认我们真的想要删除
现在这些文件已被删除，因为我们创建了工作流并清理了
让我们运行工作流
也会验证工作流是否成功，通过查看表格文件
以及运行查询对表格 使用ethanna作为下一主题的一部分
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/013_Udemy - Data Engineering using AWS Data Analytics part2 p13 17. Run Glue Workflow and Validate.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为前一个主题的一部分
我们创建了胶水工作流
我们也清理了目标数据集以及目录表
现在是时候运行工作流并确保它成功运行
让我们转到服务这里
然后进入胶水控制台
在运行它之前请确保您在工作流中
确保您在数据库 flights db 中没任何表
您可以看到没有表
因为它们作为清理的一部分被删除
作为创建工作流后的一部分
作为前一个主题的一部分，我现在可以去工作流
让我们选择这个工作流并查看完整的管道
您可以看到一切都很干净
那些额外的东西现在也不见了，开始
您只需说操作并然后运行
但在运行之前 确保入口点是按需
您可以在这里看到
它是按需
我们选择了第一个
您可以看到触发类型为按需
因为它是按需 我们应该能够转到操作
然后说运行 它将处理运行此工作流
您可以通过点击此来监控
然后转到详细信息
运行不是一个超链接
因此您将无法从这里监控
在这里您将不会得到太多详细信息
您可以转到历史记录
然后您可以选择这个
然后您应该能够点击查看详细信息
以查看此工作流的进度
您可以看到此次运行的详细信息
您可以通过点击此来停止运行
如果您想要的话
让我们等待一切完成
然后我们将实际验证表是否成功创建以及表中的数据是否可用
以及数据在表中是否可用
您可以看到有关不同颜色的详细信息
以及这里的图标
首先成功
这是运行
这是停止，失败和超时 这些是图标
您还会看到工作流的一些信息
让我滚动或让我移动
界面非常粗糙，我认为这是由于我使用 safari
Safari可能不与胶水接口很友好
现在您可以看到触发已成功
任务已触发
目前它如何运行爬取
一旦爬虫完成
我们应该能够看到是否已创建了我们的飞行db的表
第一个表只不过是飞行csv
然而，我们必须等到它完全完成
现在您可以看到爬虫已成功运行
它转到了下一个触发
即使下一个触发已成功运行
现在它实际上正在运行胶水工作
您可以通过在这里转到表来验证
您可以看到源表已创建
它只不过是飞行csv
并且您还可以转到工作
选择工作
移动此上
您可以看到工作正在无问题运行
我们必须等到这项工作成功
然后下一步将是针对目标数据集创建目录表
让我们等待并停留在工作流详细页
如果它没有正确显示
您可以做的是
您可以返回工作流
选择工作流然后历史
然后你应该能够选择这个
然后点击查看详情
你应该能够看到当前正在运行的图表
甚至在运行触发之前工作已完成
您可以通过转到s三来验证
让我从这里转到s三
然后搜索itv flights
让我们等待Web控制台加载
然后搜索itv flights
然后点击这个
您应该能够看到这里的航班火花
您可以看到文件
它实际上在文件顶部创建了表
一旦表创建
我们应该能够转到表并查看目标表
名为flights spark
然后我们应该能够使用ea对两个表运行查询
即使工作流正在运行
我们应该能够转到ethana并对源表运行查询
因为源表已经创建
源表只不过是飞行csv
所以我们应该能够运行这个
但我会使用这个
我已经有了这个飞行csv
确保我们在正确的数据库中
数据库名为flights db，选择并运行查询
你应该能看到输出
稍等片刻 你可以看到源表有五千万到四千八百九十四
爬虫根据目标位置仍在运行
一旦完成
我们应该能看到目标表
让我们等到这变为完成状态
我选择并检查状态
它说 取消
这意味着爬虫也完成了
它只是在完成工作
你可以去表
让我使用窗口
你可以点击这里进入glue目录
然后说aws glue
你应该能看到flights para表
我们也可以去ea作为flights db的一部分
我们应该能看到新表，那就是flight spark
我们应该能对这张表运行查询
我们将运行计数查询以及星型查询
预览数据，计数与flight csv匹配
我再次运行以与flight csv比较计数
与flight csv相同
我们也能通过运行此查询预览数据
这意味着整个工作流已成功运行
让我们去glue控制台
确保最后爬虫也成功
这就是你应该能创建工作流的方式 运行并验证，确保一切如预期
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/014_Udemy - Data Engineering using AWS Data Analytics part2 p14 1. Introduction - Spark History Server for Glue.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为模块的一部分
我将演示如何使用两种不同的方法设置Spark问题
一种是使用AWS Cloud Formation在AWS本身
第二种是设置Spark
在我们的本地计算机上使用Docker
我们必须设置Spark的原因
是为了解决与提交的Glue工作相关的Spark作业问题
如果你记得 当我创建Glue工作时
我选择了Spark qi进行监控
我也指定了S3位置来写入日志
你可以通过选择Glue工作
然后转到详细信息
在这里查看详细信息
你也可以通过转到编辑工作
然后转到监控选项
如果Spark UI启用
我们需要提供S3位置来写入日志
我们可以在这里部署Spark
我们应该能够解决与提交的Glue工作相关的Spark作业问题
这些作业是由于我们的Glue工作提交的
我已经在我的机器上设置好了
我将提供有关如何处理此问题的材料
一旦您在AWS或本地设置好了Par
您应该能够访问它
界面将看起来像这样
在这种情况下，我将转到本地设置
您可以在这里查看详细信息
这是Parquet，日志通过此Web界面传递
您可以在这里查看日志
这是与创建作业时指定的日志有关
让我们深入了解如何处理此事
一旦设置完成
当我们进入与Glue相关的高级内容时
无论何时相关
我将向您展示如何解释日志
使用Spark UI来调试和监控Spark
作业，这些作业是由于Glue工作提交的 作业
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/015_Udemy - Data Engineering using AWS Data Analytics part2 p15 2. Setup Spark History Server on AWS.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们来了解如何设置Spark任务以用于我们的蓝工
使用AWS云形成模板
您可以实际搜索启动Spark历史服务器胶水并按Enter
您可以访问此链接
它将实际为您提供关于两种方法的说明
一种是使用云形成
这将作为该主题的一部分进行覆盖
另一种是在本地容器中使用Docker
这将作为后续主题的一部分进行覆盖
现在您可以仅在适当区域点击启动堆栈
在我这种情况下是北弗吉尼亚
您需要根据您的项目进行选择
然后点击启动堆栈
它将带您进入云形成UI
一旦云形成启动
您只需确保它选择了适当的模板
它实际上就是我们的Spark ML
然后您可以在这里点击下一步
我们需要为堆栈命名
让我命名为itv航班
因为我们正在尝试监控与itv航班项目相关的日志
无论项目是什么
您只需在这里给出项目名称
将有多个任务提交到该项目
所有任务都将以相同的Spark集群位置进行配置
我们将使用它来监控所有这些Spark任务
我们理解稍后需要设置位置
我现在仅根据我的项目名称给出了堆栈名称
如果我滚动向下
我们需要提供I
需要白名单以访问此Spark集群
它将部署在此服务器上
在我这种情况下，这只是我的马克
我只需要在这里打开一个标签
搜索我的IP
获取此IP
将其粘贴在这里
然后输入三二
您可以保留历史端口
如它 如果您想更改
您可以现在更改
与丢失的日记相比
您可以转到现有的胶水任务
您可以选择我们之前运行的任务
然后转到详细信息
您可以在这里查看有关此Spark任务的详细信息
如果您在这里看不到
这意味着您没有将Spark任务配置为胶水
任务
你只需要去行动
编辑工作
然后监控选项
你必须选择这个，并在这里提供位置
因此，在我们的情况下，位置无非就是这个，我来使用这个并粘贴在这里
它必须是a3
不是a3 确保你将其更改为a3
然后滚动 在这里提供密码
这是一个新密码
这不是现有密码
让我输入这个密码
然后我们需要选择适当的vpc id以及子网
当涉及到子网时
你必须选择一个公共访问开放的子网
你可以在这里查看说明
如果你实际上滚动到子网部分
它说这地区的实例id是
你可以使用你vpc中的任何子网
你必须能够从你的客户网络到达子网
如果你想通过互联网访问
你必须使用有互联网网关的公共子网
在我这里，我的大部分子网都有互联网网关
因此我在这里选择了一个现有的子网
然后我可以点击下一步
你可以审查所选的选项
然后点击创建堆栈
但在那之前确保你确认
然后点击创建堆栈
现在堆栈正在被创建
它正在进行中
它很可能通过
如果不通过 我们需要调试问题并进一步处理
我们做了这么多
我们只是点击了启动堆栈
我们确认了模板被选择
然后我们输入了堆栈的名称
白名单了ip地址
配置了vpc以及子网
我们也输入了keystone密码
然后只是点击了创建堆栈
你可以通过点击这个刷新并查看进度
你可以在这里查看详细信息
它仍在被创建
你可以看到创建已完成
最新消息会在顶部，因为它默认按降序排序
按时间排序你可以去输出
让我们刷新这个
我认为输出还在生成中
我们需要再等一段时间
实际上它因为某种原因正在回滚
我们必须解决这个问题
让我们回到这里，让我们看看详细信息
它不提供任何详细信息
为什么它正在回滚
你可以在这里看到消息
它已经回滚 因为该类型的不可用性
我可能必须更改类型并重试
让我再次访问堆栈
让我以不同类型重试
我将回到输出以
在它被配置之前，我需要先访问那里
让我删除这个
通过选择它，然后点击删除
这将处理删除堆栈
我将转到完整步骤
然后我将回到这里，这次调查正在成功创建
问题是与您选择的子网有关
您选择的可用区，该子网的可用区是s to one e
在该区
没有t三类型的实例，t三小，中，等等
一旦您更改子网与c one c的可用区
我能够无问题创建环境
所以，我们选择了模板
然后作为子网，我们选择了可用区
该区有t三和micro小，中，等等类型的实例
让我们等待服务器配置完成
然后我们可以转到输出
我们应该能够获取连接spark UI的详细信息
使用将可用的服务器公共DNS
完成该过程大约需要11分钟
但最终我们有了服务器
它还部署了spark在上面
我们应该能够通过转到输出
然后滚动下来
我们应该能够复制这个
转到输入，粘贴
按回车
它将显示这个
因为我们还没有完全配置https以访问这个
所以不用太担心
只需点击显示详细信息
然后访问网站
输入您的本地密码
如果你使用的是mac
它将提示管理员密码以登录到网站
应该使用HTTPS进行连接
但是没有正确配置的证书
在这种情况下，我正在输入我的当地密码
然后它会带我到火花
一旦你开始运行这些工作
您将在这里看到已完成的工作锁定
你应该能够解决这些问题
即使现在，我们已经提前运行了与itv航班相关的工作
你可以看到，这些未完成的申请的一部分
我不知道为什么它会出现在未完成申请中
即使这些工作已成功完成
但现在不要过于担心这些事情
我们将探讨如何解决Spark作业的问题
在未来合适的时候
确保你有Spark历史服务器
无论是AWS还是本地出于成本原因
如果你不想在AWS上设置Spark HD服务器
你可以遵循后续主题，实际在本地使用Docker进行配置
然而，你需要有足够的资源来在本地设置
这取决于你 无论你是选择aws还是本地
但我将提供本地以及后续主题的部分说明
如果你想采用基于aws的方法 你可以跳过本模块的其余部分，转到下一个模块
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/016_Udemy - Data Engineering using AWS Data Analytics part2 p16 3. Clone AWS Glue Samples repository.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在追求在本地设置Spark UI的过程中
以便我们能够解决遇到的问题
让我们克隆一个名为AWS Glue Samples的仓库
然后我们会进一步处理
以便在本地设置Spark环境
你可以通过搜索GitHub仓库来探索
搜索Glue Samples
一旦你到达仓库这里
你可以实际上去Utilities
在Utilities部分你有Spark UI
我们对这个感兴趣
因此我们需要克隆这个仓库
然后我们应该能够使用此仓库来启动Docker容器
在那里我们可以访问Spark UI
以访问我们的Spark日志
这些日志是在创建Glue工作时配置的
让我们深入探讨
通过本节结束时你将理解如何克隆
你只需在仓库的根目录中
展开并复制到终端
确保你在适当的文件夹中
然后运行git clone并粘贴并按Enter
你应该能够克隆仓库
如果你使用的是Windows 你应该能够通过安装PowerShell来处理
或者你也可以使用像Git Bash的工具
无论你偏好什么你都可以选择
然后你可以继续前进
如果你在这里运行lf和lt
你会看到一个名为AWS Glue Samples的文件夹
你可以实际上进入AWS Glue Samples
然后Utilities
你应该能够看到Spark UI文件夹
如果你进入Spark UI文件夹
你会看到palm.xml和Docker文件
这些都是用于创建Docker容器的
关于如何设置这些的说明
你可以在这里找到设置说明
然后滚动到它这里Spark
你可以从这里获取说明 我将带你走过
如何使用这些说明 实际上启动Docker容器
以访问我们的Glue工作相关的Spark日志 这些日志是在创建Glue工作时配置的
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/017_Udemy - Data Engineering using AWS Data Analytics part2 p17 5. Build Glue Spark UI Container.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们理解如何构建图像
以便我们可以为我们的glue工作设置spark
一旦你进入文件夹
Spark UI作为仓库的一部分，之前话题中克隆
你应该能看到两个文件
一个是palm dot xml，第二个是docker文件
这两者在这个文件夹中是非常重要的
确认你看到docker文件以及palm dot xml
你所需要做的就是 你只需运行此命令来构建图像
以启动parkway作为docker容器的一部分
让我们粘贴到此处
然后按回车键来构建图像
你可以看到它已成功为我们构建
在我们这个案例中 在我这个案例中可能需要一些时间
因为我之前做过
所以我有一个缓存的图像 这就是为什么它稍微快一些
但对于你
可能需要相当长的时间 一旦完成
你应该能够使用这个图像并启动容器
然而
在使用此图像并启动容器之前 你需要设置环境变量 让我们理解如何设置这些环境变量并使用这个命令启动容器
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/018_Udemy - Data Engineering using AWS Data Analytics part2 p18 7. Update IAM Policy Permissions.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止，我们已经克隆了glue样本的github仓库
我们也创建了glue spark qi的镜像
然而，在创建并启动它之前
我们需要设置这些环境变量
这些变量没有其他内容，只有日志
下划线区域访问密钥
我们还需要确保我们将要使用的任何凭据
在这些s三文件夹中有权限
只有这样它才会工作。到目前为止，在探索glue的过程中
我们运行一个简单的glue作业对航班数据集
生成日志的位置什么都没有
但是是这个s三文件夹下的spark hyphen locks
叫做itv hyphen flights
所以我们需要使用对这个文件夹有权限的用户凭据
让我们从用户开始
使用的用户
将执行操作的用户 将是axing the ui
我们需要使用那个用户的凭据
这个用户是itv github用户，我们在之前创建了
并且我们附加了策略
让我们审查用户和策略
并检查策略对这个文件夹是否有权限
如果不 我们需要
嗯 刷新策略
然后我们需要进一步审查详细信息
关于用户和策略
我们需要进一步访问控制台
我们可以扩展这个 我们应该能够点击
我是最近列出的一部分
或者我们也可以在搜索栏中搜索
我是，然后转到i管理控制台
然后你可以转到用户
用户名是itv github用户
这是我们将要探索的glue的深度的用户
让我们点击这个 让我们审查策略
我们可以点击这个查看策略
你可以实际查看策略详细信息
你可以看到详细信息
ah策略
你只对itv github有列表权限
还有在itv github上星对象
这什么都没有，只有获取对象，放置对象和删除对象，因为这是列表
我们应该能够将其他文件夹添加到其中
所以我们需要更新这个策略
我们只需要点击这个
然后点击编辑策略
转到这里Jason
然后我们应该能够说arn aws s three列列列
Itv连字符航班
这是我们想要获取权限的桶
因此我们必须将其作为资源的一部分添加
就动作而言，列表的桶以及星星对象
这就是获取对象，上传对象以及删除对象
我们必须在这里添加itv航班
所以让我 saying 逗号和然后粘贴
我只需将github替换为航班
现在策略已更新
我们可以点击审查策略然后保存更改
然后我们可以点击脉冲以退出
或者我们可以转到用户并点击此GitHub用户
因为我们已将访问锁定所需的权限附加到航班
我们应该能够使用aws cli进行验证
这是我们在docker jupyter lab环境中配置的
让我们进入docker容器
然后让我们验证我们是否能够访问此桶
在进一步之前
我需要确保容器正在运行
我可以说docker p s在这里
目前容器未运行
所以我必须说docker ps hyphen在这里
我们创建的容器实际上是aws analytics
所以我必须说docker start aws analytics
它将为我们启动容器
然后我可以转到浏览器
然后说localhost列九九九九斜杠lab
现在 我在数字环境中
我应该能够打开终端
然后我说aws s three s three列斜杠
斜杠itv连字符航班
它失败了，因为我没有指定配置文件
让我指定配置文件
配置文件实际上是itv github
现在你可以看到桶中的文件夹
itv连字符航班
这就是你应该能够确保的
我们将用于配置我们的spark的用户
Qi有对spark locks文件夹所需的权限
这是我们想要查看的文件夹
或者您想使用我们正在本地使用docker设置的spark ware访问
由于您已成功为用户配置了访问spark locks的权限
现在 是时候启动与spark相关的ah容器了
Ui与ah此日志目录
并且我们应该使用相同的凭据，这些是我们作为部分此环境使用的 让我们深入细节
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/019_Udemy - Data Engineering using AWS Data Analytics part2 p19 8. Start Glue Spark UI Container.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们知道需要配置哪个文件夹
我们也准备好了用于启动容器的凭据
然而，而不是直接使用docker one开始容器
我将单独创建容器并启动它
否则 每当你运行这个命令时
它会创建一个新的容器
这不是一个好主意，尽管如此
你可以实际获取这些凭据的相关详细信息
去我们的docker容器
Jupyter Lab在哪里运行
AWS CLI在哪里配置
并且我们需要在这台PC或Mac上使用这种信息
以便这些信息可以传递给另一个Docker容器
在那里Glue Parkway将被启动
话虽如此 我可以去dot aws
然后在我的家目录中查找credentials
我应该能够看到此用户的访问密钥和秘密密钥
他有Spark权限
离子锁定在itv航班中
使用将可配置
Spark 所以让我回到这里，首先让我作为终端的一部分设置这些环境变量
第一个环境变量只是日志
下划线ar 所以我可以说日志
下划线dir
等于s3
冒号斜杠
划掉itv-航班
然后划掉锁
这是我们有很多的文件夹
我们需要向前
划掉在最后
然而此时我无法访问此路径
使用我有的用户，我必须首先配置配置文件
然后继续
所以让我运行aws
配置在这里
然后以itv github的配置提供个人资料
这是我以该身份进行身份验证的个人资料
访问密钥就是这个
让我粘贴在这里
秘密密钥就是这个
让我粘贴在这里
现在已使用这些凭据创建了个人资料
我应该能够通过说aws后跟s three来验证
路径就是这个s three
itv flights和配置就是itv github
你可以在这里看到文件夹
这意味着使用此配置文件
我能够访问锁
或者使用与用户相关的凭据
我们对此spark锁授予了权限
我现在能够访问桶中的文件夹
而不是使用称为docker run的命令来创建容器
以及启动容器
我将其分为两部分
一个是创建容器
然后我将实际单独启动容器
每当我们停止容器
它不会创建一个新容器
如果你使用docker run 它总是创建一个新容器，话虽如此
让我在这里创建一个脚本
脚本名将是create_underscore_container.dot h
我必须设置这三个变量
让我现在复制这个东西
与日志和分数有关
这就是这个
所以我复制这个路径
然后让我打开create container.h并粘贴在这里的课程
然后我们必须指定访问密钥
我可以从我的jupyter环境中获取访问密钥
我在那里验证了凭据
此外 我也在我的mac直接验证过了
无论我从哪个地方都应该能够复制
粘贴这些详细信息
让我来看看脚本
然后粘贴到访问密钥
然后粘贴秘密密钥
从这里我们已经配置了三个环境变量
让我从材料中复制这个docker run脚本
这是我们应该复制的命令
让我粘贴在这里
让我分成多行
在更改这个命令之前
现在我们不再使用run
我们将使用create来首先创建容器
然后我们将使用start来启动容器
所以让我改一下你的one为create
我们之所以必须将这些环境变量作为脚本的一部分指定
当我们运行这个脚本时
在创建容器时会使用它们
你可以看到 我们使用docker创建命令作为部分，使用日志访问密钥ID和秘密访问密钥
现在，让我进行一些更改
让我做一些更改
我们不应该这样做
因为我们不在这里启动容器
我们只是创建容器
我们可以完全删除这条线
然后在镜像名称之前
我会添加一行
短划线短划线名称
这是名称 以便我们知道容器的名称
我们应该能够通过名称管理此容器
让我添加这个
然后让我出这个，然后通过说source创建容器
创建_容器点h
它将为我们创建容器
你应该能够通过说docker验证
Ps短划线这里
grep spark ui
您可以查看有关容器的详细信息
您可以通过说docker开始
启动spark ui
端口绑定到零八零
默认情况下 您可以实际上转到详细信息这里
如果您转到说明
如果您审查它
说短划线p八零八零
我们没有更改此
您也可以通过检查脚本验证
创建_容器点h
您可以看到端口号为零八零通过说localhost
然后端口号八零八零
作为浏览器的一部分
我们应该能够访问与提交的spark作业相关的锁
让我說localhost call zero
八零 我们应该能够看到历史记录服务器
锁在此桶下
我们可以使用此ui访问这些锁
这样我们就可以交互式地调试问题
如果作业失败
甚至如果您想理解作业的性能
这将很有帮助
如果您想查看详细信息
您可以点击应用id
然后您可以查看详细信息
您可以看到阶段
存储环境等
这就是您应该能够访问glue的方法
使用包含spark ui的docker容器访问spark作业锁
我们必须克隆仓库
确保环境变量已设置
创建容器
使用本地主机和端口号启动容器
如果我们的作业出现问题，我们应该能够进行故障排除
我们也应该能够理解性能相关的方面 如果你想理解为什么作业以某种方式运行
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/020_Udemy - Data Engineering using AWS Data Analytics part2 p20 1. Prerequisites for Glue Catalog Tables.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们理解胶水爬虫的要求
以爬取元数据并创建胶水目录表
我们已经看到了一些关于创建胶水目录表的例子
现在我们从GitHub数据的角度来看待胶水目录表
我们需要在S3或其他支持的数据存储中有数据
使用胶水爬虫来爬取元数据并创建表
强烈建议对数据进行结构化
如果你对数据进行了结构化
我们应该能够使用爬取功能自动创建表
否则我们需要手动创建表
正如我们在数据集中对一般锁定所做的那样
如果你从文本文件中读取数据
那么数据应该有标题
以便数据可以爬取并创建胶水目录表
胶水应该有适当的权限
这样我能够访问S3桶
否则胶水无法爬取数据并创建目录表
使用数据的元数据
请记住这一点
你需要将结构化的数据上传到S3
然后拥有正确的权限 胶水可以爬取元数据并为您创建表
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/021_Udemy - Data Engineering using AWS Data Analytics part2 p21 3. Steps for Creating Catalog Tables.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们一步一步地来防御目录表
使用s3作为源
您需要将数据上传到s3
您不需要上传所有数据
至少上传数据集的数量
对于json，至少需要上传能够覆盖所有方案的数据
在某些数据集中可能缺少某些字段
在其他数据集中可能存在
我们需要确保至少有这样数量的数据
以便能够覆盖所有情况
以便能够创建没有太多空白的glue目录表
即使您上传的数据集添加了额外的字段
您可以爬取并刷新表
但如果您删除某些字段
那么在开始时可能会遇到一些挑战
确保在爬取和创建表之前尽可能多地覆盖字段
所以使用json数据将数据上传到s3
然后创建爬虫
我们需要为爬虫命名
我们需要配置一个角色
以及配置源
从该源获取数据并创建表
确保您遵循所有这些步骤，以便能够无问题创建表
没有数据
您将无法创建任何表 尤其是使用爬虫
您可以手动创建表
但手动创建表
这是自动化创建表的目的
以及刷新表的能力
因此我们应该依赖此选项 数据被复制并创建表 使用爬虫
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/022_Udemy - Data Engineering using AWS Data Analytics part2 p22 5. Download Data Set.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们下载GitHub活动数据
以便我们可以将其上传到S3以学习数据工程
使用AWS分析服务三天
这将大约为7GB
这将花费一些时间来下载
请确保您记住这一点
我们需要有一个良好的数据集来实际理解构建解决方案的价值
使用AWS分析或甚至其他相关堆栈
您可以使用wget将数据下载到本地机器
wget将作为mac的一部分以及Ubuntu和Windows上可用
我不确定你是否能完全使用w get
作为powershell的一部分
但如果你使用git best
我认为w get会对你有用
确保你理解可以用于Windows目的的工具
但对于mac和linux
你应该能够使用w get
我们将下载三天的数据
这是我们将要下载的日期
我们可以使用该网站提供的说明下载文件
我们可以点击这个
我们应该能够访问到这个档案网站
你可以看到指示
这是您可以使用的命令，以下载一个文件
如果您想下载给定日期的所有文件
您必须使用这种方法
您可以说年份-
从零到二的日期-从零到二
给定日期下载
因此，使用此方法将下载文件到我们的本地机器上使用w get
我们可以像这样首先创建一个名为下载的文件夹
我正在创建目录
这将处理为那时创建目录
我可以说cd并转到目录
我必须删除mk l在这里
我在目录中 我应该能够使用双命令
命令是作为材料提供的一部分
您可以将此复制到下载为二
二一 January 十三为整日
你可以使用双门
像这样 当我从材料中复制粘贴到这个项目窗口时
它会添加这些反向百叶窗
这就是为什么它失败了
我们可以在mac上删除这些反向斜杠
如果他们被生成你应该能够下载
你可以看到文件正在下载
这些文件下载需要一些时间
如果你想利用你的电脑的容量并并行运行这些文件
如果你使用的是Mac或者你想要...
你可以使用Notepad++提交这些东西
让我打开一个新的标签页
然后我复制并粘贴这个东西使用Notepad++
像这样
所以我能够使用多线程下载文件
现在正在下载
我们也可以做同样的事情
我正在使用Notepad++
这样我就不需要在你的情况下使用多个标签页
如果你不熟悉Notepad++
你可以打开多个标签或终端窗口
你应该能够并行下载多个日期
你也可以围绕这个脚本
如果你熟悉脚本或编程使用Python
现在文件正在下载于三天
让我们看看第一天发生了什么
它还在下载中
一旦它们被下载
然后我们实际上需要将这些文件上传到S3
然后继续
与13相关的文件已成功下载
你可以通过运行这里来验证
你可以看到所有与13相关的文件
与14相关的文件在这里没有下载
这意味着我可能已经下载到long文件夹
是的 我实际上在我的家目录本身运行了它
你将在我的家目录中看到所有文件
你可以在这里看到文件
所以我可以做这些事情
我可以将我的家目录中的文件移动到这个命名约定
目前下载仍在进行中
一次下载已完成
是的 两次下载已完成
我应该能够将我的家目录中的所有文件移动到此下载
所以现在我可以说hyphen ltl
我应该能够看到此处的所有文件
包括2021年1月13日至2021年1月14日
以及2021年1月15日
你也可以运行名为du hyphen h的命令
如果你使用的是Mac或基于Linux的环境
你应该能够检查所有文件的大小
我们有4.5GB
不是7.7GB，而是4.5GB的数据在此目录下
我们应该能够使用这个来上传至S3并继续
让我们了解如何将这些文件上传到S3 使用AWS SSO S3控制台以及命令行
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/023_Udemy - Data Engineering using AWS Data Analytics part2 p23 7. Upload data to s3.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们已经成功地下载了第十三日的文件
2021年1月14日和15日
让我们将这些文件上传到S3中
我们可以使用网页控制台或命令行
让我们走一遍与网页控制台相关的步骤
然后我们将回到命令行界面，关于网页控制台
你可以去浏览器
你可以去 aws点amazon点com
然后你可以去s三
你应该能够根据itv-hyphen-github进行筛选
如果你按照步骤操作
你应该拥有这个存储桶
如果没有 你可以点击创建存储桶
并以itv-hyphen-github命名创建存储桶
我们已经创建了几个名为landing和raw的文件夹
如果你没有它们
只需点击创建文件夹，确保这些文件夹现在已创建
我们可以进入landing
我们需要在这个文件夹中创建一个名为g activity的新文件夹
我们将把这些文件上传到这个文件夹中
点击这里创建文件夹
确保我们在文件夹内部
如果你想使用aws web控制台上传文件
只需点击这里或这里
你应该能够点击添加文件来添加文件到这个文件夹
你可以选择所有这些文件
然后点击真实上传
然后你可以说上传
它会处理上传文件
如果你不熟悉命令行界面
确保你至少使用网页控制台上传文件
然而，我推荐使用命令行界面，因为它现在更快
让我取消这个
也让我确认没有上传任何文件
如果有 我会删除它们
现在没有上传任何文件
让我们回到终端并
尝试使用命令行方法将文件复制到此位置
命令无非是
AWS S3 CP
如果你想获取帮助
你可以说AWS S3 CP帮助
你应该能看到几个选项，除了参数
例如本地路径和AWS S3
我从本地路径复制文件到AWS S3
我们将使用的选项无非是递归
让我再次运行帮助
然后按空格键向下移动
你可以看到有一个选项叫做递归
将使用此选项递归
将所有文件从本地文件系统复制到S3中
让我称之为队列 然后说aws ccp点
这意味着当前目录
工作目录中的所有文件将被复制
如果你在最后使用递归，位置将不再是itv-github
登录geh活动
在这里我们必须使用正斜杠
然后我们可以说减号减号递归
我们也可以说减号减号配置
使用适当的配置
在这种情况下，配置无非就是itv github
你应该已经现在在github下配置了凭据
使用那些具有写入s3桶权限的凭据
这无非就是itv github now
让我们按回车 它会负责将文件从本地文件系统复制到s3
你可以看到它已经开始了
这取决于你的网络带宽
它将花费它应该花费的时间
让我们等到命令成功运行
然后我们将验证文件是否正确复制到s三中
由于使用单一线程复制所有文件需要太长时间
我使用了三个并行线程
像这样使用排除和包括来复制与十三相关的文件
十四和十五
在并行 你可以在这里查看详细信息
你也可以运行tail hypha
是的 没有hub可以查看进度
你可以看到它在三个不同线程中发生变化
这就是为什么它刷新得这么快
让我们等到所有文件从本地文件系统复制到s three
使用这三个线程
然后我们会继续
我们可以加快复制过程
使用多个线程像这样
我们必须使用额外的选项与 cp 一起使用
例如 exclude Include
等等，以使用不同线程复制互斥的文件子集
现在你可以看到所有文件已成功上传到 S3 中
然而，位置不正确
我们应该将其复制到 ITV GitHub 登陆 GitHub 活动
然而，我已将其复制到 ITV GitHub GitHub 活动
我们可以将文件移动到适当的文件夹
要么使用aws网络控制台或CLI
让我首先刷新这个
确保所有72个文件都在这里
然后我可以实际通过点击这个选择所有文件
然后我可以说操作移动
我可以实际浏览三个
或者我可以直接通过在这里输入指定目的地
所以我可以说itv github
着陆geh ch活动
所以这是我想把所有文件复制或移动到的文件夹
然后我可以说移动这里
它会处理移动所有文件
从我们的itv github活动到itv github着陆活动
我们的itv github活动到itv github着陆g活动
要么 你可以像这样使用aws网络控制台
或者我们也可以使用移动命令来移动文件
使用aws CLI
要移动 你可以探索aws s three mv命令
你可以说aws s three和v帮助
你应该能看到这里的帮助
你应该能够使用这个并移动它
如果你想使用命令行
你可以自己探索并进一步
移动将相对更快
因为我们是从s three的一个位置移动到s three的另一个位置
然而，当谈到cp命令时
我们从本地文件系统复制文件到s three
因此它相对较慢
作为这个话题的一部分
我们已经成功将文件从本地文件系统上传到s three
取决于你的偏好
你可以选择aws
子控制台或命令行
如果你想并行
你可以并行使用no hub
你应该能够使用包含和排除，以便使用多个线程
你可以从本地文件系统复制互斥的文件子集到s three
因为你已经成功将文件从本地文件系统上传到s three
现在是时候探索glue目录选项了 以对这些s three文件创建表
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/024_Udemy - Data Engineering using AWS Data Analytics part2 p24 9. Create Glue Catalog Database - itvghlandingdb.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


正如我们已经成功将GitHub活动数据上传到S3，现在
让我们继续在Blue Catalog中创建数据库
然后我们将看看如何在Blue Catalog中创建表
我将进入AWS管理控制台
然后我们实际上可以转到Glue在分析部分
这里是Glue
一旦你点击这个 它将带你到Glue控制台
有两种创建数据库的方法
一种是通过前往数据库
然后通过点击添加数据库
这是爬虫本身的一部分
你可以选择创建一个新的数据库
你可以选择这两种方法之一
但更正式的方法是首先创建数据库
然后定义爬虫在数据库中创建目录表
让我们继续通过点击添加数据库来创建数据库
这里 我们将为数据库命名
在这里我们为落地数据集创建数据库
因此我给这个名字
然后我们可以说 create 会自动为我们创建数据库
你可以在这里看到数据库
现在是我们使用它的时候了
创建者创建与 github 登陆相关的数据库表 数据集
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/025_Udemy - Data Engineering using AWS Data Analytics part2 p25 11. Create Glue Catalog Table - ghactivity.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止，我们已经成功将GitHub活动数据上传到S3
我们还创建了一个名为ITV G Landing DB的数据库，用于那个设置
现在是时候为我们创建一个爬虫了
然后使用那个爬虫来创建Glue目录表
你可以去爬虫那里点击添加爬虫
或者你可以去数据库那里
点击这个数据库
然后你可以去数据库的表那里
然后你可以实际扩展表
然后你可以点击使用爬虫添加表
你可以使用任何一种方法
或者你也可以直接去表那里
你不需要在数据库的表中
关于你正在的数据库
你应该能够扩展表
然后说使用爬虫添加表
它会实际上让你在一个页面上选择数据库
你应该能够选择适当的数据库来将表导入数据库
我们想要复制
爬虫名称什么也不是
G活动
登陆爬虫
所以我要给这个名字
因为我们正在尝试通过爬行登陆文件夹来创建蓝目录表
因此我给了登陆爬虫在主名后面
然后我可以点击下一步下一步
你需要在这里提供S3桶路径我们的S3文件夹路径
你可以去S3控制台并复制S3 ray从这里粘贴
你也可以从这里选择
确保你无论是复制粘贴还是选择
而不是打字 如果你打字
如果有拼写错误
它会抛出错误如访问被拒绝
它不会说文件夹不存在
因为用户通常在S3桶和对象上有受限的权限
因此它只会抛出访问被拒绝
而不是说不存在
如果你给了名字有拼写错误
我现在可以点击下一步这里
然后说下一步
因为我们只有一个数据存储，我们希望用它来创建目录表
我们不需要选择是
在这里我们需要选择不
然后我会说下一步现在我会创建一个新的
这个角色
真正的名字什么也不是GitHub
它会自动为Glue爬虫在文件夹上给予权限
以便实际爬行并创建表
所以我们不需要太担心将这个附加到脉冲上
现在会自动处理
我们应该能够点击下一步
我们将选择按需运行
我们也可以安排抓取器定期运行并刷新表
现在 我将选择一次按需
然后说在这里你可以选择现有数据库
数据库只不过是itv landing db
如果数据库不存在
你也可以从这里创建数据库
你不需要太担心其他事情
即使是关于前缀的
我们已经适当地创建了文件夹
文件夹名无非就是g活动
我想创建表格为g h活动本身
因此我不会添加前缀
如果你想要添加前缀
你可以在这里添加前缀
人们倾向于给前缀
例如着陆原始
等等 在表名前也要这样
但我将只使用活动g
那么让我说明年然后完成
如果你给前缀
根据文件夹名，表名将被附加
如果我说着陆，表名将是着陆活动
你不能摆脱文件夹名
在给表命名时
作为创建目录表的一部分
使用爬虫 您只能提供前缀
您不能完全替换现在所说的表名
创建了爬虫 爬虫无非就是一个落地爬虫
您应该能够选择这个
然后说运行爬虫
它会负责运行这个爬虫
并且输出将会是
您将在数据库中看到一个表
被命名为itv
名为活动着陆db
让我们等待直到表格创建
然后我们会进一步
现在爬虫状态更改为停止
我们应该能够去表格
我们可以在这里看到表格
表格名称无非是g活动
如果你点击表格
你应该能够看到表格的结构
我们有ID类型
Rao 负载
公共创建者 未评分
尚未 并且所有作为列
大多数列的类型是结构体
您可以看到结构体
是结构体类型
负载是结构体类型
并且也是或结构体类型
如果您想了解更多关于这些构造字段
您可以点击这些
您应该能够看到那些结构体列的完整结构,那些列在构造类型中
让我们理解当我们使用爬取数据时发生了什么
它实际上会访问指定的位置
在文件夹中穿过文件
根据这个数据集推断模式
然后它会在glue目录中实际创建表
一旦表在glue目录中创建
我们应该能够使用ea等服务对表运行查询
或者我们应该能够使用emr处理数据
甚至第三方服务,如databricks
随着我们进一步深入,我们会了解这些细节
目前我们已经成功创建了爬取器
并且使用该爬取器我们已经成功创建了表
现在我们已经理解了当我们运行爬取器时会发生什么
它会从现有数据中推断模式
并在glue目录中创建目录表
然后它会创建目录表在glue目录中 并且它会从文件中推断模式
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/026_Udemy - Data Engineering using AWS Data Analytics part2 p26 13. Running Queries using Athena - ghactivity.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


正如您已成功使用爬取创建了Glue目录表
让我们验证该表
使用乙醇验证使用ea
您可以实际访问服务
然后转到aws
分析
点击这里ea
如果您首次进入athena
您可能需要配置查询结果
如果提示您这样做
确保在继续之前配置查询结果
您可以实际扩展这里数据库部分
您应该能够看到itv gelanding
Db这里您应该能够看到活动表
这是由glue爬虫创建的
当我们运行它时 然后我们应该能够扩展此并应该能够看到此处的列
使用这些 我们应该能够运行查询
所以让我打开页面这里
让我们从获取计数开始
我只需要说select count of one
我们需要确保在此处选择了适当的数据库
否则可能无法识别表
数据库已选择这里
因此我应该能够说from g activity
它将运行查询对itvg landing db数据库中的表
让我选择并运行此
我可以在末尾添加semi column
这是可选的 如果您想可以添加它现在您看到它实际上正在运行查询
您应该能够看到计数片刻
您可以看到有八百三十九万
三万八千二百二十九个事件在三天内对github
现在您应该能够使用此查询获取计数
以及distinct report id计数以获取有关新创建的仓库的详细信息
这就是您应该能够选择新创建的仓库
类型必须为create event
和f类型必须为repository
当我们运行此查询select count of one
以及distinct rapper id计数以获取新仓库的详细信息
这些是计数
因此与新仓库相关的总事件数为三万四千零六十九
然而实际上创建的新仓库数量仅为三万四千
一百五十八九
如果您想预览一些仓库
您可以使用这个查询
我们说self trapper from jo
这是字段 这将给我们id和url的详细信息
因此，我只从结构类型字段中选择
您可以在这里检查数据类型
它是结构类型
它包含id, name, url
可能还有一些额外的字段，这里不可见
我们将知道我们拥有的额外字段
一旦我们运行此查询
让我们选择并运行此查询以预览十个仓库
以获取详细信息 例如id, url等
您可以在这里看到数据
这是id
这是名称，这是一个URL
仓库只包含三个字段
这就是您应该能够从特定字段中选择数据的方式
即使它是结构类型
让我们获取新创建的仓库的数量
这是您可以首先运行的查询
让我们理解此created at字段的结构
然后我们将运行此查询以获取created at字段的结构
您可以说 选择
创建自g活动限制十个
然后我们应该能够运行此查询
让我们看看数据如何
数据以ISO日期格式提供
这意味着它包含日期部分以及时间戳部分
时间戳部分使用t在日期中间分开
如果我提取这些前十个字符
我们应该能够获取日期
我们应该能够按日期获取计数
通过按它分组
这就是您应该能够获取新仓库的计数
每天创建的新仓库
我们有三天的数据
因此将获得一个每个日期
连同计数以及新仓库事件的唯一计数
让我们运行此并预览输出
现在您看到输出
我们获得了日期，计数和唯一计数
这就是我们可以使用ethana运行对glue目录表的ad hoc查询的方式
我们也可以使用报告工具如tableau连接到ethana
快速站点
点击
查看 等 我们应该能够使用ethera在s3上构建漂亮的仪表板
使用glue
数据 使用glue 目录表格
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/027_Udemy - Data Engineering using AWS Data Analytics part2 p27 15. Crawling Multiple Folders.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们理解如何爬取多个数据源并使用单个爬虫创建多个表
使用单个爬虫
为此我们需要一个数据集
目前我们没有这样的数据集
我们可以实际爬取多个文件夹并从中创建多个表
我在我的当地机器上有一个名为零售db json的文件夹
这是我的mac
你可以看到有一个名为零售db json的文件夹
它有六个文件夹，文件夹中包含json文件
我想将此上传到s3
然后使用爬取功能来创建多个表格，使用这些多个文件夹
那么我们首先创建一个存储桶
我将创建一个新的存储桶
在这个情况下存储桶的名称将是itv 零售
一旦我给出存储桶的名称
现在我应该能够滚动并点击创建存储桶
它将负责创建名为itv 零售的存储桶
由于存储桶已成功创建
让我们看看如何使用cli将数据上传到此存储桶
然而要使用cli
我们需要授予这个桶的权限给用户
我们使用我们实际为该用户配置了配置文件
我必须去服务并然后去
我在 我必须搜索适当的策略
这与相关的用户相关联
在这个情况下，用户只不过是itv github用户
所以这是用户
我们必须去这个用户
然后我们必须扩展这个策略
点击编辑策略
我们要授予对该策略的权限
将零售策略应用于此
我们可以在这里点击编辑策略
转到jason
然后我可以复制这个并
将其粘贴到此处 将航班更改为零售
与对象上的操作情况相同
这样我们就可以实际复制或删除此桶中的对象
所以我必须复制这些行中的一行
然后粘贴
然后说零售这里，现在我可以查看政策并保存更改
这将实际为用户授予权限
我们已经为CLI配置了一个配置文件
使用用户的凭据
因此我应该能够访问桶
使用AWS CLI
让我们去终端并运行它
然后继续在这里
我在隧道里
我可以说aws s three s three column /
Slash itv -
零售配置文件
Itv github
你应该能在这个桶中看到文件夹
到目前为止，那里什么也没有
这就是为什么它什么也没显示
现在 在下载下
我们已经验证过这一点
有一个名为db jason的文件夹
我想将这个文件夹复制到itv零售桶中
我们可以这样做的方式是
使用aws s three cp命令
我们可以在这里指定本地文件夹名称
本地文件夹名称就是零售_ db_ json
让我向前
然后在末尾加上 / 然后我们必须指定目标
这就是s three column
Slash / itv - retail /
零售_ dv_ json
然后加上 - - recursive
然后加上 profile itv github
现在按回车键，它会为我们处理复制文件
桶的名称就是itv - retail
你应该能在名为
零售 dvjason的下面看到文件夹
你应该能在每个文件夹的子文件夹和文件中看到
文件已成功复制
让我们通过访问aws s three控制台进行验证
让我访问s three控制台
让我搜索itv retail桶
然后进入那个桶
你可以进入零售桶
你可以看到文件夹
零售 dv jason在这里
你可以看到子文件夹
如果你点击orders
你应该能在这里看到文件
这些文件以json格式存在
因此我们应该能够创建一个爬虫
来爬取这些文件夹并为我们创建表格
让我访问aws glue控制台
然后点击爬虫
然后添加爬虫来创建新的爬虫
这个爬虫的名称
我想给它起名为retail crawler
然后点击下一步
然后我们可以将这些设置为默认值
然后点击下一步
数据存储类型是s3
桶路径就是这个
我们需要复制s3
我们需要给所有子文件夹的父文件夹
这样我们就可以自动扫描所有子文件夹并创建表格
当我们运行调用时
现在我们可以转到glue控制台并粘贴路径
如果你在这些不同的文件夹中有这些文件夹
那么我们实际上可以添加多个数据存储
在这种情况下只需添加一个数据存储
因为所有子文件夹都在一个父文件夹下
零售个体化现在
我可以点击下一步
我们不需要添加其他数据存储
在这种情况下 假设你有一个文件夹中的订单和其他项目和carbis
客户 部门和产品在另一个文件夹中
让我们将它们命名为零售db json one和零售db json two
在这种情况下作为第一部分数据存储
您需要指定路径为itv retail be json one
然后您需要选择是
然后下一步您可以指定itv retail retail dp jason two的路径
然后您可以继续前进
它将处理扫描这两个数据存储中的所有子文件夹
并为我们创建多个表格
在这种情况下，因为我们所有子文件夹都在零售dbation下
我们只扫描这个
因此我们可以选择不并点击下一步
我将创建一个名为aws glue服务角色的新iam角色
它可能已经存在
如果它存在 我将选择现有角色
我将选择零售
如果不存在 它将创建并继续前进
让我们点击下一步并查看角色是否已存在
角色不存在 因此它将为我们创建具有目标桶所需的所有权限的角色
itv retail 我们可以将其保留为按需运行
您可以看到有其他选项，我们可以定期安排
在这种情况下，我们只是使用按需运行
然后点击下一步以继续
我们将添加一个名为retail underscore db的新数据库
因此我点击了添加数据库
我给它起了一个名字叫retail and score db
让我点击创建以在数据库下创建表格
当我们运行这个爬虫时，我应该能够点击下一步
我们可以回顾我们所做的选择
然后点击完成，爬虫将被创建
现在我们应该能够通过选择这个来运行爬虫
并点击运行爬虫
我们已经知道如何运行爬虫
现在我们将使用Web控制台来验证是否创建了表
让我们理解如何使用这个
使用AWS CLI
将详细解释如何使用AWS CLI运行爬虫
还将了解如何使用AWS CLI和Web控制台验证是否创建了表
这样 你将能够熟练使用AWS CLI来管理目录
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/028_Udemy - Data Engineering using AWS Data Analytics part2 p28 17. Managing Glue Catalog using AWS CLI.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们可以管理胶水目录会议
使用胶水
胶水目录会议只不过是爬虫
数据库表等
让我们了解如何触发爬虫
以及如何使用aws胶水命令验证表是否已创建
我们已经成功将我们的零售db json数据复制到桶中
零售在一个名为零售db json的文件夹下
我们创建了一个爬虫来爬取该文件夹并首先创建表
我们需要确保用户具有通过cli管理胶水目录的所需权限
如果我在这个基于dual的docker环境中
我已配置了aws cli并启动终端
我们应该能够验证我们是否能够管理aws
胶水组件
通过运行简单的命令 我们可以简单地说aws胶水列出爬虫
它应该显示爬虫
如果它按预期工作
它说区域需要配置
并且也使用配置文件配置了凭据
我需要在这里使用配置文件
让我输入-hyphen-profile配置文件的名称
是itv-github
现在可以回车
但它仍然抱怨区域
我们需要提供区域
区域是us-east-1
你可以在这里看到 北弗吉尼亚是us-east-1
因此我需要指定-hyphen-region和区域名称
即us-east-1
现在可以回车 你可以看到它说权限被拒绝
它说 在调用爬虫时
用户未被授权
这意味着权限被拒绝
因此我们需要为该组中的用户提供适当的权限
您的用户名是itv-github用户
组名是itv-github组
itv-github组应该具有适当的权限
实际上允许该组中的用户运行这些命令
所以让我们去 我是
我们可以去服务 然后点击iam
这将带我们到iam控制台
然后我们可以实际去组
组名是itv-github组
选择它 我们可以在这里添加一项政策
应该附加的脉冲实际上就是aws glue服务角色
我们可以添加以下政策
现在我们应该能够来到这里在终端中运行此命令
这将确保我们有适当的权限来实际管理目录
我们可能需要等待几秒钟才能使权限生效
您可以看到输出
现在我们有四个爬虫
一个是航班数据爬虫
第二个是航班Spark数据爬虫
Dh活动落地爬虫和零售爬虫
一旦您验证了这一点
您可以看到爬虫
现在您可以实际上获取有关零售爬虫的爬虫详细信息
使用以下命令
aws glue获取爬虫名称实际上为retail爬虫
由于有空间 我们必须使用双引号像这样
然后我们必须使用配置文件
配置文件实际上为itv github并且区域
区域实际上为s east one
我们应该能够获取有关爬虫的详细信息
它已失败 因为在区域中有拼写错误
让我复制这个
然后让我复制这个
然后让我说区域s east one并按Enter
现在我们应该能够看到输出
因此这些是爬虫的详细信息
零售爬虫
到目前为止我们还没有运行过一次
因此您将无法在glue数据库中看到任何表
让我们转到数据库
甚至retail db数据库本身现在也没有创建
让我们从命令行触发爬虫
您可以使用称为start crawler的命令
所以我们只需说aws glue start hyphen crawler
名称实际上为retail爬虫
配置文件实际上为itv github
然后区域实际上为us hyphen east hyphen one
然后按Enter
它将为我们启动爬虫
我们可以等到爬虫完成
然后我们可以实际验证是否为我们创建了表
我们还可以获取运行中的爬取状态
您可以说aws glue
帮助并按Enter搜索quality
在这里我们有批处理门爬虫
创建爬虫
删除爬虫
获取爬虫
获取爬取矩阵
获取爬虫
列出爬虫
启动爬虫调度
停止爬虫调度
更新调用者
更新爬虫调度
等等
可能我们可以通过获取爬虫来获取状态
让我们运行获取爬取命令
我们之前也运行过这个命令
看看它是否提供关于运行中的爬取的状态详情
你可以在这里看到
状态显示为运行
你可以实际运行这个命令来验证爬虫是否正在运行
现在状态更改为停止
我们应该能够使用CLI列出数据库和表
我们也可以通过AWS Web控制台进行验证
让我们首先使用AWS Web控制台进行验证
你只需前往这里
然后你可以在这里看到零售DB数据库，你可以点击这里
然后你可以点击零售DB中的数据表
你应该能够在这里看到表列表
你也可以前往ethanna
你应该能够对这些数据库表运行查询
以确保可以从这些数据库表中查询数据
Little DB数据库会自动为我们选择
我们只需点击这里
创建一个新的标签页
然后我们应该能够运行查询
例如，选择orders表中的计数
让我们在运行时运行这个
我们也可以说从orders表中选择星号，限制为10
以查看前几条记录 在运行此查询之前，我们可以先预览数据
让我们检查前一个查询的输出
让我向下滚动
你可以看到orders中的计数
现在是68300
我可以选择这个 我应该能够运行此查询
以预览数据
以确保数据以预期方式呈现
它正确呈现了
因此我们很好
我们能够从命令行触发爬虫
我们能够通过控制台验证表是否已创建
并且我们也有使用以太坊的查询
现在我们也可以使用aws cli列出数据库和表
如果你回到aws cli这里
你应该能够运行命令
名为list databases的指令可以用于列出带有itv github配置的数据库
然后，地区我们-东
连字符一然后按回车
实际上获取数据库
不仅仅是数据库
所以让我们运行获取数据库并获取数据库名称
你可以按回车
你应该能在这里看到输出
我们有一个数据库名为jhdb
然后是itvdh着陆db通用db航班db默认db
我们也应该看到零售db
让我们看看哪里我错过了零售db
是的 最后一个是零售db
像这样 你应该能够列出数据库
你也可以通过说获取表格来从数据库中获取表格
然后数据库名为零售和得分db
它将实际获取有关表格的详细信息
您可以在这里查看每个表的详细信息
它提供了所有必要的信息
让我们审查其中的一个
这是产品
数据库名为零售db
您可以在这里查看列详细信息
然后位置输入格式
输出格式等等
输入格式是文本输入格式
实际数据是json格式
这就是为什么你可以在这里看到调查
实际上使用的是json调查
你可以甚至获取列名和其他详细信息
这就是你应该能够获取数据库中所有表详细信息的方式
你也可以使用获取表并获取一个表的信息
你只需像这样说获取表
你必须在数据库名称上方指定数据库名称
你也可以指定名称来传递表名
在这种情况下 让我们获取关于产品表的详细信息
你应该能够看到产品表的详细信息
这就是你应该如何使用aws cli
与glue目录进行交互并管理glue目录中的所有组件
你应该能够触发爬虫
你应该能够创建爬虫
你应该能够创建表等
然而
通常我们使用命令行或脚本来启动和停止调用者
或者通过安排它来使用一些脚本
使用实际的调度工具
这就是为什么我们应该熟悉这一点
爬虫通常仅使用Web控制台创建
但可以使用命令行触发
或者脚本来安排这些爬取部分的调度
每当数据刷新时
以便表格也能自动刷新为我们
因此我们应该对aws
CLI或auto three感到舒适
两者都是为了自动化触发爬虫的过程和创建表格
以及使用诸如获取数据库，获取表格等命令
甚至可以验证一旦爬虫运行，数据库和表格看起来如何
16： 等等 这就是为什么你必须对aws
CLI或auto three都非常熟悉 两者
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/029_Udemy - Data Engineering using AWS Data Analytics part2 p29 19. Managing Glue Catalog using Python Boto3.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你已经理解了如何使用 AWS Glue 目录，接下来让我们探索如何使用 Python Auto Three 管理目录。
使用 AWS CLI 或 Python Auto Three，我们可以触发与 AWS Glue 相关的任务，或者自动化管理目录。
我们可以从外部调度器触发与 AWS Glue 相关的任务，或者自动化管理目录。
让我们探索如何使用 Python Auto Three 管理 AWS Glue 目录。
让我们探索如何使用 AWS Glue 管理目录。
那么让我来删除所有这些表，通过去操作
然后说删除表
我们也可以使用aws cli来删除这些表或者甚至自动三
但现在我是使用网页控制台删除
我们也可以删除数据库
如果你想要删除数据库
所以让我们选择零售数据库，然后说删除数据库来删除数据库
现在让我去到这个数据库环境来实际生成自动三代码
来列出爬虫
来启动爬虫
并且验证表格是否已创建
当爬虫运行时
我们也将了解如何检查爬虫是否仍在运行或已完成
因此，在这种情况下，我们需要导入water three
您需要确保water three已经安装
并且用户的凭据也已经配置
我们通过其尝试访问的凭据应该已经配置
我们已经在名为itv github的配置文件下进行了配置
当我们使用both to three时
我们可以有多种方式传递配置文件
一种方法是设置一个名为aws profile的环境变量，该变量的值是配置文件的名称
这就是itv github
在我们的情况下，使用python
你可以这样设置配置文件
你需要导入一个名为this voice的库
这是一个纯python库
然后我们要说voice dot envion dot set default
它没有被设置，但set default
然后我们要说aws underscore profile
这是我们需要设置的环境变量
否则我们无法确定应该使用哪个配置文件
如果这是命令行界面 我们可以像这样指定配置文件
通过设置环境变量aws_profile并指定适当的配置文件名，我们可以获得类似的行为
这实际上就是itv github
我们应该能够创建自动三客户并管理glue目录组件
让我们运行这个
我们还需要设置区域
如果你查看所有命令的命令
我们设置了区域
如果我们没有设置区域
将无法使用glue运行aws命令
让我们首先在不设置区域的情况下运行它
然后我们将看看我们正在进入什么
然后我们将重新启动这个内核
然后我们将设置其他环境变量并继续
因此，在这种情况下，我说glue_ client等于三点客户
然后，我们必须像这样传递glue字符串
这将为我们创建客户端
然而，它说没有区域错误
如果你滚动下来
它实际上说你必须指定一个区域
指定区域的一种方法是使用环境变量本身
环境变量的区域是aws_default_region
我将详细介绍
我们可以使用os像这样查看可以传递的所有环境变量的详细信息
但首先让我们验证
然后我们将详细说明
你可以实际上说语音环境.set_default
属性名称是aws_default_region，其中包含下划线
区域名称是us east one现在我们可以运行这个
然后我们可以运行这个
现在创建了客户端
你应该能说
Glue client.dot list crawlers列出爬虫
我们使用了aws glue list crawlers来列出爬虫
我们必须使用 list crawlers和glue client来实际列出爬虫
使用water three
当我说早些时候 我谈论的是aws cli现在我们谈论的是auto three
你可以在这里看到颜色列表
你应该能够使用key crawler names访问所有爬虫的名称
通过使用此键
它已经写了一个dict
我们可以实际上传递此键
我们可以获取爬虫列表的Python列表
然后实际上可以获取爬虫名称
并获取有关特定爬虫的更多详细信息
例如 在这种情况下，如果我想获取有关零售爬虫的详细信息
它是列表中的第四个元素
因此，我可以说after three像这样
实际上会返回一个零售爬虫
我应该能够使用这个
我可以说color等于glue client list
Scrollers of crawler
Names of three
这将给我们带来零售爬虫
我们可以通过说crawl像这样进行验证
为了获取爬虫的详细信息
现在我们可以使用胶水客户端
然后说获取爬虫
伟大的爬虫需要一个参数
那就是名字
使用这个名字
我们应该能够获取爬虫的详细信息
所以在这种情况下我可以说名字等于爬虫
或者我们也可以硬编码说零售爬虫
我们应该能够获取零售爬虫的详细信息
在这里你可以看到目前它处于成功状态
你可以看到关于它创建时间的其他信息
等等
你也可以获取爬虫相关的数据库名称和其他元数据
现在您可以使用启动爬虫功能开始爬取
您可以通过说glue client. start crawler来实际探索
然后您可以说问号来获取帮助
您可以看到它接受名称作为参数
因此我们可以说
名称等于爬虫
这就是一个零售爬虫
它会在我们运行时实际启动爬虫
目前它正在等待直到完成
因此将无法在此进行验证
然而您可以转到命令行
您应该能够运行获取爬取在这里
您可以从这里检查状态
已成功
这是一个大型爬取状态
当前状态实际上是正在运行
所以目前它正在运行
但之前的状态现在是成功的，实际上它已经开始运行
可能花了一些时间才启动
这就是为什么它等待
如果你想检查这个爬虫的状态
你应该能够再次运行这个获取爬虫
你应该能够得到这个状态
它正在运行
如果你想读这部分
你可以做的是
你可以实际上在这里粘贴它
然后它是爬虫的一部分
所以有一个大的子文档是爬虫
其中爬虫的一个属性是状态
所以我们可以说爬虫的状态来获取状态
现在状态改变后它停止了
我们应该能够获取关于数据库的详细信息
以及使用获取数据库和获取表函数获取表
让我们在这里添加一个单元格
然后我们看看获取数据库的帮助，说是glue client点
获取下划线数据库问号并运行这个
你可以看到有一个名为获取数据库的函数
你应该能够列出数据库
你可以传递这些附加参数
但现在我不探索这些参数
我将仅运行获取数据库来列出数据库
我只需删除这个问号并将其替换为圆括号像这样
我们应该能够获取数据库
这也是一个字典，键称为数据库列表啊
键数据库列表包含数据库列表以获取数据库列表
我们可以这样说数据库列表像这样
它将返回数据库列表
然而，列表中的每个元素都是字典类型
包含名称，描述
位置新 或创建时间
创建表默认权限
等等以获取名称
我们只需从这个名称获取值
在这种情况下，我将遍历此列表
我将打印数据库名称
我所需要做的就是 我必须说对于db在glue client点
获取数据库数据库列表
这将在列表中
列表是类型列表字典
因此，我可以说打印db
所以db将是字典类型在这里
因为我们正在处理列表字典
然后因为它是字典
我可以说db of name以获取数据库名称
我们可以看到数据库名称在这里
如果你想获取表列表
有一个名为获取表的函数
让我们看看获取表的帮助
让我们运行这个，你可以看到它需要几个参数
让我们爬回以查看参数列表
然后我们将使用适当的参数从数据库获取表详情
帮助非常大
让我快速呼叫一下
你可以看到关于获取表的详细信息在这里
我们必须传递一个数据库名称以获取属于数据库的表的详细信息
因此，在这种情况下，我可以只说获取表
数据库名称可以传递给名为数据库名称的参数
数据库名称为零售下划线db
我们可以运行这个，你可以看到它写了表列表
所以我们可以使用此tablist键以获取表列表
并且表列表也是字典列表在这里
因此，每个与特定表相关的元素都是字典类型
你可以从这里到这里查看详细信息
好的 如果你想要从数据库中获取表格的名字
我们所需要做的就是
我们需要处理这个列表
使用名称来引用表格的名称
你可以这样操作：for table in glue client.get_tables
数据库名称等于db of table list
然后我们可以说print table of name像这样
我们应该能够获取表格的名字
如果你想要获取特定表格的详细信息
让我们看看一个叫做gettable的函数的帮助
那么我们可以说glue client dot
Get tables或get table
然后我们可以滚动向下
你可以看到它需要两个主要参数
一个是数据库名称和表名称
我们应该能够将这个函数的数据库名称和表名称传递过去
我们应该能够获取关于表的详细信息
所以我已经传递了数据库名称
现在我必须传递表名称
参数名为name
和表名为
假设为orders，让我们运行这个
你可以看到输出
输出与orders相关
你可以看到列
数据类型 文件格式等等
这就是你应该能够获取爬虫详细信息的方式
数据库表等
使用自动三
您可以使用AWS Web控制台创建爬虫
但是，您可以使用Water Three自动运行爬虫
触发爬虫的过程
话虽如此，有几件事我想要强调
因为我们已成功使用自动三验证
现在让我们转到Glue控制台并确保这些表已创建
让我前往数据库
在运行之前我们已经清理了这些表
使用Both Three
现在你又看到了这些表格
所以我们清理了这些表格
再次运行爬虫
使用自动三 我们使用自动三进行了验证
我们也使用aws web控制台进行了验证
你也应该熟悉适当的环境变量
这样你就可以轻松地设置ah
所有使用自动三所需的环境变量
你可以通过说aws bought three list of environment variables在谷歌周围搜索
这将重定向我们到官方文档
你应该去配置这里
你应该在这里看到所有环境变量
所以对于配置文件 这是aws下划线配置文件区域
这是aws默认下划线区域
取决于你的设备
你可以探索其他环境变量并开始使用它作为创建客户端的一部分
不仅限于glue
但你试图创建客户端的任何服务
你可以使用适当的环境变量并继续
到目前为止，我们已经了解了glue目录的大部分细节
我们也理解了如何管理glue目录
不仅使用aws cli 也使用auto three
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/030_Udemy - Data Engineering using AWS Data Analytics part2 p30 1. Update IAM Role for Glue Job.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们正在创建一个glue作业
该作业将把数据从landing复制到raw
在创建glue作业时
我们需要为作业关联一个角色，我们将要关联的角色
应该在landing文件夹中具有读取权限，在raw文件夹中具有写入权限
如果我们处理的是多个文件夹
那么我们需要在源文件夹中具有读取权限
并在目标文件夹中具有写入权限
让我们了解如何更新策略
以便我们可以实际将关联策略的角色附加到glue作业
在创建过程中
以便工作可以在源文件上读取并在目标文件上写入
以操作与角色相关的策略
你可以去服务
然后您可以转到角色
在这种情况下 我们将处理与aws glue服务相关的github角色
这是我们在定义爬取时创建的角色
所以角色就是这个
如果你点击这个
你可以看到这些角色绑定了脉冲
总共有两个脉冲 一个是aws clue服务角色策略
另一个是aws glue服务角色github策略
这些都是策略 不是角色
名字有点误导
现在您可以扩展此策略
您应该能够审查权限
我们在itv github桶的这个文件夹中有获取和放置对象
目标文件夹没有权限
这无非是授予读取权限
以及所有文件夹内的写入权限
我们可以打开权限到itv-github
以便工作能够写入到raw文件夹
一旦运行编辑策略
我们可以点击编辑策略
然后我们可以转到json
然后我们可以向下滚动这里
然后我们可以删除这部分从这一行
我们只需要说 itv 斜杠 github 斜杠 star
这将负责提供 both get object
以及 put object 在所有此桶内的文件夹下
现在我们可以说 review policy
然后保存更改
然后我们应该能够使用此脉冲绑定的角色
它只不过是 aws glue 服务角色
Github 我们应该能够创建工作 这将负责从 landing zone 读取数据并将数据写入 raw zone
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/031_Udemy - Data Engineering using AWS Data Analytics part2 p31 3. Generate baseline Glue Job.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


正如您已授权所需的权限，以便从源读取数据并写入目标，
让我们创建一个基准作业，
我们将读取JSON数据并将其写入Parquet格式，
我们不会进行分区，
至少目前不会， 我们将尽量保持简单，
以确保我们能够创建并运行作业，
然后我们将根据AWS Glue探索相关API进行定制，
在转换数据的过程中创建基准作业，
您可以转到服务，
然后点击AWS Glue，
这将带您进入AWS Glue控制台作为AWS Glue控制台的一部分，
您应该去作业这里，
然后说添加作业，
我们之前已经走过这些步骤，
也使用航班数据，
我们现在在使用我们的GitHub存档数据集，
现在， 当涉及到名称时，
我想要命名为GitHub JSON到Parquet，
这是作业名称，
我可以实际上选择，
我的角色， IAM角色必须是AWS Glue服务角色，
GitHub， 它应该具有在源读取权限以及在目标写入权限，
类型是Spark Glue buneeds two，
您可以在这里看到不同选项，
然而，我们选择这个，
这就是Glue版本二，
当我们实际创建作业时，
它会创建脚本， 我们将在后续详细讨论这些细节，
我们需要确保我们审查高级属性，
我们有一个称为作业书签的东西，
我们将在下一模块的下一节中讨论作业书签，
目前我们保持禁用，
我们可以展开监控选项，
我们可以实际选择作业矩阵，
连续日志以及Parquet，
当涉及到Spark qi时，
我想要使用GitHub桶，
因此我说它是s3 itv github，
然后Spark Python locks，
所以它将使用GitHub桶中的这个文件夹，
实际复制Spark locks，
这样我们就可以查看作业的历史记录，
现在， 我们可以滚动向下， 我们可以展开啊
这些其他部分也征税
我们不需要太担心
安全配置 脚本库和作业参数你应该滚动查看
如果你有任何额外的库需要传递
你可以在这里配置这些事情
你必须确保你选择了适当的数量
以便作业能够及时完成
目前 我将其保留为默认设置
我们是按需付费
因此只要我们处理的数据量足够多
数据集不会浪费太多钱
一旦作业完成
集群将被终止
因此我们可以将工作者数量保留为默认值
即10
然而如果你想要更快完成作业
根据你处理的数据
你必须确保你使用了适当的工作者数量
你可能需要根据数据量增加数量
你也必须根据作业的特性进行设置
你必须对提供工作者数量保持谨慎
根据你的需求
目前我们不会深入探讨这些细节
让我们继续
你可以将一切保留为默认设置
然后你应该能够点击下一步
它要求我们选择数据库名称和表名称
你可以向下滚动
我们感兴趣的是jh activity，位于itv g landing db中
你也可以在这里搜索表名
如果表和数据库太多
它会过滤出你正在寻找的详细信息
当我搜索activity时
它只显示了一个表
那就是activity
我们可以选择这个 然后我们应该能够点击下一步
我们正在将数据从json转换为parquet
因此我们需要在这里更改模式
然后我们可以点击下一步
它询问目标
我们将在数据目标中创建表
因此我需要选择这个
我们将使用s3
我们将数据dump到目标中
表不会自动创建
但数据将被dump到文件夹中
默认格式为json
然而，我们感兴趣的是Parquet格式
因此，我需要扩展这一点并选择Parker
数据将自动使用Snappy进行压缩
当我们选择Paret时 我们将在运行作业后审查这一点
然后您可以指定目标路径
目标路径只不过是itv-hyphen
GitHub raw和g活动
这就是我想要将文件dump的位置
现在我可以说下一步
它将实际上向我们呈现这个界面
我们可以移除列
我们不想把它复制到目标中
在这种情况下，我们希望复制所有列
因此我们现在不会删除这些内容，我现在可以滚动浏览。
然后我可以说保存工作并编辑脚本
它将重定向我们到脚本编辑器
你应该能够编辑脚本
一旦你详细理解了ap
目前不会改变脚本的任何内容
我们只是运行它，看看数据是否通过此基准脚本复制 然后我们将实际探索应用程序并按我们的要求进行定制
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/032_Udemy - Data Engineering using AWS Data Analytics part2 p32 5. Running baseline Glue Job.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


因为你成功地使用glue创建了基准工作并关联了脚本
现在是时候运行它了
了解如何审核以及如何验证它
使用ea ethan将使我们能够对表运行查询
为此我们需要创建表
我们将逐步运行工作以进行验证
通过创建目录表
然后使用ethera运行查询
所以在这种情况下运行 你只需点击运行工作
你可以查看所做的选择
现在应该禁用工作书签
至于集群大小，也可以进行设置
你可以去安全配置并往下滚动
你可以查看工人数量设置为10
一旦你审查了这些事情
如果你想自定义一些东西，你可以自定义
我们将深入探讨一些细节
随着我们进一步推进
目前我们不会改变任何东西
我们只需点击运行任务，以确保任务已启动
我们只需等待一段时间，以确保任务能够无问题启动
然后你会在这里看到任务的进度
你也会看到进度条
因为我们实际上创建了任务
我们选择了持续监控选项
这就是为什么你应该能在这里看到进度条作为部分进度
让我们等待任务启动
然后我们继续
现在你可以看到任务正在运行
你可以在这里看到进度
它还在运行 你可以在这里看到状态
此外，你也可以实际上去glue控制台，然后转到工作
选择工作，你应该能看到这里的进度
这也意味着它将继续运行
让我稍微移动一下
让我稍微放大一点
你可以在这里看到状态正在运行
如果你想回到早期的界面
你只需进入脚本
点击编辑脚本
你应该会看到这样的界面
在这里你可以看到进度条
它几乎要完成了
一旦完成
状态会在这里改变
你也可以在这里查看锁
你应该能看到锁
如果有其他的也
实际上你可以去云观察
你应该也能在云观察中看到锁
我们也可以配置Spark服务器指向锁的位置
在那里锁被写入
我们应该能够通过与这个Spark历史服务器一起提供的Spark方式解决历史作业的问题
然而，目前我没有将Spark服务器指向这个地点
因此我将无法使用它
即使作业完成后
目前，我们必须使用Glue控制台进行监控
我们还得忍受监控
使用Glue控制台本身进行监控
作业仍在运行
让我们等到它完成
然后我们再继续
看起来作业失败了
实际上你可以去锁那里看看为什么失败了
很可能是权限问题
被拒绝 实际上无法删除键
它失败的原因是因为它试图删除
下划线临时文件和文件夹
然而，我们在S3桶中并没有删除权限
这就是为什么它会失败
我们应该做的是
我们可以去 这里是控制台
去适当的角色
那就是AWS Glue服务角色
GitHub 然后我们必须去这个策略
点击编辑策略
去这里看JSON
而不是说S3获取到S3放置对象
我们可以做的是
我们可以删除一行
去掉逗号然后改为星号对象
现在我们应该能够审查策略
保存更改 然后我们应该能够运行作业
然而数据已经复制
因为我们有权限
它失败的原因是无法删除一些文件
再次运行它
并且看到作业成功
我们应该做的是我们去S3控制台
去目标文件夹
清理数据 然后我们再运行
所以让我们去清理S3桶后运行作业
所以让我转到这个标签
让我进入s三这里
然后进入原始
进入活动
选择这里的所有内容
然后说删除
确保你永久复制粘贴这个
删除或输入它
然后点击删除对象以删除所有对象
然后对象将从这个文件夹删除
现在我们应该能够进入glue控制台并运行这个作业
点击运行作业这次它将无任何问题工作
一旦作业完成
我们将审查目标位置
并且我们还将尝试在创建目录表后运行一些ea查询
以确保数据没有任何问题复制
现在作业已成功运行
你可以看到状态更改为一项作业这里
然后灰色运行作业
即使进度条未完成
作业已完成 我们可以通过进入s三管理控制台进行验证
让我进入这个文件夹在itv github桶下
文件夹结构无非就是itv
Github或活动
我们应该能够看到使用snappy算法压缩的parquet文件
这将确认文件已复制到目标
使用parquet文件格式与snappy压缩
然而 最好运行一些查询对此数据
以确保数据如预期运行查询首先
我们必须创建glue目录表对此位置
以便我们可以实际进入aws glue控制台这里
让我们点击这个进入glue控制台主页面
这将关闭glue作业界面
并将实际将我们带到这个
现在我们应该能够进入爬虫
然后我们可以说添加爬虫
爬虫名称将是g活动然后raw爬虫
这是爬虫名称
让我点击下一步这里
让我点击下一步
文件的位置无非就是这个我们可以实际点击
复制s三栏这里
然后粘贴到这里
然后我们可以点击下一步
说下一步更新iam角色中的政策
无非就是这个aws glue服务角色github
然后点击下一步
它在抱怨
因为角色是手动创建的
它不是由服务创建的来解决这个问题
实际上我们可以选择现有的m角色
它只不过是aws glue服务角色github
然后点击下一步
然后我们可以点击下一步
我们将创建一个新的数据库
它只不过是itv g原始db
然后我们可以说创建
然后我们可以说下一步
现在我们可以审查我们所做的选择并点击完成
爬虫将被创建
我们应该能够选择爬虫并运行爬虫
通过点击这个按钮
它将负责运行爬虫
一旦完成
我们应该能够看到在itv g或db数据库中的名为g活动
我们应该能够运行查询对表
使用ethera运行查询首先
我们必须创建glue目录表对此位置
以便我们可以实际上去aws glue控制台这里
让我们点击这个去glue控制台主页面
它将关闭glue工作界面
它将实际上带我们到这
现在我们应该能够去爬虫
然后可以说添加爬虫
列名将是g
让我检查现有爬虫名
g活动然后原始爬虫
这是爬虫名
让我下说下一步
让我点击下一步
列名是g
文件的位置是nothing
但此我们可以实际上点击复制三杠这里
然后粘贴于此
然后我们可以说下一步
然后点击下一步
更新策略在iam角色
它只不过是这个aws glue服务角色github
然后点击下一步
它抱怨
因为角色是手动创建的
它不是由服务创建的来解决这个问题
我们可以实际上选择现有的m
它只不过是aws glue服务角色github
然后点击下一步
然后我们可以点击下一步
我们将创建一个新的数据库
它只不过是itv g原始db
然后我们可以说创建
然后我们可以说下一步现在
我们可以审查我们所做的选择并点击完成
爬虫将被创建
我们应该能够选择爬虫并运行爬虫
通过点击这个按钮
它将负责运行爬虫
一旦完成
我们应该能够在itv g或db数据库中看到表格
名为geactivity的表格
我们应该能够使用ethera对表格运行查询
实际上工作仍然失败
让我们通过点击这个来理解为什么它失败
它处于临时状态
因为它正在尝试删除那些临时文件
但现在它似乎与数据有关
让我们在这里滚动并查看
如果它实际上提供了关于为什么它失败的任何见解
你可以在这里看到问题
它与一些java io异常有关
这是因为某些字段具有null值导致一些问题
在写入标准parquet文件格式时
这是与glue相关的一个问题
现在不要太担心
即使我们看到差异，随着我们进一步前进
并尝试部分表并稍作定制
然后将数据复制到目标位置
这个问题将自动解决，因为我们已经创建了一个基线工作
我们已经运行了它 并且我们使用ea进行了验证
然而，我们看到工作失败
并且我们还看到了与数据计数的一些差异
让我们继续前进并探索api
定制我们的代码
重新运行它，以便数据如预期被复制 并且所有数据从着陆复制到原始
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/033_Udemy - Data Engineering using AWS Data Analytics part2 p33 7. Glue Script for Partitioning Data.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


当我们之前运行这个github json到glue job时
它只是复制了json文件到para文件格式
源文件无非就是itv github桶内的着陆文件夹
目标文件无非就是itv github桶下的raw文件夹中的love文件夹
我们有 这些是创建的文件
让我清理一下 然后让我们按年份，月份和日期分区数据
首先重构glue job By refactoring the glue job first
让我清理一下
我正在选中这里的所有内容
然后说删除
然后我们必须输入或粘贴永久删除消息
然后文件将被删除
然后我们将实际进行工作定制并运行它
这样数据就会被分区
在这种情况下我已经有了代码
我现在只是复制粘贴代码
API的详细信息将作为其他主题的一部分进行覆盖
目前 我只是要把这段代码复制粘贴到编辑脚本部分
然后我们来审查代码
这是代码生成器本身生成的那一部分
我现在没有修改任何东西，首先最重要的是
我正在将动态帧转换为数据帧
两个数据帧 数据源零的类型是动态帧
你可以看到它是蓝背景的结果
创建动态帧
我正在将其转换为数据框，以便我可以利用数据框
这样我就可以利用数据框
Ap 这些函数中的大多数也可以在动态框架上使用
但我已经看到它们在行为上有一些差异
这就是我为什么倾向于将动态框架转换为数据框的原因
一旦数据处理完毕，我将使用数据框来处理数据
我将将其转换回动态框架
以便我可以利用动态框架
APIs将数据以文件的形式写入或任何目标
我们希望写入
它可以是数据库
所以如果你看这段代码
它主要是将动态帧转换为数据帧
然后添加了三列年、月和日
这些是从created衍生出来的
所以如果你看数据created在使用连字符mm
连字符dd，并且也有时间戳
这将处理获取我们前十个字符
前十个字符只不过是四位年份
连字符两位数月
然后是连字符两位数日
这将处理提取四位数格式的年份部分
这将处理提取月份部分
这将处理从created中提取日期部分
现在数据框将包含动态框所有必要字段
这是我们从上面的json中获取的
它将有三个字段：年份
月份和日期
如果您想在写入目标位置时传递数据
使用动态框API
我们需要将此数据框转换为动态框
这正是这里发生的
一旦转换完成
我们可以使用glue上下文.dot.write_dynamic_frame
并从选项中
作为那一部分
我们可以传递动态框，这就是这个
然后连接类型是s3
因为我们试图将数据写入s3
大部分代码都是自动生成
我所做的唯一更改
是模式案例年份
月份和日期 它将实际处理创建那些部分，其名称等于month
等于和等于
来自created的值
然而，这些都将放在文件夹结构中
这是非常直接的代码
我们将在其他主题中探索API
我们已经有了代码
我们可以实际上保存这个
我们可以点击保存这里
一旦保存完成 您应该能够通过点击这个并点击工作按钮来运行此脚本
我们需要在脚本末尾有这个工作
让我们确保我们现在有这个
我们可以实际上点击保存
然后在点击运行工作之前
让我们确认目标位置中没有文件
您可以看到现在没有文件
我们应该能够说运行工作
然后点击一个工作
在运行工作之前我们没有自定义任何东西
我们不会进行任何自定义
即使工作已成功完成
我们应该能够在目标位置的不同文件夹结构中看到文件
文件夹将根据年份 月份和日期命名 这些是从created中提取的
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/034_Udemy - Data Engineering using AWS Data Analytics part2 p34 9. Validating using Athena.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


工作成功完成后
让我们确保一切都已验证
我们在s3桶中验证文件
我们还将运行查询
使用ethana运行查询
使用ethana 我们需要确保glue目录表已刷新
让我们详细检查所有细节
在这种情况下，我将进入s3管理控制台
刷新此页面以查看文件是否正确创建
您可以看到有一个名为二十的文件夹
二十一个月等于零一
然后他们等于十三
十四和十五 因为我们源位置有三天的文件
这三天文件的副本都被复制到此目标位置
正如我们所见，这里的文件
现在是时候对这些数据运行查询了
我们需要确保glue目录表已刷新
所以让我们转到glue控制台
在这种情况下，我正在使用此页面本身
单击aws glue
一旦您在aws glue控制台中
转到数据库
然后单击itv g raw db
这是我们之前为raag活动创建的表数据库
就是这样 您可以单击这些表
然后您可以单击此
您可以看到现有列
您可以单击一些属性
您可以获得更多详细信息
目前该表未分区
因此您将不会看到诸如年份
月份和日期
我们需要刷新此信息
您可以通过转到爬行者来刷新此信息
我们有这个爬行者
我们可以选择这个
我们可以单击运行爬行者
一旦爬行者执行完成
您可以实际转到表
您应该能够给出表结构
并且表结构应具有模式，然后我们将能够使用ea进行验证
当我们实际将数据复制到目标位置时
使用blue 我们没有指定表
我们只指定了s3，因为文件以特定格式写入s3
我们希望爬取数据集并创建一个表
以便我们可以使用ethana在此表上运行ad hoc查询
这就是我们为什么必须照顾好这一点的原因
如果有直接写入蓝色目录表的选项
你可以利用这一点，数据将直接返回到glue目录表中
但在我们的情况下，如果你审查了glue作业
我们只指定了s三存储桶和连接类型s三
在我们写入数据时，我们没有任何地方指定了glue目录
然而，在读取数据时
我们实际上说的是从目录，我们给出了关于数据库名称的详细信息
表名 等 因此，它能够从源表中读取数据
没有指定文件的路径
但当涉及到写入时
我们使用的是类型s三
并且我们将s三的路径作为路径属性的一部分
因此，输出返回到此位置
因为我们在这里没有直接指定glue目录
在test三位置发生的任何更改
它们不会自动在glue目录表中刷新
通过运行爬虫
它会处理刷新表结构
现在它正在更改为停止状态
我们应该能够转到表格
我们可以点击这个g活动
你可以点击查看部分以在这里查看详细信息
而且如果你回到上一页
我们在看到的列名中，你可以滚动下来
你可以看到额外的列year
Month和day
它们属于部分，你可以在这里看到详细信息 这就是你应该能够刷新glue目录表的方式
一旦glue目录表刷新完毕
你应该能够使用ea运行查询
一旦glue目录表刷新完毕
你应该能够使用ea运行查询
你可以去ethana服务
它就在这里
然后我们应该能够对源表和目标表运行查询
以确认现在没有丢失数据
让我们对这两个源表和目标表运行这些查询
源表在itvhlanding db数据库中
表名就是你的活动
它实际上指向了s三存储桶中的着陆位置
让我选择这个count查询并运行这个
这将花费一些时间
让我们等待查询执行
然后我们实际上会复制计数并粘贴在这里
然后我们实际上会对raw db运行此查询，看看计数是否匹配
你可以在这里看到总计数
让我复制这个并粘贴在这里，现在让我运行此查询
这个查询将实际提供有关新创建的仓库的详细信息
在那三天内
如果有重复项
独特的计数将消除这些重复项
让我们运行这个
您可以在这里看到计数
这是总计数
这是独特的计数
现在这个数据中有一些重复项，我们可以将数据库更改为itv
G 或 db
或者我们可以在表名前加上数据库名
你可以在表名前加上数据库名
你可以选择这个查询
你应该能够运行这个查询
如果一切都成功复制没有遇到任何问题，计数应该匹配
如果计数不匹配
这意味着有bug
要么是我们的代码 或者是glue的问题
也可能是glue的问题 你可以看到计数
现在计数正好匹配
这意味着所有数据都是从源复制到目标
你也可以运行此查询以获取新存储库的计数
这些是在2021年1月13日、14日和15日添加的
一月十三 十四和十五
一旦我们得到计数
我们应该能够与这些值相匹配
如果计数匹配
这将确保我成功复制而没有在中间失去任何东西，就像这样
你应该为你的项目要求制定标准验证
并确保这些验证是为了确保数据质量
在这个情况下，从着陆到法律
唯一的目的是将数据从jason复制到parker
也还有党派
我们使用胶水处理了这个问题 我们已经验证以确保数据没有任何问题被复制
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/035_Udemy - Data Engineering using AWS Data Analytics part2 p35 1. Introduction to Glue Job Boomarks.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


截至现在 我们已经将着陆区的数据复制到原始区，共十三
十四和十五
当我们真正处理着陆区的数据摄入自动化时
文件将以增量方式添加到着陆区
随着文件以增量方式添加到着陆区
我们可能想要捕获数据处理并将其存储到区
这叫做增量处理
处理增量处理的一种方式
尤其是使用glue作业，没有什么比利用glue提供的作业书签更有效
如果你想了解如何配置它
你可以点击一个作业，然后转到高级选项
你可以在这里查看详细信息
你可以启用作业市场
你应该能够运行作业
它将处理保存之前的作业状态
并从那里继续作为部分模块
我们将实际了解我们如何利用glue
作业书签并处理增量处理
随着文件添加到源
这就是着陆区 在这种情况下
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/036_Udemy - Data Engineering using AWS Data Analytics part2 p36 2. Cleaning up the data.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们清理原始层的数据
以便我们可以了解如何使用glue job书签进行增量数据处理
你可以使用aws web控制台或命令行删除
如果你想使用aws web控制台删除
你可以通过s3控制台部分访问s3控制台
你可以实际上转到桶
然后你可以转到原始
然后你可以转到活动
你应该能够选择这个文件夹
然后点击删除
它将为您处理删除文件夹
如果你想使用命令行
你应该能够访问你的环境，其中aws cli已配置
在我这种情况下 我在docker中配置了
我有双环境以及命令行在此
我应该能够转到命令行
aws cli在此已安装
并且使用该配置文件创建了配置文件
我应该能够删除s3中的文件
我只需说文件新
然后aws s3 rm s3 column
斜杠斜杠itv-github
raw-g-activity
斜杠斜杠递归
你可以按回车
它将为您处理删除文件
它说无法找到凭据
因为我没有指定配置文件
所以我必须说配置文件
配置文件就是itv-github
现在我应该能够运行这个
它将为您处理删除文件
您可以看到所有文件从这个位置被删除
如果你想使用a3
如果你不熟悉命令行
你应该能够使用s3删除
我应该能够刷新这个
并将看到此位置中无文件或文件夹
实际上甚至文件夹也被删除
所以我可以向上一级
您可以看到文件夹在ra
所以无论哪种方式对你来说最方便
确保您使用它并清理数据
以便我们可以使用glue job书签爆炸增量加载 数据
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/037_Udemy - Data Engineering using AWS Data Analytics part2 p37 4. Overview of AWS Glue CLI.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们对AWS CLI与Glue做一个概述
作为Glue的一部分 我们有几个组件
比如爬虫 目录
作业触发器等
我们应该能够使用低主命令来管理所有这些组件
这就是AWS Glue
你可以通过说aws glue help来获取帮助
我们通常使用的一个命令是列出作业
你可以得到这样列表的工作帮助
你应该能够使用这个命令列出工作
你也可以指定区域
如果你的配置中还没有定义区域
让我们深入细节，这样你就能理解我在说什么了
让我进入jupyter环境，我在那里配置了cli
我应该能够说aws glue，看看glue是否可用
从命令行 你也可以通过说ais glue help这样得到帮助
你应该能够看到所有可用的选项
AWS 粘合剂列出作业
我们列出了要删除的作业
我们删除了作业等等，我们将探索一些这些
随着我们进一步深入
我正在退出
您可以通过说 aws glue list 来实际列出作业
按 Enter，作业
然而 我们需要权限
并且我们还必须指定区域，不指定区域
现在不会起作用
它抱怨地区
我可以通过说短划线地区像这样跳过地区
然后说美东
短划一线 这是我的地区
你必须探索你的地区
你必须传递适当的值作为地区的一部分
现在你应该能够按回车
仍然 它可能会失败
因为我们没有指定配置文件
让我们添加配置文件
我可以说减号减号配置文件
配置文件什么也不是，它只是itv
Github
让我们运行这个并看看它是否实际列出了工作或没有
你可以看到它正在列出工作
你可以将这些工作与使用aws glue控制台我们拥有的任何工作进行比较
让我去这里访问glue控制台
我将去这里找工作
你可以看到有三份工作
这些是用这个命令列出的工作
你还需要确保用户所属的组有适当的策略
这样他才能在命令行上运行这些命令
如果这些命令因为权限问题失败
你应该做的是
你可以去aws控制台
然后转到iam的部分
在iam中 你应该能够转到用户部分
在这种情况下 配置在itv github配置文件中的凭据
属于名为itv github的用户
我可以转到该用户
一旦我点击用户
我应该能够看到附加给用户的策略
这些是通过组附加给该用户的策略
我们感兴趣的策略就是aws glue服务角色
如果该策略没有通过组附加给用户
你需要确保该策略附加给组
如果你希望这样做
你只需点击这里的组链接
然后转到权限
说附加策略，例如glue
然后选择aws glue服务角色策略，将其附加给该组
这样用户就可以通过命令行使用命令行管理glue服务
然后你应该能够通过命令行运行适当的glue命令进行验证
一旦这样做了 你不仅能在命令行上运行glue命令
你应该能够使用auto three实际上使用python作为编程语言 并自动化这个过程
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/038_Udemy - Data Engineering using AWS Data Analytics part2 p38 6. Run Job using Bookmark.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们使用胶水工作书签运行任务
将启用胶水工作级别的书签
然后我们将实际处理运行它
你也可以在作业级别禁用
并且每当你运行它
你可以实际上启用并运行它
它将仅适用于正在运行的运行
在这种情况下，我们将直接在作业级别启用
让我转到aws胶水控制台
我们可以转到服务
然后aws胶水
然后我们可以实际转到作业
我们必须转到相关作业
在这种情况下，它只不过是一个github jason to parker
你应该能够说编辑作业
甚至你可以转到编辑脚本并从那里编辑作业
一旦你在这个弹出窗口中
你可以实际上滚动
你可以转到高级选项
启用作业书签
然后你可以现在保存书签
你应该能够选择这个
你可以转到脚本
然后点击编辑脚本，以便您可以进入脚本编辑器
你应该能够点击从这里运行作业
或者我们可以直接从那里运行作业
我也只想使用脚本编辑器
这就是我为什么进入脚本
不需要
您必须来到脚本编辑器
每当您想使用胶水控制台运行作业
现在您可以审查高级属性
您可以看到书签已启用
我们应该能够点击一个作业
实际上从landon复制数据到razon
现在作业运行已完成
我们应该能够转到aws cli
并且我们应该能够通过运行aws s three s three column slash
Slash itv hyphen github
Raw g activity recursive
然后配置文件 itv
github我按Enter
您可以在这里看到文件被复制
你也可以转到ethana控制台
你应该能够运行查询以查看计数是否为800万
三万九千八百二十九
让我们验证一下
在这种情况下，我正在对itvg的activity表运行
Hrdb 我们不需要刷新胶水目录
因为我们仍在处理只有三天的数据
您可以看到计数与在着陆区看到的计数相匹配
这意味着数据已成功复制
唯一不同的是这次我们启用了书签并运行了作业，而不是之前的一次
因为我们启用了作业书签
如果我们这次运行作业，因为新文件没有添加到源位置
作业将不会复制任何数据
一旦我们开始将文件复制到源位置
每当我们以启用书签的方式运行此作业
它将只捕获自上次以来添加的附加文件 它会处理数据并将其复制到目标位置
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/039_Udemy - Data Engineering using AWS Data Analytics part2 p39 8. Validate Bookmark using AWS CLI.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们更多地了解aws，以获取关于工作工作运行的信息，以及工作的当前书签
包括工作的当前书签
首先 我们列出工作
我们将获取我们aws环境中所有工作的详细信息
一旦我们获取所有工作
然后使用特定工作
我们应该能够获取工作的详细信息
它将包含与工作相关的所有元数据
你将理解 一旦我获得了关于github jason的详细信息，以便为给定的工作安排一个任务
可以有多次运行
如果你想以更高的层次获取所有运行的详细信息
那么你可以使用获取工作运行
它将实际给出每个提交的工作和ID
选择感兴趣的工作
你应该能够获取工作详细信息
这就是工作的样子
当涉及到书签时
收藏夹是工作级别
因此您可以传递工作名称
您应该能够获取该工作的收藏夹
您也可以重置工作标记
使用这个命令
将不再使用这个
但如果您想要重置
您可以使用这个命令重置
让我们看看这些命令的实际操作
我将使用我创建的基于docker的环境，该环境包含jupyter
我将创建一个新的笔记本
我将把笔记本命名为验证书签
使用命令行界面
让我缩小这个
我可以实际上将这个命令复制到这个单元格
然而，我们必须使用百分比百分比资产
它将实际使用shell魔法
然后我们应该能够运行这个命令
如果你想在终端中使用
你应该能够使用终端运行这个
你可以正确地复制这个
你应该能看到工作的详细信息
我们有三个工作
航班csv到parquet
Github jason到parquet和流测试
我们现在对这个感兴趣
如果你想了解这个特定工作的详细信息
你应该使用的命令是
在aws glue本身下获取工作
我们必须传递工作名称和附加信息以进行身份验证
获取工作详情
让我复制这个
然后让我粘贴到这里
在这种情况下，我们正在尝试获取这份工作的详细信息
这是终极的GitHub Jason到Parquet
现在我们应该能够运行这个
让我们在这里审查输出
我们得到了名称，角色，脚本位置，默认参数
这些参数包括临时目录，启用连续云观察日志，启用矩阵
以及I会Spark你的方式
这是主要的一个工作书签选项
它说工作书签启用，语言是Python
事件锁定路径是什么都没有这个一个
这会给出与工作相关的所有信息
如果你想要获取这份工作运行
你可以使用AWS Glue下的获取工作运行
你应该能够获取到所有工作及详细信息
让我们在这里粘贴命令并运行，看看它提供了所有哪些详细信息
你可以查看这个输出
它实际上返回了一个列表
列表包含所有工作运行
你可以看到与每个工作运行提供的详细信息
不仅提供了ID
但也提供了附加信息
例如尝试到工作名称，开始等
这是与几乎所有与这份工作相关的工作运行有关的情况
当涉及到这个输出时
它是按开始顺序降序排列的
所以最新的工作和详细信息会在顶部
你可以选择这份工作的ID
你可以实际上通过使用
获取工作运行
来获取与这份工作运行相关的详细信息
让我们复制这个然后粘贴到这里
然后替换工作ID
然后运行这个
你可以在这里看到输出
它给出了关于这份工作运行的详细信息
输出几乎与前一个命令相同
只有区别是前一个命令实际上返回了关于这份工作的所有详细信息
这个只写了最新的工作和详细信息
当涉及到一些属性时
你应该参考的主要属性是没有什么的工作状态
它应该是成功的，如果不是，那么你需要查看状态为什么那样
这有助于我们解决问题
如果你想要获取书签详细信息
那么你可以使用这个命令
你必须使用 获取工作书签器来获取书签详细信息
让我们复制这个并粘贴到这里
然后运行这个
它会给我们工作书签的详细信息
你可以看到工作名称，你可以在这里看到书签详细信息
这就是你应该如何获取工作书签详细信息的方式
这些信息将用于后续运行
这样，只使用自本次运行以来添加的文件
而不是从头开始运行
至于重置工作书签
你可以使用工作书签
你应该能够删除书签
一旦你提交工作后重置了工作书签
它会从源位置重新处理数据
我们当时拥有的所有数据
基于逻辑
它会尝试捕获所有数据并将其写入目标
这就是你应该能够达到的水平
这是aws cli获取工作书签详细信息的方式
我们不仅可以使用aws cli
我们还可以使用python auto three方法实际获取工作书签详细信息 如果你希望自动化一些围绕它的事情
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/040_Udemy - Data Engineering using AWS Data Analytics part2 p40 10. Add new data to landing.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


胶水工作书签的主要目的是什么，除了
以增量方式处理数据
数据被添加到源
每次我们运行该工作时
它应该只处理自上次无胶水工作书签运行以来添加的文件
我们需要程序化地注意这一点
作为我们逻辑的一部分，换句话说，为了模拟和验证
胶水工作书签是否按预期工作
我们需要向源位置添加额外数据
源位置就是这一个我可以复制这个
然后我可以进入终端
然后我应该能够运行aws s three
源位置，配置文件
itv github并按Enter
您可以看到与2021年相关的文件
1月13日和14日
以及15日
现在我想向此位置添加与2021年相关的文件
1月16日 为此，首先
我需要下载文件
然后我必须将文件上传到S3
我可以使用这个命令来下载文件
但是，要在运行此命令之前
我想创建一个文件夹并进入该文件夹
让我创建一个文件夹
下载活动
然后让我进入该文件夹
我现在在那个文件夹里
我应该能够运行此命令
以下载2021年1月16日的所有文件
它将为我们处理下载所有24个文件
根据您的网络速度
可能需要一些时间
您必须等到所有文件都下载完成
一旦文件下载完成
我们可以实际上复制此命令
然后粘贴到此处
实际上将文件从本地文件夹上传到S3
所有文件都已下载完成
我们应该能够粘贴此命令以将文件上传到S3
现在文件已上传到S3
根据我们的网络速度，上传可能需要一些时间
请耐心等待，直到所有文件都上传完成
然后您应该能够运行此命令以确保与2021年1月16日相关的所有文件都已复制到itv github存储桶的着陆区
您可以看到2021年1月16日的所有文件
早些时候
1月16日
早些时候
我们只有到2021年
现在是1月15日，我们应该能够运行带有书签启用的工作
你可以看到，只与2021年相关的被处理
1月16日被处理并复制到目标位置
让我们详细看看如何运行工作
然后我们会验证它是否以增量方式处理了数据 在增量模式下
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/041_Udemy - Data Engineering using AWS Data Analytics part2 p41 12. Rerun Glue Job using Bookmark.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们使用胶水作业书签重新运行胶水作业
这是我们到目前为止在探索胶水作业书签时遵循的步骤
首先我们确保在razone中清理了数据
然后我们在一月十三日十四日运行作业之前启用了书签以将数据复制到razone
以及从着陆到razone的十五
让我们运行这些命令来验证目标位置
以确保我们在这个位置有文件
然后我们会进一步
我在这里将使用aws cli
让我粘贴这个命令
我们可能需要添加配置文件
让我同时添加配置文件到命令
然后我会在这里再次运行这个
我只需要说减号
减号配置文件 itv
github 现在我可以复制这个
然后粘贴到这里
你应该能在目标位置看到文件夹
你可以实际使用递归运行这个命令
这样你就可以实际获取到文件
而不是只获取文件夹级别的详细信息
现在你应该能看到文件了
你可以看到 有与2021年约翰相关的文件
十三日十四日以及十五日
当涉及到验证书签时
我们可以使用这个命令
你可以复制这个
然后粘贴到cli
你应该能看到书签详细信息
作业名称不正确
我需要修复作业名称
然后我需要重新运行这个
作业名称就是这个让我复制这个然后替换这个名字
然后我应该能运行这个来验证
当运行作业时
你可以使用aws glue控制台
你可以选择这个作业然后说一个作业这是运行作业的一种方式
另一种方式你应该能直接使用aws cli
在aws glue下
有一个名为start job run的命令
使用start job run
你应该能传递作业名称和其他详细信息
你应该能从命令行开始运行作业
让我们复制这个然后粘贴到这里
因为它的作业名称失败了
我需要在这个中修复作业名称
作业名称就是github jason to parquet
让我复制这个名字
替换这里，然后让我复制这个名字
然后将其粘贴到这里
现在您可以看到工作ID
这意味着工作已成功提交
一旦工作成功提交
您可以使用AWS Blue控制台实际查看工作
或者您应该能够运行评论
如果您想使用Glue控制台
只需转到Glue控制台
确保刷新此页面
然后您可以实际选择工作
让我关闭这个
您可以在这里查看详细信息
工作仍在运行
如果您想使用命令行
您需要确保使用工作运行ID与
使用命令获取工作
我们应该能够获取该特定工作的详细信息
让我复制这个
让我粘贴在这里
这就是您应该能够获取工作详细信息的方式
一旦工作提交
工作仍在运行
我们必须等到它成功完成
然后我们将实际使用命令行进行验证 然后我们将实际使用Ethanna作为下一主题进行详细验证
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/042_Udemy - Data Engineering using AWS Data Analytics part2 p42 14. Validate Job Bookmark and Files for Incremental run.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为前一个主题的一部分
我们已经重新运行了胶水作业，以实际处理数据，直到2020年1月16日
使用作业书签
让我们验证以确保2021年1月16日的增量运行符合预期
我们预计会这样
我们将审查AWS中的胶水作业书签文件等详细信息
以及我们可以运行此命令以实际获取书签详细信息
您可以复制并粘贴此命令到命令行中
您应该能够看到有关书签的详细信息
这与我们昨天看到的不同
您可以看到有关作为处理一部分的文件详细信息
然后您应该能够运行此命令以实际查看目标位置中的文件
在这种情况下，您将看到文件夹
您可以看到有一个名为2021年1月16日的文件夹
此外，您可以运行此命令以实际获取有关这些文件的详细信息
您应该看到与2021年相关的文件之间的时间差异
约翰13 14 15和16为16
我们应该看到其他人的最新时间
我们应该看到与同一时间相关的时间
如果您查看与2020年相关的文件
2020年1月16日 您可以看到所有这些文件实际上都在2021年2月15日午夜复制
您可以在这里查看详细信息
所有这些文件都与2021年相关
2020年1月16日
这些是与2022年1月15日相关的文件
14
以及13 如果您查看时间戳
大约在晚上11点35分
而对于16日，则在午夜之后
这意味着作为增量运行的一部分
仅处理和复制与2021年相关的文件到目标
2021年1月16日
这是如何验证并获取有关书签的详细信息
这是我们应该能够验证的
以及获取有关书签的详细信息
因为我们已经进行了初步验证
现在是时候进行详细验证了
为此 我们可能需要召回表格
以便新部分可供我们查询
然后，我们需要使用ethanol运行查询
让我们详细查看爬取表格的详细信息 然后，我们将查看使用ethernet运行查询的详细信息以完成验证
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/043_Udemy - Data Engineering using AWS Data Analytics part2 p43 16. Recrawl the Glue Catalog Table using CLI.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止，我们已经重新做了2021年一月十六日的增量处理工作
我们使用glue job bookmark重新做了2021年一月十六日的增量处理工作
我们还验证了文件是否已复制到目标
然而，让我们理解我们是否可以使用现有表来查询
然后转到适当的数据库
在这种情况下，数据库实际上就是它
H或db 您可以在这里看到表
该表实际上就是活动
如果你点击它并转到查看分区
您仍然看到只与第十三相关的分区
第十四 第十五 尽管第十六的数据已经处理并复制到目标位置
没有分区确保第十六部分和后来的第十六部分已添加
您必须刷新表格
以便作为Glue目录的一部分，相关的表格元数据得到刷新
或者 您可以使用Web控制台来刷新表格
你必须去爬虫那里
选择合适的爬虫
这只是一个活动
或者爬虫选择并说
运行爬虫 它会为你处理运行爬虫的工作
你可能需要为此自动化
你需要理解你如何实际上以编程方式爬取这个
使用命令行
在这种情况下，我将演示如何审查表格详情
使用命令行和爬取
这样元数据可以刷新，实际上获取表格的详细信息
你可以使用 获取表格
你必须传递数据库名称和表格名称
并且必须使用名称传递表格名称
你应该能够获取表格的详细信息
你也需要添加配置文件
配置文件什么也没有，只是github
并且我们必须添加一个区域，什么也没有，只是美国东部一
让我们为这些命令添加这些
这样它们就能按预期工作
我忘了之前添加那些，现在添加
我正在为这些命令添加配置文件以及区域
现在 让我们运行这个命令
它会处理关于表的详细信息
让我复制并粘贴为cli的一部分，让我们审查输出
这里给出了关于表的详细信息
表名就是jh activity
数据库名称无非就是itv
Hdb 您可以在这里看到列级别的详细信息
它提供了与表中所有列相关的所有详细信息
它还会实际说表格是部分的一部分
然而 它不提供与自己相关的详细信息
要获取有关部分的详细信息，您必须运行
这是获取分区的一个子命令，位于aws下
如果您运行此命令
你应该能看到零件的详细信息
目前我们有三个零件
我们应该能在这里看到关于这三个零件的详细信息
让我们等到这运行完成
然后我们会检查输出结果
命令已成功运行
你应该能看到详细信息
它也提供了列的详细信息
然而，你可以在这里看到与零件相关的值
这是关于2021年1月14日的
然后我们有关于2021年1月13日的部分和细节
然后是15日
似乎这些人并没有按照名字排序
这里不一定是这样，你可以看到2021年第一个人的名字是15
这就是你应该通过命令行获取人员详细信息的方式
然而 如果你想刷新表格以包括2021年
1月16日
你应该爬取表格以重新获取表格
你可以使用这个命令
它将处理遍历所有子文件夹
并为每个子文件夹添加部分
让我复制这个并粘贴到这里
现在实际上已经开始爬虫
实际上运行爬虫需要一些时间
并刷新部分
你可以在任何时候查看正在运行的爬虫的状态
无论是使用命令行还是通过使用控制台来使用命令行
你必须在aws glue中找到合适的子命令
你应该能够得到详细信息
如果你想使用胶水控制台进行审查
你应该来这里
点击爬虫
然后你应该能在这里看到详细信息
这是我们完成的爬虫
它已经完成
一旦完成，你应该能够转到表格
让我转到数据库
数据库就是itv g hrdb
然后表格
然后进行活动
然后查看人员
我们应该在这里看到新的人员
你可以在这里看到
你也可以通过这条命令来获取所有零件的详细信息
我们应该在这个列表中看到与16相关的零件
让我们滚动查看
这是与14相关的
这是与13相关的
这是与15相关的
这是与这相关的
这就是你应该如何利用命令行
来实际运行爬取并刷新零件
并且也审查零件，因为表格被召回
现在是时候使用ethernet运行查询了
以确保数据没有遗漏地复制 使用glue job bookmarks
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/044_Udemy - Data Engineering using AWS Data Analytics part2 p44 18. Run Athena Queries for Data Validation.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为探索新工作书签的一部分
首先我们将数据复制到2020年和2020年1月13日、14日和15日
然后，我们验证数据是否正确复制
然后，我们对2020年和1月16日的工作进行了重新处理
使用工作书签
我们发现与2020年和1月16日相关的数据已复制到目标
然而，我们没有进行详细的验证
以确保数据是否正确复制
为了进行详细的验证
使用ea运行查询是一个很好的做法
这样我们就可以比较和对比结果看起来如何
从数据角度来看
一切如预期
让我们通过ea web控制台来运行这些查询
这样我们就可以实际验证数据级别的细节
我正在使用aws控制台
搜索athena
点击这个进入athena查询编辑器
已经 我们之前已经运行了这些查询
因此，它们应该成为这些水龙头的一部分
让我检查它们是否属于这些水龙头
在这里你可以看到详细信息
当我们对2021年进行查询时
一月十三号 十四号和十五号
我们看到了八百万
三十 九千 八二九记录现在
如果我们运行这个 我们应该能看到比这些更多的
让我们运行这个，看看它是否能运行
你可以看到它实际上抱怨党派方案的不匹配
这是由于当我们实际使用glue目录添加分区时
Athena可能无法在没有任何问题的情况下选择它们
如果是这样的话 我们必须从athena中删除分区
然后我们必须用传统的hive命令重新创建表
然后我们必须进一步深入研究以获取更多细节
实际上你可以通过这份文件进行查看
它实际上提供了一些关于这个问题的详细信息
每当零件表格有更新时
这时athena可能无法自动获取
如果使用glue目录刷新表格
你可以在这里查看详情
这就是我们遇到的模式不匹配问题
如何避免这个问题
我们只需要删除这些零件
然后我们需要通过运行修复表命令来刷新它
让我们处理这个问题
删除这个人
我们需要找出应该删除哪个人
我们添加了与2021年相关的新分区
一月十六日 只需删除分区并运行此命令
称为修复表
让我们运行此命令
我正在使用ea web控制台
添加这个命令在最后
然后运行这个命令
它会处理删除分区
现在你应该能通过说显示分区来看到分区
你可以看到只有三个人
他们与第十三、第十四和第十五有关
现在刷新这个
我们必须使用这个传统高命令
现在我们必须使用这个传统高命令
来刷新这个
这就是mk修复表
然后粘贴表名
选择它运行查询
一旦查询成功运行
我们应该能够通过运行show partcommand来审查人员
现在我们应该能够看到与2021年相关的部件
现在十六号一月 以及我们将新的部件添加到ethanna
现在我们应该能够运行此查询以获取计数
包括2021年
总十六数据
现在查询已成功运行
让我关闭这个
你应该能在这里看到计数
计数是一千万到十三万四千
二十九对八百三十万
三十 九千八百二十九在添加二零二零和一般十六之前
你可以根据日期获取数据
通过运行此查询
这将实际给我们提供按日期添加的新存储库的数量
让我们运行这个并审查数量
我们应该能够看到与2021年1月16日相关的数据
以及输出部分的数据
现在你可以在这里看到详细信息
它包括2021年1月16日
这就是你应该能够使用ethanol来验证数据级别的方式
这就是你应该能够使用glue作业书签的方式
以实际增量方式处理数据
并且验证数据是否正确复制
我们已经在文件级别进行了验证
然后我们刷新了部分
然后我们在数据级别运行了查询 以确认2021年1月16日的数据已复制
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/045_Udemy - Data Engineering using AWS Data Analytics part2 p45 1. Planning of EMR Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为这个主题的一部分
让我们来规划emr集群
emr集群的一个常见用例是设置用于数据工程
以及b或ad hoc查询
当我们实际为数据工程和b ad hoc查询设置集群时
我们将使用调度工具如airflow来安排和运行数据工程作业
这是用于处理数据的一个用例，使用相同的集群
这将最终用于报告
因为我们也将使用此集群进行报告
我们将通过工具如tableau click
视图 power bi 等
使用sparse gdbc连接器进行报告
我们还可以使用spark sql运行网络查询，利用databricks环境
我们还可以使用jule环境运行与数据科学相关的作业
因为我们将用于数据工程和数据分析目的
我们需要确定在集群上需要设置的mr配置
以确保成本得到控制
我们需要利用自动扩展功能
当我们需要资源时
集群应扩展
当我们不需要资源时
集群应缩放
我们需要将此作为设置emr集群时的一个标准
如我们简要理解了如何规划emr集群的详细信息
现在让我们设置emr集群
如我们简要理解了如何规划emr集群的详细信息
现在让我们设置emr集群 然后我们将实际了解如何用于不同目的
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/046_Udemy - Data Engineering using AWS Data Analytics part2 p46 2. Create EC2 Key Pair.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为这个主题的一部分 让我们详细讨论如何设置易于维护的这里
我们可以利用易于维护的它来连接到易于两个实例
这些是proby作为直接
当我们使用emr并部署emr集群时，还有几种其他服务
我们可能想直接连接到节点
即使在那些情况下
易于维护的配对很有用
让我们详细讨论创建一个新的
易于维护的它 作为这个主题的一部分
我们可以去aws amazon com
我们需要有一个活跃的aws账户
一旦我们在aws amazon com
我们可以实际登录到aws管理控制台
Keeper是与e
C Two服务相关的组件 因此我们必须去易于服务
如果您没有看到e two作为服务之一
您可以在here下搜索它
然后您可以实际去e
C Two仪表板，单击此
作为e c two仪表板的一部分，左侧
您有网络和安全，在其中您有密钥对
您可以单击密钥对
然后您可以为我们创建一个新的密钥对
让我实际创建一个密钥对
我将其命名为itv
现场演示
这里有一些与这相似的名称
这是我将要使用的一个
我们应该能够滚动并查看
我们可以实际说创建Keeper
它将为我们创建Keeper
如果您观察当我实际创建此密钥时
它已为我们下载了文件
您可以实际扩大此内容
您应该能够看到此文件
它只是itv live demos pam
我们应该能够将其放在一个特定位置
以便我们可以使用它连接到我们的
E c Two实例，使用此私钥
我们将使用此密钥对
在部署服务如emr集群时
当时 我们必须选择一个特定的Keeper
我们选择的任何密钥对
相应的公钥将复制到这些节点上
私钥已经存在于我们的PC或Mac上
我们需要指定这一点
创建一个用于连接到easy to instance的密钥对
这些实例是否与yr等服务相关联
你将在后续了解这些细节
稍后再处理 目前我们已创建了keeper
我们已经下载了pem文件
我们需要将其放置在适当的位置并赋予适当的权限
这样我们以后可以用它
我现在在这里说清楚
如果我去我的终端
应该能够通过说 on ltl 验证文件的存在
波浪线斜杠下载
然后是 itv live demos m
我们可以在下载文件夹中看到文件
我们应该能够将此文件移动到标准位置
标准位置就是波浪线点 ssh
这无非就是我家目录中一个隐藏文件夹
现在让我指定文件的路径
它只是下载
然后是itv live demos dot pem
目标位置只是tilde slash dot ssh
现在itv live demos m被复制到tilde slash dot ssh
我们可以通过说siphon ltl tilde slash dot ssh来验证
itv live demos dot m
我们可以查看此文件的属性
文件属于itv
该组中的成员是staff
其他人也有读取权限
我们不应该对这个文件有这些权限
无论是小组成员
还是工作人员，也包括其他人
因此，我们需要从这个文件中移除这些读取权限
在Linux或Mac上，我们可以通过运行chmod命令来实现
在这种情况下 我可以说ch mod六零零
这样我们就通过说零零为其他人和小组来禁用文件的读取权限
现在我可以说tilde /.ssh itv live demos. pem
现在我们可以说一个siphon ltr
Tilde slash dot h itv live demos dot m
您可以在这里看到pem文件，其中只有honor有读写权限
我们应该能够使用这个itv live demos keeper
每当我们创建e c two实例
或者每当我们创建eml集群
一旦使用此keeper创建了easy two实例
我们应该能够使用此pem文件连接到这些节点
只要节点使用an itv live demos创建
所以keeper会查看详细信息
所以我们在创建easy two实例时
或者在启动emr集群时
让我们等到那一刻 我们将看到这是如何发挥作用的
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/047_Udemy - Data Engineering using AWS Data Analytics part2 p47 3. Setup EMR Cluster with Spark.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为这个主题的一部分
我们将使用高级选项设置一个emr集群
我们也可以使用快速选项
但是我们将在稍后的时间讨论快速选项，目前
因为我们想要设置一个集群
以及数据工程将使用高级选项设置emr集群
该集群将配置有自动扩展功能
并且将用最小配置始终运行
让我们深入细节
所以我打开并切换到该标签
并且让我移动这个到屏幕的末尾
让我转到aws.amazon.com
我正在进入aws控制台
让我登录这里
我需要输入用户名和密码
以及也输入密码
让我处理这些事情，现在我需要输入密码
这将处理我登录到aws控制台的事情
一旦我们在aws管理控制台中
我们可以实际上在这里搜索emr
并且然后点击这个
这将带我们到emr控制台
一旦我们在emr控制台中
我们应该能够创建集群
让我们深入细节
一旦仪表板显示
现在仪表板已显示
我们应该能够点击创建集群以开始创建集群
您可以看到，默认情况下它将我们带到快速选项
然而，我们将使用高级选项
让我实际上点击高级选项这里
让我选择最新版本
那就是emr 6.3，并且我不想选择这个
我想要spark
我想要yarn
我不想要pig
我只想要hadoop和spark在这个上
这就是我为什么选择这两个
我们还将使用glue目录对于spark元数据表
而不是使用high目录出于这个原因
我选择了这个
我将使用单个主节点
这就是我为什么没有选择这个一旦
我选择了ah
hadoop spark 并且使用aws glue目录对于spark表元数据
我们可以实际上点击下一步现在
您可以做的是
您可以在这里选择统一的实例组
还有实例舰队的概念
我们将在稍后的时间讨论这些事情，目前
我们将使用统一的实例组
现在让我们向下滚动并审查网络设置
谈到网络时
我将使用默认的VPC
如果您想使用其他VPC
您可以从这里选择VPC
一旦您选择了特定的VPC
我所使用的所有子网都会在默认的VPC中显示
我将使用us-c-to-one-year子网
一旦网络配置完成
然后我们需要审查集群节点和实例，并根据集群节点的要求进行必要的更改
实例将使用默认设置
一个主节点
两个核心节点和零任务
这三者之间有什么区别
实际上将在稍后的时间详细说明
让我们使用这个配置
一个主节点 两个核心节点和零任务
现在让我们向下滚动并启用集群缩放
谈到集群缩放时
有两种类型 一种是EMR管理缩放
第二种 一种是自定义自动缩放策略
在这种情况下我们将使用EMR魔法缩放
如果您向下滚动 您应该能够在这里配置核心任务单元
这不是关于主节点
它只是关于这里的核心任务单元
根据我的计划
我将使用2作为最小值
这是上限
谈到按需限制时
将使用2个最大核心节点
将使用2个
其余的节点将是任务节点
这意味着那些节点将不会用于在HDFS中存储文件
并且除了2个节点之外
其余的将是spot实例
AWS有一个spot实例的概念
您应该了解这一点
我将稍后介绍spot实例
现在让我们专注于配置，最小值为8，最大值为8
2个按需限制和2个最大核心节点
其余的6个节点将是任务节点
并且他们将是
每当我们需要它们时
这意味着当集群变得非常繁忙时
如果您认为数据处理需要更多的节点
这将添加更多的节点
否则，它将通过终止不必要的实例来缩小规模
这就是我们如何控制成本的
话虽如此，现在
我们可以滚动浏览 谈到根设备
EBS卷的大小 如果你想要改变
你可以改变 我现在将使用10
这意味着根设备10GB
对于集群中的每个节点
现在我们点击下一步
我们可以滚动查看并更改集群的名称
在这种情况下，我给它命名为itv
来自github emmr
当我们启动集群时
当我们提交任务时
将为我们的aws mr应用程序生成锁定
我们希望锁定这些内容，以便即使集群下线也不会受到影响
我们可以审查日志并加倍处理问题
出于这个原因 我们需要启用日志记录
我们可以指定一个特定的位置
在这个情况下，我正在使用与这个集群相关的锁的默认位置
如果你想要启用调试
你可以启用这个
如果你想要启用终止生产
你可以在这里启用终止生产
终止保护将确保没有人意外终止集群
话虽如此，现在我们可以滚动浏览
然后点击下一步
然后点击创建集群
然而，我们需要确保我们选择了合适的ah以保持其这里
这样集群将以特定的key per创建，其中私钥位于mission上
公钥将复制到集群的主节点上
然后您将能够使用适当的ah密钥连接到主节点
作为前一个主题的一部分
所以keeper将在这种情况下使用它
所以让我来选择v live演示
然后我们可以说创建集群
因为我们在我们的任务上有相应的私钥
公钥将被复制到集群的主节点上
将能够使用
与这个键盘相关的私钥来连接到集群的主节点上
一旦集群运行起来，那就是说
我们已经成功启动了集群创建过程
让我们等到集群启动 然后我们将实际继续
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/048_Udemy - Data Engineering using AWS Data Analytics part2 p48 4. Understanding Summary of AWS EMR Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为集群创建后
让我们审查与该集群相关的所有小细节
我们从概述开始
我们有其他标签页
同样 我们可以查看这些详细信息并了解内部情况
让我们从概述开始
我们了解集群的想法
这在自动化方面非常有用
如果您想停止集群
增加对集群的信心等
它还告诉我们集群是什么时候创建的
到目前为止
已经过去了两分钟
在上一步完成后
它会等待 你也可以配置以这种方式完成所有步骤后终止
如果您实际以步骤执行方式创建集群
我们将在后一点时间理解那些细节
然后是终止生产
这主要是为了防止人们意外终止集群
在这种情况下 如果您尝试终止
它会说此集群有终止生产
您必须关闭终止生产以继续
即使在命令行
您不会意外终止
当终止保护开启时
我们必须明确
终止生产
然后我们才能终止此集群
我们将在后一点时间回来
一个重要的方面是主公共dns
我们可以利用这个，我们应该能够连接到这个
我将在后一点时间演示如何连接到此主节点，目前
请记住这是一个非常重要的信息
这将使我们能够连接到集群的主节点的配置详细信息
您可以看到emr版本
目前我们正在运行emr版本6.3.0
或包含hadoop分发和spark
hadoop版本为3.2.1
这是亚马逊版本的hadoop
当谈到spark时
它是spark 3.1.1
然后是锁ui
这是我们emr集群相关的锁将被保存
我们没有使用emfs
因此它说emfs一致性视图已禁用
您还可以获取有关应用程序用户界面的详细信息
根据服务
这些设置在集群上
你可能能够连接到与emr相关的不同应用程序用户界面
所有这些详细信息都会在这里显示
一旦集群完全运行
然后是网络和硬件详细信息
我们设置了一个主节点和两个核心节点
有了emr的魔法扩展
所以这给我们提供了网络和硬件的详细信息
目前可用性仅限于此子网内的一年内
它与我的默认vpc相关联
然后是安全和访问
密钥名无非就是itv live demos
这将给您一个关于您应该使用哪个密钥的想法
以连接到集群的主节点
这也将实际上给我们提供关于easy to实例配置的详细信息
是的 角色和如此
这些实际上将控制对集群的权限
您还可以通过这些详细信息获取与主节点和核心相关的安全组详细信息
如果您在访问主节点时遇到困难
如果您需要打开端口
您需要单击安全组并打开端口
您应该能够访问在主节点上运行的服务
了解安全组的相关性对你来说非常重要
这是正在创建的集群的简要总结
目前它正在运行
它正在更改为运行 这意味着集群已启动并运行
这些信息非常有用
当我们想要与集群交互时，我们会查看那些详细信息 随着我们进一步前进
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/049_Udemy - Data Engineering using AWS Data Analytics part2 p49 5. Review EMR Cluster Application User Interfaces.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在让我们理解与yr集群相关的应用程序用户界面
如果你记得 我们除了Hadoop和Spark创建了这个集群
我们没有使用任何其他组件
我们使用了Glue目录来管理Spark的表存储
我们没有使用任何位于顶部的服务
如果你记得
你可以实际上转到集群
你应该能够点击创建集群
然后你可以转到高级选项
一旦我们选择了版本
我们只取消了
嗨你 你是猪
我们确保只选择了Hadoop和Spark
我们使用了emr六点零来运行ea六点三
我们只选择了Hadoop和Spark
其余的都被取消了
因为我们的集群只有Hadoop和Spark
如果我们回到这个已经运行起来的集群
并点击应用程序用户界面
我们看到只有yeah
和Timeline服务器和Spark历史服务器
这些都是与Hadoop和Spark相关的服务
你也可以访问hdss名称节点和资源管理器
以及Spark历史服务器
只要这些端口是打开的
目前 这样的隧道没有启用
有两种方式可以访问这些东西
一去安全组
打开这些端口以启用这样的隧道
并尝试使用主节点上的隧道访问这些东西
我们还有核心以及任务节点
核心以及任务节点要么运行数据节点或节点管理器
或者两者 如果你想要访问特定ah任务核心的数据节点或节点管理器
你应该能够识别出它的公共DNS
并且你应该能够利用端口号
我们通常不连接到ora任务节点
我们通常只连接到这些服务
这些服务通常配置在集群的主节点上
只要这些端口是打开的
或者这样的隧道被配置
我们应该能够访问这些东西
我们将在后续时间点看到如何访问这些东西
你也可以在这里获得一个高级应用程序历史
一旦我们开始提交作业
我们将回到这里
一旦我们在后续时间点开始提交作业
目前 正在应用用户界面
您可以查看在此emr集群下创建的用户界面详情 根据我们设置集群时所选择的服务
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/050_Udemy - Data Engineering using AWS Data Analytics part2 p50 6. Review EMR Cluster Monitoring07 Review EMR Cluster Monitoring.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止，我们已经通过itv创建了一个集群
GitHub MMR 并且我们也已经详细讨论了摘要以及应用程序用户界面
现在让我们回顾一下监控的部分
在监控方面
你可以在集群级别、节点级别或IO级别进行监控
截至现在，我们在设置这个集群后还没有做任何工作。
那就是为什么我们在这些信息中看不到很多。
啊，是这样啊
不在集群状态中
状态不是或不是 不在io
一旦我们开始运行这些工作
你将在这份报告和仪表板中看到更多的信息
我们将在以后的某个时间点回到这个问题上
我们将在提交这些集群任务时进行审查
话虽如此 你可以将图表大小设置为大、中或闪亮
实际上你可以改变这一点
你可以看到它会变成什么样子，我更喜欢大的
所以我会使用大的
我们也可以改变这个到不同的颗粒度
要么 你可以在分钟级别或小时级别检查它
或者这个级别
你可以实际配置它是何时开始和何时结束
我们可以实际上获取报告的时间点
例如 在这种情况下我可以说两小时前开始
然后一小时前结束
然后我可以说提交
这里应该是1
然后我可以说提交
我应该能看到在两小时前和一小时前的细节
从现在开始，这些都是你可以用来调试问题的一些东西
当你遇到问题时
所以集群一旦启动并运行，任务一旦提交
目前这个集群没有动作
这就是为什么我们在这里看不到任何东西
然而一旦我们开始运行任务
如果我们回到这个
如果我们尝试查看细节 我们将开始获得更多的集群洞察
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/051_Udemy - Data Engineering using AWS Data Analytics part2 p51 7. Review EMR Cluster Hardware and Cluster Scaling Policy.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为这个主题的一部分 让我们回顾一下我们的emr集群的硬件部分
你可以通过点击这里转到硬件
你应该能在这里获取到集群的所有硬件详情
目前我们有一个主节点和两个核心
你可以实际看到主节点的配置
它有四个 我们称之为16gb内存
它拥有仅EBS存储
我们在这个服务器或实例上有64gb存储
当涉及到核心时
配置是相同的
你也可以在这里审查emr集群扩展策略详情
你可以看到emr集群扩展策略是emr管理扩展
在配置方面
我们有两个最小值，八个最大值，on demand限制的最大值不是两
如果你想要更改
你可以点击编辑 你应该能够更改属性
你也能更改你想要定义集群扩展策略的方式
目前我们正在使用是的
Emr魔法扩展
如果你想使用自定义自动扩展策略
你可以选择这个
你可以根据需要传递所需的值 目前我们将取消
这就是你应该能够审查硬件的方式
你也可以根据需要自定义
因为我们已经理解了如何审查硬件
现在让我们深入了解菜单栏中的其他选项 关于其他选项的细节
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/052_Udemy - Data Engineering using AWS Data Analytics part2 p52 8. Review EMR Cluster Configurations.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为这个主题的一部分 让我们回顾一下集群的配置以查看集群的配置
您可以点击这里 在这里，您将只能看到那些已启用或禁用的配置
在创建集群时
所有未覆盖的默认值将不会显示在这里
您还可以查看所有被覆盖的属性
在创建集群时以json格式
通过点击这里查看json
您应该能够看到所有被覆盖的属性
它显示这个属性是因为在创建集群时
我们选择了使用Glue数据目录作为Spark元数据存储表
在这种情况下 如果我在这里然后如果我点击这里
如果我去创建集群这里
如果我去高级选项
如果我选择emr六点零或此列表中
让我转到六点零o
让我取消选择这些
然后到Spark
然后我们在创建集群时选择了这个，因为这一选择
当涉及到配置时
我们看到了传统的属性，除此之外
我们还可以根据应用程序的需求写几个属性
或者通过在这里输入配置和值
或者我们也可以从s3加载json
它包含您希望在运行时添加的所有属性
无论哪种方式，当你实际覆盖属性时
如果您想要查看那些配置
一旦集群创建
在这种情况下 您必须转到配置当涉及到集群时
嗯 仪表板
所以在这个集群仪表板中，您可以在这里转到配置
您应该能够看到所有在创建集群时被覆盖的属性
目前只有一个属性
但在实际生产实现中
我们可能会添加几个属性
为了解决与应用程序相关的一些问题
我们可能会回顾在创建集群时运行时覆盖的属性
因此您应该熟悉这一点
您应该通过审查这些属性来调试集群的异常行为
从故障排除的角度来看
这对emr集群非常重要
从故障排除的角度来看 这个配置部分非常重要
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/053_Udemy - Data Engineering using AWS Data Analytics part2 p53 9. Review EMR Cluster Events.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止，我们已经创建了集群
并且我们已经回顾了应用程序用户界面
监控硬件和配置
现在让我们详细讨论事件
事件无非是
自集群启动以来触发的事情
这意味着自从我们启动集群创建以来
执行的所有任务都将被视为集群中的事件
如果你记得当我们实际创建集群时
让我再次尝试创建集群
当我们实际创建集群时
我们使用了高级选项
我们选择了emr6.0
或者让我选择emr6.0
或者我们只选择了hadoop和spark
然后我们选择了less glute catalog for spark table metadata，通过选择这个
然后我们点击了next
我们没有启用任何步骤
我们只是回顾了这个
我们启用了集群缩放这里
我们设置了最大值8，按需限制2
最大核心不设限
并且我们选择了is to 1 year这里
然后点击了next
当来到这一步时 我们输入了名称这里
并且我们确保了调试功能已启用
这是你需要记住的重要事项之一
如果你回到集群
让我实际上回到eml集群
仪表板
嗯 我已经点击了itv
github 嗯
emr集群
如果你去事件
你可以从底部查看事件
第一个事件无非是集群创建的启动
然后它执行了所有所需步骤
你可以看到它执行了一个步骤
与设置hadoop调试相关
因为我们已启用了
hadoop调试 它执行了一个步骤
它需要为集群设置一些东西
这就是为什么这个步骤在设置emr集群时运行
我们也可以添加额外的步骤
但是我们没有配置任何额外的步骤，为此原因
在设置hadoop调试后
它已经进入了一个事件
这实际上完成了所有未完成的步骤
所以这种情况下它已经记录了一个事件
表示已完成运行所有未完成步骤
然后如果您执行任何任务
例如重设集群
所有这些都将被记录为像这样的事件
这就是您如何解释事件
这将帮助您解决与重新调整或对您集群进行任何更改相关的一些问题 或任何更改
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/054_Udemy - Data Engineering using AWS Data Analytics part2 p54 10. Review EMR Cluster Steps.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


emr集群的一个常见用例无非就是步骤执行
你可以将一个高负载或spark作业配置为步骤
它将负责执行该步骤
你可以在步骤执行后终止
或者你可以继续等待
步骤执行是emr集群中一个非常重要的功能
如果你想查看关于包含在集群中步骤的详细信息
你可以来这里查看步骤
你应该能在这里查看所有步骤
尽管我们在创建集群时没有配置任何步骤
在我们接近集群创建过程的末尾时，我们已启用hadoop调试
啊 集群创建过程
它将在内部执行一个步骤
如果你执行了其他步骤
所有这些都会在这里显示
你可以使用步骤部分查看这些
你也可以向现有集群添加步骤
在这种情况下，我可以说添加步骤
我可以输入自定义java文件或流处理程序或spark应用程序
如果你还有hive
你也可以配置hive脚本步骤
此外 由于我们的集群只有hadoop和spark
我们只能看到ah
与spark相关的应用程序
在这里 我们可以选择步骤类型
然后输入名称
然后输入jar文件或zip文件的位置
然后可以在这里输入运行时参数
然后可以配置
在失败时将采取什么行动
它可以继续 或者它可以取消并等待
或者它可以终止集群
这些都是可以为步骤配置的动作
我们将在后续时间详细讨论冲突的spark作业
作为aws cr步骤的一部分
我将在后续时间详细审查所有这些
目前，请不要对太多担心
记住，你可以通过来到这里查看已执行的步骤 如果你需要，也可以向运行中的集群添加步骤
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/055_Udemy - Data Engineering using AWS Data Analytics part2 p55 11. Review EMR Cluster Bootstrap Actions.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为这个主题的一部分
我将快速回顾启动操作
在安装预部署服务到集群时，启动操作非常重要
如果你回到创建集群的过程
通过点击集群
然后点击创建集群
一旦你进入高级选项
一旦你选择了这里发布的版本
然后选择适当的服务
在这个案例中是hadoop和spark
一旦你选择了用于Spark表元数据的粘合数据目录
如果你向下滚动
这里有步骤 如果你想配置步骤
你可以配置这些步骤
如果你想阅读任何配置
你可以在这里找到
然而，我们不会处理这些事情
因为我们现在正在讨论启动操作
现在点击下一步，然后你向下滚动
这里并没有关于自举的内容
现在我们点击下一步
你可以看到有一个自举操作部分
你可以选择一个自举操作
你可以说运行
你应该能够配置并添加
你必须在s三中传递一个shell脚本
它会负责运行它
你也可以在这里进行自定义操作
你也必须在s三中传递脚本位置
你也可以向它传递运行时增强
这就是你实际执行引导配置的方式
引导配置无非就是安装某些预置的依赖项，这些依赖项是我们应用运行所需的
以便在所有集群节点上运行
我们将详细讨论confersomething的细节作为引导步骤的一部分
目前 我只想强调引导配置是如何通常配置的
如果你想知道这些引导操作的状态
可以使用yammer集群仪表板
你只需访问yammer仪表板
然后点击集群，然后转到启动动作
你应该能看到所有与配置的启动动作相关的详细信息
在创建集群时
稍后我们将详细演示
我只是想强调我们如何审查启动动作的详细信息 这就是你应该能审查的方式
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/056_Udemy - Data Engineering using AWS Data Analytics part2 p56 12. Connecting to EMR Master Node using SSH.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们理解如何通过ssh将其连接到主节点
这是公共dns，我们可以利用它通过ssh连接到主节点
我们可以复制这个
然后我们可以首先回到终端
我们可以像这样运行telnet命令，如telnet主节点的公共dns和22
只要我们能够在主节点上监听22号端口
公共dns 我们应该能够使用h来连接到主节点，然后退出
我必须使用控制关闭方括号
然后说退出
我现在已经退出了telnet
这是我们应该使用的ssh命令，我的个人密钥文件名就是nothing but
在我的家目录中添加.ssh
然后现场直播
点 m m 我必须使用这个私钥来明确地使用这个私钥。
我必须使用h减去i的波浪线
点 ssh
点 pm 然后 我不得不使用适当的 ah 用户名 在红公共
啊，为了获取正确的名称，你可以实际上点击
连接到主节点
使用se
它将实际上给我们提供关于用户名详细信息
用户名只不过是hadoop
因此，在这种情况下，我可以说hadoop at the rate
公共dns
这实际上就是这一个
然后我可以粘贴并第一次按回车
它可能会提示你说是或否
在这你必须说'是'
然后你现在可以按回车
你应该能够通过ssh进入主节点
这使用亚马逊linux运行
你可以说你的名字-here来获取这里的linux版本
你应该能够运行所有有效的linux命令
如何成为伪用户
因此你可以甚至说伪
然后你可以处理你想要作为超级用户执行的任何操作
如果你必须作为超级用户执行某些操作
让我来说伪根hyphen根
现在作为root用户切换我们
你可以在这个主节点上做任何你想做的事情
因为我们已成功连接到主节点
使用集合 将理解如何访问s三桶
如何验证python
以及如何访问你的生活
例如spark shell pi
Spark sql 等等 在本节后续主题中
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/057_Udemy - Data Engineering using AWS Data Analytics part2 p57 13. Disabling Termination Protection and Terminating the Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为这个主题的一部分
让我们了解如何终止正在运行的
是的 Emr集群
首先，您需要了解终止是否已启用
您可以实际看到终止已启用
因此无法直接终止
如果您点击终止这里
它实际上说此集群已启用终止保护
您必须关闭终止保护才能继续
有两种方法您可以直接点击这里更改
然后说关闭然后点击此
然后您应该能够关闭它
您将看到终止启用在这里并且您可以关闭它
您可以直接点击这里更改
说关闭
然后点击检查标记以保存
然后您应该能够点击以终止集群
终止保护主要是为了防止我们意外终止集群
您必须首先关闭终止保护
然后您才能终止集群
否则您将无法终止集群
即使您使用AWS CLI终止集群
如果终止已启用
然后您必须首先运行AWS CLI命令以关闭终止保护
然后您必须运行AWS CLI命令以终止集群
只要终止保护已启用，总是需要两个步骤
我再次 是的，终止保护主要是为了防止我们
意外终止集群
如果您使用集群进行I和数据工程应用
如果您终止集群
这可能会对应用程序产生重大影响
因此您需要小心终止集群
您不应该只是去终止集群 特别是如果集群在生产中支持应用程序
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/058_Udemy - Data Engineering using AWS Data Analytics part2 p58 14. Clone and Create New Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止，我们已经通过itv github emmr创建了一个集群
然后我们已经终止了它
在这期间，我们审查了所有作为这部分基于网络的集群仪表板一部分的部分
总结应用程序用户界面
监控硬件配置
事件步骤以及启动步骤操作
一旦我们审查了这些事情
然后我们禁用了终止
现在我们也终止了集群
使用终止集群或活动集群的集群配置
我们应该能够克隆并创建一个新的集群
克隆只不过是
以我们当前在github br的配置创建新的集群
终止的集群您可以在这里查看
并确认它已终止 itv github emr 集群在此处
您看到克隆选项 您可以单击此克隆
您应该能够以相同的配置克隆并创建新的集群
我们也可以转到集群
仪表板 选择合适的集群
然后点击克隆
这将带我们到这个页面
它询问我们是否想要包括步骤
在这种情况下 是的
我们希望这样做，所以我在这里保留了默认值
然后我可以取一个
它将处理将所有itv github集群的属性附加到新集群
这将终止到新集群
你可以向下滚动
你可以点击之前的
你应该能够审查一般选项
启动器操作等
对于终止的集群没有启动器操作
因此这里什么也看不到
然而 关于终止生产
它已禁用
因为我们通过禁用终止生产来终止了集群
我希望终止保护处于开启状态
因此我选择了这个
集群名称就是itv github emer
我想在这里使用相同的名称
因此我没有更改它
同样，日志位置
我们可以使用相同的位置
因此我现在没有更改这个
如果我点击之前的
你可以看到它已经选择了统一的实例组
这就是我们在创建它时选择的
如果你滚动到子网，它是在美国东部
一年 这就是我们在选择集群节点和实例时所选择的
当我们创建它时
是的 早些时候我们创建了emr集群
我们创建了一个主节点一个核心节点和零任务
这就是为什么你现在看不到任何任务
如果你滚动到集群缩放
我们已经启用了它 并且我们使用了emr管理缩放
当涉及到itv github emr时
这就是为什么这些选择甚至适用于这个新集群
当涉及到当前任务单位时
这些是我们以前输入的值，它们在这里反映
甚至evs卷大小与以前相同
如果你点击上一个
你可以实际在这里看到服务
我们选择了hadoop和spark
当我们创建集群时
这里选择了相同的服务
我们还想使用glue数据目录来存储spark表元数据
并且我们选择了这个
因此，即使在这里也选择了它
所以我们以前作为集群的一部分所做的选择
现在配置了这个新集群
我们应该能够点击下一步
然后创建集群
它将以相同的属性和配置创建集群
一旦集群创建
我们将看到如何使用ssh连接到主节点
为此我们需要使用itv live demos
密钥对 我已经在我的mac上下载了pem文件
因此，我将传递那个密钥对的路径
以连接到集群的主节点
让我们等到集群启动并运行 然后我们将实际看到如何使用ssh连接到集群的主节点
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/059_Udemy - Data Engineering using AWS Data Analytics part2 p59 15. Listing AWS S3 Buckets and Objects using AWS CLI on EMR Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止，我已经启动了mmr集群
我已经登录到集群的主节点
使用ssh
现在让我们了解如何从主节点的集群访问s3中的对象
使用aws cli从集群的主节点
到目前为止，我已经登录到集群的主节点
作为部分主节点
aws cli已经预安装和预配置
你可以实际上输入aws并按回车
你可以看到与aws相关的帮助
它也已经预先配置好了
这意味着你应该能够访问桶和对象
无需设置密钥或其他任何东西
原因是我们有一个与集群关联的角色
我们可以通过进入集群的总结来查看这个集群相关的角色
如果你滚动下来
你可以看到与eml集群相关的角色
我们有一个简单的默认角色
还有一个默认角色
让我们审查这些角色
那么我们将从eEC two默认角色开始
让我复制这个
让我说
我在这里
让我点击
一旦我来到这里
让我点击控制
让我粘贴角色名称在这里
我们可以点击这个角色
它将实际上将我们带入权限
如果你向下滚动
这个角色相关的策略
权限被分配到策略中
我们可以扩展这个
我们应该能够点击Jason来审查权限
将其附加到此策略
如果你审查权限
你会意识到默认情况下
Emer将能够管理AWS中的大部分服务
尤其是如果你向下滚动到S3
你可以看到这有一颗星星在这里
这意味着我们应该能够无任何限制地管理所有的s三桶以及对象
让我回到终端
让我们回到终端
这个终端与我们的emr集群的主节点相关联
我们已经安装了aws cli并且配置好了，这都是因为角色的原因
如果我说aws s three
我应该能够列出与我账户相关联的所有桶
我不想列出与我账户相关联的所有桶在这里
我只想查看名为as的文件桶中的文件
它在GitHub上使用短横线
如果你按Enter 它将实际列出此存储桶中的文件夹以及文件
我们有这些子文件夹
然后有一个名为testing的文件
让我们回顾一下在itv github下的landing文件夹中的文件或文件夹
我只需说aws s three l less
itv github
然后/slash/然后landing
我们需要有前进
斜杠在最后 这样我们就可以实际列出此位置中的文件夹以及文件
现在您可以看到有一个名为activity的文件夹
然后有一个名为a0的文件
现在 让我们实际运行aws s three命令在此文件夹上
以查看该文件夹中的内容
我们有2021年的文件
一月 第十三 第十四
第十五和第十六
这是早些时候复制的
这就是你应该能够从集群的主节点访问S3中的桶以及对象
使用AWS CLI
我们也可以使用hdfa cla列出S3中的桶以及对象
让我们进入下一主题的详细信息
关于这些细节 让我们进入这些细节作为下一主题的一部分
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/060_Udemy - Data Engineering using AWS Data Analytics part2 p60 16. Listing AWS S3 Buckets and Objects using HDFS CLI on EMR Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为前一个主题的一部分
我们看到了如何从EMR集群的主节点上使用AWS CLI访问S3桶以及其中的对象
作为这个主题的一部分
我们将了解如何使用Hadoop或HDFS Ali访问S3桶以及其中的对象
我们有两种方法实际上可以访问HDFS中的文件或S3对象使用Hadoop或HDL
它们是Hadoop FS或HDFS
Dfs 两者都只不过是彼此的别名
实际上可以说如何说s-
然后斜杠或甚至hdfs dfs s-
斜杠
两者实际上写成相同的输出
您可以在这里看到
这是how do的输出
这是his dfs的输出
Dfs有 你看到默认情况下这里显示相同的输出
它将实际显示来自hdfs的文件和文件夹
当我们没有指定任何uri时
这与这个相同
Hdfs dfs hyphen
然后是dfs列三斜杠
即使现在，我们也会看到相同的文件夹
这些文件夹实际上来自hdfs位置
如果你终止集群
这些文件夹中的任何内容都将消失
现在 我们也可以访问S3中的对象和桶
使用hdfs或hadoop
的方式你可以访问s3桶或对象
是说hdfs dfs
然后减号
你必须说明你在这里
然后冒号 然后正斜杠
正斜杠itv
Github
然后斜杠
这将实际给我们显示这个文件夹中的所有子文件夹
我们有子文件夹也有文件
我们应该在这里都能看到
这里有个拼写错误
这次我改正了拼写错误，我应该能在这里看到子文件夹
让我们检查一下landing文件夹下的子文件夹
我只是在说他的dfs dfs hyphen s three column斜杠
斜杠itv hyphen github
然后闪存 然后降落在这里也需要
你需要在最后使用正斜杠
否则你可能无法看到此位置的子文件夹或文件
让我们删除正斜杠并看看这里的输出会如何
如果我删除正斜杠
我将只能看到它到itv github landing
如果它是aws s three命令
当它来hdfs dfs hyphen
我错了
它实际上甚至列出了子文件夹
即使我们没有在这里指定正斜杠
所以当涉及到aws s three命令时，有一点不同
在这里我们有一个dfs hyphen命令
如果你想列出所有在activity下面的文件或文件夹
我只需要添加activity
我们应该能够访问此位置下的所有文件和文件夹
我们可以看到此位置的所有文件
我们有与2021年相关的文件
一月十三日至2021年一月
一月十六日 包括十四日和十五日在bein
这就是你应该能够访问s three桶的方式
以及使用hadoop或hdfs cli作为上一主题
我们已经看到了如何使用aws cli访问s three桶以及对象 在此 我们已经看到了如何使用hadoopl i访问相同的桶和对象
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/061_Udemy - Data Engineering using AWS Data Analytics part2 p61 17. Managing Files in AWS s3 using HDFS CLI on EMR Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为前两个主题的一部分
我们已经看到了如何使用aws cli来访问s three中的文件
除了hadoop或hdfs之外
我现在让我们理解如何在s三中管理文件
使用hdfs或hadoop
我们已经知道如何使用aws cli将文件复制到s3中
课程开始时
因此将不会使用aws cli来实际将新文件复制到s3中
将要用于将文件复制到三个位置的地方实际上什么也不是
这个有三个斜杠
划掉itv
连字符 github
然后着陆
然后活动已经
我们在这个地点有关于2021年1月13日、14日、15日和16日的文件
现在我们会在2021年下载新的文件
一月十七日在集群的主节点上
然后我们会使用他的dfs cli将文件复制到s three中
在下载之前
让我实际上创建一个名为活动的文件夹在这里
那么让我来进入那个文件夹
包含数据的网站就是jark dot org
让我来到jark dot org
现在 我应该能够在2021年1月17日下载文件
通过使用这个命令作为参考
让我来复制这个
然后让我来回到与集群主节点相关的隧道
让我来粘贴
让我将日期更改为2021年1月17日
这里 现在应该是2021年
我按回车 稍后会将所有文件下载到2021年
一分钟后，1月17日
一旦下载完成
我们应该能够使用hdfs
从本地文件系统复制文件到hdfs
在这种情况下 指定的目标位置实际上就是一个有效的s三位置
这意味着文件将自动复制到目标位置
我们可以使用的命令就是hdfs
Dfs Hyphen从本地复制
或者我们也可以使用hyphen put
我想将所有文件从这个位置复制到
所以我实际上说的是星号
目标位置就是三个斜杠
斜杠itv
Hyphen github斜杠landing
然后进行活动
现在让我点击 进入本地文件系统中的文件，活动将被复制到目标位置
让我们检查文件是否已成功复制
我们将实际处理审查过程
一旦所有文件都已成功复制或失败
目前仍在进行中
我们必须等到它们实际被复制
现在 文件似乎已被复制
让我们检查他们是否成功复制
为此 我们只需要复制这个
然后说aws
S three
然后粘贴这个
我们应该能够在2021年1月17日看到文件
以及
我们也可以使用hdfs
Dfs hyphen
然后粘贴这个路径
我们应该能看到与2021年相关的文件
通常听起来也不错
这就是你应该在s3中管理文件的方式
使用hdfs或hadoop cli
如果你想删除文件
你也可以删除它们
你只需要说hdfs
rm
然后你只需要使用模式指定文件
在这种情况下，我想删除与2021年相关的文件
2021年1月17日 出于这个原因
我说三个列
斜杠斜杠 GitHub
登陆活动
2021年1月17日
如果我说星号并按回车
这将只删除与日期相关的文件
其他文件将保留到2021年
2021年1月13日、14日、15日和16日的文件将保持不变
我们可以通过运行这个来验证
Hdfs Dfs有一个在所有AWS S3命令的目标位置
我们应该能够看到2021年之后的文件
2021年1月13日、14日、15日和16日
这就是你应该在S3中管理文件的方式
即使使用Hadoop或HDFS
我 当你实际启动时
是的 一个集群
我们可以通过两种方式管理文件
使用aws 以及hdfi 当涉及到s三中的文件时
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/062_Udemy - Data Engineering using AWS Data Analytics part2 p62 1. Deploying Applications using AWS EMR - Introduction.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为系列讲座之前的部分章节模块
我们已经看到了有关设置pmr集群的所有细节
现在我们还没有看到如何在本节中部署应用程序
一系列讲座中 我们将看看如何实际在运行中的mr集群上部署应用程序
如何编辑到步骤
并且如何仅使用步骤创建集群
我们将仔细检查所有这些细节
从讲授系列Python基于应用的部署视角 在这个模块中
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/063_Udemy - Data Engineering using AWS Data Analytics part2 p63 2. Setup EMR Cluster to deploy applications.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为这个讲座的一部分
我将创建一个新的emr集群
使用iam用户
我不会使用root账户
我之所以不使用 root 账户的原因
是因为无法在emr集群上创建笔记本
那是使用根账户配置的
如果无论如何 如果你想在之上创建笔记本的话
啊，啊，是的 Emr集群
然后你需要创建Emr集群
使用IAM账户
而不是根账户
我已经有了IAM账户
如果你不确定如何创建IAM账户
我已经覆盖了那些细节
作为与AWS相关的一部分
你必须查看详细信息
确保你有一个具有根权限的IAM账户，然后你可以继续
我也会快速回顾我正在使用的现有iam用户
这样您就能理解在创建集群的过程中创建iam用户的过程
并在其上创建笔记本电脑以使其简单
我已经以管理员权限创建了iam用户
这样我们不需要太担心权限问题
稍后
让我点击分配到控制台
在这种情况下，我正在使用iam用户登录，而不是root用户
当涉及到登录和使用iam用户时
你必须输入账户
ID或账户 告诉我们
在我这种情况下，账户 teus 只不过是 v 我的用户名
我将要使用的只不过是 ea demo
我已经有一个密码
密码会自动在这里填充
让我点击登录
如果你想使用根账户登录
你可以点击这里 然而
在这种情况下，我正在使用我的身份验证账户登录
如果你去aws amazon.com
如果你点击登录AWS账户
或者如果你点击这里登录
在这种情况下，这是私人浏览器
因此它显示登录和创建AWS账户
现在没有现金交易
如果我点击登录，默认情况下
它将选择真实用户转到窗口
我之前已经展示过
你需要选择用户
然后你可以在这里输入账户ID并点击下一步
它会负责完成登录
在这个例子中，我是用户或账户
我已经自动选择了登录对
我输入了账户ID，用户名和密码
在这个例子中，我使用了账户ID的别名，而不是账户本身
如果你没有账户，
请确保你提供一个账户并继续
让我点击登录
我现在在aws web控制台中
让我首先审查权限
我需要这样做
然后我会继续
在这个例子中，审查iam用户的权限
我会转到iam仪表板作为iam仪表板的一部分
你可以转到用户在用户中
你可以搜索
是的
emr演示用户 这就是用户你可以点击这个
它会带你到这个用户的页面
你可以看到这有一个附加的政策
它是什么管理员访问
这将几乎给你的账户相关的所有权限
我是用户
这就是你应该能够授予权限的方式 为了简单起见
使用这些来处理所需的一切
我是用户，话说回来，在生产环境中
iam用户可能没有管理员访问权限
你将只获得你应该得到的权限
如果你打算使用这些iam用户创建集群
如果你没有写入权限
你必须与你的aws管理员团队联系
并在你的国家进行下一步
他们会确保他们给你所有的必要权限
现在，使用此账户创建集群，让我搜索emr
让我点击emr
在这里，我将点击创建集群
让我转到高级选项
让我选择laser
在这个例子中，我将选择最新的一个
那就是6.4.0
我从列表中选择
我想选择how do 3.2.1
我不想要你
我想要spark 3.1.2
我不想要猪
因此我已经取消选择
我想设置双企业网关
这就是我想在这个上设置的东西
当涉及到Spark表元数据时
我想启用AWS Glue数据目录
从这些东西上
我不想选择任何这些
让我点击下一步
当涉及到集群组成时
我想使用统一实例组
当涉及到网络时
我想留下默认设置
而不是使用us east one f
我将选择us east to one year
这是这里唯一与网络相关的更改
当涉及到主时
我想选择一个实例
当涉及到核心时
我想选择只有一个实例
我想把成本降到最低
这就是我为什么选择这里只有一个实例
当涉及到任务时
我不想使用任务实例
如果你想要 你可以实际上启用集群缩放
你可以点击这个
你应该能够选择核心任务
单位进行自动缩放
我实际上选择最小值为一
最大值为两个个on demand
限制为一最大核心节点是一个
这就是我想要在这种情况下启用自动缩放的方式
最多会达到两个
他们已经启用了自动终止
如果集群闲置很长时间
你可以在这里选择时间
以便集群可以自动终止
在这种情况下我将在集群闲置30分钟时终止
让我选择30分钟
现在我可以点击下一步
当涉及到集群名称时
我想命名为itv hyphen
github hyphen
让我检查现有的名称
然后我会继续
让我登录到amazon com一次
一旦我登录
我要去emr
集群名称像这样
itv github emr
让我使用相同的命名约定
让我称之为v github emmr
让我滚动一下
让我点击下一步
当谈到易于维护时
让我检查一下我是否在我的机器上有私钥
然后我会继续
让我实际上打开我的终端
我忘记了我现在有的账户
让我开一个新终端
现在我可以说一个连字符
ltl 波浪号
斜杠 点h然后回车
这些都是我在我机器上拥有的
现在我会使用这个
itv live demos
所以我让我选择v live demos
现在让我点击创建集群
它会为我们创建集群
我们需要等待集群设置完成
然后我们会继续
我们也会看到如何设置笔记本
这样我们就可以通过aws cr本身访问笔记本
我所说的本本只是
我们可以在这些运行的集群上创建笔记本
集群会处理它
一旦集群运行
让我们在这里暂停直到集群运行
然后我们会继续
我正在结束这个讲座
一旦它运行
我会验证我是否能够使用set连接到它 然后我会设置笔记本对抗这个集群
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/064_Udemy - Data Engineering using AWS Data Analytics part2 p64 3. Validate SSH Connectivity to Master node of AWS EMR Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为这次快速讲座的一部分，我将讲解如何登录到主节点
我将演示如何登录到主节点
使用一组命令 我还将验证我是否能够运行aws命令
为此，我只需复制公共DNS
然后转到终端
在终端中
我应该能够说h hyphen
我tilde slash dot ssh
然后私钥名称是这个itv
Live demos
Dot pam Then easy to hyphen user
Then at the rate
Then the public dns
然后可以按回车
它
可能会提示您输入a或no 如果提示您只需说yes
您应该能够登录到这个终端
目前
我正在集群的主节点上 我应该能够运行任何命令
我想运行
作为这个主节点
我应该能够启动一个spark shell
我应该能够启动pi spark
我应该能够启动spark sql
我应该能够使用spark submit提交作业
我可以运行aws命令来实际查看
如果我有三种文件
所以等等
让我们验证我是否有一个命令 因为我将使用很多来实际验证文件
所以每当我们运行作业时
为了那个原因
我想确保aws命令没有任何问题
我可以只是说aws s three
然后then i can say yes
Three column slash Slash itv
Hyphen retail
这是一个桶
我有作为part of my a second
如果我有访问s three
它将显示该位置中的文件
您可以在该位置看到文件和文件夹
所以有一个零售桶
有这些文件夹和文件
此外，我们还可以运行hdfs命令与集群的hdfs交互
我们可以说dfs
Dfs 短划线 然后斜杠
我们应该能够看到这些文件作为这个部分的一部分
如何集群
我们应该能够启动spark shell或spark submit或pi
Spark或spark sql
根据我们的需求，我们将在这些命令需要时运行它们
目前我们能够无问题登录到主节点
我们也能够运行诸如aws的命令
S three ls列出文件
和hdfs Dfs从hdfs列出文件
alien命令主要用于在aws s three bucket中列出文件
因为我们能够使用命令界面与集群交互
现在让我们详细讨论如何在这个集群上设置笔记本 然后我们将处理使用不同方法部署应用程序
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/065_Udemy - Data Engineering using AWS Data Analytics part2 p65 4. Setup Jupyter Notebook Environment on EMR Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


是的 我们已经成功启动了我们的emr集群
让我们继续在上面设置笔记本
AWS EML提供了一个称为笔记本的东西
你可以在这里看到 你应该能够在现有的emr集群上创建笔记本
或者你也可以在新集群创建过程中创建笔记本
在这种情况下 我将使用创建现有集群上的笔记本的方法，需要注意的是，集群应该由用户创建
请记住，集群应该由用户创建
不是root用户
也尝试仅使用IAM用户创建笔记本
如果你尝试使用root账户创建笔记本，可能会遇到一些问题，尽管这样说
一旦你登录到AWS控制台
使用该账户
你应该能够访问EMR中的部分笔记本
仪表板 点击创建笔记本
这将带你到这个页面
在这种情况下，你可以实际指定笔记本名称
我将为其命名为itv
然后nb 这是我的笔记本名称
将选择一个现有的集群
让我点击这个并选择一个已经运行的集群一旦选择
然后我们可以实际选择默认的安全组或现有安全组
无论对你来说多么方便
在这种情况下我将选择默认安全组
我们也将使用aws服务角色
那就是ea笔记本默认角色
当涉及到为这个笔记本保存文件的位置时
我将选择默认位置
如果你想选择现有位置
你应该能够选择这个 你应该能够在us east one中选择现有位置
然而，我将使用默认的s3位置
这个位置将用于放置我们的笔记本
这些笔记本是使用dupter基于的环境开发的，也就是说，在选择了大多数默认设置后
你应该能够点击创建笔记本
它将为您创建笔记本，一但笔记本创建完成
要么你可以在jupyter中打开，要么在jupyter lab中打开
在这种情况下 当我们提到笔记本时
它只不过是笔记本环境
所以我们已经在现有的emr集群上设置了笔记本环境
我们可以使用jupyter笔记本界面或jupyter lab界面打开
为了实际在这个笔记本环境中创建和管理笔记本
在原始的jupyter环境和jupyter lab环境中有一些细微的差异
随着我们进一步打开环境，你会理解的
使用jupyter和jupyter lab
当我们实际与它互动时
你将理解其中的差异
让我首先点击打开在jupyter中
嗯 你可以看到它打开了一个弹出窗口说
如果你启用了弹出窗口拦截器
为了这个域名禁用你的拦截器并再次尝试
让我实际上点击这个
让我首先关闭这个并让我点击这个
它会处理在标签中打开页面
你可以看到它正在使用笔记本界面打开
我们有在现有emr集群上的jupyter环境
这个jupyter笔记本环境正在使用原始jupyter界面打开
在加载过程中
我也会点击打开jupyter lab
让我回到这个然后让我点击在这里打开jupyter lab
你可以看到它正在使用jupyter lab界面打开界面
这是jupyter界面
这是jupyter lab界面
现在你可以看到jupyter lab界面没有出现问题
因此，双界面也不会有问题
当涉及到与笔记本交互的最原始方式时
当涉及到jupyter lab时
它只不过是在jupyter基础上的一层包装
我们将有一些高级功能
我们可以在左侧有一个侧边栏
这将使我们能够快速访问笔记本
与原始的jupyter环境相比
这是原始的jupyter环境
这是双lab环境
Dupta lab也在jupyter上运行
然而，当涉及到原始的jupyter环境时
它将看起来像这样
与jupyter lab相比，它不那么用户友好
至于jupyter lab
你可以在这里看到所有笔记本
你应该能够访问那些东西
你可以最小化这个
你可以扩大以快速访问笔记本
你可以进一步
我将尽可能多地使用Dupta lab环境
你可以继续使用基于jupyter的环境
如果你对探索感兴趣
只使用基于jupyter的环境
两者都将使用类似的环境提供笔记本
因此，你应该能够根据你的选择跟随
无论是基于jupyter的环境还是Dupta lab环境
话虽如此 让我关闭基于Dupta的环境
这里 我们已经有了笔记本
我们可以打开这个笔记本
它将是空的 里面不会有任何东西
它只会创建一个笔记本
当我们实际上设置环境时
当我们打开笔记本时
没有与之关联的核
你可以在这里看到 它说没有核
你可以从这些核中选择一个与emr集群交互
你可以选择y spark或spark
这实际上就是spark与scala spark r
或者你可以实际上选择python3
在这种情况下我会选择python3
让我点击 选择现在
我应该能说打印
然后hello
然后world
然后我应该能通过说shift和enter运行它
你可以看到我能看见hello
World这里 这意味着基于du的环境部署在我们emr集群上
我们应该能利用这一点
要么用于交互式开发笔记本
要么用于利用代码
以了解涉及在spark litapis中开发应用的api
当涉及到基于jupyter的环境在eml集群上
通常它们由数据科学家用于交互式开发与模型
以及由测试者和其他用户用于在我们的数据上运行一些ad hoc分析
我们通常不使用dupta作为正式应用开发的ide
与数据工程应用相关我们使用
例如pycharor
Spider或visual studio code来处理我们应用的开发
不是jupyter
当涉及到这个环境的使用
我们会尝试使用它来运行一些ad hoc分析
然后在运行我们的应用时需要
我们有基于tutor的环境准备好了我们使用
每当需要 当我们需要
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/066_Udemy - Data Engineering using AWS Data Analytics part2 p66 5. Create required AWS s3 Bucket.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


是的 这次我们将在aws mr上运行应用程序
让我们创建一个存储桶
以便我们有一个隔离的存储桶来源数据
以及持久化数据
在运行应用程序在aws mmr的过程中
当我们实际设置aws集群时
如果你在设置集群时不更改任何角色
默认选择的角色将获得对s three的完全访问权限
这意味着你应该能够列出存储桶
你应该能够将桶添加到账户中
你也应该能够从你的账户中删除桶，那就是说在生产环境中
在生产环境中将不会获得这样的权限
即使在开发环境中
当你涉及到账户时
你将不会获得这样的权限
你可能需要与aws行政团队合作
在你们组织内部处理获取创建桶所需的权限
让我创建一个桶，输入aws s three yb
yami的意思是创建桶
可以利用它来创建桶
在这种情况下，桶名将是itv 斜杠 github
斜杠 emr
我想在us east one地区创建这个桶，为此
我实际上说的是斜杠 斜杠 区域 us 斜杠 east one
我们不需要传递任何配置文件
因为集群的主节点将继承在aws上的权限
s3使用iam角色
我们不需要太担心添加额外的配置文件
让我按回车 它将会创建桶
然而 我们需要以s三ua的形式传递uri
为此目的
我只需像这样更新url
桶名应该是s三列
然后斜杠斜杠
然后桶名
记住，桶名在所有aws账户中都是唯一的
因此，你必须以某种独特的名称创建桶
你不能使用相同的名称
而我正在使用这里 它不能有v-开头的github
但可以有-开头的emr
你可以使用你的姓名首字母
你应该能够通过说你的姓名首字母来创建存储桶
-github emmr
如果这是常见的姓名首字母
那么可能会遇到冲突
你只需要让你的桶名尽可能的唯一
现在桶已经准备好了
我们应该能够通过说 aws s three 来进行验证
然后我可以说 grep
然后我可以搜索 github
我有多个包含 github 的桶
你可以在这里看到所有的桶
这就是你应该能够列出桶的方式
我们也可以获取给定桶内的文件夹和文件
通过说 aws s three airless
然后桶名就是 itv
连字符 github
连字符 emr
连字符 连字符 recursive
实际上会以递归的方式获取到所有文件
到目前为止，在这个桶内我没有任何文件夹或文件
这就是为什么没有写任何东西
是的 桶已成功创建
现在是时候通过创建适当的布局将数据上传到这个桶中 我们将在下一节课中详细讨论这些细节
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/067_Udemy - Data Engineering using AWS Data Analytics part2 p67 6. Upload GHActivity Data to s3.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


是的 作为在aws cml上运行应用程序的一部分
到目前为止，我们创建了一个存储桶
存储桶的名称是itv-hyphen-github
hyphen-emr 现在我们需要将数据上传到那个存储桶中
在上传数据到存储桶之前
我们需要将文件下载到本地
或者我们需要使用适当的库在python中
从api获取响应并直接上传到s3
我们将使用正式的方法
我们将使用w get命令将文件下载到本地文件系统
然后我们会将其上传到S3
使用aws s3 cp命令或hdfs dfs copyfromlocal命令
我将使用正式的
如果你想要 你可以使用后者，话虽如此
当涉及到网站时
与数据集相关的
它只是jk.org而已
我们已经看到在过去也是如此
你可以去这个页面 然后向下滚动
你应该能看到关于re的评论
如果你想为某一天获取活动
你必须使用这个命令
它将尝试在2015年1月1日下载文件
如果你使用这个命令
然而 我将在2021年1月13日下载文件
话说回来 让我复制这个
我先去终端这里
让我创建一个工作饮食
它只不过是在我的家目录下载
然后作为这个文件夹的一部分活动
我将首先下载文件
然后我将上传至s三
我必须使用破折号p
只有这样才能创建多级日记
让我输入p
现在日记已经创建
你可以通过说查找下载或斜杠下载来验证
你可以看到它有一个子文件夹
这些文件夹没有其他文件或文件夹
现在我应该能够进入下载
然后我们应该在那个文件夹中执行活动
我们应该能够说获取十个https
一旦你输入https
你应该能够说数据点g h档案点org
2021年1月13日
然后连字符
然后在花括号中
你可以说0.0.23闭合花括号这里
然后给出扩展名为json.gz
它会下载当天的所有24个文件
你可以在这里看到
它将只需花费一点时间
它将快速下载所有文件
因为我们正试图从集群的主节点上下载
主节点和我们有数据集的网络服务之间的互联网速度非常快
因此我们应该能够快速下载文件
你可以看到它只用了25秒就下载了所有文件
它下载了24个文件
文件的大小是1.4GB
现在
让我清除屏幕 添加的文件作为我们本地文件系统的一部分可用 现在是时候将这些文件上传到S3了
我应该能够使用称为aws的命令
S3
让我清除屏幕
然后反斜杠 然后cp从当前目录
我想将文件上传到S3
S3中的位置是itv
然后github
然后ea
这是我的桶名
然后指定文件夹的路径
文件应该上传到
即使文件夹不存在
它会自动为我们创建那些文件夹并上传数据
现在我会说prod
然后着陆
然后jactivity
确保在最后加上正斜杠
然后它才会以那个名字创建一个文件夹并将文件复制到该位置 现在我需要使用换行
这样我才能以可读格式输入aws s3 cp命令
当涉及到这个位置时
最终我们可能会有与不同日期相关的文件
无论我们有多少文件
我们只想上传与特定日期相关的文件
在使用aws s3 cp命令时
我们可以使用排除和包含的组合
我们可以选择我们感兴趣的文件首先上传到S3
在排除中，我们需要说星号
这意味着我们正在尝试排除掉我们想要包含的一切之后剩下的一切
现在包含将生效
包括的任何内容都将在排除掉一切之后只包括那些文件
因为i和排除星号是包括的一部分
我们可以指定模式
模式就是2021年1月13日
这是我想要上传数据的日期
我可以在这里使用星号然后关闭双引号
如果你想要上传2021年1月14日的数据
你需要下载到这个文件夹
你可能会得到2021年1月13日和14日的数据
在我们的本地文件夹中
然后你可以实际指定2021年1月14日的星号在这里
以上传只包含2021年1月14日的数据到目标位置
在包含之后
我们必须使用递归
因为我们必须复制所有数据
使用当前目录中的模式递归地复制文件
这就是为什么我们需要使用减号
减号递归像这样
现在你可以按回车键
你可以看到文件正在上传到s3
一旦文件上传到s3
当我们上传这些文件时
由于网络速度和aws的速度
文件已经上传到s3
我们应该能够通过aws s3来验证
然后指定s3
我上到gh活动像这样
你可以看到所有与2021年1月13日相关的文件
在这个位置可用
这意味着我们为1天设置了数据
现在我将其作为练习留给你
下载2022年1月14日的文件
以及2021年1月15日的文件
确保你将文件下载到本地文件夹
本地文件夹就是这个
下载2021年1月14日和15日的文件
确保你将文件上传到s3
使用这个命令作为参考
说到这里
确保你将文件下载到2021年1月14日
以及2021年1月15日
并上传到s3
因为你将使用那些数据集
也用于验证我们的应用程序
最终 没有那些东西你只能验证到一定程度
你将无法完成整个演示
我将在AWS上运行应用程序方面给予尊重 集群
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/068_Udemy - Data Engineering using AWS Data Analytics part2 p68 7. Validate Application using AWS EMR Compatible Versions.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为早期部分的一部分
我们已经详细讨论了Spark应用程序开发生命周期
使用Python作为编程语言
我们构建了一个名为itv-jh-activity的应用程序
它将负责从落地区获取数据
并将数据转换为Parquet文件格式以写入原始区。话说回来
嗯 我们可能会使用任何
嗯 我们想在开发期间使用的Python版本和Spark版本
但我们需要确保它已验证
使用集群中存在的版本
在我们构建zip文件并在集群上部署zip文件之前
在这个案例中我没有使用太多模块
所以我只使用了Python作为编程语言
我只使用了Pi Spark
让我们审查集群中Python和Pi Spark的版本
我们将尽可能多地复制开发环境中的内容
然后我们就会继续前进
让我转到AWS网页控制台
在这个情况下，我在集群页面
你可以看到我正在使用Spark 3.12
我们不确定关于Python版本的情况
我们可以实际上去终端
你可以登录到主节点
在这个情况下，我已经登录到主节点
这就是主节点本身
我可以说Python并按回车
你可以看到Python版本实际上为
Python 3.7
说到开发环境
我用的是python 3.6进行开发
你可以在这里查看
让我转到pie charm这里
如果我打开终端
你可以看到我使用的是python 3.6
你也可以从这里查看版本
然而，是的
我们的应用程序是一个非常小的应用程序
我们使用python 3.6没有问题
验证更好
至少使用spark三點一點二
这是集群的版本
然后我们应该能够构建zip文件
我们应该能够利用该文件在我们的集群上部署并验证
当涉及到这个spark问题时
我已在安装spark零一点二，以防你的应用程序未构建
使用spark点一点二
你可以这样做
你可以从这个虚拟环境中脱激活
你可以通过说“hyphen rf”来删除文件夹
然后通过说你连字符 v e 和 v
这是处理它的一种方式
这样它就会在本地环境中为你清理所有模块
你将不会碰到太多关于这方面的问题
与你复杂应用中使用的模型相关的冲突
也可能有其他更好的选择
但这是确保应用程序与兼容版本进行验证的一种方式
根据我们将要部署的目标环境
现在 与这个虚拟环境相关的一切都消失了
实际上我们可以创建一个新的虚拟环境
我可以说python然后按tab键
我有python3.6
我有python3.8和python3.9
我现在只想使用python3.6
所以我说的是python3.6
然后输入hyphen yam v和v
然后你输入hyphen v e和v
它会负责创建虚拟环境
全新的 一旦虚拟环境创建完成，我们就可以使用一个全新的环境
我们应该能够激活它
通过输入 source jh -v 和 v/bin/activate 并回车
现在虚拟环境已经激活
可能需要一些时间来反映
因为目前这个虚拟环境中还没有任何与Spark相关的模块
因此它应该显示其他内容
但现在它还没有刷新
这就是为什么你没有看到任何错误
这将是强制刷新的一种方式
让我们看看如何刷新这个
我可能可以做的是
实际上我可以去解释器设置
我现在应该能处理好
你可以看到它实际上正在处理刷新过程
让我们等到刷新完成
你会看到错误
因为Spark模块在这里没有安装
你可以看到它现在说未解析的引用pi Spark
现在我们可以通过安装适当的spark问题来解决这个问题
为了理解什么是适当的spark问题
基于我们的目标环境
我们可以去这个emr页面
你可以看到它只不过是spark three one two
我应该能够在这个虚拟环境中安装spark at one two
通过说pip
安装pi spark
然后双等号
然后三点一点到二点回车
现在它将实际安装pi spark
将其作为虚拟环境的一部分安装为三点一和二
一旦安装完成
一旦产品刷新
错误应该消失
你也可以通过运行
根据之前提供的指示
我在软件应用程序开发生命周期中详细覆盖了spark
然后您可以继续
在此情况下，我不会详细说明那些细节
我将使用这些程序构建zip文件
并负责将文件作为emr集群的一部分进行部署
以运行应用程序以验证
它是否按预期工作
使用emr现在
让我们等待此刷新完成
它没有刷新
我们可以实际点击此
您可以看到在刷新后它正在刷新
您会看到这些从这里消失
让我们等到刷新完成
现在我们可以继续
错误应该消失并且它们现在已经消失
使用我们在此环境中的程序
我们应该能够构建zip文件
文件命令实际上就是zip python r和zip文件名
我们可以实际使用所有相关的程序来运行此应用程序
应该包含的程序只不过是这些
让我谈谈y on lt
然后start.py
还有其他程序，不仅仅是这一个
我的意思是错误的文件夹
这就是为什么我们只能看到main.py
我现在可以进入名为zero six production is code的文件夹
我应该能够运行on lt start.py
您可以在这里看到python程序
我们需要将所有这些程序包括在zip文件中
以便我们可以无问题地验证我们的应用程序
现在我应该能够通过说zip hyphen
R i v hyphen activity
然后.dot.zip
我可以说star.py
它将包括所有在此处的python脚本
如果您想排除任何东西
您可以仅指定感兴趣的文件名
或者您可以详细解释explosive命令
以了解如何排除您不感兴趣的文件
目前
我正在包括所有在此位置处的python脚本 使用这些程序构建zip文件
将此zip文件作为部分包含在内
您可以看到
文件创建没有遇到任何问题
我们也可以使用
称为和python l a到文件名的路径
我们应该能够查看zip文件中所有文件的内容
它包含五个程序，因为zip文件已经准备好
现在是时候使用不同的方法在aws emr上部署它了 并验证它是否按预期运行
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/069_Udemy - Data Engineering using AWS Data Analytics part2 p69 8. Deploy Application to AWS EMR Master Node.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


当我们准备好文件时，文件名无非就是itv-zip
我们应该能够将此文件复制到主节点和valder
我们是否能够使用spark submit
在emr集群的主节点上运行
然而，这不是在emr集群上运行spark应用的最常见方式
我们通常使用称为步骤执行的东西
我将在随后的讲座中详细介绍步骤执行的细节，首先
让我们将此文件复制到主节点上
连同app.py
让我们使用spark submit命令进行验证
利用已经获得的技能和知识
当我们在集群的马什节点上运行应用程序时
实际上会作为后续讲座的一部分进行步骤执行
现在我们需要为那时选择这个文件夹
运行名为pw的命令
这是我有zip文件的文件夹
我可以复制这个 让我转到终端这里
让我试试 scp
短划线 然后我波浪线斜杠点
如我所说 私钥文件应该是itv
现场演示 点pam
这是集群的私钥
用于创建集群
现在我应该能够粘贴路径
然后我必须说itv短划线jh活动点zip
然后我必须指定是的
EMR主节点详情
那个主节点的用户名无非就是easy-to-hyphen-user@redhat
我必须使用那个主节点的公共dns
我可以去浏览器
我可以复制这个
让我回到终端这里
让我粘贴它
然后说column
然后说tilde now
我可以按下回车键 它 将负责从本地机器复制itvg活动到集群的主节点上的home目录中
然而，我将实际上在该目录中创建一个文件夹
一个目标服务器，那就是我们集群的主节点
并将zip文件复制到那里
因为我们也需要有app.py文件，以及其他zip文件
以便我们可以实际运行spark submit命令来运行我们的应用程序
首先，让我使用命令登录到服务器上
一次
我使用set命令登录到服务器
让我输入mk dr itv hyphen g activity
这是我想要存放zip文件的文件夹
以及app.py
现在让我输入yon lt
现在让我移动itv hyphen activity dot zip到itv hyphen activity now
让我退出这里
让我使用相同的scp命令作为参考
这次而不是复制itv hyphen activity dot zip
我将复制驱动程序
这就是app.py
app.py也在这个文件夹内
所以我应该能够像这样说app.py，作为目标的一部分
而不是仅仅指定tilde
这实际上就是家目录
我想添加一个itv-hyphen-activity
所以app.py将直接复制到目标服务器上的文件夹中
现在已复制
我应该能够登录到服务器
然后我应该能够在itv-hyphen-activity上运行python
我们应该能够同时看到文件以及app.py
当我们在集群的主节点上准备好应用程序后
现在是时候为我们设置适当的环境变量了
然后使用spark submit运行应用程序
一旦我们理解了如何在集群的主节点上使用spark submit运行应用程序
利用这些技能实际上可以处理运行任务 在接下来的讲座中，我们将使用步骤执行作为后续部分的一部分
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/070_Udemy - Data Engineering using AWS Data Analytics part2 p70 9. Create user space for ec2-user on AWS EMR Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为前一次讲座的一部分
我们已经将运行在eml集群主节点上的应用程序所需的文件复制了过来
然而，在运行它之前 我们需要确保easy to hyphen用户能够运行应用程序
如果不行 我们需要确保我们已经设置了preisites
为了运行这些应用程序，也就是说，为了验证
hyphen用户是否能够运行应用程序
我们应该能够启动pi spark或spark al
这将实际上告诉我们我们是否能够运行应用程序
使用名为easy to hyphen user的用户非常简单
让我输入pi spark并按回车
让我们看看它是否会启动
你可以看到它失败了
它失败的原因是我们没有为这个用户设置用户空间
名为easy to hyphen user的用户作为spark的一部分
是的 L运行在
啊 是的 Anmo
我们需要确保我们有足够的用户空间来运行这个将要运行应用程序的用户
如果你滚动浏览并查看详细信息
它说对/user拒绝访问
你可以看到这边的箭头
我们遇到这个问题的原因是因为
每当我们尝试使用特定用户运行应用程序时
它会尝试创建一个称为Spark Staging的文件夹，类似于/user
斜杠文件夹，该文件夹以运行应用程序的用户名称命名
在这个情况下，它只是斜杠user
轻松切割用户
此时 我们没有一个叫做轻松切割用户的文件夹
这就是为什么它失败了
我们还需要确保文件夹由用户拥有
这将运行应用程序
在这种情况下，它就是轻松切割用户本身
所以我们需要一个名为
轻松切割用户在用户下的文件夹
所有者也应该是轻松切割用户
让我们回顾一下 无论我们是否在斜杠用户下容易使用用户，还是那样子
我能说他的df
深度优先搜索用户
几乎每个人都会对命令的成功运行有读取权限
您可以在这里查看输出
有一个名为的文件夹
容易让用户连接 由易于连字符用户自己拥有
话虽如此，创建文件夹的责任在于我们
也要确保文件夹由easy to hyphen用户拥有
我们应该能够使用这个命令
所以做hyphen新hdfs
Sdf s是hdfs的超级用户
然后我们必须使用称为hdfs dfs
Hyphen mk d l slash用户slash
E c two hyphen用户
让我按回车 它将为我们创建文件夹
在这种情况下，这个hdfs只不过是用户名
这个sdfs是主要命令
Dfs是该命令中的子命令，可以与文件系统交互
Hyphen mk主要是创建文件夹
文件夹路径只不过是这个
现在我们也需要确保这个文件夹由easy to hyphen用户拥有
让我们检查权限和所有权与cc two hyphen用户相关
然后我们将再次进行
我可以运行hdfs dfs hyphen on slash用户
这次你将能够看到easy to hyphen用户
然而，与该文件夹相关的所有权仍然属于hdfs
不是easy to hyphen用户
因此我们需要确保所有权更改为easy to hyphen用户
为此我们可以使用称为伪hyphen的命令
New hdfs dfs dfs
然后c h one ion r递归
所有者只不过是easy to hyphen用户
我会更改组也到easy to hyphen用户
然后我可以说slash用户slash easy to hyphen用户现在
让我按 Enter所有权将更改为easy to hyphen用户本身
现在我们可以说hdfs dfs hyphen on slash用户
这次我们可以看到easy to hyphen用户由easy to hyphen用户本身所有
现在我们应该能够无问题启动spark
这次你将不会抛出任何错误
你将能够进入spark基于的cli
使用python作为编程语言
我们应该能够处理编写spark代码作为这部分cli的一部分
你可以看到没有任何问题
我们也可以说并按回车它将实际写入对象的类型
这正是pi spark dot
Sql dot session Dot spark session that being said
让我离开这个
现在我们应该能够使用e
C Two hyphen用户运行在itv hyphen activity部署的应用程序 让我们作为下一讲一部分去查看那些细节
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/071_Udemy - Data Engineering using AWS Data Analytics part2 p71 10. Run Spark Application using spark-submit on AWS EMR Master Node.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


截至现在 我们已经成功在集群的主节点上的一个文件夹下部署了应用程序
我们也创建了一个文件夹或用户空间
以便用户可以轻松地在hdfs中创建用户
这样我们就可以利用易用性来提交Spark应用程序
使用Spark提交命令
现在我们应该能够运行应用程序
然而 在运行应用程序之前
根据我们的应用程序
我们需要设置几个环境变量
如果你不确定应该设置哪些环境变量来运行此应用程序
请确保你回到这个应用程序开发生命周期部分
我在那里覆盖了开发
以及作为局部的
以及我们的大数据集群，即生产
我在那里详细地覆盖了所有环境变量
你只需要去看那些讲座
然后你可以回到这里
我想设置的第一个环境变量就是vion
我想将其设置为prod
在这种情况下 我可以说export
等于prod
这将确保它实际上使用yarn来运行此应用程序
因为我们的mr集群基于Spark和yarn
它将自动选择yarn
它将负责运行我们的应用程序
选择将由我们作为应用程序本身构建的智能做出
当我们设置prod时
它将尝试利用yarn
如果yarn在集群中可用
它将实际负责执行我们的应用程序
让我按Enter键设置它为prod
下一个是source_目录
实际的属性名是src和score
环境变量名也可以说是c和score
它必须是一个基于三的路径
桶名是itv-github
然后是emr
里面有一个叫做prod的东西
然后是landing
然后是jactivity
这是我们的源文件所在处
我们需要设置路径到此路径
我们需要确保路径以正斜杠结尾
以确保它是有效的路径
我可以做的是
我可以使用aws s3
在这个路径上
看看是否有文件
让我运行aws s three paste
输入路径
你可以看到该位置有文件
很快，通过运行应用程序，这将跨越这些文件，设置环境变量
下一个环境变量实际上就是源文件格式
你可以看到这些文件是json格式
因此，源文件格式应该设置为json
我可以这样设置，像这样导出源文件
下划线格式
它是杰森
现在来到目标目录
目标目录什么也不是
是的，又是三
目标位置也基于S3
桶什么也不是，它v
短划线github
短划线emr
然后产
然后原始的 然后活动
这是我们的目标文件夹
我们希望将数据写入的地方
在我们应用程序使用它之后
现在 目标设置为这个
目标文件格式应该是parquet
因此，我们必须设置名为tgt的环境变量
文件格式_
这应该是设置为帕克
我现在这样做
我们可以实际上说e和v并审查所有已设置的环境变量
你可以实际上查找源饮食源文件格式
目标目录在这些东西上面
所以我们也在此设置
让我们滚动上来并查看envion的值
他们应该在某处
我无法找到
让我滚动到这里
我们有源文件
设置java home lang hist control
你可以在这里看到这些东西的环境
如果需要 我们也需要设置pi spark和score python
然而 到目前为止，在这个emr集群中默认的python就是37
你可以通过说pi spark来验证
你可以看到pi spark python是37
因此我们不需要设置环境变量
Pi spark可以转换Python到Python三
你可以通过查看这来确认Python版本
它只是Python 3.7.10
它也在开始时显示Python版本
现在，除了已经设置的环境变量之外
我们需要设置一个更多的环境变量
它只是我们希望处理数据的日期
在这种情况下的环境变量只是src_file_pattern
所以让我输入export src_file_pattern
这次我们将验证2021年
一月十三 所以我不得不说二零二一年
零一年
然后十三
现在我们已经设置了所有环境变量
所以我们实际上可以说历史
然后grep
Export回车
您可以查看已设置的环境变量
这些是现在设置的变量
我们应该能够运行这个应用程序
我现在可以进入名为itv activity的文件夹
让我运行lf和lt来查看这些文件
在这个文件夹中我们有它
我们有一个活动点zip
以及应用程序点py
使用这两个 我们应该能够运行应用程序，利用spark submit命令
我们可以通过这种方式实际获取spark
提交命令像这样
你可以说 spark have 和 submit
让我换行
我可以说 hyphen hyphen master yarn
这在 eml 上是可选的
因为默认的 ah 框架，它将用于运行 spark 作业的，无非就是 yarn 自己
因此，即使你跳过了 master yarn
这将是良好的
我们需要有 pi python 文件
我们必须在这里指定 zip 文件名
它无非就是 itv hyphen dh activity dot zip
然后我们需要指定程序文件名
在这个例子中我将进行换行
然后我将在这里指定程序文件名
程序文件名就是app.py
这是驱动程序，我们将用它来触发应用程序
它将利用作为本部分的Python程序
导入并使用文件来处理并写入目标数据
现在我按回车
我们将看看应用程序是否成功运行
它似乎已经启动
让我们等到它完成
然后实际验证数据是否处理成功
现在应用程序已成功运行
你可以在这里查看日志
没有其他的了 一切都似乎正常
我们应该能够在目标位置验证
目标位置无非就是作为分数的一部分设置的tgt
让我实际说
E v 然后抓取然后tgt
这是我们现在将目标文件返回的位置
我可以说aws s three ls粘贴
你应该能看到
文件夹等于2021
为了确认我是否使用parquet文件格式有文件
我应该能够通过实际上说破折号破折号递归来改进这一点
实际上按上箭头
它应该回到aws sls命令
它没有回到aws s命令
让我复制这个
让我粘贴在这里
然后说破折号破折号递归
它将实际遍历目标位置中所有子文件夹中的活动
你应该能看到所有文件在这里作为你的等于2021
我们有一个按月份的文件夹
等于1 然后他们等于13
然后你可以看到有很多parquet文件使用snappy压缩算法
即使检查这些文件是一个很好的常识
检查 这不够好
我们应该能够在这些文件中验证数据
以确保数据符合我们的预期
作为下次讲座的一部分
我们将详细讨论如何在这些文件中验证数据
通过利用在集群上设置的笔记本环境
虽然我正在演示使用笔记本环境
如果您想使用cla自己探索
您应该能够启动pi spark
并运行我将在笔记本中运行的相同代码
我们开始看代码 你可以选择你喜欢的环境来运行这段代码
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/072_Udemy - Data Engineering using AWS Data Analytics part2 p72 11. Validate Data using Jupyter Notebooks on AWS EMR Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


是的 我们已经在aws mr集群上成功验证了itvh活动应用程序
现在是时候验证这些文件中的数据了
这些文件是生成的
我们可以看到这些文件
现在是时候深入数据相关细节了
看看数据是否符合我们的预期 如果你想使用cla
你可以通过spark启动
你应该能够编写代码
然而
我将演示 使用设置在emr集群上的本书
这样
你将理解emr集群上的本书环境的目的
它可以用于ad hoc分析
单元测试等
我们不需要挣扎于命令行界面
相反 我们可以实际上使用设置在emr集群上的本书环境
当涉及到路径时
这就是文件所在的地方
因此让我复制这个
让我实际上进入笔记本环境
这是设置在emr集群上的本书环境
如果你看不到这个
你可以实际上进入yr控制台
转到笔记本并点击在dulab中打开
它将使用此界面打开
这只是dupta lab基于的界面
现在我们可以更改这里内核
我将使用现有笔记本本身
此笔记本是当我们实际上在现有集群上创建笔记本环境时自动设置的
首先我们需要将内核更改为pi spark
让我更改内核为pi spark
让我点击 选择现在笔记本的内核只不过是pi spark
你可以在这里查看
现在我们应该能够说spark运行此单元格
你可以看到类型
当你实际运行时
如果这是第一次，它可能需要一些时间
过一会儿 它将实际上立即返回
那就是说 这会自动曝光
每当我们将内核更改为pi spark时
我们不需要编写代码来创建名为spark的对象
这是spark会话
现在我能做的是
我可以通过名称创建变量df
让我称它为df
我可以实际上说park点
然后读取点
然后par的文件位置就是我们从控制器隧道复制的内容
这就是我们复制的内容
我已经在这里粘贴了
现在让我运行这个
它将创建一个名为df的对象
它是一个数据框类型
在这种情况下它失败了
说它是无效的语法
原因是我在这里没有反斜杠
现在让我运行它
它将正常工作 它将创建一个名为df的变量
它是一个数据框类型
一旦它被创建 我们应该能够通过说df来预览数据
我们应该能够通过说df点打印模式来预览模式
我们可以通过说df点计数来获取计数，等等
然而，在探索那些函数之前
让我折叠这个
让我稍微放大一点
我正在尝试放大
我现在能放大了
我可以说df点打印模式来查看这个数据框的模式
你可以在这里看到完整的模式
在审查模式之后
我们也可以说df点显示来预览一些数据
在这种情况下我说df点显示
它将实际显示数据框中的二十个记录
让我们等结果显示出来
然后我们将实际查看这里显示的输出
现在你应该能看到输出
然后让我实际上获取计数
我只需说df点计数
我们应该能看到计数
让我们等这个运行
我们应该能看到计数
计数只不过是两千八百二十九万零一千一百一十条记录在这个数据框中
如果你想要后来过滤数据到2021年1月13日本身
你应该能够说df点过滤
然后你可以说有一个字段叫has created yet
让我们审查打印模式的输出
然后我们会使用适当的字段名
在打印模式中
有一个字段叫college created unscore yet
让我们先审查数据
然后我们将进一步审查数据或查看数据
我正在说 df 点 select
然后创建 yet 然后让我说
所以让我打印 20 条记录与 truncate 等于 false
让我运行这个
我们应该能看到 created
Yet 值在这里
它有数据以及时间戳
日期和时间戳通过大写的 t 分开
现在我应该能够使用 substring
并且使用 creators 和 score 的子字符串在逗号
一逗号十 这将实际返回这个时间戳的日期部分
我们应该能够与 2021 年比较
一月十三日来获取所有属于该日期的记录
让我们在这里编写代码
在这种情况下我将使用双引号
然后说 sub st
然后 created _
yet 一逗号十
让我们确保十字符足够
我们有四个数字 l
然后两个数字 month
然后两个数字 date
这构成了八个字符在上面
我们有两个破折号 这意味着我们有十字符
现在 我可以说 substring of created 在 一逗号十 双引号
等于
我们应该能够与 2021 年比较
让我运行这个来看看它是否按预期工作
它正在创建数据框
然而，我无法预览数据
要预览数据
让我 say show
让我预览数据
到目前为止，我们只有与 2021 年相关的数据
通常只有 13 号 因此您将不会看到任何差异
然而，当我们向此目标位置添加更多数据时，运行其他日期
然后我们将根据不同日期看到不同的结果
我们说完了
让我清除这个输出
让我选择这个
让我获取计数
到目前为止，计数应该与以前所见相同
这就是二十万八
二十九千一百一十一
您可以在这里看到输出
您也可以说 df 点 group by
作为分组的一部分，你可以有这样的逻辑
创建的at one,十的子字符串
我可以包括作为单个课程的一部分
我们可以实际上为这个提供elias
我们需要在这里使用调用函数
在使用调用函数后像这样
我应该能够像这样使用别名，给这个派生字段一个有意义的名称
这实际上就是created_underscore_date_now
我应该能够说
Ag count of lit off
然后星号在sping之后
Count of little of星号像这样
我们应该能够提供别名，通过说elias
然后活动_underscore_count
你可以给你的选择名称
在我的情况下 我正在使用这个名字
我应该能够说点然后点然后我应该能够说so over
在运行这之前 我们需要导入调用count和lit函数
我可以添加 并在此单元格中说from pi spark dot sql dot functions
导入count调用lit now
我应该能够导入这些三个函数
然后运行这个
然而它失败了
因为我没有适当地使用函数
而不是以这种方式使用
我必须使用不同的方法
我在这里应该做的是，而不是导入调用
我必须使用exp
我在这里应该使用exp函数now让我再次运行此
您可以看到此代码已成功运行
您可以查看输出以及到目前为止的目标位置
只有与2021年相关的数据
一月十三号
因此我们只能看到这一项 一旦您为其他两个日期运行应用程序，设置适当的环境变量
这是源文件模式
您应该能够看到额外的日期在这里
如果您实际上在此代码片段中运行验证
在运行那两个日期之后
我说
我将将其作为练习留给您，设置适当的环境变量
并在2021年1月14日运行应用程序
以及2021年1月15日
并且您可以回到基于tutor的环境
或使用pi spark实际运行任何代码片段
我在这里显示以验证其他日期
到目前为止 我们可以通过使用spark submit从主节点运行应用程序
然而，更正式的方法是在下一节课中作为步骤执行部分
我们将详细讨论如何进行步骤执行的细节 在不进入主节点的情况下运行我们的应用程序
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/073_Udemy - Data Engineering using AWS Data Analytics part2 p73 12. Clone and Start Auto Terminated AWS EMR Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止，我们已经在cmr集群上成功验证了应用程序
然而，当我们设置集群时
我们选择了自动终止
这意味着如果集群闲置30分钟
它将在此情况下自动终止
因为它已被终止 您可以在这里查看详细信息
我们需要再次设置集群，一旦集群设置完成
我们需要重新部署应用程序
然后应用程序才能再次运行
让我重新创建集群，并将其进一步扩展到重新创建集群
我可以在这里选择集群
然后说 克隆
它询问我们是否想包括步骤
让我们说好 让我们点击克隆
它将处理啊
选择我们以前选择的选项
我们只需点击这个就能创建集群
现在集群正在设置中
你可以去配置详情中了解在这个集群上正在设置什么
集群正在使用以太坊64点O与spark3点1点2和duenterprise网关设置
话说回来
一旦集群启动并运行
我们需要重新部署我们的应用程序
以便我们可以再次运行它
让我们等到它启动并运行
然后我们会处理重新部署我们的应用程序
然后我们会继续
集群还在启动中
在启动过程中 我们应该能够选择公共dns
我们应该能够连接到它
使用se
在这个情况下，公共dns会改变之前的公共dns将不再起作用
让我点击这个 这样我就可以复制这个公共dns
让我转到终端
我应该能够说h hyphen i tilde slash dot h
然后进行现场演示
这就是私钥，我们必须使用它
它是密钥对的一部分
用于创建集群
现在 我说容易-连字符
用户在速率粘贴
输入公共dns
说 yes
你应该能够进入集群的主节点
启动这个集群的服务需要一点时间
但我们能够连接到主节点
当集群启动时
我们应该能够将应用程序复制到这个
这样我们就可以部署它并且我们可以进一步验证
我将创建一个名为itv-hyphen-activity的文件夹
这是我想要部署应用程序的文件夹
我可以回到pie插件
我应该能够再次运行cp命令
然后我可以继续
实际上我将使用终端本身
而不是pie插件 让我退出这里
让我搜索cp，这是我以前使用过的
这是我以前使用过的scp命令
我只需要将公共DNS替换为新的
让我删除这里的公共DNS
让我改为新的
这就是新的
让我复制一次
让我回到终端
让我粘贴在这里
让我按回车
在这种情况下，我正在尝试将app.py复制到itv
我也会复制zip文件
我只需要将app.py替换为命令中的文件
这样文件也被复制了
让我删除app.py
让我输入itv-hyphen-activity.zip按回车
现在文件也已经复制到集群的主节点上
我可以使用such连接到主节点，现在应该能够运行和
让我们看看
itv-hyphen-activity
你可以看到app.py以及b的gzip文件夹
这意味着我们已经没有问题的重新部署了我们的应用程序
让我退出这里
让我回到浏览器
让我们刷新以确认集群是否正在运行
你可以看到它现在是运行状态
现在我们应该能够去笔记本
笔记本也因为闲置时间停止
我们应该能够点击这个
我们应该能够通过点击这个来启动
然而底层集群已经被删除
因此我们可能需要看看是否能够编辑并使用新集群
是的 你可以看到有一个名为更改集群的选项
我们可以点击更改集群
然后点击
选择现有的集群
通过集群选择这个运行中的集群
然后你可以说更改集群并启动笔记本
你可以看到笔记本正在启动
一旦笔记本启动
你应该能够点击打开duter lab
这样我们就可以转到dulab基于的环境
让我们等待直到这完全运行
然后我们实际上也会去笔记本
并看看笔记本是否仍然存在
现在 笔记本环境也已经在我们的集群上运行
我们应该能够点击打开jupyter lab
这样基于jupyter lab的环境
可以无问题地打开
我们可能需要点击这个
来实际绕过弹出窗口并无问题地启动jupyter lab
一旦jupyter lab环境启动
你应该能够看到我们之前更新的带有验证代码的笔记本
让我双击它
你应该能够在这里看到笔记本
我们现在有了之前使用的所有代码
让我清除所有输出
右键点击这个
然后说清除所有输出
我们应该能够通过运行这个打印语句来验证它是否正在运行
如果它工作 那么我们实际上也可以运行其余的代码
当我们实际运行第一个单元格时
这是一个启动Spark应用程序
因为内核是pi spark
一旦它打印出来
我们应该能够运行其余的代码
为了确认处理过的数据仍然存在作为部分的一部分
它是三还是什么
请记住，当我实际上在2020年1月13日验证了该应用程序时
我 它拥有在2021年1月14日和15日运行的权限
我已经为2021年跑了两次
一月十四日和十五日
现在当我通过运行这些单元格来验证时
你将会看到与
不仅与2021年1月13日有关
也与14和15有关
让我运行这个来创建数据框
实际创建这个数据框需要一些时间
一旦创建完成
我们应该能够打印出模式
预览数据 我不会运行这个
如果你想 你可以跑
我将直接进入更早的计数
我们已经看到了超过二十万条记录
现在将会更多
因为我们有三天的数据
我们需要等到这被运行
然后我们将实际预览这里的输出
你可以看到我们有超过八百万条记录
因为我们已经运行了三天
让我们说运行这个来看创建的实际上这不需要
我可以直接跳到最后一个并先运行这段代码
让我运行这个以获取2020年1月13日的计数
我也会复制几次
我也会运行14和15
所以让我数14
也让我数15
一旦13完成
然后让我运行14
让我们看看14的计数
我们也会看看15的计数
运行后
你可以看到我们有2.85
七百万在2020年1月14日
关于2020年2020年15
让我们看看有多少数据我们有
我们应该能够很快看到计数
你可以看到在这个日期我们有超过260万条记录
让我导入这三个函数
也让我运行这个
现在你可以查看由这个代码生成的输出
截至现在，我们已加载了三天的数据
因此，我们得到了这三天的每个数据统计，话说回来
这就是你应该设置集群的方式
同时，在集群上设置笔记本
如果你的集群被终止
一旦完成 你应该像这样进行验证
请记住，虽然我只演示了2021年1月13日，但我把它留给你作为练习，运行2021年1月14日
我留下了一个练习，运行2021年1月14日
第十五课，确保三天内完成所有练习，以便您对设置环境变量和运行应用程序感到舒适，使用适当的Spark提交命令。
到目前为止，我们使用默认部署模式运行了该应用程序，这意味着客户端。
在运行Spark在YARN时，我们可以使用两种部署模式之一，一种是客户端，另一种是集群。
当我们进入步骤执行时，我们实际上会进入客户端或集群模式。
第六课，这种模式被称为客户端。
第七课，当我们在YARN上运行Spark时。
第八课，我们可以使用两种部署模式之一，一种是客户端，另一种是集群。
第九课，当我们实际进入步骤执行时。
第十课，当我们实际进入步骤执行时。
我们需要确保
我们理解如何可以在集群模式下运行spark submit命令
当你尝试在集群模式下运行spark submit时
你需要以不同的方式设置环境变量
随着我们继续深入这个模块，我们会详细介绍这些细节
在深入探讨在集群模式下运行作业的详细信息之前
让我们确保我们完全清理了数据
我们也将探讨集群模式与客户端模式的区别
然后我们将看看我们如何使用集群模式运行这些作业
然后我们将实际进入步骤执行，当进入步骤执行时
你将理解如何将spark应用程序添加到集群的步骤执行中
并了解如何在不使用命令行界面的情况下运行我们的应用程序
理解步骤执行的好处有很多 一旦我们深入这些细节，你就会理解
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/074_Udemy - Data Engineering using AWS Data Analytics part2 p74 13. Delete Data Populated by GHAcitivity Application using AWS EMR Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止，我们已经理解了如何在集群上运行Spark应用程序
使用传统方法
在进入步骤执行之前
让我们确保所有数据都已删除
这些数据已经填充到了目标文件夹中
因为我们将处理与相同日期相关的数据
现在 让我们向上滚动
让我们使用我们已经将数据复制到目标的路径
这是基础文件夹
我可以复制这个
我可以进入终端
让我连接到集群的Marsh节点
然后我应该能够使用aws s three rm命令
我应该能够传递ua
让我添加一些内容
在最后添加一个斜杠 添加一些换行
我只需要说减号
减号rea 所以现在我可以清理文件夹中的所有内容
让我按Enter 它将负责删除文件夹中的所有内容
是的 数据已成功清理
现在是时候探索两种不同部署模式之间的区别了
它们就是集群和客户端 然后 我们将详细讨论如何在Mar集群上使用步骤执行提交Spark应用程序
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/075_Udemy - Data Engineering using AWS Data Analytics part2 p75 14. Differences between Spark Client and Cluster Deployment Modes.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为这次讲座的一部分
让我们详细探讨两种可用的部署方式
当我们使用yarn运行spark应用程序以获取有关这两种部署方式的详细信息时
实际上可以说spark hyphen submit
然后你可以说 hyphen hyphen help 然后按回车
你应该能够看到这条命令的使用方法
如果你滚动到顶部
有一个叫做部署模式的东西
你可以在这里查看详细信息
它说 是否在本地启动驱动程序
在客户端还是在集群内部的工作机器上启动
默认是客户端
所以我们运行命令时说spark submit
然后输入-hyphen hyphen master yarn
然后输入-hyphen hyphen pi files the file
然后输入app dot 这就是我们的驱动程序文件
实际上在客户端模式下运行
我们可以在运行前设置所需的环境变量
我们应该能够无问题地使用客户端模式运行
但是设置环境变量并使用集群模式运行应用程序的方法将不会起作用
原因是环境变量是在本地设置的
然而，驱动程序将在集群中的一个工作节点上运行
为了更好地理解这一点
让我们转到图表
这是官方Spark文档的一部分，您可以查看该图表
您可以实际访问浏览器
让我在这里打开一个标签页
我将使用此标签页
我可以说spark.apache.org
一旦我们在这个页面
我们应该能够去文档
我们可以选择最新版本
一旦我们去到最新版本
你可以实际上去部署
然后你实际上可以点击为你
这将是一个图表在这里
这就是你应该理解的图表
所以当我们谈到我们的集群
我们有一个主节点和一个工作节点
当涉及到Spark时
工作节点就是这个
我们连接的主节点是我们拥有的
无论我们运行哪个Spark提交命令
都会有一个Spark上下文和驱动程序与之相关
当我们实际使用客户端模式时
驱动程序和Spark上下文将在我们提交Spark提交命令的节点上启动
然而，当我们使用集群模式时
而不是在集群的 mash 节点上运行这个驱动程序
当我们使用 spark submit 命令时
它将使用工作节点
当我们在主节点上设置环境变量时
这些环境变量不会自动传播到工作节点
这就是为什么在集群模式下设置环境变量并运行此 Spark 应用程序将不工作
为了克服这一点，我们需要将环境变量作为 spark submit 命令的一部分设置
只有这样，它才会按我们的期望工作
只是为了演示设置本地环境变量并运行应用程序不工作
让我尝试设置环境变量并在集群模式下运行应用程序
作为下一节课的一部分，它将失败
我将实际演示如何在集群模式下运行
通过设置环境变量作为spark提交命令的一部分
现在让我转到这个终端
我将设置环境变量
然后，我将使用集群模式运行应用程序
那就是说 第一个环境变量就是on
它应该被设置为plot
之后，我必须设置称为src的东西
并且得分的位置除了s三列/ /
然后itv- github
hyphen emr
然后prod
然后landing
然后活动
这是基础文件夹
我们在这里有文件 所以我们可以说aws来验证
s三
我们应该能够使用环境变量，使用美元符号
而不是输入URL
你应该能够在这里看到文件夹，没有其他问题
我应该在这个导出命令的末尾添加正斜杠
让我添加，然后让我再次运行
我们应该能够在这个位置看到文件
现在，下一个参数应该是源文件格式
我可以说导出sc
下划线文件 下划线格式
它应该设置为json
因为我们的文件源是json文件格式
然后是目标文件夹
目标文件夹应该设置为s three colon
斜杠itv
连字符github
连字符yammer
然后prod
然后raw
然后活动
这是目标文件夹，我们希望将数据写入其中
说到目标文件格式
应该是parquet
因此，让我说目标文件格式
然后最后我们必须设置源文件模式
让我输入导出slc_文件
模式
在这种情况下，我将尝试在2021年1月13日验证数据
2021年1月13日 因此，让我来说二零二一
连字符零一连字符十三
现在运行我们的复制所需的所有环境变量都已设置好
让我使用spark submit命令并运行该应用程序
我只能说 spark-提交
这次我将使用集群作为部署模式来运行
在指定主YARN后
我应该能够说破折号破折号
启用连字模式
那么我不得不说集群现在
我现在按下回车
我可以说破折号破折号破折号文件
我应该能够指定文件名
然后我必须传递驱动程序文件名
这就是app.py
记住，当涉及到spark提交命令时
你必须从spark提交开始
你可以将这些控制增强放在任何顺序
你不必遵循这个顺序
最后你必须指定程序文件名
你不能在控制增强项之间将程序文件名放在前面
控制增强项 语法是提交spark
然后所有控制参数在任何顺序中
连同相应的值
然后在最后放上程序文件名
现在按回车
你可以看到它失败了
让我们看看它是否失败了
因为它在集群模式下运行
或者一些其他事情
在这个情况下它失败了
因为容易让用户分词
在这个新的集群中，用户空间没有他的空间
不是因为现在在集群模式下运行这个spark提交命令
让我们照顾好设置用户空间以便于让用户分词
正如我们早先看到的
我们遇到这个问题的原因是因为我的集群在中间被终止
在我开始集群后我开始了集群
我已经验证了一切
但是我还没有为 easy to hyphen user 设置用户空间
现在我尝试运行 spark submit 命令时
它无法修复这个问题
我必须说 pseudo hyphen u
Hdfs dfs dfs hyphen mk d l slash user slash
Easy to hyphen user
它将负责创建
Easy to have a user 在 slash user 文件夹下
我们也需要为这个文件夹更改所有权
为 easy to hyphen user 自己
我可以说 pseudo hyphen you his dfs
His dfs dfs hyphen c h on hyphen capital r
Easy to hyphen user colon
E c two hyphen u
然后 slash user slash e c two hyphen user
现在 让我按回车
它将更改该文件夹的所有权
让我们尝试运行这个 spark submit 命令，看看它是否会成功
或者它会失败
很可能它会失败
我们将调试它为什么失败
我们也知道我们正在尝试使用集群模式运行
通过在本地节点设置环境变量
当我们尝试以集群模式运行时
它将不会传播这些环境变量
因此在哪个 worker 节点上 driver 程序将运行
因此它将失败
你可以看到它已经失败
如果你查看日志
它不会非常明显，因为工作将在集群模式下运行
并且与 driver 程序相关的实际日志将作为 spark history 服务器的一部分
我们需要访问 spark history 服务器以实际调试问题
但在这种情况下 我确定这是因为环境变量没有传播到 driver 程序运行的 worker 节点
让我们再次回顾这个图
然后我们将实际使用集群模式运行应用程序
正确设置环境变量
在这种情况下，如果你看看这个图
让我们假设这是集群的主节点
并且集群中有一个 worker 节点
当我连接到集群的主节点并使用 spark submit 命令时
使用默认或客户端模式
将创建 driver 程序在同一节点上我们运行 spark submit 命令
因为我们在运行 spark submit 命令之前设置了环境变量
因此 driver 程序将创建在同一节点上
所有与环境变量相关的值将没有任何问题地传播到我们的 driver 程序
当涉及到集群模式时 现在，当它到达 worker 节点时，所有值都将没有问题地传播到 driver 程序
当我们在集群模式下运行时
驾驶员程序将在集群中的一个工作节点上运行
在我们这种情况下 目前我们只有一个工作节点
当我们实际使用集群模式运行spark submit时
spark submit将从主节点上运行
我们也在主节点上设置了环境变量
然而 现在驾驶员程序将在工作节点上运行
由于驾驶员程序在工作节点上运行
在主节点上设置的环境变量不会传播到工作节点上
我们的程序
这就是为什么我们遇到这个问题的原因
如果你想进一步调试这个问题
你可以实际上到这个作业的spark UI
你应该能够看到详细信息
很可能你会得到一些关于环境变量的提示
或者它会实际上说一些值缺失
这些值应该被传递给正在运行的程序
既然我们已经理解了在集群模式下运行的副作用
让我们继续在集群模式下运行它，看看我们如何传递环境变量
以确保环境变量也会传播到驾驶员程序
我们需要确保环境变量作为spark submit命令的一部分传递 使用特定的语法
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/076_Udemy - Data Engineering using AWS Data Analytics part2 p76 15. Running Spark Application using Cluster Mode on AWS EMR Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为前一个部分的一部分
我们已经详细讨论了适用于Spark作业的两种不同部署方式
啊 在YARN上
它们只不过是一个客户端和一个集群
当涉及到使用客户端模式运行应用程序时
我们只需要使用export命令导出所有环境变量
然后我们应该能够在客户端模式下运行
但在集群模式下这不会起作用
因为环境变量不会传播到驱动程序上
司机程序将在与稀疏提交命令提交不同的使命上运行
话虽如此，当运行使用部署模式集群的Spark提交时
通过传递所需的应用程序
如果您需要传递环境变量
您必须像这样传递环境变量
您必须使用连字符
连字符conf 然后在双引号中您必须指定像这样的键
它应该是spark点点app大师e
V 然后是一个点
然后是环境变量的键
然后是等于对应的环境变量的值
这适用于所有环境变量
然后是与我们的环境变量相关的键
使用这种方法 我们应该能够以集群模式运行我们的应用程序
请注意，这种不会在客户端模式下工作，为了在客户端模式下工作
我们必须使用export命令导出所有环境变量
然后我们应该能够无问题地以客户端模式运行spark submit
让我在这个命令上即兴发挥
根据我运行的环境
现在 说到神经元
它应该是绘图
说到源目录
它应该是s三
我在这里说s三
然后冒号 然后双斜线
然后桶名就是itv
短横 G活动
实际上是一个短横
Github
然后emr
然后我们有prod
然后landing
然后活动
这是源目录
我需要在末尾添加前缀
因为现在我们正在提到三个部分 目标只是着陆点的替代品
其余的部分与源相同
因此让我粘贴在这里
让我实际上将着陆替换为也
我想设置源文件模式为2021年1月13日
让我将其更改为2020年1月13日
现在当涉及到pi文件时
所以文件在本地文件系统中
因此让我直接传递文件
像这样
当涉及到驱动程序时 它也是本地文件系统的一部分
因此我可以像这样指定app
让我执行这个命令
我应该能够将其作为终端的一部分粘贴
它将无问题运行应用程序
让我们运行这个并看看它是否能够处理数据
到2021年1月13日
并正确地写入目标位置吗
一旦它成功运行
我们可以回到笔记本环境
我们应该能够运行我们的验证代码以确保数据已复制
按照我们的期望
让我们等到它完全运行
然后我们将继续
你可以看到它已成功运行
最终状态为成功
然而
对我们来说重要的是首先验证 让我们验证文件写入的目标位置
但此一我应该能够使用aws s three命令查看
如果此位置有文件
在这种情况下我说aws s three
让我换行
让我粘贴路径
然后让我换行
让我输入减号
减号递归
它将递归地使用此基文件夹获取所有文件
你可以看到文件后来自2021年
一月十三号
到目前为止我们只处理了数据相关的数据
因此所有这些文件都与之相关
你也可以在这里检查分区
这就是你应该能够使用spark提交运行应用程序的方式
利用集群模式
我们需要确保环境变量像这样传递
您已成功使用集群模式运行应用程序
我们还可以看到文件
让我们转到数据
我们将使用您创建的笔记本
在这种情况下，我在笔记本中
我们应该能够运行此以创建数据框
我已清除所有输出
我们以前有过
如果您在运行之前想要清除所有输出
实际上您可以选择笔记本的任何部分并点击清除所有输出
现在数据框已创建
我们应该能够通过运行此来获取计数
如我们所见，我们只有数据相关的2021年1月13日
我们应该得到约289万，如这里所见
我们也可以运行此以查看我们是否只有2021年1月13日或其他日期的数据
让我们运行此并查看详细信息
它将返回仅一个记录，那就是与2021年13日相关的记录
这意味着我们已成功清理数据后使用集群模式运行数据
而不是在aws cmr集群上运行应用程序
使用spark submit命令
而是使用其他可行的替代方案
而不是使用spark submit
我们将使用其他可行的替代方案
其中之一就是aws cr的步骤执行
让我们了解如何利用这一点 以提交我们的应用程序使用cms步骤执行
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/077_Udemy - Data Engineering using AWS Data Analytics part2 p77 16. Overview of Adding Pyspark Application as Step to AWS EMR Cluster.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


到目前为止，我们已经详细讨论了如何在aws集群上运行spark应用程序
使用传统的spark提交命令
请记住，这并不是我们运行spark应用程序的方法
我们不会在emr集群上运行spark应用程序
一个可行的替代方案是使用称为步骤执行的东西
这是aws eml的一部分
在本讲座以及随后的讲座中，我们将详细讨论如何运行spark应用程序
利用步骤执行，这是aws eml的一部分
让我详细说明一下选项
让我们详细说明一下选项
我们通过探索UI已经完成了
我需要去EMR AWS控制台
我在这里点击取消
不需要 我在这里去集群
现在我可以点击这个
它将带我到集群页面
一旦你在集群页面
你可以看到有一个叫做步骤的东西
你可以点击步骤
你可以通过点击添加步骤
在这里添加步骤，就像向现有集群添加步骤一样
我们可以创建集群来执行步骤并自动终止
稍后我会详细说明
让我们为正在运行的集群添加步骤
在添加步骤的选项中
第一个是步骤类型
你可以展开它
你可以看到，我们可以传递自定义JAR流应用或Spark应用
在这种情况下，我们必须选择Spark应用来运行我们的基于Spark的pi应用程序
然后我们必须选择部署模式
它可以是客户端或集群
如果你想使用客户端
那么你必须传递环境变量
与选择集群的方式不同，集群更实用
我会详细解释客户端
但我不会用客户端演示
但如果你想要 你也可以使用基于客户端的方法
但这有点棘手，话虽如此
我们将选择集群
然后我们必须指定这里所有选项
如果你回到这里
我们有这个和别的
你可以忽略这个
你不需要太担心它
即使你可以忽略这个
因为我们已经选择了一个集群作为部署模式
我们只需要复制这个就完了
在指定这些在spark提交选项后
你必须指定应用程序的位置
应用程序位置应该指向驱动程序
也就是这个
我们需要确保它作为s3的一部分
因为我们正在尝试运行集群模式
如果你尝试运行客户端模式
你也可以使用本地路径
你可以指定s3路径
或者你可以指定本地路径
因为我们打算使用集群模式
我们需要确保我们的应用程序部署在s3上
在这种情况下，我们需要从s3桶中选择app.py
此外，作为spark提交选项的一部分，我们还需要指定python-hyphen-py-files
这可以通过s3 url传递
因此，我们需要确保文件和应用程序.py都在s3上
并且我们应该能够使用这些细节作为spark提交选项
如我早前所提到的
我们需要确保应用程序位置从这里选择
使用s3 如我早前所提到的
我们需要确保应用程序位置从这里选择
使用s3 如果我们的程序需要任何参数
你需要传递这些参数
使用空格作为参数之间的分隔符
然后你有一个叫做action on failure的东西
这将实际告诉你在步骤失败时应该做什么
你是否想要继续或取消并等待或终止集群
取决于你的设备
你可以在这里选择
因为我正在现有集群上运行步骤
我将选择继续
我们将再次查看这些选项
以便我们可以验证应用程序
我只是想回顾这里的选项
然后我们将实际进行这个过程
无论需要什么来运行应用程序
利用此步骤
我们将运行它 并且也会验证 运行
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/078_Udemy - Data Engineering using AWS Data Analytics part2 p78 17. Deploy Spark Application to AWS S3.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在这个阶段，我们在探索运行Spark应用程序的过程，作为步骤的一部分。
使用CMR集群进行操作。
为此， 我们需要确保代码部署在S3上，在本课中，
我将详细介绍如何在S3上部署代码。
让我进入隧道这里。
让我退出并运行应用程序。
我们需要复制这两个文件，也就是驱动程序和驱动程序的程序。文件的位置以及驱动程序程序的位置，实际上就是这个。让我实际上扩展这个。
文件以及驱动程序程序的位置，实际上就是这个。让我实际上扩展这个。
这就是我有文件以及app.py的路径。
我需要复制这两个
然后让我从这里复制路径
现在说绝对路径
让我去这里 让我说cd粘贴
让我运行seven lt
确认我们有itv和activity dot zip
以及app dot py在这个现在
我应该能说aws s three cp itv hyphen activity dot zip
然后s three columns Slash我将使用路径作为itv-hyphen-github
Hyphen-emr
Then app
这是文件夹，我想将此文件复制到此文件夹中
确保在末尾添加斜杠
否则itv-hyphen-gzip将被复制到bucket中的app文件夹中
现在将被复制为itv-hyphen-gzip
但在app文件夹中
让我按Enter
现在文件已成功上传
我可以通过说aws s three来验证
然后这条路径让我复制这条
让我清除屏幕
让我输入aws s three粘贴
按回车
我应该能看到这个文件在这里复制
这就是文件 我也需要复制驱动程序
这只是一个点py
因为我们需要传递它
以便于spark submit命令能够实际使用它来启动该应用
我只需要说aws s three cp
然后app.py
然后我需要说s three column
斜杠itv-github
然后-emf
斜杠app斜杠
你需要确保在最后
否则app.py会被拷贝为app本身
现在让我按回车键，并在这个位置运行aws
我们应该能够在这个位置看到文件以及.py文件
因为zip文件以及abp都已部署
现在是时候添加步骤并在顶部运行它作为 您的集群将负责在下一节课中处理它
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/079_Udemy - Data Engineering using AWS Data Analytics part2 p79 18. Running Spark Applications as AWS EMR Steps in client mode.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


目前我们已经成功地将应用程序部署在S3上
现在是时候将我们作为集群的一部分运行
谈到部署模式
就EMR集群中的步骤而言
我们有集群模式和客户端模式
在本讲座中 我将详细介绍如何实际使用客户端模式运行步骤
然而，我不会使用客户端模式进行演示
因为它有点棘手
使用集群模式更为常见
因此，我将使用集群模式进行演示，尽管
如果你想使用客户端模式
你可以看到当你选择客户端时
当涉及到应用程序位置时
你可以指定你的应用程序文件的本地路径
无论是作为数据库文件还是驱动程序
你可以指定对集群主节点的本地路径
我们在尝试在这个步骤上运行这个步骤
在应用程序位置和文件位置之后
我们还需要传递环境变量
我们已经看到，当我们实际使用客户端模式运行时
我们不会能够传递环境变量
我们需要使用export命令设置环境变量
在这种情况下，我们不会能够直接传递环境变量
这正在进行中
这没什么，这只是启动
让我取消这个
如果你想使用客户端模式运行你的步骤
如果你的应用程序需要环境变量
那么你需要确保作为你集群的启动程序
你包括export命令来导出环境变量
在这种情况下 让我转到集群
让我点击创建集群
让我转到高级选项
你可以选择你想要的任何技术
然后点击下一步
你没有bootstrap选项
作为第一页或第二页的第三页
这就是一般的集群设置
你会看到启动选项
让我点击下一步
你可以在这里看到启动操作
你可以实际指定脚本
实际上使用那些export命令
你可以实际将脚本上传到S3
你可以指定S3中的位置
每当集群启动时
它将实际启动集群
无论文件中有什么命令
所有这些命令都应该被执行
因此，您将看到环境变量
一旦使用此启动类型方法设置了环境变量
我们应该能够在客户端模式下运行步骤
此外 应该将环境变量传递给应用程序的任何变量都将由启动程序传递
话虽如此
如果您尝试使用您自己的集群，这种方法将起作用
仅用于执行围绕您的集群创建过程步骤
您需要确保自动化并创建具有适当环境变量的脚本
将其作为启动操作添加
您可以继续
尤其是如果您考虑源模式
每次运行时都应该更改
我们需要为源文件模式提供值
并在启动时使用它
以便我们可以使用客户端模式运行步骤
这就是实际在客户端模式下运行步骤所需的
您需要确保在启动集群本身时设置了所有环境变量
现在，当谈到集群模式时
我们将演示的内容
我们可以利用现有集群本身
让我实际上点击这个
让我转到耶 Emr这里
让我转到这个集群页面
让我转到步骤
让我实际上点击添加步骤
我们只需要确保我们使用spark应用程序
部署模型是默认的集群方式
我们应该能够提供所有spark提交选项
我们应该能够在cmr集群上作为步骤运行我们的应用程序
集群模式
为此 我们需要使用此命令作为参考
然后我们可以继续
我将在下次讲座中演示如何使用集群模式运行步骤
我们已经理解了在客户端模式下运行步骤所需的内容
如果应用程序需要环境变量
那么我们需要确保在客户端模式下运行应用程序时处理了启动 利用emr集群上的步骤
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/080_Udemy - Data Engineering using AWS Data Analytics part2 p80 19. Running Spark Applications as AWS EMR Steps in cluster mode.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为这次讲座的一部分
让我们详细讲解如何将我们的应用程序添加到现有Ambari集群中
在这种情况下，您需要确保步骤类型是Spark应用程序
我们可以指定名称
让我称之为itv g h活动
这是我应用程序的名称
这将是一个有意义的名称
当涉及到Spark提交选项时，部署模式应该是集群模式
让我们回顾一下我们以前使用过的Spark提交命令
这没什么，你可以忽略这两个
默认情况下，它会是on
并且这里已经选择了集群模式作为部署模式
因此我们可以忽略这两个
现在我们需要将所有这些内容复制到Spark提交选项中
您可以在这里看到
作为这次 我们需要复制并粘贴这些
然而，在复制和粘贴之前
我们需要确保文件模式更改为2021年1月14日
因为对于2021年1月13日
它已经运行过
当涉及到pi文件时
我们需要指定itv-hyphen-zip的位置
我可以回到我的终端
我可以回顾一下我们以前运行的命令
这是关于itvg活动.zip的
我需要复制这个
然后我需要去pie charm
让我粘贴在这里
这是pi文件的路径
这也是a p的路径
我应该能够复制这些并粘贴到Spark上
提交选项
当涉及到应用程序位置时
我可以点击这里，现在我应该能够去itv
uh hyphen github
hyphen 然后app然后app.py
现在我应该能够选择
现在您可以看到tapis没有任何问题地选择了
应用程序不需要任何参数
因此我们不需要添加任何部分作为参数
现在 让我们审查失败操作
当涉及到失败操作时
我们有三个选项
一个是终止集群
如果您选择终止集群
当步骤失败时 它将会终止集群
当涉及到继续和取消和等待
继续和取消的方式行为相似
当我们只有一个步骤
但如果你有多个步骤
当我们说继续
它将会进入下一步骤
如果你说取消和等待
它将会停留在它失败的地方
而不是进入下一步骤
在这种情况下 因为我们只有一个步骤
我们不管我们选择继续还是取消和等待
我将选择继续
我可以说添加
现在你可以看到tab.py被选中没有出任何问题
该应用程序不需要任何参数
因此我们不需要添加任何参数
现在让我们回顾失败操作
当涉及到操作 在失败时
我们有三个选项
一个是终止集群
如果你选择终止集群
当步骤失败时 它将会终止集群
当涉及到继续和取消和等待
继续和取消的方式行为相似
当我们只有一个步骤
但如果你有多个步骤
当我们说继续
它将会进入下一步骤
如果你说取消和等待
它将会停留在它失败的地方
而不是进入下一步骤
在这种情况下 因为我们只有一个步骤
我们不管我们选择继续还是取消和等待
我将选择继续
现在我可以说添加
现在这一步骤已经添加到集群
我们应该能够刷新并查看当前状态
它仍然处于待定状态
它将需要一些时间 它将实际开始运行
当涉及到这一步骤
因为我们在集群中有限的容量
它将需要几分钟来完成步骤
让我们等待直到这一步骤运行
然后我们将进一步处理
最初，该步骤处于待定状态
现在，它正在更改为失败状态
你也可以看到，对于这个步骤没有生成锁定
让我们看看为什么没有生成锁定
我们可以展开这个
你可以实际转到错误
在这种情况下，错误是因为有一些语法问题
由于存在一些语法问题
它甚至没有尝试提交应用程序
因此，在这里没有生成锁定
话说回来，我们应该能够复制这个
让我复制，然后让我转到我的pie charm
让我粘贴在这里
是的 一切都改为一行
这些反斜杠导致问题
它破坏了语法
因此，我们需要确保这些反斜杠已被移除
让我移除这个反斜杠
也让我移除下一个
有五个或六个
我们必须移除它们全部
之后，我们需要移除这个
然后，这个，还有这个
我们也需要移除这个
现在我们已经从这条线移除了所有反斜杠
我们可以回到浏览器
在这种情况下，我们不能编辑这个步骤
我们需要克隆这个步骤
并且我们需要修复它
或者我们也可以完全从头开始一个新的步骤
让我选择克隆步骤
你可以看到，它选择了步骤类型为自定义jar
并且它直接使用了spark submit命令与命令行上的一个jar文件
我不想使用这个
我想使用spark应用本身
让我使用集群模式
当涉及到spark submit选项时
我们不应该复制这整行
我们需要清理一下这个
然后我们需要复制其余部分
所以我们需要移除这个spark submit部署模式集群或部署更多集群
并且我们也需要确保这部分已被从这移除
现在我们需要复制这行
粘贴在这里
然后我们需要选择app.py为应用程序位置
让我转到s三浏览器
让我转到itv
在github emr中，我们有app文件夹
在我们有app.py
让我选择这个 让我点击选择
让我输入添加
现在 步骤将添加到集群中
让我们看看这次是否会执行
我选择了spark应用作为名称
我没有更改名称，这目前没问题
我们将看看它是否会运行
使用这个名称本身
理想情况下，我们应该更改为一个有意义的名称，比如这个
以便我们稍后可以双锁定
尽管我如前所述提到了有意义的名称
运行此应用程序将花费一些时间
让我们等到应用程序完全运行或失败
然后我们将实际看到下一步需要做什么
几乎两分钟后
我们可以看到此步骤的状态为运行
你可以刷新 你可以看到它是否正在运行或失败
它仍在运行 它似乎运行正常 让它运行 作为下一讲，我们将验证它是否已成功运行
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/081_Udemy - Data Engineering using AWS Data Analytics part2 p81 20. Validate AWS EMR Step Execution of Spark Application.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为前一次讲座的一部分 我们已经详细讨论了如何在ema集群中添加spark应用程序作为步骤
该应用程序成功运行
然而，集群被终止
您可以在这里查看集群的状态
当谈到应用程序的状态时
它说已完成
我们可以查看详细信息
通过查看日志来确定应用程序是否完全成功运行
然而 如果你想要对数据进行验证
那么这是不可能的
我们知道我们需要启动集群
然后使用笔记本环境进行验证
或者我们可以在数据集上创建一张表并使用ea方法
我现在不会深入这些细节
因为我已经涵盖了如何验证的内容
如果你已经在运行集群
你应该能够运行相同的代码
在上面的集群上使用笔记本
你应该能够验证这些数据
让我点击这里的查看锁定
你可以看到这里有多个链接
一个是控制器
第二个是标准错误
你可以忽略这个日志并突出显示
因为它们没有关联的链接
要审查应用程序的锁定
你应该能够点击标准错误
它将下载文件
你可以去下载文件夹
你应该能够打开文件
在这种情况下，文件已经打开
你可以实际滚动查看应用程序是否成功运行
你应该全面审查日志来确认它已无挑战地执行了所有内容
你可以看到一切都看起来很干净
这里 它说已完成
并且没有错误，这意味着我们可以继续前进
即使集群在此时已被终止
我们应该能够用一条命令来对抗s三，看看文件是否已经创建
在前一次运行中 我们处理的是2021年的数据
一月十四日 因此我们应该能够看到与验证相关的文件
实际上我们可以去隧道
我们应该能够运行一个名为aws的命令
S three然后s three列斜杠斜杠
然后itv
Github
雅马 然后推动
然后生
然后活动
最后 我们需要有一个斜杠
现在 我们应该能够在这里看到文件夹
与您的等于2021年看到该年度所有文件夹和文件
等于2021年的文件夹
我可以实际上在最后添加递归
让我说斜杠
斜杠重复然后按回车
我们应该能够看到此位置的所有文件夹和文件
基础位置是
你可以看到这一个2020年和1月14日
在这个上面 你应该也能看到2020年1月13日的文件
这就是你应该能够进行健壮性检查的方式
即使集群没有运行
然而 如果你想进行数据验证
你可能需要启动集群
在上面部署笔记本并运行代码片段
你之前看到的数据验证代码片段
在这种情况下 我不打算演示这些事情你应该现在很熟悉
如果你想进行数据验证只需启动集群
在上面设置笔记本并运行给你的数据验证代码
话说回来
我们已经理解了如何运行spark应用程序作为步骤在ea集群上
你应该对这个过程非常熟悉 这是部署spark应用程序在emr集群的最常见方式
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/082_Udemy - Data Engineering using AWS Data Analytics part2 p82 1. Building Streaming Pipeline using Kinesis.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为这个模型的一部分 我们将看到如何使用canis构建一个简单的流处理管道
canis是aws原生的流处理服务，可以用来处理流数据的摄入
经常 你可能想要捕获你的应用程序生成的日志
并将数据摄入到数据湖或s3中
然后你想要进一步处理
目前 我们将专注于如何将日志摄入到s3中
为此 我们将有一个称为general_underscore_locks的东西
设置在一个名为gw_zero_two的服务器上
在我这个案例中，我将使用这台服务器
在你这个案例中，你可以使用任何你想要的next_base服务器
你最好使用send或red_hat服务器
一旦你在服务器上生成了日志
你可以配置consist组件来将消息发送到目标
让我演示一下它会是什么样子
这里有一个部署的应用程序
它会生成流式日志
它是一个模拟器 它不是实际的web应用程序
如果你已经有一个生成日志的web应用程序
你可以配置canis来读取这些日志
并确保它们被摄入到s3中
因为我没有实际的web应用程序来演示
我已经复制了cloudera的general_score_locks应用程序
并且我围绕它构建了内容
日志在这里生成
underscore_locks
access.log
你可以在这里看到文件
你也可以运行tail -f在这个文件上
你可以看到日志一直在被生成到这个文件
然后你会配置服务器上的agent来捕获这些日志消息
消息会被推送到你的交付流
这个流实际上就是gen_underscore_locks_underscore_s3
让我展示一下这个流
我去canis
并且这里是你可以实际查看流的地方
用于这个模块的流实际上就是general_score_locks_s3
并且这个score_locks_underscore_s3会将数据写入到s3桶中
桶名是itv_gen_locks
并且文件夹就是locks在这个s3桶中
你可以实际看到gw_zero_two上生成的消息
由我们的gen应用程序被实时写入到s3中
会有两到三分钟的延迟
因为canst组件内置的缓冲
你应该能在这里看到日志
作为这个模型的一部分
我们将构建一个简单的流处理管道
将访问日志文件中的消息读取到桶中
在接下来的部分中
我们将实际看到如何自定义这一点 我们还将涵盖几个额外的场景
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/083_Udemy - Data Engineering using AWS Data Analytics part2 p83 2. Rotating Logs.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


后续在流媒体管道中使用的演示将使用日志文件
我们有一个叫做gen locks的东西，部署在我们的服务器上
它实际上模拟了有人访问我们的网站
我们可能有所有可能访问我们网站的客户
这是被模拟的 事件被记录到一个名为访问日志的锁文件中
在这种情况下 与遗留应用程序常见的一个问题是，日志轮换可能未启用，这可能会导致下游问题
这可能会下游造成问题
在我们这种情况下 日志旋转并未启用
那就是为什么如果你看文件的大小
这只是下划线锁定
锁 然后x点对数
这个网站只有三个点
我们每天应该旋转四个G字节
为了确保尺寸不会随着时间的推移而增长
如果它继续以日志文件的形式增长
然后，在某个时刻或另一个时刻
它实际上会困扰我们，最终会导致一团糟
所以我们需要确保锁文件被生成
不仅生成 还要在频繁的间隔内进行轮换
所以让我们配置一个日志轮换
以便这个文件每天轮换
为此我们有一个名为log rotate的工具
它几乎适用于所有Linux平台
如果你使用的是红帽风味
你可以使用 你应该能够安装log rotate
如果你使用的是ubuntu
只需在谷歌上搜索 你应该能够无问题安装log rotate
命令只是伪m hyphen y install log rotate
如果你使用的是那些如centrofederal的flavor
等等 现在我们必须配置这个log rotate
以便文件在定期间隔内旋转
通常每个应用服务器或Web服务器都有一个配置文件
文件的位置就是/etc/rotate.d
只有在log rotate安装后，这个才会出现
我们可以使用这些标准配置文件之一作为参考
我们应该能够为world gen应用配置日志轮转
所以我将使用x作为参考
所以让我假装
cp
etc log
这是nginx的完整路径
如果你感到不舒服
你可以这样做 你可以首先进入这个目录
让我来做
所以cd c log rotated
然后cp和x和gen underscore locks
我正在复制和x作为gen score locks
然后我将替换适当的行到适当的值
以确保我们的gen locks也被旋转
使用此模拟器生成的锁文件也将被旋转
我在这里必须说伪cp和x gen underscore locks
现在文件已复制
我应该能够说伪vi gen underscore locks
现在您可以看到它与我们的nginx有关，我们的锁文件位置是nothing but
can underscore logs和then logs
我想要给实际的文件名
而不是使用模式
如果你说星log
以log结尾的任何文件都将被旋转
在这种情况下 我只想旋转访问日志
当涉及到文件的权限时
因为这是我们的最后
我们允许人们运行应用程序来写入文本文件权限是nothing but
这些几乎每个人都可以实际写入此文件
您可以在这里看到
并且您可以访问访问日志的父目录
这是nothing but gen underscore logs
您可以看到权限是nothing but读取和执行对所有学生，无论他们被分配到学生组
他们将能够更新此文件夹中的文件
因此我们必须使用称为su的东西来实际运行
作为该用户来旋转锁
所以首先
我们应该做的是我们必须使用称为su的东西
我们的用户是nothing but training
所以我们必须重复两次
我不知道为什么
我们不得不重复两次
我不知道为什么 我们不得不重复两次
我还没有探索过
如果你感兴趣 你可以探索并让我知道
然后这不需要
我们希望每天旋转
我们希望保留十个日志文件
因此，任何比十天老的文件都将自动被删除，这样一来
您不会随着时间的推移浪费太多的存储空间
然后missing
好的 我们可以删除不格式化的
我们可以删除压缩的意思
所有旋转的锁文件
我们的文档文件
在我们的情况下我们将有十个
那些十个文件将在它们被旋转后压缩
你将看到每个旋转的文件都有一个gz扩展名
让我们启用压缩
删除这个删除这个
删除这些行
我们也需要一些叫做复制截断的东西
现在我们应该能够保存这个
退出这个
要验证你可以这样做
你可以说伪日志旋转减减d是干运行
然后给出锁文件的位置
不是锁的位置 但是给出配置文件的位置
这是etc日志旋转点d然后gen下划线锁
你可以看到它说旋转模式是这个一天后十个旋转
空锁文件被旋转
warlocks被移除
切换uid到144和gid到145
所以你不需要担心这个
这将是旋转的锁文件
日志不需要旋转
日志已经被旋转
切换uid到零和gid到零
如果你多次连续运行
你可能会看到这条消息
你可以忽略这个消息
这意味着我们的配置是正确的文件将被旋转
即使我们安排了日志旋转
有时候我们可能会想强迫旋转
你可以通过使用日志旋转来实际强迫它
你可以实际指定配置文件直接
在这个例子中我在开头添加了伪
这样我可以以超级用户运行
让我们看看发生了什么
它立即回来了
验证一下在gen下划线锁
锁的位置
到目前为止文件没有被旋转
如果它没有被旋转
如果你想无论如何旋转
你可以实际说减f像这样
然后按回车
它需要时间 Which is which means it is actually rotating the file
Let's wait until the rotation is done
Then we'll actually review what's happening under this location
Let me copy this and let's wait until log rotation is done now
Log rotation is done
Let me copy this and then run the cell as having an ear on this location
And you can see that the previous log file is compressed and you have gz at the end
Let's run it once again
So as of now the logmasses are not being written to the access log file
That's why you are not seeing anything in case
If you want to make sure that it is validated
You can actually run this log generator application
I think the way you can actually run this is nothing but gen underscore locks
Actually we have a python
Not python script shell script
We just have to say start underscore locks
Dot h now the locks will be generated and you can actually weld it
Now you can see if you want to rotate again
You can actually say log rotate once again
Like this and you will see second log file
And you can see that the names are changed for each and every old log file
The reason why it is changing these names is because we have configured log rotate
To be ten beyond ten files
They will be automatically deleted
So this is how you should be able to enable log rotation
It is very important that it is configured
So that you can consume the data
If you want Or you can just let the log files deleted after a period of time
Depending upon the ss within your organization for your application
You typically configure the a number of rotations
Or we can also specify based on number of days
You can actually say thirty one days
And it will actually delete those files
Which are older than thirty one days
Based upon the sls You have to explore the options that are available as part of the log rotate
And you have to use them now
Log rotation is done
We need to ensure that it is done on a regular basis for that
Either we have to schedule using cron or some other enterprise level scheduler
I can just say cron tab hyphen e
So that lonny opened in edit mode
Then i can actually say star star star star star
Which means it will be rotating every day every minute
Which is not required
Let me actually replace this
It means it will be rotated every hour
Actually i can even ignore this and go to every day
所以每天
好的 实际上
如果我说r零
每天午夜
它会旋转
好的 所以让我用这个集会每天在r
这意味着大约在午夜
我们需要做的事情就是旋转文件
我们只需要说伪
因为我正在旋转作为训练训练不适合
因此我不得不说伪
然后我必须实际上说log rotate hyphen f
文件名无非是etc
log rotate点d然后下划线locks
然后我们应该能够像这样运行
为了验证它如预期旋转
我们现在可以做的是
让我替换两个星星在这里
这意味着每分钟旋转
好的
如果它每分钟旋转
然后我可以回来并更改它为零在这里
所以现在每天在零旋转
让我保存这个已经安装
让我们等到它变成33
这样我们就可以验证它是否在33旋转
现在是33
我们应该能够通过说si lt gen下划线locks
locks您可以在这里看到详细信息
您可以看到它在33旋转
您可以看到旧文件名为a2
这是之前的1
这是2 现在是3
每分钟它都在旋转
这意味着功能在预期中工作
我们所需要做的就是
我们只需要回到调度程序
然后更改它为零在这里
这意味着文件现在将每天旋转一次
这就是你应该能够配置日志旋转的方式
并且启用日志旋转
按照他所说
他是
所以在这种情况下我每天旋转
无论什么evil sla
您可以按照顺序旋转
因为我们能够以定期间隔旋转locks
现在是我们处理剩下的东西的时候了 在这份文件上建立流处理管道
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/084_Udemy - Data Engineering using AWS Data Analytics part2 p84 3. Setup Kinesis Firehose Agent.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们理解如何在生成锁的服务器上设置代理工具
代理的文件实际上就是canis或aws应用到的代理工具
在canis下
这将实际帮助你从生成锁的服务器推送锁
有两个canis流或交付流
canis中有两种类型的流
一种是一致性流，另一种是流
我们将稍后了解它们的含义
与目标有关
你可以在实际生成锁的服务器上配置代理工具
但在配置代理的之前
你需要确保它已经设置好
让我们来走一遍设置所需的步骤，总共有五个
确保这个服务器没有生成锁的文件夹
在这种情况下 锁文件是在这个位置生成的
在_ lo locks下
你可以在这里看到一个叫做访问日志的文件
所以这就是模拟的
如果有人访问我们的网站或移动应用
如果你想 你可以通过说 tail hyphen
F gen
Underscore locks
Locks
访问日志，你可以在这里看到消息
你可以看到 如果有人访问我们的网站
实际上会生成日志消息
这只是一个模拟器
如果你想要 你可以按照提供的说明进行设置，这些说明是作为早期主题一部分提供的。
正如我们有一个服务器在那里生成区块
现在我们可以在这个基础上安装kell agent
然后我们将看看如何配置
为了确保消息能够推送到任何目标
它们本应被发送到
实际上为代理人设置了一个文件
你可以实际上去浏览器并搜索安装canis文件主机代理工具在Linux上
它将带你到aws文档
你可以点击这个链接
你可以向下滚动
你可以稍后查看步骤来下载和安装代理
你可以点击这里
我会在资源中提供这个链接
另外 你可以直接使用这个链接
你应该能够来到这里
你应该能够安装货币代理
现在 让我复制这个，你可以实际运行这个来安装cancis代理
如果失败了 我们会看看其他替代方案
很可能它会成功
让我们看看如果实际上你去到这
它说使用红帽企业linux设置代理
你必须遵循这个 好的现在我明白了为什么它失败了
当我说 yum
安装hyphen y a cis agent
如果你在服务器上设置
这是建立在e
c two 尤其是使用亚马逊linux
那么你可以运行这个，它会工作，因为它是亚马逊图像
它会实际上拥有所有库或包，从这些工具可以安装
但如果你使用非亚马逊linux ami作为部分aws
或者如果你试图在web应用程序服务器上设置
这些运行在外
aws 那么你必须使用这个
它会处理从s three安装cis代理的
服务器应该有互联网访问权限
并且它应该能够无障碍地与s three通信
那么你应该能够使用这个命令来直接安装代理
让我复制这个，让我粘贴这个
作为部分s three bucket
我们有一个rpm文件
并且它负责下载并在此服务器上安装
这就是你应该能够安装cis代理的方式
让我们理解这里面有什么
如果你去斜杠edc
你可以看到有一个文件夹叫aws consists
你可以实际上进入那个文件夹
it c aws canis
然后说iphone lt
你有agent jason
你有agent rod等等
一旦安装 你也可以在之后启动代理
我们将实际上进入详细信息关于conferring
让我们看看它是否会启动还是会失败
我们可以复制这个命令来启动
如果你使用红帽最新版本，例如toy seven
红帽七等
你可以实际上使用system ctl
所以你可以说pseudo system l start aws cis agent
这个命令服务
a cis agent start也会工作
但是，它已经过时了
现在，它是用于检查状态的旧命令
你可以做的事情是
你可以实际上用状态替换这个start，看看代是不是已经启动
你可以看到代已成功启动
所以我们已经使用这个命令安装了代
从s3安装hyphen y
然后我们实际上通过运行这个命令启动了
也有在必要时的不同步骤
如果你不想直接从s3使用yum install
因为你将在我们的企业中服务
可能没有互联网访问权限来连接到这个s3桶并下载和安装
你可以实际上从github上传到服务器获取仓库
然后你可以实际上去你有这个设置脚本的位置
然后你可以说./setup --install像这样
这样代就被手动安装了
所以你可以像这样直接使用a3桶安装
或者你可以按照这些说明使用github仓库设置代
确保你以默认配置开始并测试代是否启动
在处理从我们的日志文件读取配置之前
因为我们已成功安装并验证我们的代正在运行
现在是我们实际上将日志文件配置为目标
然而，我们需要首先有目标作为下一部分的主题
我们将看到如何创建交付流
然后我们将回到这里并配置访问日志文件
以便使用此cancis代将其写入一致流
让我们稍后详细讨论如何创建一致流 然后我们将回到这里看如何使用这个代来写入
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/085_Udemy - Data Engineering using AWS Data Analytics part2 p85 4. Create Kinesis Firehose Delivery Stream.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


正如我们在服务器上安装了货币
代理
我们从哪里想要推送锁定
现在是时候让我们有一些目标，消息可以被推送
然后我们可以实际配置我们的火来推送消息到这个目标
让我们进入kis在aws控制台
所以我正在登录aws这里
如果你在这里看不到cancis
我最近列出了
这就是我为什么看到这些
你可以实际上滚动到aws
分析服务
你可以看到这里 分析，你应该能看到这里
要么你可以通过这里进入并点击这里
或者你可以直接这里搜索说这里，然后选择这里
当你进入canis
你可以看到有三种类型的会议在canis中
一个是数据流
第二个是数据火洞
第三个是数据分析
在这种情况下，只是为了从kis火收集数据给代理
并将数据写入某些目标
在我们的情况下，我们将使用s3
我们只能使用数据火流，数据分析是为了不同的目的
我们将在后一点谈论那些，到目前为止
我们从这个文件获取数据
Was代理，我们希望将其写入某些目标
在我们的情况下，它将是s3
我们只能使用数据火流
它将有与源相关的目标指令
因为data file for不知道锁定文件在哪里生成
它将不会帮助我们配置
然而，我们必须遵循推送策略
并且我们必须配置代理将消息推送到这个火
因此，我们将配置此火房的目标
并且我们将配置火Was将消息推送到这个目标
一旦它被创建
你可以点击创建交付流来实际创建交付团队
让我命名为gen underscore locks
我们希望将其写入s3
这就是为什么我在末尾有s3
对于名称 当涉及到来源时
你有一致的数据流
在某些情况下，我们可以实际上将数据放入数据流
它可以作为火洞的力
这样消息就可以发送到目标
当我说火洞
我正在谈论流流和火洞是一样的
Firehouse agent 是我们在服务器上安装的一个
在这里锁被生成
在这种情况下，我们不会从数据流中收到消息
这叫做拉取策略
从数据流获取消息到目标
我们遵循推送策略
在这里，部署在服务器上的代理
在这里锁被生成
会推送消息
因此，我们必须选择这个，因为它说推送或其他来源
你也可以编写自己的发布者或生产者，将消息生产出到这个流
如果你对我们的情况感兴趣
它是其他来源
因为我们将要配置cancis
文件代理将消息推送到这个流
一旦你选择这个
然后你可以点击下一步
你也可以应用某些低级转换
为此，你必须使用lambda
我们将实际上详细说明如何应用低级转换
当消息传递到目标时
使用lambda稍后
让我们禁用这个 然后当消息被写入到目标时
你可能想改变记录格式
如果不那样
消息被写入到交付流
从firehouse agent
消息将以相同的格式写入到目标
在这种情况下，我们不会处理任何记录格式转换
因此，我现在禁用它
点击下一步 如果你遵循顺序
你有选择与来源相关的选项
然后转换
然后目标
在这种情况下，目的地就是s3
你可以选择s3桶
或者你可以创建一个新桶
所以让我实际上命名为itv
连字符gen logs
所以我创建一个新桶
好的，然后s3前缀
如果你想根据某些信息自定义文件的名称
你可以实际上在这里有前缀
现在，我只会说locks
我不会改变任何事情
所以在itv和logs下
实际上会有一个名为logs的子文件夹
然后它会开始将文件复制到该位置
让我们向下滚动，然后点击下一步
说到缓冲大小以使消息更快
我想将其减少到1b
这是最小值
你可以低于1b
说到缓冲和间隔
最小值是60
所以每分钟我们都会看到消息
或者当它达到1d时
在我们的情况下，日志消息在一分钟内没有生成太多
这可能到达这个
因此，每一分钟都会看到消息被写入目标
一旦端到端管道配置完成
所以现在我们实际上将缓冲区设置为1并将缓冲间隔设置为60
然后当文件写入到s3
如果你想压缩 你可以启用压缩
你可以选择这些压缩算法之一
我们现在禁用它
如果你想加密数据
你也可以加密
我正在禁用甚至加密
错误日志 我想配置错误日志
因此我正在启用它
锁将在云观察中生成
云观察是在本地aws
所有我们在aws生态系统中的服务器的日志消息都将返回到
现在，我们正在启用它
我们将看看如何处理问题
如果你遇到任何问题
通过浪费云监控锁
现在滚动并查看您需要创建i am角色
或者更新现有的
i am角色 以便权限保持不变
这样交付流可以写入s3
如果你真的理解这一点
我们正在尝试将消息写入s3
正在尝试写入消息的服务
它需要权限
AWS有一个概念
我需要重新审视它
与它有关的概念需要理解其中的细微差别，目前
我们需要有一个角色
以便kansas文件hogestream可以向s3写入文件
这将为我们处理
你不需要太担心自定义或选择现有的
现在您可以点击创建或更新
我是角色 它将负责更新
我是具有写入消息到s three所需权限的角色
然后你可以点击下一步
我们已经配置了源
我们已经配置了转换
我们没有应任何转换
无论是角色级转换，还是记录级转换
然后当来到目标时
我们有cons three并且我们已经将其配置为默认设置
现在我们应该能够点击创建交付流
你应该能够在网页控制台仪表板中看到交付流
现在需要一点时间，你可以看到新交付流正在创建
你可以点击这个，你应该能够监控
你可以去日志
如果你想通过访问云观察控制台来从监控中解决问题
然而，我们只创建了流
我们还没有将此与文件连接起来，以将消息发送到s three
现在是时候为我们配置文件了
以便代理将消息写入此流 我们必须确保数据流入s three
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/086_Udemy - Data Engineering using AWS Data Analytics part2 p86 5. Planning the Pipeline.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在建立一个基本的流处理管道时，使用consists的一部分
让我们回顾一下正在处理的内容
到目前为止，我们设置了一个叫做general underscore locks的东西
它只是一个日志模拟器
它会将日志消息生成到一个名为log_under_this_location的文件中
你可以在这里看到x.log文件
这里我们也设置了canas firehouse agent
我们使用了默认配置
你可以通过说sudo system l status aws hyphen canis hyphen agent来验证
你可以看到代理正在无问题运行
如果你去aws web控制台并转到仪表板
我们已经创建了一个名为itv通用交付流的东西
它被配置为将数据写入s3桶
然而，我们还没有配置将数据写入此交付流
我们需要确保代理的cancis文件
该文件以默认配置运行
从max日志文件读取数据并将其写入channel_underscore_logs
channel_underscore_s3_stream
此流应负责将消息发送到s3桶
让我们了解在gw zero to com上需要采取哪些措施
在哪里可以看到文件代理正在运行
实际上你可以来到这里的终端
然后转到名为/etc/aws的位置
你应该看到一个名为agent.json的文件
你可以在这里看到
你可以查看它包含的内容
它设置了一个名为cloudwatch emit matrix的参数为true
这将实际负责将消息写入云观察
当由canas fire生成的日志被创建时
然后你需要指定consistent和point 5以及与你相关的任何其他点
并且你还需要在这里配置锁定文件，在这里配置流名称
要么使用取消流要么使用delustream
在我们这个案例中 因为我们已经创建了一个交付流来写入s3
我们需要使用这个
我们可以删除这个
我们可以使用这个
所以这将负责将消息写入到我们的流
这就是一个下划线锁定
下划线s3从访问锁定文件
一旦我们请求更改
你也需要记住，代理需要在此ah上具有权限
包括开发流
以便代理可以向其推送消息
为此，我们需要根据权限创建IAM角色
然后，我们需要使用访问密钥和秘密密钥来写入
以生成评分日志和评分三交付流
如果您使用AWS C实例本身生成日志
那么您可能不需要创建凭据
例如，AWS的访问密钥和秘密密钥
但如果你在aws之外运行，那么你必须确保你传递访问密钥和秘密密钥
这样消息可以被写入到流中
访问密钥和秘密密钥需要根据iam用户生成
我们为了这个目的创建的iam用户
我将带你走完这些步骤
然后我们将配置cancis将消息写入到流中
这实际上就是general score locks和score s three
它会以这个名字将消息写入到s three中
s three桶的名字就是itv unlocks
它会包含一个名为locks的文件夹
当你一切如预期工作时，你会看到消息
所以让我们开始首先创建具有所需权限的iam用户
然后我们将实际使用它来配置代理的配置文件
以及源和目标
这实际上就是日志文件和交付流 这是实际上的日志文件和交付流
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/087_Udemy - Data Engineering using AWS Data Analytics part2 p87 6. Create IAM Group and User.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在构建使用canis的简单流处理管道的过程中
让我们继续创建iam用户
用户凭据将被用于配置
不仅最初的顺式剂
但是也要验证文件是否正确地写入到s three中
首先我们会处理创建用户以及角色
然后我们将实际讨论与水炮相关的政策
然后当我们谈论根据s三进行验证时
我们将看看如何配置策略从s three bucket读取
所以，要实际创建iam用户
你必须去服务中心
如果你最近访问过
你可能会在这里看到 如果没有
你可以从这里搜索
你应该能打开
我是网页控制台
我是身份和访问管理
我不打算详细说明
我是嗯 我将直接跳到创建用户注册
如果你感兴趣的话
你需要通过参加适当的课程来进一步探索
但要构建流管道
我在这里覆盖的内容应该足够长久
你理解我为什么要做某些选择
我会尝试在相关地方解释
所以在这种情况下
我们创建iam用户并生成凭据来配置cancis文件
是因为kansas fireagent在aws的范围之外运行
生态系统或aws云基础设施
它完全在另一个服务器上
你的网站或移动应用程序可能在你
本地数据中心运行
你可能想使用aws来构建数据湖并解决分析问题
你可能需要配置画布
超出了基础设施的范围
因为你试图设置超出了基础设施的范围
你需要有一个具有访问密钥和秘密密钥的用户
以便您可以为代理配置文件
这也适用于其他服务
现在您可以实际点击
用户说添加用户
让我们给用户名itv gen locks用户
所以我们实际上是将其命名为itv gen locks
用户gen locks代表我们的应用程序
用户代表您
itv只是一个前缀
以便我可以根据这个过滤
当有需要时
所以用户名是解锁用户
我只需要程序访问权限
这个用户不需要任何网页控制访问权限
这些用户被称为功能用户
他们只负责集成我们的服务
没有人会使用此用户的凭据访问网页控制台
因此程序访问权限足够了
它将生成访问密钥和秘密密钥
然后您可以点击权限
您可以通过直接附加策略将权限附加到用户
或者您可以将用户添加到组中
在这种情况下，我们将将用户添加到组中
我想创建一个新的组
然后您可以点击创建组
您可以通过创建用户的方式创建用户
您可以创建组
更正式的方式是先创建组，然后创建用户
因此，而不是创建用户然后创建组
让我们取消
转到组
然后创建组
组名只是itv can locks组
现在我们可以说下一步暂时不附加任何策略
我们将在下一个时间点回来处理策略
脉冲实际上会赋予组权限
我们希望配置交付流的写入权限
因此我们需要探索那些权限
我们需要构建自定义策略来写入特定流
我会详细说明
一旦创建了组和用户
现在我们可以创建组
组已创建
您可以查看它
组名是itv gen locks组
没有关联的用户
现在您可以选择并说添加用户到组
您可以添加现有用户
或者您可以创建一个新用户并将其分配给该组
所以让我创建一个用户
您可以点击添加用户
用户名只是itv gen locks用户程序访问权限
我们可以将用户添加到组中，已经选中了
组名只是itv gen lox组
如果您想应用税
您可以在这里应用税
然后审查
您可以审查该用户属于该组
然后您可以说创建用户
目前该组没有附加任何策略，因此该用户对任何内容都没有权限
因此此用户对任何内容都没有权限
现在您可以实际下载CSV文件
它将包含访问密钥
一个秘密密钥 让我们下载这个，我们必须使用这个
或者你也可以点击显示
现在您可以将其进一步用于我们的canfireagent
我们需要在这里添加额外的属性作为此代理的一部分
带有适当的密钥和值
这些详细信息将作为值
我们已经下载了CSV文件
我们可以关闭这个 确保你下载
否则你将无法看到这些凭据
如果你忘记了
你只需重新生成凭据
如果它已经在其他地方被使用
你必须确保所有那些区域都更新为新凭据
以及如何生成新凭据
你只需去用户
用户名无非是itv和锁
你点击这个
然后你可以去到安全凭证
你可以看到活跃的一个
你可以删除这个
你可以实际说创建访问密钥
它会为你生成新的密钥
这就是你如何生成新密钥的方式
如果你忘记下载CSV或其他文件
我认为它会生成JSON文件
让我们回顾一下它实际上是只有CSV
所以CSV将包含访问密钥和秘密密钥
你可以用任何标准编辑器打开
让我使用Visual Studio Code打开
我想我在这有Visual Studio Code，你可以看到详细信息
所以这是用户名
这是密钥ID
而这是秘密密钥
秘密密钥从这里开始到这里结束
所以我们必须使用它来在我们的防火墙代理中进行配置
无论如何 使用它 我们需要确保您也具有所需的权限
让我们了解如何为该用户授予所需的权限
通过政策授予一组权限
因此需要创建政策
这些政策需要被附加到该组
因为组已经授予该用户权限
组继承的所有权限都将适用于该用户
政策将直接适用于用户
如果您使用的是aws
实例本身 不需要生成
这些很长
你使用被称为我是角色的东西
E c 二是aws中的一个服务
这也是aws中的一个服务，用于服务间的通信
最好使用称为iam角色的东西
如果需要
我会稍后覆盖，目前
因为我们正在设置canca
这超出了基础设施的范围
我们需要有一个具有访问密钥和秘密密钥的用户
这就是创建的
但是该用户没有权限 让我们详细探讨如何授予权限
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/088_Udemy - Data Engineering using AWS Data Analytics part2 p88 7. Granting Permissions to IAM User using Policy.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


正如我们已经成功创建了用户
让我们将用户附加到策略
以便用户可以向gen_underscore_logs写入消息
underscore_s_3
我们流 我们应该非常小心通过策略赋予用户的权限
权限不应该是打开的
权限应该是什么
让我们回顾在canis中我们有什么
我将在这里作为canis delistream的一部分
我们有一个称为通用的东西
underscore_logs和underscore_s_3，适用于AWS中的每个组件
我们有一个称为arn的东西
arn代表aws资源名称
我猜这就是n for交付流
gen_underscore_logs underscore_s
我们需要使用这个信息
并且我们需要创建一个自定义策略
如何创建自定义策略
让我点击这个
我正在打开一个标签
我们可以去pulse
然后点击创建策略
在这种情况下，它没有要求我们在开始时给我们命名
它要求我们创建策略
无论是使用视觉编辑器还是json来创建自定义pulse
最容易的方法是使用json，而不是视觉编辑器
我们可以点击json
它给我们一些基本的模板
然而 我们不必太担心为几乎所有常见的场景生成json
在线可以获得大多数常用场景的json
你可以通过这种方式搜索
这是自定义策略来写入的类型
Delistream并且你可以点击这个链接
它说使用亚马逊canis控制访问数据
我是我会分享这个作为参考部分资源
如果你滚动
你会在这里找到json
你可以复制这个
然后转到这里
我是管理控制台
替换这个为这个
这是我们的交付流的arn
所以我们需要复制这个
让我复制这个
在这里替换
你可以审查策略
只要政策与用户关联
用户将只能向此主题写入
您还可以查看其他权限
您拥有的权限 它有删除交付流的权限
放入记录批次和更新目的地
这就是用户将获得的权限
我们将向其附加此策略
我们将通过组附加策略
我将稍后向您展示这些步骤
现在 您可以给名称
名称只是itv gen logs策略
现在您可以滚动查看权限
您可以实际创建策略
用户仍然不会继承权限
因为策略没有附加到用户
您可以直接将策略附加到用户或通过组
使用组会更好
我们可以为不同目的创建多个用户
我们可以给所有这些用户所有这些权限
我们将通过组附加策略
如何照顾它
您可以搜索策略
或者您可以点击因为它刚刚创建
然后您可以转到策略使用
然后您可以说附加
您应该能够附加到组或用户
在这种情况下我将附加到组
让我滚动到这里
我应该能在底部看到组
现在你看到组在这里
我组
您应该能够附加策略
这是附加策略的一种方式
另一种方式是您可以转到组并从组界面附加策略
您可以转到组
在这种情况下我们谈论的是itv和锁组
我们可以选择这个
然后我们可以说好的
这里与附加策略相关的选项
我们可以点击此打开
您应该能从这里附加策略
您可以点击附加策略
您应该能在这里搜索
您应该能附加策略
由于策略已经附加
它可能不会作为过滤器的一部分显示
您可以忽略它
我只是想演示您如何从组界面附加策略
你可以从策略接口或组接口附加策略
无论哪种方式对你来说都方便
你可以选择并附加策略到组中
因为组包含用户，一旦附加策略，用户权限就会被锁定
用户会自动继承权限
现在用户权限被锁定
你有权限向主题写入
Gen underscore Logs underscore
当我提到主题时 我指的是cancis stream
主题是我们在kafka中使用的术语
他们对kafka也有基本的理解
我有时也将交付流称为主题
但实际上它被称为一致性流
我们通常讨论谁能向一致性交付流写入
现在是时候配置代理使用这些凭据了 这样gw zero two上的代理就能向一致性流写入消息
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/089_Udemy - Data Engineering using AWS Data Analytics part2 p89 8. Configure Kinesis Firehose Agent.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


正如你已经有了所有必要的组件
现在是时候为我们的canis firehouse agent配置了
以便将我们的消息发送到我们的consist stream
这个stream实际上就是general underscore locks
Underscore s three 为了写入到这个stream
我们需要配置这个agent
我们需要访问
can secret key for the user
我们用它来尝试将我们的消息发送到stream
并且我们还需要传递关于文件模式的额外信息
以及deli
让我们打开这个agent jason文件
位置实际上就是etc aws consists
文件实际上就是agent jason
它是由root拥有的
我登录为training
因此我必须使用pseudo来打开这个文件
只要作为这个用户我有pseudo权限
我应该能够打开它
我将使用编辑器
你也可以使用nano
如果你对它感到舒适
如果你对基于linux的数据不熟悉
例如使用nano etc
你应该能够删除这个文件的内容
然后你应该使用你选择的编辑器
确保你创建了agent jason文件
然后将其复制回这个agent ro
无论对你来说什么是方便的
我们现在就按照那个做现在我可以使用v和pseudo打开这个agent jason
因为文件是由root拥有的
而我是作为training登录的
你可以看到内容在这里
首先我们需要禁用cloud dot emit matrix
因为我们将要发送消息的用户
在此刻只有与delustream相关的权限
无法对locks和score s three进行操作
我们没有为任何aws服务提供任何权限
因此我们需要禁用它
这样做的目的是捕获aws文件生成的锁
并将其记录到云watch中
云watch是一个aws服务
用于跟踪锁
不是lock是locks
现在让我真正地说false在这里
我们可以留下consistent endpoint和endpoint
或者删除任何对你来说方便的内容
现在我们需要配置凭证
当用户创建时，凭证会生成，以防万一
如果你忘了下载凭证
你可以这样做 你可以回到用户
去用户那里，也就是itv解锁
去安全凭证
你将无法看到现有访问密钥的秘密密钥
如果你忘了 你可以删除它
你可以点击创建访问密钥
然后下载CSV文件并用你喜欢的编辑器打开
在这个例子中，我已经使用Visual Studio Code打开了
我应该能够使用这个访问密钥和秘密密钥
配置凭据
这样代理就可以写入流
通用评分锁定和评分s three now
如果我回到这里
我必须同时输入访问密钥和秘密密钥的键和值
那些键和值是什么
如果你回到这个文章
使用这些，我们已经设置了堪萨斯
除了关于下载和安装代理的说明外，我们还设置了500个
它还包含了关于代理的配置说明
你可以在这里点击代理的配置设置
你可以向下滚动
你看到有一个新的层
如果你选择选举ID轴关键
ID AWS秘密访问密钥等等
我们必须使用这两个属性
所以让我复制这个并现在粘贴在这里
我将其留空
我将其替换
我也需要秘密访问密钥
然后双引号
然后逗号 确保你有逗号
在两个属性之后
因为我们仍然有其他的json
需要更新
或者已经存在
有时你可能会忘记这个评论
即便如此，它也可能引起一些问题
我们必须清理这些东西
并不需要 我们可以删除这两行
他们不需要 或者你可以留空
它也不会受伤
但当涉及到将使用的流量时，我们将使用交付流
这涉及到癌症领域
因此我们可以删除这五行
所以我可以说删除财务的五
所有的五行都消失了
我们只剩下一个条目
文件模式必须是我们的访问日志，流必须是下划线日志
分数为3 让我们更新信息
我们从访问密钥ID开始
确保云观察器发射矩阵为false
至于访问密钥ID
我可以去视觉工作室代码并复制访问密钥ID，然后将其粘贴在这里
我没有正确复制
我必须处于插入模式
请确保您非常小心
如果您使用变量
有时您可能在命令模式下粘贴它
它可能不会正确粘贴
现在访问密钥已复制
您可以验证它 然后您可以实际复制并粘贴密钥
现在访问密钥和密钥都已复制
现在我们需要传递访问日志文件，使用完全限定的路径
这就是can_underscore_logs
access.log
至于流，流名就是general_underscore_logs
underscore_s3
所以您可以复制这个并粘贴在这里
所有必需的配置都已完成
您可以实际复制这个
退出并验证文件的位置是否正确
您也可以说tail -f来确认locks正在生成
现在locks正在生成
但它们还没有流入流
因为我们还没有重启代理
让我们重启代理
并检查locks以确保一切如预期 我会在下一个主题中覆盖那些
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/090_Udemy - Data Engineering using AWS Data Analytics part2 p90 9. Start and Validate Agent.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以我们已成功配置了kis
但我们尚未验证
在重启代理之前
让我们审查代理的内容jason
并确保我们有所有必要的信息
Cloudwatch发送的矩阵必须为false
因为我们没有权限写入到cloudwatch
但我们已配置了访问密钥ID和秘密访问密钥
这些密钥具有权限实际写入到aws中的general score locks和score three交付流
这些凭据与用户相关
该用户具有政策
该政策继承了实际写入到一致流中的权限
general score locks和score s three
这是生成锁的位置
而这是交付流
我们有所有必要的信息用于我们的简单流处理管道
现在我们可以重启我们的aws is代理并说sudo system l
我们启动aws hyphen kis hyphen agent
一旦你运行这个
代理将被重启
你需要做的第一件事是检查状态
以确认代理是否正在运行
你可以说status并查看代理是否正在运行
代理正在运行 但这并不意味着一切正在适当地进行为了确保一切正在适当地进行
你可以做 你可以去where
啊 log aws
Cis agent 这是生成锁的位置
你可以进入这个文件夹
然后说ion lt你可以选择最新的日志文件
你可以说tail hyphen f
这样随着锁的生成
我们应该能看到这里屏幕被刷新
你可以按回车键你可以看到详情
目前没有错误
一切正在正确地流动
如果你在其中弄错了任何信息
那么你将开始看到错误
如果你想验证错误将如何看起来
你可以打开agent rod json
所以我可以去
Etc Aws can assist
然后agent json你可以将名称更改为其他内容
让我们说for that is not there
即使它在那里 用户没有权限
因为我们已给予明确的权限
显式权限生成下划线锁
下划线s3现在
如果我保存这个
然后如果我使用此命令重启
现在代理已重启
您可以检查状态
您可以看到它正在运行
然而，如果我们验证锁并说tail hyphen f
您将看到许多错误
因为目标不正确
它说代理正在启动
您将很快开始看到错误
这些错误可能需要一些时间
现在您可以看到其他内容
它说400代码访问被拒绝
因为我们配置的用户
代理的权限仅限于通用评分锁
下划线s3 因为它对其他流没有权限
即使流不存在
它甚至不能验证
这就是它在说
现在访问被拒绝
要解决这些问题
您需要关注您作为代理的json文件
您必须确保您提供所有信息准确无误
在这种情况下名称不正确
因此我们必须将其更改为正确的一个
那就是下划线s3
其他常见错误是提供错误的文件模式
或错误的访问密钥和秘密访问密钥
或它可以是emit matrix为真
对于复杂的代配置，可能会有其他
但对于简单的
您通常会看到访问被拒绝，因为它是关于访问源
这就是这个或目标
或与凭据有关
您可能提供了不正确的信息
因此您可能没有目标上的权限
因此您将看到访问被拒绝
有时您可能会看到无法连接等问题
这意味着端点必须配置
我们不需要太担心端点
只要我们使用aws基础设施之外的服务器
我们不需要显式指定端点
但当您实际在aws中配置时
如果锁在aws本身生成
如果您想在两个不同的vpc中捕获它们
您可能需要担心端点
但通常您尝试在外部基础设施中配置时
那么你不需要太担心端点
你需要记住的唯一东西就是访问密钥
秘密访问密钥 文件模式交付流
如果你有权限
你可以将云发射矩阵设置为true
否则你必须将其设置为false
这是基本的代理配置
让我保存这个
出了这个
让我重启
让我检查状态
因为它正在运行
我们也可以说尾减f
现在你应该能看到成功消息
你应该不再看到错误
锁正在x.log中生成
代理缓冲消息需要一点时间
并将消息以批处理方式发送到目标
我认为默认设置是一b和一分钟
如果我没记错
无论触发哪个
它将 实际上会将消息推送到电视流
这无非就是下划线锁
下划线s3 所以这就是来源
这就是目标
你可以看到它已经通过了92条记录
你很快就会看到已发送消息
话说回来
如果你想调整缓冲设置
你可以去这篇文章
你可以在这里查看其他设置
你有这个初始的溶液文件模式
会有流数据处理选项
聚合大小字节
等等 你可以扩展这些选项
你可以设置阈值
关于时间和大小
这与时间有关
这与大小有关
默认为一分钟
关于时间
关于大小
默认是b
无论哪个先发生
它会触发并通知团队
让我们回到锁这里
你可以看到消息已成功发送到流中
转到堪萨斯
仪表板 然后打开火烈鸟流
然后转到监控
你应该能在这里看到图表
这将实际显示一致性流的进度
将消息发送到一致性流
你可以在这里看到小点
站点不是很大
因此你不会看到图表很合适
尽管正在生成消息
但你不会看到适当的图表
因为发送的消息数量很小
然而 你应该能够通过查看这些锁来验证
以确认消息正在推送到流中
流什么都不做
它将仅将消息推送到s3
你可以通过访问s3存储桶来验证
我们的s3存储桶只不过是itv解锁
然后我们可以实际打开桶
itv-hyphen-gen-logs以查看消息流入此s3存储桶
你可能需要等待一段时间
你可以看到锁
2021 锁是使用的前缀
你可以点击这个
然后零一 然后19
然后22
然后你可以看到实际的文件在这里
你可以打开这些东西
它将包含日志消息
因为它们被写入访问日志中
你可以下载并查看
这就是你应该能够在配置后启动代理并验证
你可以看到命名约定不如预期
在这种情况下，我必须修复流
其中前缀必须在末尾有正斜杠
以便在locks下创建文件夹
这可以通过以下方式完成
你可以去服务
你可以去canis
然后你可以实际去交付流
流只不过是下划线locks和score s3
你可以实际点击编辑这里我们谈论的是目标或目的地
这就是s3
所以我们需要将这个前缀更改为locks斜杠
然后我们应该能够保存这个
当消息正在写入时
它将返回到locks下的subfolder
让我转到s3
现在可能需要一些时间来反映这个更改
因为它在中间使用了缓冲
它不会立即发送消息，而是在缓冲一段时间后发送
你将在配置中看到消息流动
对于gen locks s3流，缓冲设置无非是1MB或1分钟
抱歉 不是1分钟
1MB和1分钟
我猜是的 1 1B和1分钟
我们可以实际上返回到这里进行审查
我们可以实际上转到canis
然后我们可以转到交付流
然后我们应该能够点击通用附件
然后滚动 缓冲设置无非是1B或60秒
到现在我们应该能够看到消息
让我们回到s3这里
让我们进入itv locks bucket
你可以看到这里的bucket
哦我点击了错误的bucket
这不是v app measures
它必须是itv gen locks
让我点击这个
我们还没有看到locks文件夹创建
可能需要一些时间
让我们等到我们看到该文件夹
然后我们将审查文件是否成功写入
现在文件夹已经创建
你可以看到locks文件夹在gen locks下
三bucket 你可以点击这个
然后你的月日期或和文件在这里
你可以下载此文件并查看
你应该能够看到消息，因为它们正在被生成到x.log
所以您可以这样下载
你可以选择这个 然后操作
下载 然后使用您的fabric编辑器打开此文件
在这种情况下我将使用visual studio code
让我们说并让我选择visual studio code
我应该能够看到消息
这就是你应该能够配置一个简单的流处理管道的方式 并在每一步后验证以确保数据无问题地流动
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/091_Udemy - Data Engineering using AWS Data Analytics part2 p91 10. Conclusion - Building Simple Steaming Pipeline.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为模块的一部分
我们已经成功创建了一个简单的管道，用于读取生成到访问日志中的消息
并将其上传到S3
您可以在这里看到文件
我们还下载并审查了正在生成的消息
这就是它们的样子
让我实际上打开这个
我想它不见了
让我再次打开
在这里
我只需要点击 点击此处以打开
我想用
视觉 studio code打开
您可以看到返回到S3的日志消息
因此，消息通过firewas传递流
当我说firewas时，我是指canis
我是通过canis传递流将消息上传到S3
现在您应该了解如何处理此数据
如何自定义传递流的行为
以更合适的方式写入数据
取决于我们将要使用的消费者
并且我们还可能需要探索与自定义
对于代理本身的此文件相关的选项 我们将在模块的后续部分详细讨论所有这些细节
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/092_Udemy - Data Engineering using AWS Data Analytics part2 p92 1. Customizing s3 folder using Kinesis Delivery Stream.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


作为使用kansas构建简单流处理管道的一部分
我们已经创建了一个名为通用分数锁定的流
下划线 s 三 实际上会接收消息
由五个房屋代理人生成并由三个桶写入
实际上你可以向下滚动并前往亚马逊的s三目的地
实际上你可以查看详细信息
水桶名称是 itv 连字符 gen 日志，前缀是 lock
如果你点击这个
我正在使用命令点击器
这样它就可以在不从同一页重定向的情况下在新标签中打开
如果我访问我打开的这个标签
您可以看到锁
这个名字来自前缀
无论您在这里给出的是什么
基于这一点 您可以在这里看到一个文件夹
您可以点击这里
然后默认情况下
子文件夹和锁的名称约定在这里就是四个
两位数的月份
两位数的日期和两位数的日期
在这种情况下 我想为各种原因自定义它
其中一个原因是我想使用像katha这样的工具来处理这些数据
通过为这创建目录表
为了将这些文件夹解释为部分
我们需要有一个名为等于他们的列
因此我们需要自定义文件夹命名约定
通过更改为四位数
第二级年份
等于四位数字 年份或我们想要使用的任何列名
月份和日期的情况也是如此
等等 在这种情况下
让我们理解我们如何可以自定义前缀
以便数据每天更新
以及年份、月份和日期的命名约定
等等
所有年份、月份和日期都会显示，我们不会使用层级文件夹结构
现在，会给定日期在日期层级文件夹中保存数据
让我们详细讨论如何自定义它
当你实际创建交付流时
它提供了关于如何使用时间戳表达式的帮助
为了自定义文件夹命名约定
因为它在这里没有显示
我将创建一个新的交付流
如何做到这一点
点击并选择控制
点击Windows 点击并选择Mac
您可以说创建流
现在您可以给一些随机名称
或者一些测试
或者您想给的任何名称
然后您可以将其他来源留作来源
接下来我们不会处理任何记录
接下来这是我们可以实际配置目的地的地方
如果您仔细查看详细信息
它提供了有关如何自定义文件夹结构的命名约定信息的相关信息
您可以点击这里
了解更多 我会将此作为资源提供
您可以直接使用它
您可以使用时间戳命名空间
并使用表达式提取您正在查找的信息
语法将像这样
您可以说结果等于fireho
作为输出类型
这是我们的输出类型
所以您不需要太担心这一点
这是我们用于月份和日期的
我们将使用它来演示这一点
好的现在 让我回到这里
让我关闭这个
我们不会创建此交付流
将编辑现有流
我们可以说编辑这里
然后滚动到前缀
您可以做 您可以说这里等于月份等于日期等于
然后我们应该做的是
我们必须复制这一部分
您需要在开头有一个感叹号
您也可以复制到闭合括号
我们可以编辑现在
您可以实际上删除dml
好的现在您可以使用相同的东西对于月份的最后一天
确保格式正确
也在格式的顶部
确保您在括号内有时间戳列的格式像这样
您需要在感叹号之前打开括号
您需要在结束处有一个大括号
您需要在大括号的末尾有一个感叹号
您需要斜杠
否则文件名将连接到日期
这不正确 这将实际处理自定义文件夹结构以适应所有文件，话虽如此
当你使用这些自定义前缀时
您还需要为错误设置前缀
我将其命名为erl
或者您想给什么
我将其放在桶级别
因此所有错误都将进入该文件夹
现在我们拥有所有必要的信息以进行自定义
以便文件写入自定义命名文件夹
现在您可以保存它，一旦保存
如果您等待一分钟或更长时间
您将开始看到使用新文件夹结构的文件
所以让我们等待一分钟或更长时间
然后我们将审查位置
Itv 短横线 gen locks here
我已经等了一分钟
现在您可以实际点击锁下的锁和锁桶
您可以看到文件夹等于2021
然后月份等于零一
然后they等于19
您可以在这里看到文件
没有我们的文件夹
这就是您应该能够自定义文件夹结构的方式
在使用consistent delivery stream将文件写入s3时
根据您的要求可能会有其他表达式
您可以使用此处提供的材料解析它们
我将提供此链接作为参考
以便您可以审查
并且您可以根据您的要求进一步扩展 作为您应用程序的一部分
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/093_Udemy - Data Engineering using AWS Data Analytics part2 p93 2. Create Policy to read from s3 Bucket.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们理解如何创建自定义策略
以便应用程序只能从这个桶中读取
桶的名称是itv 斜杠 gen locks
你可以实际上去
我在这里，桶的名称是it
我们有锁 你需要记住这一点
你可以去策略
你可以实际上说创建策略
我们可以使用视觉编辑器来创建策略
在这种情况下，我们试图获取读取
对某个桶的唯一访问权限
如果你通过视觉编辑器探索
可能会变得复杂
可能会有一些示例
我们应该能够谷歌搜索，我们应该能够使用JSON编辑器
我们应该能够直接更新这一点
所以让我们稍后再详细讨论
我是为特定的S3桶
我会提供链接作为参考
一旦我探索
你可以实际上点击这个，你可以使用这个作为参考
然而 它也会给予正确的权限
你需要确保你将星号对象更改为其他内容
我会稍后突出显示这一点，你可以复制这个
然后回到
我在管理控制台这里
实际上不是这个
这个是替换这个
你可以看到源a n这里
所以我们需要理解itv和locks桶的资源
如果我去S3网页控制台
所以让我打开网页控制台这里
如果我去itv和locks桶
让我滚动一下，它显示了所有桶
你可以现在滚动
你可以去itv和locks这里
然后去属性
你可以在这里获取资源名称
所以你可以点击这个
然后你可以去这里，你可以替换模板中的任意内容
在这种情况下，我们试图给所有文件夹赋予权限
这些是桶中的文件夹
这不仅包括文件夹
还包括文件 在S3中
没有文件夹的概念
所有的都是对象 这将为您提供对桶中所有对象的访问权限
现在说到行动
就所有对象而言
我们说星型对象
如果你查看这个提供的材料
它说s三个星型对象
弹性对象行动使用通配符作为行动名称
所有行动声明允许获取对象
删除对象，放置对象和任何其他行动以单词对象结束
所以这种情况下我只想给予只读权限
我不想让消费者删除任何对象
或者将新对象放入这个S三桶中
因此，而不是说塑料对象
我将只说获取对象
所以现在我可以去这个
我是管理控制台
然后说获取到对象
所以它只有列出特定桶的权限
他们从那个桶中获取对象
桶名是itv连字符gen locks
现在我们可以审查策略
然后我们可以给名字
所以名字什么都不是，只是itv gen los three read only
这是策略名称
现在我可以创建策略
一旦我们创建了策略，就可以向用户授予权限
我们可以实际上去用户这里
我们感兴趣的用户什么都不是，只是itv unlocks用户
它已经属于该组
我们只是想将策略附加到该组
这样任何添加到该组的用户都可以
继承S3桶的权限
现在可以从那里读取
我可以点击附加策略
我在组中导航到策略
所以我去了用户
点击用户 然后获取用户所属的组
一旦我在组中
我正在实际附加策略
你可以直接通过组
同样，值得注意的是，策略名称只是itv gen s三读策略
我可以选择这个
然后我可以附加策略
现在我们有了itv gen lopolicy和itv gen lo s三读策略
这主要是为了将消息推送到流中从消防站
这主要是为了消费者从s三读取数据
现在 用户将具有所需的权限
这样我们就可以使用与该用户关联的同一凭据
配置库如auto三
这样我们就可以读取文件 使用基于Python的方法将在后续主题中详细说明
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/094_Udemy - Data Engineering using AWS Data Analytics part2 p94 3. Validate s3 access using AWS CLI.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


正如我们已经附上了这一政策
称为itv gen locks
S三仅用于用户
我 V 通过组访问gen louser
称为itv gen logs组
让我们确保
确保gen louser是否有权限访问s三存储桶内的对象
我现在解锁了
我们可以利用aws cli在我们的Windows或Mac上
或者甚至两个基本系统提供的
我们有aws cli安装以验证aws cli是否安装
如果你使用Windows 你可以使用powershell或git bash
如果你使用Mac，最好与远程服务器交互
如果你使用Mac
你可以打开隧道
即使你想要
你可以打开隧道
你可以直接输入aws并验证
如果你没有看到这样的输出
这意味着你的AWS未安装成功
你需要在谷歌上搜索并解决在AWS上安装AWS CLI的问题
所以安装AWS CLI并选择与你相关的选项，然后继续
一旦你安装完成
然后你可以使用一个名为aws configure的命令
来实际配置用户的凭据
在这个情况下，用户就是itv和locks用户
但是 如果你只说aws configure
它可能会替换现有的凭证
通常我们可以创建配置文件来管理不同账户的凭证
所以在这种情况下我已经有了凭证
你可以实际查看该位置
AWS凭证在home目录下
你可以看到大小是三五六
这意味着凭证已经是该文件的一部分
而我想添加新的
我们可以使用称为此配置文件的东西来做到这一点
所以，在这个情况下，我们不会说aws configure
我们可以说aws configure hyphen hyphen profile
我们可以给定配置文件名称
我给定的名称为itv gen locks
现在 它会提示输入访问密钥
ID和秘密密钥
我们需要输入这些信息
如果你记得 当我创建用户时
Itv 和锁定用户
我已经下载了CSV文件
该文件包含访问密钥和秘密密钥
它还在我的下载文件夹中
文件夹 我可以来这里，你可以看到文件名为新用户凭据
点CSV 我可以用文本编辑或视觉工作室代码打开
让我使用视觉工作室代码打开
你可以在这里看到内容，现在，我可以复制并粘贴这里
以及这个，首先秘密访问密钥
你可以忽略区域名称
这不太重要
甚至默认输出格式可以是节点
现在我已经配置了配置文件
Itv 和锁定这些凭据
然而，如果我说aws s three
它将使用默认配置文件
我想使用这个配置文件
因此，我可以说短划线短划线配置文件itv gen locks
所以每当我想要使用这些凭据
我必须使用这个配置文件
以便我可以进行身份验证
使用这些凭据对aws服务或组件进行身份验证
现在让我运行这个
这可能会抛出一个异常
说权限被拒绝
因为我们没有对策略进行列表桶
操作权限
我们只能列出一个桶
I tv 解锁 但并非一切，现在如何仅列出itv gen locks
桶你必须要明确指定
你可以说itv 生成解锁空格
然后短划线短划线配置文件itv gen locks
现在你应该能够看到itv 解锁中的内容
我们有locks和locks
2021 现在我们应该能够访问此中的对象
在这种情况下，我们感兴趣的是lo
每当您尝试访问对象
如果是文件夹 你必须有
如果我这样搜索
然后它才会起作用 否则它将不会显示该文件夹的内容
你应该总是有一个正斜杠在最后
现在我们感兴趣的是
等于2021
我忘了在最后添加一个正斜杠
这就是为什么它没有显示该文件夹的内容
要获取文件夹的内容
我可以在这里添加斜杠
然后与月份相关
我可以添加month等于zero one
然后斜杠和也 day 等于 nineteen
斜杠并按回车
它将实际显示在此文件夹下创建的对象
自我们启动此管道以来已创建这么多对象
这就是您可以实际验证的方式
您是否可以访问桶中的对象 一旦政策通过组附加到用户
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/095_Udemy - Data Engineering using AWS Data Analytics part2 p95 4. Setup Python Virtual Environment to explore boto3.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在读取这个对象中的文件的追求中
在这个我们称之为桶中，我们有和lo
让我们设置一个基于Python的环境
以便我们可以利用基于Python的自动三库
从对象或从常规文件系统角度来看，这些文件
从三角度来看，它们被称为对象
所以，在这个情况下，我将进入终端
然后，我将实际上进入我的项目文件夹
作为内部部分，我想创建一个名为process gen locks或gen locks的文件夹
S三
我想给它命名为gen lock s
S三 就是这样
现在文件夹已经创建了 让我首先进入gen lock s，首先我想做的是
我想创建一个虚拟环境
以便我可以在那个环境中设置自动三
您可以通过使用Python 3来创建虚拟环境
您不应该使用Python 2
如果您有Python 2
您只需要确保您安装了Python 3 您可以通过运行此命令来创建虚拟环境
Python 3
hyphen m v e和v
您可以给虚拟环境起一个名字 让我起名为gen locks s three v e和v
它将负责创建虚拟环境
但在做之前
让我们回顾一下将用于创建虚拟环境的Python版本
您可以运行Python 3
您可以看到我的Python 3是三点七九
如果您使用的是三点六或三点七或三点八
它应该可以无问题工作
但如果您使用其他版本，可能会有不兼容性问题
如果您在遵循内容时遇到任何问题
尝试使用我正在使用的同一版本
并看看它是否按预期工作
现在让我退出这个并运行此命令以创建虚拟环境
它将在此位置下的S三中创建一个名为该名的文件夹
您可以实际上说source activate以激活此虚拟环境
然后您可以说pip install auto three
它将在此虚拟环境中安装auto three
一旦auto three安装完成
您可以使用提供给您的API来读取S three中的文件
使用这些API
通过船到达三
我将在jule基环境中演示
因此，在这个自动三的顶部，我也设置了dulab在这个虚拟环境中
你可以简单地说pip install jupyter lab
它会为我们安装jupyter lab
现在jupyter lab已经安装
我可以说jupiter lab然后按回车
它会启动网络浏览器
它会重定向到网络浏览器
我们应该能够使用这个交互式地探索bottom three库
我将只是验证是否在环境中可用auto three库
或者不点击这个python three
然后我可以说import bottle three
现在你可以看到导入成功
这意味着我们创建的虚拟环境有两个库
使用这些我们应该能够探索api
并从文件中读取数据
这些是通过我们的流处理管道被摄入到s three中的 让我们进入细节作为后续主题之一
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/096_Udemy - Data Engineering using AWS Data Analytics part2 p96 5. Validating access to s3 using Python boto3.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你是否成功在通用虚拟环境中设置了auto three以及dulab
让我们了解如何使用auto three访问s three桶
我已将duter笔记本重命名为zero
一《探索使用auto three访问s three》
让我删除这个未命名的i y和b
然后使用这个笔记本，它就是《探索使用auto three访问s three》
你需要像这样导入auto three
然后你实际上可以创建一个客户端
你可以给这个对象起任何你想要的名字
这将代表s three的客户端
我在给它一个s三下划线客户
两者到三有一个名为as客户的函数
所以我们可以说auto三点客户
然后我们应该传递服务
所以在这种情况下当我们说s三
对象将属于类型s三客户
如果你传递EC two
它将是后pc二客户，因此等等
根据你想要访问的a服务
你必须创建客户
因此现在
您可以实际检查s three客户端的类型
它只不过是一个核心客户端s three
所以它会暴露所有相关的api，用于访问s three桶和底层对象
现在，s three客户端在没有指定配置文件的情况下创建
它将使用默认配置文件
在我这种情况下，配置文件是针对aws根账户
所以如果我说 s three客户端列出桶
这将给我们桶的名称
你将看到所有的桶
因为我的默认配置对aws下的所有内容都有访问权限
现在我必须配置对抗itv通用日志配置
在创建客户端之前
你应该做的就是
你必须设置一个名为aws_underscore_profile的环境变量
这是你可以配置一个自定义配置来访问s three的一种方式
使用分配给该配置或该配置下的基础凭证的权限
现在我让重启这个内核
让我运行这个导入auto three
然后创建s三客户端之前，添加到单元格中
让我导入voice，这是纯python库
它导出了一个名为as envion的功能
设置默认环境变量，环境变量就是aws_profile
全大写，你可以给名字像这样
实际的值就是我们创建的配置文件
我们在配置凭证时创建了它
所以我使用了aws_configure_hyphen_hyphen_profile_itv_gen_logs
我配置了只读账户针对itv和logs配置文件
所以我要传递这里，我解锁了，然后我可以运行这个
环境变量已经设置
如果我运行这个
客户端将使用此配置文件创建
您可以实际检查客户端的类型
然后您可以运行这个
它将抛出权限被拒绝的错误
因为配置文件下的凭据没有权限列出所有桶
然而它具有访问itv的权限
使用只读权限访问lobucket
因此我们可以使用称为list的函数
下划线objects
让我清除这里输出并说s three client list
下划线objects 让我们获取帮助
如果您不熟悉jupyter问号是帮助的快捷方式
您可以说help of s three client list objects
或s three client list of question mark并运行
您应该能够看到有关此函数的详细信息
您可以获取语法以及语义
如果您滚动
应该可以看到详细信息
这是您应该查看的一个 我认为它在上面某个地方
让我向上滚动
是的 这就是您应该查看的一个
list objects接受多个参数
bucket limiter和coding type marker等
对于这一点 相关的只有bucket和prefix
您首先可以给出bucket
bucket name是itv-hyphen-gen-locks
这是bucket name
让我们运行并看看它是否会列出对象
您可以看到详细信息
如果您想给prefix以获取数据
只与文件夹相关的是等于2021
您可以说prefix等于locks
所以我可以运行这个
您应该能看到结果
它实际上仍然给出locks-2021
我想在这里给出prefix
所以我可以说locks year
让我们运行并查看输出
您可以看到对象名包含等于2021
所以prefix会做所需的魔法以获取具有特定prefix的文件
这就是您应该能够使用适当配置文件访问s3桶中的对象的方式
您只需使用aws configure设置配置文件
通过设置环境变量
这就是您应该能够访问s3桶中的对象的方式
带有适当配置的AWS配置文件
然后你需要创建客户端 你需要进一步处理
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/097_Udemy - Data Engineering using AWS Data Analytics part2 p97 6. Read Content from s3 object.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们了解如何使用Python自动读取s three对象或文件中的内容。
我们早先已经看到如何创建s three客户端
并且验证我们是否能够从s三存储桶获取详细信息
或者不使用个人资料
现在只对特定桶具有只读权限的那些
关于从三个对象中读取内容
你必须遵循这些步骤
你需要使用适当的配置创建一个C客户端。
然后你必须获得一个对象名称
我们可以使用一个称为最低分数对象的函数
可以提供每请求高达一千个对象
你应该理解如何逐步获取更多文件
如果你有超过一千个对象
稍后我们会看到这些细节
在大多数情况下，对象和文件可以互换
就S3而言
我们称之为对象
通常我们称之为文件
通常我们也称之为文件名
但在我们的世界中，我们称之为键
所以对象键和文件名几乎相同
在大多数情况下
我们可以选择一个对象键或名称并将其传递以获取一个分数对象
连同桶名称
获取一个带有字节流传体的对象
我们可以将体解码为字符串
如果数据可以转换为字符串
一旦它被转换为字符串
我们可以使用相关字符串操作函数进行进一步处理数据
根据我们的要求
我们先采取行动
我们需要导入auto three使用配置文件
我们应该能够创建一个客户端来设置配置文件
我们可以使用这种方法，我们也见过这种方法
如果你想了解关于分数对象的帮助
你可以实际上运行这个
你可以查看详细信息
语法看起来像这样
在客户端顶部
你有一个名为分数对象的函数
这些需要什么
不同的论据将主要使用桶和前缀来读取文件或对象。
开始的锁等于如此这般
所以，让我们从桶和前缀开始
随着我们进一步进行，我们将处理其他论点
其他的重要论点无非是标记而已
马克斯键 等等 现在让我在这里清除输出，然后我将调用学校对象的列表。
跳过桶名称和一个前缀
现在您可以运行这个
您应该能看到输出
首先您有响应元数据
然后是被称为内容的东西
我们感兴趣的是内容
您可以看到内容包含一个列表
这就是为什么这里有一个方括号
它包含几个元素
每个元素都是无类型的字典
您可以检查字典这里它包含键last
修改的etag大小
存储类 等等
我们感兴趣的是键
这将给我们文件名或对象密钥
让我们选择一个对象
我将复制这个
您也可以程序化地选择
程序化地选择响应s three _ objects
您可以说内容
您应该能看到内容的详细信息
忽略响应元数据
这将实际返回内容
您可以看到列表中的元素
如果我清除这个输出，按默认
它将显示最多1000个对象
如果您想进一步处理
您需要使用标记和最大密钥
您应该能够获取其他对象密钥
您也可以实际获取lleon s three objects of contents
您可以看到它有1000个
所以默认情况下一次获取1000个对象
要获取一个对象密钥
您可以使用s three objects of contents of zero
因为s three _ objects of contents 什么也不是列表
我们可以使用零来获取第一个对象密钥详细信息
我们可以运行这个
您可以看到详细信息
这是无类型的字典
我们感兴趣的只是键
这就是您应该能够访问密钥的方式
让我复制这个
让我添加一个单元格这里
粘贴并运行
您可以看到对象密钥或文件名
现在我们可以将其分配给s three和score和score密钥
一旦分配
那么我们必须调用get _ object
如果您看一下get和score对象的语法
它实际上需要桶名和对象密钥或文件名
它会实际返回响应
你可以在这里看到语法
最重要的参数无非就是桶和键
你必须提供这两个
你应该能够获取到主体
响应将包含一个名为主体的属性
类型是字节流
我们需要进一步处理
所以让我们在这里传递桶名和键
实际上这应该是一个三对象键
不是对象键
现在我可以运行这个
你可以得到这个s three对象的类型
让我首先得到这个类型的信息
它是字典类型
你可以忽略这个
因为我们已经知道如何处理字典
现在你应该能够查看对象的详细信息
你可以看到它有响应元数据
然后它有body，body是瓶口流体类型
我们应该能够通过说s三个对象的身体来获取身体
获取这个对象上可用的功能列表
我们应该能够在这里运行帮助
你可以实际上下滚动并查看所有可用函数的详细信息
主要功能是读取
你可以读取，只要文件不大
我们可以使用读取，我们可以读取整个文件
如果文件很大
那么你可能需要分块读取，为此你需要探索分块
在我们的情况下，我们经常将文件写入s三
文件大小不是很大
因此我们应该可以直接读取
让我清除这个输出
然后让我读取正文的内容
你可以看到它正在被读取为字节流
因为你以b开头
然后现在所有的东西都在单行代码中
让我清除这个输出
你可以将这个字节流转换为字符串
通过在读取输出上调用decode，使用uf-8
只要它是自然字符串或自然英语
它将无任何问题地工作
让我运行这个
它实际上从内存中清除了
这就是为什么你在这里看到空字符串的原因
你应该验证的方式再次
你必须运行这个
然后你实际上可以运行这个
我们应该能够看到输出字符串 a 一旦我们读取并查看内容
内容已经从内存中清除
这就是为什么当我们运行这个时
它将显示空字符串
现在显示实际字符串
因为我们直接运行这个后
一旦对象创建并直接使用body上的read
然后调用decode将字符串转换为
这就是为什么现在起作用
让我清除这个输出，让我们看一下完整的代码
所以我们只需创建客户端
我认为创建客户端的步骤在这里缺失
所以我也必须添加s three _ client
等于auto three. client of s three
一旦客户端创建
我们只需调用list objects
从生成的1000个对象中获取一个对象的键
然后将该对象键
与存储桶名称一起传递以获取响应
响应包含body
我们必须使用read和decode
实际将文件的内容转换为字符串
现在运行这个
名为contents的文件将包含字符串
这将是一个非常大的字符串
我想将这个字符串转换为列表
以便我可以进一步处理它
你可以像这样使用split lines
文件和score records将包含字符串中的每一行
作为列表中的单个元素
你可以在这里看到
记录类型为列表 我们可以看到列表中的前三个元素
这就是你应该能够从s three对象读取内容到字符串 并进一步处理它
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/098_Udemy - Data Engineering using AWS Data Analytics part2 p98 7. Read multiple s3 Objects.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们理解如何将多个对象的内容读取到集合中
我们之前已经看到了如何将一个对象的内容读取到集合中
现在我们谈论的是将多个对象读取到一个集合中
首先我们将使用适当的配置文件创建客户端
然后，我们需要获取十个对象的键名或对象键
我们需要使用客户端来完成这一点
一旦我们获得了这些键
我们应该能够将这些键传递给对象以获取对象
我们应该能够读取内容
然而，我们需要将十个文件的内容读取到一个集合中
在我们这种情况下，文件大小非常小
因此，我们可以将多个文件的内容合并到一个集合中
对于大型对象或大型文件
我们可能需要将大型文件对象分成不同的块，并将其分配给一个集合
然后进行进一步处理
因此，根据对象的大小
我们需要制定一个策略
让我们开始行动
让我们导入auto three
让我们创建客户端
让我们获取十个对象
这样会处理获取十个对象详细信息的问题
我们应该能够审查内容
你可以在这里查看详细信息
现在你应该能够使用简单逻辑获取键或对象名称
我们正在尝试迭代s中的三个对象内容
然后通过这种方式提取键
这将返回一个列表
我们在列表中进行排序以创建一个变量
称为s three underscore object和 score case
写的逻辑与这个相同
这也会处理同样的事情
我们只是想将这些键放入一个集合中，这些就是这些
集合的名称就是 three underscore object underscore keys
你可以运行这个，然后验证
我们只获取键
你也可以运行这个，使用任何一种方法验证
一旦你获得了你想要获取内容的十个对象的键
你应该能够将内容放入一个集合中
我们已经看到了如何获取一个对象的内容
这是逻辑
我们将一个对象的键存储在这个变量中
我们称之为three和score对象和score键
我们调用了 获取带有bucket的_object
这是s three _object和score键
您可以在这里查看详细信息
响应，实际上就是一个score对象
包含主体
主体类型是bottle core streaming body
它有一个名为read的功能
我们应该能够调用它
read将负责将对象的内容读入内存
以字节流的形式
我们应该能够使用decode将其转换为字符串
这将负责将内容转换为字符串
然后我们可以使用像split lines这样的函数
将对象的内容转换为字符串列表
在这种情况下 文件的每一行都将成为列表中的元素
一旦我们创建了列表
当我们遍历所有属于这个three_on_score_update_on_school_keys的元素时
我们应该能够将其连接到一个综合列表中
我们可以将所有列表合并到一个列表中
如果你有两个这样的列表
l_one等于one two three，l_two等于three four five
你可以使用加法运算符
你应该能够连接两个列表
在这种情况下 当我们遍历所有对象键时
一旦我们获得一个对象的内容
我们将其分配给一个综合列表
当我们获得其他文件的内容时
我们将其附加到我们的原始综合列表中
我们将稍后详细讨论这一点
让我们运行这个并确保加法运算符按预期工作
如果你看逻辑
我们初始化了一个数据为空列表
这将是一个综合列表
它将包含所有10个文件的内容，以字符串列表的形式
然后我们遍历three_on_score_update_on_school_keys
它只是一个对象键列表
我们在每次迭代中获取对象键
然后使用对象键和桶
调用get_object
它将有一个body
body的类型是boto_core_response_streaming_body
然后 我们调用read
然后解码将内容转换为字符串
然后我们实际应用了split lines
这将从文件中提取字符串列表
我们将其附加到s_three_on_score_contents中
我们将其附加到data中
当我说data += s_three_on_score_contents时
它只是data = data + s_three_on_score_contents
这就是你应该读取所有10个文件的内容的方式
并将其合并到一个综合列表中
让我们运行这个，然后运行这个以获取数据的大小
我们得到了854行
称为数据 如果你只读一个文件
大约你会得到80到90行
你可以验证
如果你对这个代码感兴趣并运行它
我认为这将起作用
让我们运行这个
如果它失败了 那么我们会看到
所以 我正在尝试看到我们将从所有十个文件中获取内容到数据
验证非常重要
你可以在这里看到详细信息
我们可以实际上说
文件长度
下划线内容 点分割行
它将给我们集合长度
由分割行写的
如果它也写接近854
它也给出854
然后此结果不正确
你可以看到它只写122
这意味着我们从所有十个文件中获取数据到一个集合
称为数据 现在你应该能够进一步处理
通常我们应用什么样所需的转换
根据要求 然后将数据写入数据库或文件
这是典型的端到端管道
我们将在后续时间进入那些详细信息
目前我们已经看到如何从十个文件中读取内容到一个集合
只要文件大小较小
这是我们使用的一种策略
如果文件大小非常大
那么我们必须将每个文件分成多个列表
并且我们必须进一步处理那些单个列表
然后将其保存到目标
根据大小的
我们必须划分策略 我早些时候已经强调了这一点
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/099_Udemy - Data Engineering using AWS Data Analytics part2 p99 8. Get number of s3 Objects using Marker.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们详细讨论一下我们如何可以得到三个对象的数量
当三个物体的数量超过一千时
我们需要使用标记的概念
这主要是为了解释标记的概念
我们理解标记对分页的重要性
使用auto three获取列表对象并获取包含三个元素的对象的数量
从给定的桶获取S三对象元数据的一种方法是
使用列表 下划线对象
然而，列表和分数对象最多只能获取一千个对象的元数据
我们需要使用标记分页
我已经这样做了 直到我们获取到所有对象的详细信息
以下是我们可以遵循的步骤，以获取存储桶中S3对象的数量
使用标记 创建一个带有适当配置文件的S3客户端
使用标记逐步调用对象
直到获取到所有对象的详细信息
获取内容列表中的元素数量，并将其添加到对象计数中
当内容列表的大小小于一千时，我们可以退出循环
或者当比赛不存在时
作为响应的一部分
我们将使用第二种方法
我们将终止
当内容作为响应的一部分不存在时
让我们导入auto three
让我们创建客户端
然后运行这个
这将处理获取一千个对象
如果你有超过一千个对象
如果你少于一千个对象
它将获取对象的数量
让我们运行这个
你可以得到所有键作为响应的一部分
通过使用s three underscore objects上的键
s three underscore objects只不过是字典类型
因此，输出责任被分配给我们
由于list on score objects只不过是字典
让我们运行这个 你可以看到我们有哪些键
所以内容键是描述对象键的详细信息
其他键也与这些键相关
还有其他键，如标记
最大键 等等 让我们看看标记的一部分
因为我们没有输入任何东西
它是空的 我们使用默认的最大键
因此我们有几千个最大键
但是实际获取的对象详细信息数量是通过这条s三对象内容线获取的
你可以看到有几千个，这意味着
即使我们可能默认有超过一千个
每次迭代中只获取一千个对象详细信息
关于使用对象列表来获取对象详细信息
现在我们需要获取最后一个对象详细信息
这就是你可以实际获取s三对象内容类型方式的方法，它不是列表
我们可以在列表中获取最后一个元素详细信息
通过说list[-1]
这将给我们提供列表中的最后一个元素
列表中的元素包含不同的关键和关联值
因为列表中的元素类型什么也不是字典
而我们正在尝试获取键
以便我们可以使用它作为标记
因此，最后一个元素的键可以用作后续运行进行标记
以获取下一千个或更少的文件
我将这个升序标记
标记是一个变量
它将包含这个键
现在我们应该能够将这个键作为附加参数传递
到s三对象上，加上桶和前缀
以便我们获取下一组对象
让我们运行这个
这将花费一些时间
因为它正在运行 我们应该能够验证标记
现在，你可以看到传递给这个s三对象的内容
什么也不是这个，因为标记作为part of markov属性的返回作为部分返回
在s三和s三对象的响应中，什么也不是
和s三对象的内容类型是列表
它包含380个对象
因此，s三桶中的总对象数量什么也不是1380
现在
这就是你应该能够迭代的方式 直到我们获取所有对象详细信息并获取计数
我们所看到的
我已经包括作为部分这个
以获取每个迭代中的s三和s三对象的内容
我已经使用了get get不会失败
如果内容不存在为响应部分
它将返回nothing
如果有一个键内容作为响应部分
值将返回
所以either和s三对象将是列表
或none
在这种情况下，作为第一部分两个迭代将获取一千和三 分别
在s三和s三对象中
作为第三迭代的一部分
它将不会得到任何东西
然后我们通过使用这种逻辑跳出循环
否则 如果s three和score对象是列表
我们只是获取s three和score对象的长度
这与这个相似
并添加到object_underscore_count中
object_underscore_count最初被初始化为零
在每个迭代中，长度被添加到它中
当我们走出循环
对象计数将达到总对象数
我们作为s3桶的一部分
也是从一开始就 市场被设置为空字符串
当我们传递标记时
它将首先获取
一千个对象 如果你有超过一千
然后在每次迭代中我们捕获标记
我们正在将该标记作为参数传递
在调用liston score objects时
以便获取下一组对象
您可以在这里看到获取最后一个标记的逻辑
现在您可以运行此代码
一旦执行完成，您应该能够看到输出
然后当你运行score count时
您可以看到在这里是180
我们之所以看到此输出，是因为我们在每次迭代后打印了标记
很多时候我们可能需要控制批处理大小
为了做到这一点，我们可以传递一个名为max is的额外参数
我们应该能够以少于一千的数据批次处理数据
所以我们可以在这里使用max is
假设 200
所以我们一次尝试获取200个对象
并在每次迭代中查看每个批次中有多少对象
我们将对象计数添加到计数中
最后我们将返回对象计数
现在我们应该能够运行这个数据
你将更频繁地看到输出
因为我们一次迭代200个
经过七次迭代
我们将能够处理所有1380个对象
我们将能够获取对象数量
让我们等到这被运行
然后运行这个
你应该能够看到输出
这就是你应该能够使用标记的方式
以及max用于分页和处理你的数据
我们理解了如何获取对象数量
使用标记和最大键
现在让我们深入了解如何获取我们所有对象的大小 作为S3存储桶的一部分
```

### /content/drive/MyDrive/bilibili/Udemy-DataEngineeringusingAWSDataAnalyticspart2/100_Udemy - Data Engineering using AWS Data Analytics part2 p100 9. Get size of s3 Objects using Marker.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们来详细说明一下如何获取s three对象的大小
使用max案例和标记作为score对象的列表中的一部分
我们将在获取s three对象计数的基础上进行改进
这是我们用来获取s three对象计数以获取大小的代码
我们只需要在每个对象中使用size属性
我们应该能够将其添加到获取大小中
代替获取s three对象长度以获取计数
我们需要使用简单的for循环在每次迭代中获取所有对象的大小
让我们详细说明一下，以便您理解
这里是我们需要遵循的步骤，首先
我们需要创建一个合适的客户端
然后我们需要调用列表对象在页面上使用max case和标记
列表对象大小中的每个条目
除了键和其他详细信息
我们需要添加每个条目的尺寸
以获取我们s three桶的总尺寸
每个条目的尺寸将以字节为单位
你可能需要使用一些开源库将其转换为兆字节
让我们创建s three客户端
让我们运行此以获取所有键
我们有输出中的一部分
我们有内容和几件其他东西
在这种情况下，我们必须使用我们作为内容的一部分的内容。内容包含列表
如果你实际上得到第一个元素的详细信息，像这样
你可以看到它是字典类型
我们这里有键名
在上面 我们还有尺寸
所以我们必须选择这个尺寸并将其添加到总尺寸中
从这个字典中获取大小
我们能做的是，一旦获取到元素，
我们可以只传递属性size
我们应该能够获取到size
这次s three _objects里有一千个对象的详细信息
我们只需要获取每个对象的size，
然后累加得到所有一千个对象的总size
这就是我们如何照顾到
我们可以创建一个全局变量叫object on score size并初始化为零
然后我们可以遍历所有的s three on score objects在每次迭代中
我将其命名为s three on score object
我们应该能够通过说分数对象的大小来获取详细信息，像这样
我们只需要在这两个对象上添加分数大小
要么 你可以说来自学校大小的对象
加上等于s三个大小对象
或者分数大小的对象
等于分数大小的对象
加上分数大小对象的s三
让我们运行这个
它失败了 它说字符串索引必须为整数
在这种情况下，我应该使用像这样的内容
现在我们应该能够运行这个
它失败的原因是
我正在直接使用s三在score对象上，并且score对象是类型
它有一个名为contents的属性，contents
包含对象列表
我们必须仅处理对象列表以获取对象列表
我可以说三score对象的内容像这样
我应该能够在这里打印对象大小，只是说对象_大小
然而，它将以字节为单位，为了以兆字节为单位
我们可以使用此文件大小库
我不确定它是否是标准库
但它能满足目的
我正在安装它
然后我正在导入size来自harry文件大小
我正在使用size获取所有对象的大小
我们有15兆字节大小的文件作为第一部分
现在我们必须获取所有数据的总大小
我们必须捕获标记
并且我读取直到所有数据都被处理
我们总是以获取对象数量的方式看到这一点
我们只需将此for循环替换为对象的长度
它将给我们带来使用此前缀的所有对象的大小
让我们运行这个
它将打印标记
我们有1380个文件
并且它将打印标记两次
然后我们应该能够运行这个来验证总大小
我们可以运行这个以获得兆字节大小
我们有20兆字节大小的数据在itv-hyphen-gen locks中，使用此前缀
这就是你应该如何使用标记以增量方式获取文件
并再次获取总大小
我再说一遍，如果你想控制最大键的数量
你可以像这样传递max keys参数
你可以传递你想要传递的任何数字
它应该小于1000
然后你应该能够以可管理的数量处理对象 然后你应该能够处理对象
```