### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/001_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p01 001 Welcome!.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


你好，欢迎来到这门课程
我叫尼科莱·舒拉
我很高兴能成为您在这门课程中的导师
我知道您想尽快开始
因此我们将立即开始
我只需要花一到两分钟时间来快速自我介绍
我是尼科莱，我的热情是数据
以及教学
我是一名数学家
我开始作为一名商业智能顾问工作
然后转型为数据科学家
后来我更多地转向数据工程方向
所以你看到我没有学习计算机科学
所以我必须从零开始学习一切
我相信这正是帮助我意识到学习那些技能时所面临的挑战
因此这门课程不仅可以从我丰富的经验中受益
还可以从我面临的挑战中受益
现在我只想把这些见解与你分享
因为我说过，教学加上数据是我的一大热情
我真的想看到你成功并实现你的职业目标
这正是给我最大动力的事情
顺便说一句，我是德国人
所以如果你注意到一点口音
这就是来源
如果你想联系我
最好的方式是通过领英或Instagram
这也是我分享更多关于数据工程和数据科学的见解的地方
在你完成这门课程后
你也可以提到我
我真的很高兴祝贺你
在Instagram上
我还会分享一些更个人的事情 所以请随意关注我
我真的想让你从这门课程中获得更多
这就是我为什么鼓励你设定一个目标
以在特定天数内完成这门课程
比如20天，每天安排30到60分钟
并跟进这门课程，投资于你自己的技能
我相信对自己做出承诺，跟进这一点，会有很大不同
你也可以在播放器中调整播放速度
有些人告诉我我讲得太快
所以调整这个可能会帮助你适应
所以如果你准备好了
那么我们一起开始这段旅程
设定你的目标，记住我在这里支持你每一步
我很期待看到你的进步
所以让我们立即深入其中
设定你的目标，记住我在这里支持你每一步 所以我很期待看到你的进步
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/002_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p02 002 About the exam & this course.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在在我们深入探讨所有单个考试主题之前
我认为先快速谈谈这门课程的设置是有意义的
这样你们就知道可以期待什么
我们也会简要谈谈考试
这也是重要的
这样你们就知道可以期待什么，并以最佳方式应对
当然，我们是为了数据工程师副学士认证而准备这门课程
所以我们要确保你们对这场考试有最佳的准备
但为什么这实际上有意义呢
让我们快速谈谈这一点
因为实际上这对你的职业生涯可以产生巨大影响
以专家身份定位自己，而这些认证在行业内被高度认可
所以这可以真正以专家身份定位自己
这应该是一个很好的动机，让你们坚持下去
即使有时准备考试可能会有点困难
我知道对我们所有人来说，找到时间准备考试是很困难的
像这样 但这真的很值得
这对你的职业生涯可以产生巨大影响
以专家身份在这个高度需求的领域定位自己
所以我想在开始时发送一点小小的激励
我们将涵盖的内容
当然，所有考试要求的内容
这是这门课程的目标
在这里，你会看到关于这场认证的详细信息
如果我们点击这个链接
我们会看到这场认证的详细信息
也会看到考试的信息
这是一个副学士认证
考试时间为一百三十分钟
但你作为非英语母语者可以额外获得三十分钟
因此，在本课程结束时，我们也会快速覆盖
你如何获得那些额外三十分钟
这相对容易
当然，我们也会走遍整个考试设置和时间安排
在本课程结束时
所以你可以期待六十五个考试问题
要么是选择题，要么是多选
这里 选择题
一个选项是正确的
多选
多个选项是正确的
考试费用为一百五十美元
我们会走遍这是如何设置的
以及如何在本课程结束时注册考试
这里重要的是
如果你滚动到考试指南
在这里你可以看到考试的详细信息
他们说你应该至少有一到两年的实际经验
两年到三年的数据工程经验
我认为如果你能密切跟随这门课程
你将能够做到
无论怎样
但这是aws推荐的
这里如果我们滚动下来
我们可以看到重要的部分
那就是这个考试是关于什么的
在这里你可以看到这四个领域
这门课程将帮助你为每个单独的领域做准备
然而，这门课程的内容，我认为有必要以稍微不同的方式组织它
所以我只是想让你知道，课程的结构可能会看起来有点不同
但是实际上，这门课程中的所有内容都涵盖了考试范围
所以这里看起来可能有点令人畏惧和复杂
所以我已经确保了这一切以有意义的方式组织
所以如果你滚动下去
这门课程实际上非常紧密地与考试范围相一致
但有时它仍然与这有很大不同
所以这里我只是确保所有与考试相关的内容都被覆盖了
所以这里我已经确保所有与考试相关的内容都被覆盖了
有时候你会看到
比如一个主题，比如s和s
或者一些可能对考试来说不是特别相关的东西
比如我们有aws config
例如这可能不是考试中最相关的主题
因此，我们也相应地稍微简短地覆盖它
另一方面
例如，像aws glue或亚马逊红shift这样的服务
这些对考试来说非常重要
因此，当然我们会深入探讨它们
所以我只是想让你知道，你不会感到惊讶
因为某些主题比较长，某些则比较短
这实际上是有意为之，因为它对考试的相关性
当然，我们也有为考试准备的示范
这只是实际理解，看看这是如何工作的
即使这并不为考试所必需
我认为这非常有用，因为它有助于理解和记忆
同时也有助于获得实际技能
这样你就能真正看到它在实际中如何运作
即使这不为考试所必需
我强烈推荐进行这些演示
这使得它更加实用
本课程的目标是轻松通过考试
这是一个具有挑战性的考试
但我也知道，通过这种准备，你实际上已经为通过考试做好了充分的准备
此外，你还会对数据工程的所有主题有一个良好的实际理解
最后，及格分数是一千分中的720分
本课程的目标是让你达到850分以上
我认为这是非常现实的
如果你紧密跟随这门课程并完成我们所列出的所有步骤
所以这是关于这门课程
还有关于考试
我们也想简要说明一下，我们将在下次讲座中设置一个免费试用账户
所以你也可以这样做
这不必要
但这是一件好事
这样你可以获得额外的实践知识
你有考试信息
如果你跟着我们刚刚看到的这个链接
以及考试
如我们所说 考试需要130分钟
但你可以作为非母语人士获得额外的30分钟
这就是你可以这样做的方式
我们将在课程和考试结束时也看一下考试问题
我们有65个问题
要么是多选，要么是单选
这些问题是基于场景的问题
所以你需要找到最合适的解决方案
这意味着可能是最简单的解决方案
或者可能是最可靠的解决方案
或者你有一些要求
比如它应该高度可用
对于这些场景
你应该找到最佳解决方案
例如 你需要设置一个etl过程，从s3桶中提取数据
转换数据并将其加载到红移中
这将是最简单的解决方案
正确答案是aws glue
你可能也能用其他方式做
但你有最简单的解决方案
在这个情况下将是glue
这只是一个考试问题的例子
当然，我们会为考试提供所有必要的东西
这就是关于这门课程和考试的所有内容
现在我们将深入主题，开始课程和主题 希望你们兴奋，下次讲座见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/003_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p03 003 Important tips for this course.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我现在想给一些额外的小贴士作为食谱
基本上为了通过考试
那么你该怎么做
所以首先这很简单
你只需要基本上按照这个课程的每一步骤来
在这里我们一步一步地列出了所有内容
我也推荐观看演示
这样你也会对主题有更深的理解
我推荐要非常一致地做
我推荐每天至少做30到60分钟
即使少于这个时间
持续做也很好
这样你真的会坚持下去
我们也有多个测验
这有助于巩固知识
另外你也可以使用幻灯片
我也会把它们给你
你可以阅读它们
快速回顾一些重要的事情
重要的是也要做练习测试
这也是课程的一部分
在最后 我们会有一个完整的模拟考试
这对你了解考试非常有用
也了解你的知识
这样你就可以做好充分准备
你也能发现你的弱点
并消除它们
最后你可以预约考试
我们也会讲解如何做
我会给你一些最后的考试技巧
然后你就可以自信了
因为你知道你真的为考试做好了准备
然后你就可以通过考试
在我们开始之前有几个非常重要的小贴士
在我们开始一些讲座之前
你也会找到一些额外的资源
你会看到这个网站
有一个资源选项
然后你可以下载一个文件
如果这有必要
所以你会在所有讲座中找到资源
如果你有任何问题
你可以在视频下方找到
也有一个问答部分
你也可以在那里提问
此外你在这个课程的早期也会被提示
留下评分或评论
我对何时出现没有影响
但是很早
当然，收到你的评价我非常高兴
这对我来说意义重大
但如果对你来说太早了
你也可以点击
稍后问我或课程结束时问我
当然，一旦你完成这门课程
请随意在领英上分享
我非常高兴祝贺你的成功 那么我们开始，深入探讨我们的第一个主题
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/004_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p04 005 Signup for AWS Free Trial.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看我们如何在AWS上设置账户
您只需访问aws
Amazon.com，在这里您可以创建AWS账户
所以我们只需点击这里
在这里我们需要设置一个根用户电子邮件地址
这是我们的主要电子邮件地址
根电子邮件地址
我们稍后可以设置额外的用户
额外的电子邮件地址
但这将是根用户
在这种情况下，我将在这里设置我的电子邮件地址
然后，我们也可以使用AWS账户名称
这仅帮助我们
如果我们有多个账户，了解这个账户的目的
例如，我们可以有一个产品，我们可以有
例如，我们可以有一个产品类账户和一个非产品类账户
或者如果我们有不同的部门
我们可以有不同的账户用于不同的部门
在我们这个案例中 当然，只需设置一个单一账户是可以的
在我们提供电子邮件地址后
我们需要查看我们的电子邮件账户
来找到这个验证代码，我们需要复制
然后在这里粘贴，仅用于验证我们的电子邮件
在我们完成之后
我们可以设置我们的密码
然后，我们需要提供一些联系信息
然后，我们需要提供我们的账单信息
正如这里提到的
在这种情况下，我们不会因为使用低于三档限制而收费
但需要有
当然，用于超出的费用的信用卡
在下一讲中
我们也会快速查看账单并为此目的设置预算
所以这里我们也要注意，在这种情况下，会有一个一美元的
仅用于验证您账户的交易金额
这将用于验证您的身份
您将得到回扣
或者您实际上不会为此付费
这将被退还
然后，在我们验证信用卡后
我们现在也准备好确认我们的身份
这可以通过验证代码来完成
例如，在这里我们需要提供一个电话号码
以进行快速身份确认
在我现在提供了我的手机号码后
我只需在这里输入
我从AWS收到的短信中的验证代码
所以我将输入此代码，然后点击
继续这里关于支持计划的讨论
当然我们可以使用基本的支持
也就是免费的
然后点击完成注册
这样我们就已经在aws设置了账户
现在我们可以去aws管理控制台
在这里我们可以决定是否用根用户登录
就像我们之前所说的，这是一个账户所有者
根用户
之后我们也可以设置iam用户
对于日常任务
我们也可以使用这些iam用户
现在我们将使用根用户登录
然后我们完成这个
现在我们在aws管理控制台
当然现在我们想要了解这是如何工作的
获得一些概述，就像我们提到的设置我们的预算 这就是我们在下一节课要做的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/005_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p05 006 AWS S3 - Basics.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在让我们谈谈aws的主要构建块之一
这是一个w s three
这是主要的建筑模块之一
因为它是我们的主要存储解决方案
所以这就是为什么这是我们aws账户中最重要的一些事物和服务
因为我们总是需要存储
并且这是解决这个问题的主要方案，其中"s"代表简单存储服务
这就是它旨在提供的，并且它正在做的。
它是一种成本效益高且简单的所谓的对象存储
这就意味着我们有所谓的桶
它们只是我们文件的容器
然后我们有对象
这些是我们可以存储在我们桶中的实际文件
正如我们在这里看到的那样，这是我们的组织方式
在这里，我们可以看到我们在s3中选择了一个桶
在这里，我们可以看到这一点
网页界面 我们可以在这里也有一个文件夹结构
因此，我们也可以通过文件夹来组织这些
但我们也可以直接将文件上传到我们的桶中
所以我们可以登录我们的账户
在这里我们也可以管理这些
上传文件等等
让我们在实际中看看
也仅仅在一瞬间
正如我所提到的，我们有这些桶和这些对象
重要的是每个这些桶
在这里我们可以看到，它们总是基本存储或创建于一个给定的区域
这就是数据实际上存储的地方
所以我们有 当然
AWS 他们有一些数据中心
数据需要被物理存储在一个位置
这就是我们选择的区域
这可能当然对延迟有重要影响
因为我们通常希望将文件放在与其访问地点接近的地方
也许应用程序被访问的地方或数据被用户访问的地方
以便我们有较低的延迟
但我们也可以为合规原因放在不同的区域
因为可能不同地区有不同的监管合规性
因此，重要的是每个桶总是要在特定的地区创建
在这里我们可以看到，关于这三个桶的另一个非常重要的事情是
是每个桶必须有一个全球唯一的名称
这意味着在所有地区和所有账户中
所以我们不能在全世界的任何一个桶有相同的名称
这对所有地区和所有账户都是真实的
此外，我们还有几个相对简单的命名约定
它必须有3到63个字符
我们只能使用小写字母
数字 点和短划线
正如你在这里看到的
所以我们只使用这些小写短划线和数字和字母
并且我们也必须在开始和结束处使用字母和数字
这必须是一个字母或一个数字
我们不能使用IP地址作为名称
所以这只是一些基本的命名约定
现在我想介绍一点理论
但正如这是非常基础的
我们想讨论这个问题
而且这也不是很复杂
这就是键的概念
键是识别对象的东西
所以每个对象基本上都有一个键
这可以有不同的形式
所以对象的一个简单键示例是
我直接将我的文件example dot text上传到我的桶中
没有使用任何文件夹
直接上传到桶中
然后文件名也将是我的键
这样我就能在我的桶中识别出这个对象
然后我们有例子，我们可能会有一些文件夹
所以这个文件夹可以是文档文件夹
我将示例文本上传到桶中的这个文件夹
那么文件夹路径和文件名的组合将组成我的键
所以你看到我们没有桶的名称
键的一部分
只有桶中的全路径
这就是我们的键
我们可以利用这个来识别存储桶中的对象
但现在我们的s三服务有哪些重要应用场景呢
正如我们所提到的，我们可以使用各种方式
无论我们需要存储什么
这通常可以通过s三来实现
因此它的应用场景非常广泛
我们可以用它进行备份和恢复，基于这个构建网站，或者当我们需要为应用程序存储数据时
或者当我们需要存储数据以备档案时
这也可以在s三存储桶中存储，或者用于数据归档
当然，也可以基于s三存储桶构建完整的数据湖
所以为了分析目的
我们也可以 当然可以存储数据
因为我们可以存储结构化数据
非结构化数据
我们可以 如你所见
甚至可以存储pdf文件
所以任何类型的文件都可以存储
然后稍后我们可以访问那些文件
也许比如 就像半结构化的CSV文件
我们也可以使用亚马逊Athena查询数据
但这是我们稍后查看
当然
我们可以看到
我们有广泛的用例
因此，我们已经快速想要介绍不同存储类以适应这些不同用例的概念
它们具有不同的功能
因此，正如我们稍后想要详细讨论的那样
我们将在后续实践中看到更多
所以，现在我们只想介绍基础知识
因此，我们有这些不同存储类，它们针对不同的用例设计，具有不同的功能
基本上，它们可用性也不同
正如你所看到的，耐久性在所有存储类中总是相同的
你可以看到有很多九
因此，我们偶尔会称之为11个九
那么这实际上意味着什么？耐久性
这是指在一年中丢失对象的可能性
正如你所看到的，我们拥有11个九
这意味着在一年中我们可以预期丢失一百万个对象中的一个
因此，使用特定文件出现错误的可能性非常小
然后可用性的概念，这有点不同
这仅仅是服务的操作时间百分比
这意味着我们可以访问数据的百分比
当然
有时我们想要自动根据文件的年龄更改文件的存储类或文件夹 或者整个存储桶
因此，我们有生命周期规则
例如，在文件前30天，我们可能想要非常频繁地访问数据
因此，我们使用存储类3
因此，我们使用标准存储类
然后，在30天后，我们可能想要自动将其放入某些存档中
然后，我们可以使用不同的存储类
因此，使用这些生命周期规则
我们可以自动定义随着时间的变化
正如我们稍后也将详细讨论的那样
同样，关于版本控制
它允许我们访问文件的先前版本
因此，如果我们可能对文件进行一些意外修改或删除
我们还可以保留先前的版本
因此，我们可以使用版本控制获得一层额外的安全
所以，这也是我们可以启用的
正如我们所提到的
我们稍后将详细讨论这些具体细节
现在我们只想了解基础知识
因此，现在我们已经了解了这一点
现在我们想看看在实践中这看起来如何
因此，现在我们只想了解基础知识
因此，让我们跳入我们的aws账户
然后看看如何创建这些存储桶 看看这一切在实际中是如何工作的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/006_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p06 007 Create a Bucket in S3 (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看在实际中这是如何工作的
当我查看我的账户时
我总是可以通过这个搜索栏搜索我们正在使用的任何服务
在这里，我们希望使用S3
所以我可以只是搜索三
我发现了这个服务S3，现在我可以看到我已经将这个添加为收藏
这已经将S3服务添加到了导航中
所以我可以轻松地访问它
我已经这样做了
因为这样一个常用服务
所以我会把它设置为收藏
我可以通过选择它直接访问服务
在我这个案例中，我已经创建了几个桶
所以 因此，我可以在这里看到这一概览
当然，在你这个案例中，你
如果你没有创建任何桶
那么你在这里将不会看到任何东西
但我们现在可以看到我有这些桶
现在我们可以继续创建我们自己的桶
因此，如果我们想这样做
我可以在这里创建桶
然后我们可以设置我们的桶
如前所述，我们首先有区域，这个桶是在这里创建的
这决定了数据物理上存储在哪里
在哪个数据中心
再次在我们的情况下，这不会真正产生差异
但当这用于生产
那么我们当然想
在我们的用例中选择最有意义的区域
例如，接近的地方
数据被访问的地方
然后价格也会发挥作用
因为不同的地区有不同的价格
当然，合规原因在这里也非常重要
所以在我们的情况下，这不重要
我们将选择弗吉尼亚州
我现在可以选择一个存储桶名称
再次记住，我们需要遵循一些约定
最重要的是，存储桶名称必须是全球唯一的
如果我选择我的桶
我非常非常确定这个桶名在世界的某个地方已经被占用
一些账户
如果我想这样做并继续创建这个桶
我不会这样做
因为已经有一个同名的桶
有时候会发生这种情况
解决方案是
如果我们想要有一个名字
像这样的，只需添加一
是的 相对随机的一组数字
在我们的情况下我们可以说这是
让我们说我们的第一个桶
然后可能我们还需要一个数字组合
这样我们就可以使它变得独一无二
而且你可以在这里看到
桶名不能有减号或句号
所以这也会与命名约定冲突
所以选择适合你的任何数字
我会选择这个
我会快速检查这是否有效
顺便说一下
我们可以看到一些我们将在后续课程中继续讨论的选项
对于现在来说这不那么重要
但我们已经提到过
我们可以启用桶版本控制，当我们创建桶时
也可以稍后更改
我们可以添加一个所谓的文本
这也是我们将稍后讨论的一些内容
但这主要是帮助
是的 徒步成本
所以，我们可以看一下成本
基于我们设置的一些文本
例如，技术可以是一个部门的名称，或者像项目这样的东西
这样有助于我们跟踪成本
但我们还有其他一些组织目的，仅帮助标签
当我们将它们分配给特定资源时
以某种方式分组它们
但在我们的情况下，这不那么重要
而且加密是我们稍后会讨论的事情
所以现在我们将继续前进并创建这个存储桶
然后我们可以看到现在这正在创建这个存储桶
所以现在我可以来这里查看详细信息
我将直接跳转到我刚刚创建的存储桶
所以存储桶现在已经创建
当然现在我们想继续上传一些对象
看看这是如何工作的
然后看看这些对象在我们的存储桶中看起来如何 这就是我们在下一堂课将要做的事情
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/007_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p07 008 Uploading files to S3 (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


最后一讲，我们学习了如何创建一个桶
如果我们回到s3
例如，我正在使用在这里添加的这个收藏夹
我可以再次看到我所有的桶概览
在这里我们应该能找到我们创建的桶
我们也可以，因为已经有这么多桶了
我们可以总是根据创建日期对它们进行排序
在这种情况下，我会看到第一个桶就是我刚刚创建的
我也可以看到这个桶创建的区域
然后如果我想把东西上传到这个桶
我可以直接选择它
然后，我有一个非常容易使用的界面
我现在可以创建新的文件夹
或者我直接将东西上传到桶中
在我们这个案例中，我们可以首先创建一个文件夹
例如，我可以将这个文件夹命名为文档
然后我可以说，我想创建这个文件夹
在这里，我可以看到现在我可以导航到这个文件夹
我可以直接在这里上传东西
或者直接将其上传到这个文件夹中
所以我选择了这个
这就是我在顶部导航中看到的
现在让我们继续从我们的本地PC上传一些东西
在这种情况下，我可以看到我有一些文件的选项
我可以通过点击这里来做到这一点
我也可以通过我的机器拖放它
我可以只是拖放它
然后它就会在这里添加
我现在就可以继续上传
几秒钟后这将已完成
所以我们可以看到这个已成功上传
现在我们关闭它
我们可以看到这份文件现在可用
如果我选择它
我可以看到一些额外信息
例如也有这个区域
谁是所有者
有多大
类型等等
在这里我们也看到了这个对象的密钥
但在我们的情况下，这已经足够了，我们可以这样结束
现在我们想更深入地探讨数据摄取
我们首先想要概览 这就是我们在下一节课要做的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/008_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p08 009 Streaming vs Batch Ingestion.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在在我们深入之前
我们想要概览一下不同的摄取方法
我们基本上只是想在这里区分
大致来说
在两种摄取模式之间
第一种是流式摄取
这基本上是实时摄取到
例如 我们也看到稍后如何在S3桶中这样做
这与此相反的是批处理摄取
所以这里数据不是实时流式传输的
而是数据批次
通常这些是大批量的数据
所以数据会定期大量摄入
甚至可以是一次加载
或者按照特定的时间表
例如
假设每小时或每天或每周
收集的数据只是批量聚集
或者直到这一刻生成的数据
也许将被摄入
所以这就是 正如我们所说
通常在大批量和流式传输中，流式传输是我们需要时进行的事情
这意味着当我们处理非常时间敏感的数据时
我们需要立即处理这些数据
例如
当我们进行一些欺诈检测时，数据需要以实时方式处理
以便我们能够采取行动
所以这些用例非常重要
我们需要立即采取行动或非常紧急
在这些情况下，我们可能需要进行流式数据摄取
我说可能需要，是因为默认方式是批量摄取
所以，当我们有大量数据或无时间敏感数据时
我们可以自动默认为批量摄取
这样做的原因是因为它更简单
而且成本也更低
而流式数据摄取则更复杂，因为我们需要将数据立即可用
这也使得它更昂贵
我们可以使用不同的服务，而这些其他服务我们将在下次讲座中探讨
所以例如
对于流式摄入
我们可以使用像亚马逊kinesis这样的服务
我们在本节中也会看一下
对于批处理摄入
最常用的工具是aws glue
我们也可以使用aws lambda函数的帮助
这两种我们都会在本节中看一下
但现在我们先要关注aws glue来批量摄入数据
所以我们怎么做 我们想在下一节课中更详细地看一下现在
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/009_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p09 011 AWS Glue.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看数据工程中最核心的服务之一，那就是数据工程服务
这是一个线索
这是一个完全管理的etl服务，它可以很容易地加载数据
所以从数据源中提取数据
在途中进行转换
然后将其加载到另一个数据存储中
这是通过使用可视化界面实现的
在这里，我们可以轻松地通过拖放方式创建etl流
在这里，我们有各种数据服务可用
所以各种分析服务和数据存储服务可用，我们可以集成包括当然
可以将数据加载到另一个s3桶中
或者从s3桶中提取数据
但也可以加载到和从亚马逊红移和其他数据库中加载数据
此外，我们还可以看到
在这里，还有许多预建的转换 在这里，我们还可以看到一些数据源
所以这些都可以用于集成
我们也可以使用这些预建的转换
这使得一切都变得非常简单
然后，一旦我们有了这个
数据将自动提取
我们可以按计划运行这个
背后，这是自动生成脚本
然后执行这个工作
但我们不需要做任何事情
这都是在幕后
但这会被自动生成
然后执行它
在这里，您可以看到在幕后使用spark
而且我们不需要管理任何事情
这非常有用
因为我们可以专注于集成不同的数据源
转换数据
所有这些spark集群都在幕后管理
所以这当然非常可扩展
这也是无服务器功能
所以我们不需要担心幕后的基础设施
但这都会被我们管理
我们也可以很容易地扩展这
所以这是一个按需付费的服务
这意味着我们只根据计算支付费用
我们将更详细地了解这一点
但这很重要记住
成本取决于我们使用特定计算能力的时间长度
这是用简单术语
我们将更详细地了解这一点 但这很重要记住
我们将更详细地了解这一点
但这很重要记住
因此，我们也会谈论成本
这样我们的成本不会爆炸
这非常重要
这是aws glue的核心能力之一
这是一个完全管理的etl服务，非常简单
我们也可以
当然可以调整这个脚本
我们可以编辑这个，甚至做更多的定制
是的 自定义转换和自定义数据加载选项
所以这非常方便，使用起来也非常简单
此外，AWS Glue的另一个服务和组件是Glue数据目录
想象一下，我们有一个半结构化的数据文件放在我们的S3桶中
例如，一个CSV文件
现在我们想在数据湖中分析这些数据
但传统上，现在查询这里的特定列有点棘手
或者对其进行某种分析
因此，查询这些数据并不容易，可能也无法对其进行可视化等
这就是Glue数据目录发挥作用的地方
Glue数据目录
所以，我们可以运行所谓的爬虫
在ETL中，我们也可以提取模式
这可以自动推断
然后将其存储在中央数据目录中
这包括列名
这些列的数据类型
文件的格式
它是CSV文件等
然后，有了所有此类元数据信息和表模式
我们现在有能力查询这些
所以我们可以使用aws athena并以sql风格的方式查询它
这样看起来我们就像在运行一些查询
我们可以直接而不必复制数据
所以仍然以csv文件的形式存储在三个存储桶中
但我们可以使用athena直接查询它
我们还可以使用它在redshift和quick side等上查询
如我所提到的
这可以在etl工作中完成
或者我们也可以使用单独的glue爬虫来完成
他们所做的是本质上他们正在查看数据源
然后它们扫描它
然后当有一些文件时
它们推断模式
所以它们看到列名是什么
数据类型是什么等等
然后它们将其存储在我们的线索目录中
因此它可以自动分类我们的数据
这意味着数据格式是什么
所以这是一个CSV文件
这是一个Power K文件等等
然后所有这些都存储在数据目录中
这两者都是如此
无论是我们的etl掉落，还是我们的glue爬虫
这些都可以按计划运行
或者我们也可以手动运行它，或者运行etl掉落
也可以基于触发器运行
所以我们可以有某种事件来触发etl的运行
这有时也很有用
我们也可以运行增量加载
这样只加载之前未加载的数据
这是我们在etl掉落中可以做的事情
爬虫也可以以增量方式进行
这样它们只从最近添加的数据中推断模式
自从上次爬行以来
这当然更有效率
我们不需要使用那么多的处理能力
这样我们也可以管理成本
这就是对aws glue和数据目录的简要概述
当然现在我们想将其付诸实践
我们也想看看成本考虑
这样我们就可以控制成本 然后我们想 当然看看这如何在实践中工作
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/010_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p10 012 Setting Up Crawlers (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在最近的讲座中
我们已经看到了如何将我们的文件上传到aws s三号桶中
我们也已经听说过aws胶水
它允许我们创建非常简单的etl作业
所以以一种非常简单的方式
我们也听说过胶水爬虫是胶水服务的一部分
它们帮助我们创建元数据目录
这就是胶水数据目录，我们可以在这里存储元数据，例如
我们s三号桶中的文件 然后这将存储在我们数据目录中
这样我们就可以用类似于sql的风格很容易地从s三中查询数据
因此我们现在想看看这是如何一步一步工作的
首先我们要做的就是配置一个爬虫
它只是检测这个桶中元数据
然后之后我们就想看看结果是什么
然后我们也想查询数据，这就是我们在做这个的原因
因此让我们首先转到胶水
我们可以在这里搜索胶水服务
我可以在新标签中打开它
然后在这里确保你也导航到相同的位置
这样我们就可以在所有我们设置好我们的桶的地方设置一切
现在在这里，在胶水中
我们可以在左侧导航中看到当然有etl作业
稍后我们会看一下
然后在数据目录下我们可以看到一些东西，如数据库表
然后我们还可以看到爬虫
正如我们所提到的，表
最终我们将查询它们
我们用aws athena
然后我们将查询本质上称为表的数据目录中的数据
所以表只是爬虫检测的数据源的元数据的结合结果
所以我们会设置一个爬虫
它会检测我们的数据源
并提取所有元数据信息
如列名
数据类型
数据的类型，例如csv文件
例如
然后它会将其存储在表中
这就是为什么我们有那些表
然后数据库
表就在这里
这仅仅是我们表的一个非常简单的容器
基本上
这就是为什么它被称为数据库和表
即使它实际上只是一个元数据的集合
因为当我们使用aws athena时
这就是我们将要查询的
所以这就是我们看到的数据库和表界面
所以看起来它像是一个数据库，具有SQL风格的表
但本质上，它只是元数据
而数据仍然存放在我们的桶中
所以我们在这里没有复制或复制任何数据
所以让我们看看如何
首先设置爬虫
因此，我们导航到这里，爬虫
在这里，我们看到在我的情况下
我之前已经设置了几个爬虫
但我们仍然会创建一个新的爬虫
所以我们点击这里新建爬虫
我可以给这爬虫起个名字
例如 我的第一个爬虫
我可以给它一个描述
只是为了理解具体是哪个数据源等等正在被爬取
在我们这个案例中我们保持简单，只需点击这里下一步
然后我们有这个选项现在配置数据源
所以这里第一个选项在最上面意味着我们之前没有创建过表格
所以这张表格将在我们设置爬虫时创建
所以第一次运行时
这张表会自动设置
因此我们不需要将其映射到现有表中
我将在这里设置尚未
现在我们可以设置数据源
这将被爬取
因此我点击这里添加数据源
现在我可以浏览我的s3桶
当然如果我回去
我也可以选择不同类型的数据源
但是s三只是最常见的一个
所以这里如前所述
这只是来自我的账户
在这里我可以浏览一个现有的桶
当然我已经创建了很多不同的桶
我们也可以使用搜索栏
在这里我发现这个桶
我们可以在这里做的一件事是，我们可以选择顶层桶的一般情况
但有时在一个桶里有一点文件夹结构
所以我们可以有不同类型的文件
例如，我们可以为客户创建文件
我们可以为销售创建文件
当然，如果我们在这里选择一个数据源
所以 例如，一个文件夹或整个桶
所有文件需要具有相同的格式
否则我们无法从这些文件中创建良好的表格
如果所有文件有不同的模式
所以在我们的情况下，总是只有这一个文件在这里
所以我们可以选择整个文件夹
这样所有的文件都将按照相同的结构被爬取
当然，这是必须的
所有的文件都将被爬取
我们将能够查询
然后所有的这些数据
好的 因此我们选择这个文件夹，文档
我们可以选择这是一个路径
然后在接下来的爬取运行中
我们可以说所有的子文件夹都将再次被爬取
或者我们只爬取新创建的子文件夹，像这样
例如，如果我们每天有一个额外的文件夹
我们可以稍微更有效率
但在我们的情况下，我们将保持默认
因此我们将只爬取所有的子文件夹
因此我们添加一个新的数据源
我们可以看到它在这里出现
我现在可以检查这个
这就是我们要使用的一个
然后点击下一步，在这里
当然，我们必须给予正确的权限
这样爬虫可以执行其任务
因此我们可以选择现有的一个
或者我也可以创建一个新的
这样它将默认具有执行任务的所有必要权限
这就是它应该做的事情
我将在这里添加一些数字
这样这将是一个良好的名称
这并不重要
我将在这里点击创建
现在我们可以再次点击下一步
我们已经提到了我们有数据库和表
表基本上是我们最终要查询的
它本质上也是所有元数据的结果
所以表名
列名
数据类型
以及所有这些元数据
但这当然存储在数据库中
因为我们之前没有创建数据库，我们也可以快速设置一个数据库
我们可以称它为顾客
我们也可以给它一个描述
但我们将像这样创建数据库
现在我们已经设置了这个数据库，我们可以在这里放置表
一旦我们做了这个
我们可以快速刷新这里
然后我们可以选择这个数据库
当然，现在我们将设置表
表的名称实际上将由文件夹名定义
所以这里这是文档
所以这将自动成为表的名称
所以这是从数据来源的数据
但我们可以额外给一个前缀
例如我可以说表，这将是一个额外的前缀
在我们的情况下我们将只使用表
然后最后如我们所提到的我们可以定义一个时间表
所以在我们的情况下我们将只留下按需
但我们也可以每小时或每天或每周或每月运行
如我们所提到的我们将留下按需
所以我们有更多的控制权
当这个应该运行
我们可以点击下一步
在这里我们有所设置概览
一旦我们审查了这个
我们可以在这里点击创建爬虫来设置爬虫
并且它已经被创建了
这非常快
然后当然我们可以现在立即运行这个
这就是我将要做的
我在这里点击运行
与此同时我们也可以去爬虫概览
我已经创建了一些之前
但你可以看到你创建的一个
如果你跟着我
你可以看到这是一个应该运行的一个
然后一旦大约一分钟后我们将回来并看看结果
我们应该看到这额外的或新创建的表在我们的数据库中
所以让我们回来当这个完成时
大约一分钟后当我看爬虫
我可以看到这已经完成所以现在
如果我导航到我们之前创建的数据库
我现在可以看到这个表
你可能想要刷新以便能够看到这
在这里我们可以看到
当然所有的元数据现在
所以我们可以看到实际的数据在哪里
因为我们知道我们不会创建数据的副本
它仍然在桶中
没有创建任何副本
因为这是一个元数据的集合
然后也正确识别了分类
这是一个csv文件
现在我们可以继续直接查询数据
所以我们可以使用athena查看数据
我们将在稍后看一下
如果我们选择表本身
我们可以检查元数据和推断的schema是否正确
所以在这个案例中列名看起来已经被正确识别
这非常好
数据类型看起来也是正确的
所以这真的很棒
我们已经在这里的表中识别和存储了所有元数据
现在我们应该能够使用athena查询数据
这是如何工作的 我们希望在下一节课中看一下
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/011_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p11 014 AWS Athena - Overview.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在让我们快速了解一下aws athena
为了理解这是如何工作的基本原理
然后，我们将更深入地探索雅典娜
但在这之前，我们先要展示这一点
我们迅速想要得到这个非常重要的初步概述
所以，aws athena是aws的一个交互式查询服务
这意味着它允许我们查询存储在s三号桶中的文件
所以实际上所有类型的文件都是未结构化的数据
或者至少半结构化的数据
类似于CSV文件
但我们也可以查询avraw文件
Power K文件
所以，所有可以处理的文件类型，实际上存储在我们的数据目录中
我们可以使用SQL查询它们
在这里，我们可以看到我们可以在这个数据目录中运行标准SQL
我们有设置起来的数据库和表
然后我们可以在这上面运行CQ查询，具有非常高的性能
而且，它也非常可扩展
是的
实际上它也非常可扩展
如果我们有大量数据
那么这将自动为我们扩展
在这里我们又有一个无服务器基础设施
这意味着我们不必担心基础设施
但它将被为我们提供和管理所需的资源
根据需要
然后我们根据查询数量和扫描的数据量支付费用
因此我们不必担心基础设施
我们只需要这种简单的按需付费模型
我们不需要担心基础设施，我们只需要这种简单的按需付费模型
那么这是怎么工作的
在实际案例中它会是什么样子
让我们快速回顾一下
我们有文件在一个s3桶中
这就是文件的位置
而现在它们只是坐在那里
而现在如果我们运行之前见过的那些爬虫
我们可以推断出模式
我们可以设置表，也可以
当然，我们也会将它们存储在数据库中
在我们的数据目录中，Glue
现在我们有了这些表
我们可以在Athena中使用它们
在这里，我们可以使用SQL查询这些数据
但这并没有结束
我们现在可以将Athena作为数据源用于其他应用程序
我们可以使用 例如
AWS Quick Sight
在这里，我们可以可视化数据并构建报告
但我们再次不限于快速边
但我们也可以使用不同的选择
因此我们可以使用ODBC驱动程序并连接到不同的应用程序
此外，我们还有一些用例，如做一些锁分析
因此，我们可以使用Athena与SQL分析存储在S3桶中的锁
我们可以对存储在S3桶中的数据进行一些快速交谈分析
当我们有一些数据存储在S3桶中时
也许一些数据科学家
他们只是想快速获取一些
是的
做一些也许探索性的数据分析 一些文件
这非常快
非常容易
我们可以在数据上交互式地运行查询
但当然，当我们基于S3构建一个数据湖架构时
我们可以使用Athena一般查询数据
稍后也可以构建一些
可视化
一些报告
我们可以在这里基于Athena
因此，存储在我们数据湖中的文件实际上被查询
并且它们具有良好的性能
并且它们能够连接到我们的可视化工具
此外，我们还可以与Amazon Kinesis等流数据源集成
当然，稍后我们会更详细地了解Amazon Kinesis
但现在我们已经看到了这个快速概述
现在我们想要
当然也快速演示一下
因此我们理解我们实际上设置了数据目录
因此我们能够
是的 在这个上下文中建立联系
因此我们能够理解这一点 这就是我们在下一讲要做的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/012_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p12 015 Query data using Athena (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们快速演示一下雅典娜的概念
使用我们已经设置好的
之前我们设置了我们的数据库
在我们的数据库中，我们创建了这个表，这个表是通过爬虫创建的
表的结构是从一个S3桶中自动推断出来的
所以我们记得，我们有这个文件在这个文档中
所以现在自动推断了结构，使用爬虫
因此我们现在在这里有了这个表
但是现在我们也想使用这个
这就是我们做这一切的原因
所以现在我们可以查询这个文件中的数据
我们使用athena来做这个
我们可以这样做
例如 如果我只是导航回我们的主数据库客户
我有这个选项来查看数据
如果我在表格中
我在操作下面有这个选项来查看数据
然后我们被要求是否想被带到athena来预览数据
当然这会产生费用
所以当然这也是一项付费服务，所以
根据查询数量和读取数据的数量
我们需要为此付费
但在我们的情况下，实际上会非常便宜
因此我们不必太担心这个问题
现在我们看到，我们已经被带到这个界面，我们在athena中看到了
查询编辑器
当然，我们在开始时可能看到这条消息
未提供输出位置
需要提供输出位置，可以通过工作组结果配置设置或作为API输入
所以我们可以做什么
实际上，这也是我们在运行第一个查询之前会看到的信息
你需要在s3中设置查询结果位置
这样查询结果实际上存储在一个桶中
这就是athena需要的
我们可以通过这里点击编辑设置来做到这一点
在这里，我们需要在这里设置查询结果位置，我们可以只选择桶
我们之前是否创建了一个单独的桶
或者我们可以使用以前为这个目的创建的桶
所以我们可以设置一个额外的单独的桶
但我们将保持简单，只选择这个桶
所以，我只选择这个桶
实际上，让我们回到顶层并选择它
所以我在这里只勾选这个框
然后我说选择
这将创建一个额外的文件夹，用于保存查询结果
查询结果将保存在这个文件夹中
在这个桶下，我们将在下一秒看到它
我现在只是点击保存
然后设置完成
现在我们可以像这样回到编辑器
我们可以再看一下这个
在这里的左边，我们现在基本上可以看到我们要查询的数据
在这里我们可以看到我们选择了客户数据库
然后在这个数据库下面我们有这些表
我们可以展开这个表格来看
就像在数据库中一样
所以我们可以看到
这些是数据库的列和数据类型
当然，在这里我们可以看到这张表
所以看起来它实际上就像一个数据库
这就是为什么它被称为数据库和表格
因为从雅典娜的角度来看
这就是我们实际使用它的方式
现在我们也可以运行那些查询
我们看到数据库名为客户
然后这里我们有表名
我们可以将结果限制在前10行
在这种情况下我们可以像这样运行查询
或者我们也可以通过点击这个表格来预览
这里这三点
我可以说预览表格
这将打开一个新的查询并运行它
所以我们可以看到这里有不同的列名
然后我们当然也有在这些列名中的数据
现在我们可以运行查询
例如我可以说h的平均值
所以如果我这样做
我可以运行我的常规查询
所以我说平均值或例如
H 和这里我们又有了这个自动完成功能
非常方便
所以我可以从这里选择它
在这种情况下我们不需要设置这个限制
是的
然后我可以通过按Ctrl键执行查询
或者运行查询 然后我们在这里得到结果
当然，我们可以现在给它一个别名
例如 平均值
我们也可以四舍五入结果
这些都是我们在Excel中可以执行的标准操作
标准方式
如果我这样做
我现在可以看到这很好
我错了地方
当然，必须在列之后
让我再试一次
然后我们在这里看到这些别名
现在结果也被四舍五入
所以这只是一个快速的演示，告诉我们如何使用我们的数据目录中的数据
通过使用athena和查询 存放在我们s3桶中的数据
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/013_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p13 016 Federated Queries.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈一个稍微高级一点的功能
即联合资源查询
联合资源查询允许您查询数据
不仅来自S3
还可以来自其他数据源
这样您可以在关系数据源中运行SQL查询
非关系数据源
对象数据源
甚至可以创建我们自己的自定义数据源
这是一个联合资源查询
这是这个功能的好处，给我们提供了一个接口
基本上，我们可以使用Atina
我们只需使用Atina
这样我们就可以从不同的来源查询数据
为此我们使用所谓的联合资源数据源
这基本上是一段代码
它将目标数据源与Atina进行转换
您可以将其视为Atina查询引擎的扩展
我们可以自己编写代码
或者我们也可以使用一些预构建的连接器
我们有
在这种情况下
Amazon CloudWatch日志
Amazon DynamoDB 文档DB RDS
以及其他符合JDBC的关系数据源
例如MySQL和Postgres
这样我们就可以结合多个数据源
例如，如果我们想运行一个快速临时查询
我们不想构建一个非常复杂且
需要很多努力
复杂的ETL过程
但我们只是想快速进行分析
也许我们想查看一下
例如，特定客户的购买历史
也许我们还想获取他们购买的产品
可能存储在RDS中
也许我们还想获取详细的客户资料
可能存储在DynamoDB中
我们还想获取用户交互信息
我们可以使用联合资源查询将这些所有来源组合在一起
这样我们就可以在单个界面中获取所有信息
这就是联合资源查询的作用 希望对你有帮助，下次再见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/014_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p14 017 Performance & Cost.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈成本和性能
首先谈谈成本
我们只支付我们实际使用的费用
所以我们只支付我们运行的查询
我们为此付费
这是基于每个查询扫描的数据量
我们不为失败的查询付费
我们只为运行的查询付费
稍后我们将看到如何利用这一点来优化成本
所以基本上我们只需减少扫描的数据量
记住这一点
当我们谈论性能时
我们也可以降低成本
除了节省成本
我们还可以使用预留容量
如果我们提前知道需要多少容量
当然这也是一种额外的成本节省选项
现在我们来谈谈性能
这非常重要
我们如何一般优化性能
将数据分区是一个很好的方法
这意味着我们将数据结构化为分区
然后athena可以使用称为分区修剪的东西
这基本上就是在查询执行时消除
那些对查询不需要的分区
所以这些数据甚至不需要扫描
所以我们在执行所有其他查询处理之前
我们只关注相关的
例如 我们运行一个查询只需要某个区域的数据
如果数据分区为该区域
我们有这种s3目录结构
并且这些分区的元数据也存储在我们的glue数据目录中
然后我们在执行所有其他查询之前
或者athena只是消除那些对我们无关的区域
例如我们过滤
我们有一个查询 我们按日期过滤
所以我们只想看上个月的数据
我们在查询中应用了过滤器
然后athena使用分区修剪
如果数据像这样分区
所有其他不必要的分区
即来自不同月份的分区
可以很容易地消除
所以它们将被修剪
有时这当然很有用
通常我们在这种情况下使用元数据来自glue
即我们的数据目录
当我们在这种情况下处理查询时
雅典娜在实际修剪之前提交数据目录的获取分区调用
但有时这可能会影响性能
如果我们有大量的分区
为了避免这种情况
我们可以使用分区投影
因此，使用分区投影
我们不仅可以自动化分区管理
因为我们不需要手动指定我们的分区
它将由雅典娜自动完成
但它也可以加快高度分区表的查询处理
当然，如果我们有大量需要添加的额外分区
这也将容易
因为这里我们有这种分区管理的自动化
让我们看看这是如何工作的
这种分区投影消除了
在我们需要手动在线索数据目录中指定分区的需要
或者可能是外部的hive元存储
这里它将基本忽略glue的元数据
但它将在雅典亚本身构建分区
我们将只需在雅典中配置它
我们将通过指定值范围来做到这一点
以及每个分区列的投影类型
例如 让我们说，我们有一个
让我们说一个日期列
我们只需指定这是我们想要使用的分区列
用于分区投影
这在这里很有用 例如
对于日期列或数字列
所以值确实 它们是可预测的
所以日期 例如
或者可能是一些id列
在这种情况下
如果我们有大量需要手动添加的额外分区
以及高度分区的表
我们有许多的分区
这可以真正提高查询性能
所以在查询执行期间
雅典亚使用这个信息来投影分区值
而不是从glue数据目录中检索
这将减少查询执行时间
并且自动化了分区管理
因为它消除了手动创建分区的需要
这对于高度分区的表是有用的
现在，雅典亚也支持glue数据目录的分区索引
像这样 我们也可以优化查询规划和减少查询运行时间
所以当你在一个拥有大量分区的表上运行查询时
雅典娜 必须从胶水数据目录中检索所有可用的分区
并且这里我们就必须这样做
或者阿提娜必须确定哪些那些部分需要分开
哪些需要修剪
如果我们有很多分区，这可能已经相当耗时
所以当我们有一个分区索引时
现在可以只获取子集分区
而不是加载所的分区表
因此这也可以提高查询规划并减少查询运行时间
而这是我们可以在glue数据目录中设置的东西
像这样 我们可以专注于实际上需要的分区
但它仍然由aws管理在我们的数据目录中
另一种提高查询性能的方法是启用查询结果重用
首先 这实际上做了什么
它允许我们重用一个已经返回的先前查询
这可以提高性能并减少成本
尤其是对于频繁运行的查询
这样你就可以在这里看到，可以启用查询结果的重用
例如，在控制台中
然后你就不必再次扫描整个数据
但你可以基本上获取存储的结果
因为幸运的是
Athena也会将查询结果存储在S3作为CSV文件
所以这总是要做的
查询结果重用只是利用了这个事实
利用这个事实
所以在这个功能可用之前
我们也可能进入这个桶并手动查询这个文件
但现在重新使用查询结果更容易更方便
所以每当我们运行一个查询，该查询与已经运行的查询相匹配
还有最大年龄
所以这种情况下 例如，最多60分钟匹配
那么它将重用已经生成的查询结果像这样
我们不需要再次扫描所有数据
当然我们也有更快的性能
所以我们不仅降低了成本
而且提高了性能
这在我们数据源不经常变化的查询中尤其真实
因为这样重新使用我们已经生成的结果就更有意义
但这不仅仅局限于此
当我们有重复查询时
我们可以启用这个并使用这个功能
这对于大数据集尤其有用
也可以与复杂查询结合使用
例如结果并不大
但我们必须扫描以通过此结果
我们必须扫描大量数据
在这种情况下，再次使用查询结果会有益
最后，我们还有数据格式
基本上这里
最好像这样压缩数据
我们可以减少数据的大小
这也可以提高查询性能
还可以减少被扫描的数据量
因此，再次节省成本
再次尝试做同样的事情
减少被扫描的数据量是使用合适的格式
而不是使用CSV或类似的东西
我们应该为分析目的使用列格式
列格式如Parquet或Apache ORC
它们允许我们
它们再次允许我们更快地检索
因为如果数据以列存储
那就快得多
再次，存储更有效
因此数据量更少
主要好处是我们可以只选择特定列
而不必扫描其他列
如果以行格式存储如CSV
例如 我们必须扫描所有线
但我们可能只想分析一行
通常这是
是的 分析目的中更常见的做法
因此这也有助于性能 好的 希望这有所帮助，下次讲座见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/015_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p15 018 Workgroups.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈雅典娜工作组
它们在我们组织不同类型的查询方面也很重要
因为通过工作组我们可以基本隔离查询
这意味着如果我们有
例如 不同类型的团队或不同的使用案例
例如，我们可以有一个用例或一种查询，仅用于生成报告
可能在快速侧
然后也许我们有另一个用例或另一种工作负载
这将是用于某些人的hoc使用
也许数据科学家
然后我们可以为这创建不同的工作组或应用程序
我们基本上有不同的类型设置
我们也可以使用它来跟踪和控制成本
这样我们就可以分离这些不同类型的工作负载
当然，每个工作组都可以
然后有自己的配置
例如，我们可以控制查询执行设置，也可以控制结果位置
这是s3位置
这是我们存储查询结果的s3桶
对于我们在给定工作组运行的查询
这也可以指定给此工作组
我们也可以控制对工作组的访问
当然，我们可以使用正常的资源级iam权限或基于身份的iam策略
所以我们可以使用基本iam策略来控制对工作组的访问
然后，谁有权限访问此工作组
可以使用给定的工作组执行查询
在窗口中，我们将看到一个下拉菜单
我们可以选择给定的工作组来执行查询
我们还可以
当然
如我们所说 控制成本，也可以控制使用的引擎类型
这里
默认是 当然，athena sql
这通常被使用
但我们也可以设置一个apache spark启用的工作组
我们将创建一个新的工作组
并将引擎设置为apache spark
然后我们设置好之后
现在我们可以使用spark使用这个工作
这将需要在另一个工作组中设置
我们可以在每个区域创建多达1000个工作组
默认情况下，每个账户都有一个主要工作组
这总是可用的
在这里，默认权限允许所有已验证用户访问此工作组
并且不能删除它
当然 如果我们创建另一个
我们也可以使用iam来配置对这个的访问权限
这样这个才是工作组
现在我们也想最后谈谈athena的性能 这也是非常重要的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/016_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p16 019 Workgroups (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们快速看一下如何设置和配置雅典的工作组 所以我们只需导航到服务雅典
我们可以在左侧导航中看到
如果我们在左侧导航中扩展这个
现在我们在查询编辑器中
我们也可以在管理下看到工作组
正如我们所说，我们有我们的主要工作组
这总是可用的
在这里我们只需使用雅典引擎
在这里我们可以看到一些配置
我们将这个作为主要配置
当我们在查询编辑器中运行查询时
我们可以看到在这种情况下我们将使用主要工作组
但我们也可能有其他工作组我们有访问权限
如果我们想要创建另一个工作组
我们可以在这里的控制台设置它
要做到这一点
我们可以从这里创建一个新的工作组我们可以说
例如 这是假设的报告生成
像这样的东西
或者另一种类型的团队
也许是一个开发团队
我们可以像这样指定一个名字
在这里我们可以选择引擎的类型
正如我们所说，可以是雅典娜库尔用于交互式查询
或者我们也可以使用Apache Spark来使用无服务器Spark引擎
这也有可能
在我们这种情况下，我们只留下雅典娜SQL默认
在这里我们可以进行一些额外的配置，关于身份验证
我们也会将其留在默认设置
我们也可以 当然可以配置我们自己的查询结果位置
所以这里我们需要指定一个给定的桶
例如
我们也可以在这里设置使用控制
在这里我们可以设置
例如数据限制
或者我们也可以说我们想要设置一些使用警报
所以像这样，我们可以更好地控制和跟踪成本
对特定团队或工作负载来说，这会更加隔离
如果我创建了这个工作组
我看到它被创建得非常快
我们可以看到这个案例中我们也有这里的指标
所以我们可以更好地控制和跟踪成本和使用量
但我们也可以看到这是如何使用的
所以如果我现在回到查询编辑器
我现在可以使用主要工作组
但我也可以说在这种情况下我们应该使用报告生成工作组
所以这也可以在这个查询编辑器中设置
所以这只是在控制台这里一个非常简短的演示 希望对你有帮助，下次课程见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/017_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p17 021 Glue Costs.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在我们深入实施胶水手的实践之前
我们也快速地讨论一下成本
尤其是当你想要和我一起跟进
这是一个重要的考虑因素
以免超出预算
当然，这也是普遍情况
如果你想要有效地管理成本
这是一个重要的方面
因此，了解这是如何工作的是很好的
因此，让我们首先从爬虫开始
这就是我们按小时计费的地方
根据GPU的数量
GPU是数据处理单元
这基本上是计算能力来源的地方
这个按小时计费会按秒计算
所以会有一个一秒的增量
所以当它运行时
例如半小时
那么我们将支付每小时费用的一半
所以这里 然后这将只是耶
50%的1小时
基本上并且注意
这将由秒构建
但现在有10分钟的最低限度
也快速 GPU是什么
我已经提到了 这些是数据处理单元
这是基本提供的计算能力以执行
那些任务和服务
因此这些爬虫
因此它们需要这些计算能力来执行它们的任务
然后我们在这里也有资料目录我们可以免费存储高达一百万个对象
并且任何超过这个数量的对象将会以每月每十万个对象一美元的价格进行存储
但这只适用于超过100万个对象的情况
通常在我们这种情况下
当我们进行这种训练时
那么我们就不必太担心这一点
因为我们很可能不会达到一百万个对象
但是现在我们还有etl的消耗
再次，这与之前一样工作
我们有每小时的tpu使用费
所以这里我们有
这些大约是4美分
但我们将在几秒钟内深入探讨
在更多的细节中
再次，我们有按秒计费，最低增量为10
然后，我们在任何高于版本2的情况下都有1分钟的最低费用
我们有1分钟的最低费用
所以这仅适用于版本0.8和0.9
对于版本2.0及以后的任何内容
我们至少有一分钟的最低要求
但现在的问题是使用了多少个GPU
当然，我们会讨论具体的成本和数字
包括示例将在几秒钟内讨论
但首先 让我们快速讨论在执行Drop时使用了多少个GPU
嗯，这取决于首先它是可配置的
但然后它也取决于最小值是什么
所以我们知道，底层的ETL作业使用Apache Spark
在这里对于Spark
如果我们使用Spark
我们有两个GPU的最低要求
但要注意，这里是默认设置
如果我们不做任何更改
设置为10个GPU
所以如果我们想为我们的训练目的这样做
那就好降低到最低限度
因为我们没有那么多
是的 我们不需要那么多的计算能力
因此我们应该意识到这个默认设置
然后我们也有Spark Streaming这里
默认设置为2个GPU
这也可以使用2个GPU的最低要求
然后我们也有Ray Drops
它们目前处于预览状态
它们对于机器学习和AI非常有用
在这里我们使用这些MDPUs
它们是特殊的GPU，具有更高的内存
所以这里我们有两倍的内存
所以不是16GB
我们有32GB的内存
对于这些Drop
如果我们在Chop中使用它们
默认为6个MDPUs
这里有2个MDP的最低要求
这些仅适用于这些特殊的工作，这些工作对于ML和AI非常有效
是的 对于ML和AI非常有效
然后我们也有Python shell jobs
如果我们有一些不需要Spark的分布式计算能力的工作
但我们可能有一些更简单的工作
我们也可以使用这些Python shell jobs
并且我们可以使用更低的GPU数量
现在问题是
当然，在查看示例之前，这些GPU的成本是多少
所以如前所述 目前的费率是每GPU小时4.4美分
但是当然，这可能会随时间变化
所以请查看AWS网站上的信息
如果这一变化
但这是现在的价格
此外，我还想提到我们可以交互式开发我们的glue ETL代码
我们可以使用这些笔记本，它们会启动一个交互式会话
这样我们就可以
正如我提到的，我们可以在笔记本中交互式地开发代码
然后我们也要执行这些代码
然后我们也要执行这些
因此，根据会话的活动时间
并且我们执行的代码所使用的dpus数量也被构建了
并且我们也可以配置一些空闲超时
所以，我们应该非常清楚这个会话活跃和运行的时间有多长
所以当我们不再需要它们时
我们也应该关闭它们，以免成本爆炸
但是，我们又在这里有了这个可配置的闲置超时
并且这里我们也有一个一分钟的最低计费
所以这就是更常见的情况
并且也在这里
类似于我们之前讨论的其他现有粘合剂切削
我们有至少两个gpu和默认的五个tpu
现在让我们也看一下一个例子来理解实际例子中的成本
假设我们有一个基于apache spark的etl作业
它运行15分钟
所以15分钟 这是一個四分之一的小時，並且我們在這個工作中使用了六個tpu
我們記得在我們的情況下，一個dpu小時是四四美分
因此我們可以輕鬆地計算成本
它運行了四分之一的小時
并且它使用了六个TPU
所以我们只要做计算
这为这项工作带来了66美分
所以这是一个例子
然后我们也可以快速地用交互式会话来做一个例子
假设我们用笔记本电脑来交互式地开发我们的代码
我们使用了五个TPU
这是默认的
我们保持会话运行以交互式地工作两个小时
这是2.5小时
然后我们也可以进行计算
在这种情况下，这个数字是88美分
希望这能给你一些理解
考虑到这些成本
现在我们也想快速谈谈预算
这样我们就可以制定一个预算，帮助我们控制成本
在下一讲中，我们将设置一个实际的预算
实际上为零的预算
还有一个5美元的预算
只是为了提醒我们
如果你想在我的实操课程中跟着我一起学习，我建议你也设置预算。 我也建议你设置这些预算。
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/018_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p18 022 AWS Budgets.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


首先，我想讨论一下AWS预算
这是我们可以在账户中设置的东西
在成本和账单中心
在这里，我们也可以定义警报
当这些预算被超出时
这样我们就可以通过电子邮件收到通知
这对我们来说非常有用
特别是为了了解成本
是的 确保我们不会超出预算
特别是如果你想跟着做
这可以根据实际发生的成本设置
也可以根据预测的设置
这样我们就可以提前收到通知
当预测超出时
这也很有用
在我们的情况下，我们有四种不同类型的预算
最常见的一种是
我们将会使用的那种
将是成本预算
但对于一些服务，我们也可以定义特定的使用预算
所以这种情况下
这是通过某种更间接的方式，也控制了成本，仅仅通过控制使用
这不会是以成本的形式
但会以使用的形式控制
如果我们使用节省计划或预订
在我们这种情况下，我们也会设置这些预算
这样有助于我们理解节省计划和预订计划是如何高效使用的
但在我们的情况下
如前所述
最重要的会是一般的成本预算
预算是完全免费的 如果我们使用启用了动作的预算
那么我们有限制两个免费的
对于任何超过两个的预算
我们需要支付每天十美分
对于这些启用了动作的预算
我们可以启用一些与iam相关的动作
或者其他一些动作
比如，基本上会执行一些操作
但在我们的情况下，我们会发送通知
这在我们的情况下是免费的
好的
这就是我们的AWS预算
现在我们当然想进入我们的账户，在实际中设置这一切 这些是AWS预算 现在当然，我们要进入我们的账户，在实际中设置这一切
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/019_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p19 023 Setting Up Budgets (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看我们如何设置预算
在我们的aws账户中
首先，我在这里的控台，主页上
我已经可以在这个小型仪表板中看到
成本和使用情况
如果我点击这个成本
我会被导航到成本探索器
所以我可以做这个
这会直接导航我到成本探索器
所以这将是账单和成本管理服务的一部分
在这里我们可以看到成本探索器是我们打开的
所以如果我从这里选择它
我可以看到所有服务的成本预览
所以这按月和按服务分解
现在如果我们向下滚动这里
我们可以看到一个更表格化的视图
并且右边
我们现在可以过滤这些
在这里我们有不同的服务我们可以过滤到这些
例如 我只想看到雅典娜
然后我可以看到我已经使用了多少雅典娜
这么多这个服务产生了多少费用
此外，我们还可以根据更多不同的类别进行过滤
比如地区等等
另外，过滤税也是一个很常见的功能
我之前简要提到过，我们可以给我们的资源添加标签
像这样，我们可以进行非常个性化的分类
例如，可以为不同的部门设置特定的标签
这样我们就可以了解每个部门产生了多少费用
通过哪个公寓
或者通过哪个业务单位
或者通过哪个项目
所以像这样我们可以使用这些标签来分类我们的资源
为了那些成本目的
所以这是一个非常普遍的做法
而我们的情况是我们想要设置预算
而这也是我们可以在计费和成本管理中做的事情
所以这里如果我们向下滚动左侧
我也有预算
在我这种情况下，我已经设置了几个预算
实际上只有两个
但我们也想设置这两个预算
第一个是无支出预算
这将在我们超过预算一分钱时提醒我们
一旦一分钱被超过
我们会被提醒
这样我们就会收到通知，当我们跨越免费使用限制时
或者我们使用了任何产生费用的服务
此外
自从我们将会使用
例如胶水
这将产生费用
但我们希望控制这个数额
因此我们想要设置预算
在我这个案例中我将设置5美元的预算
所以这里我们可以看到
这些是我设置的预算，我们可以看到
这是已经使用的金额
这是预测的金额
在这里我们也有当前与预算的对比
所以我可以看到73%已经被使用
所以这是
是的 与预算相比
在这里也有预测值
所以这只是预测
但现在我们想要设置一个新的预算
我们基本上会设置这两个预算
为了做到这一点
我们将点击创建预算按钮
我们可以有两种方式做这件事
第一种方式是使用模板
这稍微容易一点
但实际上这个自定义方法也很简单
但在第一种方式中我们想要设置一个零支出的预算
然后之后我们再自定义
基本上就是
我们只需要四步
然后我们可以用这个自定义方法来设置
但在我们的案例中首先我们使用零支出预算
这将设置我们所需的一切
我可以给这个数字命名
因为我已经使用这个名字设置过一个预算
我可以输入一个示例电子邮件地址
因为我已经做过了
当然在你这个案例中你可以输入你自己的电子邮件地址
这就是通知
当预算超支时发送的电子邮件地址
当然你可以输入这个电子邮件地址
如果你想要收到通知
这就是所有要做的
所以我们可以点击创建预算
像这样我们简单地设置了一个快速且容易的预算
当然现在我们也想设置一个每月预算
实际上我们也可以快速查看预算
所以这里我们可以看到
这就是当前与预算的对比
这也是我们在概览中看到的
如果我们点击这里编辑
我们可以看到这四个步骤
以确保预算被设定
但我们不想编辑现有的预算
但我们想创建一个新的
这将是我们的5美元预算
所以我再次点击创建预算
我们可以再次使用这个月度成本预算模板
这再次稍微简化了一点
但在我们的情况下，很容易我们自己自定义
现在我们必须选择预算类型
我们提到了最常见的一种是成本预算
所以这只是跟踪和监控成本
所以额外成本由服务产生
我们不想只测量使用情况
而是实际产生了多少成本
因此我们使用成本预算
我可以点击下一步
现在我们可以给它起个名字
我可以说五美元预算像这样
然后我们可以设置周期，每月完全没问题
这是一项循环预算
所以它不应该到期
但每个月又应该重新进行
它应该被更新
并且起始月份就是当前月份
我们也希望保留固定的预算方法
所以这不应该自动调整
但我们只想保留一个固定的值
在这里我们可以使用现在的数字
所以你可以使用最适合你的数字
五美元应该足够了
对于我们来说，这将足够完成所有动手实践活动
因此，我将使用五美元的价值
我们可以排除一些服务
但我们想使用
当然，所有服务
因此，我们再次使用默认设置
我们点击下一步
然后在第三步我们可以配置警报
所以我可以在这里添加一个警报阈值
实际上这个预算已经被设置为一分钱
那么当超过1美分时
我们会收到警报
我们也可以根据百分比或绝对值来设置
对于我们这里的5美元预算来说，用百分比值是有意义的
这非常方便
那么这将是一个实际的4值
在这里我们可以选择任何这些选项
所以我们有80%
这是我们的美元预算
一旦达到这个预算，我们会在这里收到通知，通知会发送到我们的邮箱
同样，我们需要提供我们的邮箱地址
所以，这将是接收通知的地址
我们可以在这里设置
然后我们点击下一步，在这里我们不需要添加任何额外的操作
这意味着这是一个免费的预算
我们没有任何操作
我们点击下一步
然后，我们可以查看所有内容并创建预算
一旦完成，我们可以看到这预算已经创建
好的
一旦我们做完这些
现在我们已经准备好深入研究glue 看看实际的etl流程是如何设置的
我们想在下一节课中看看这是如何实现的
现在我们已经准备好深入研究glue 看看实际的etl流程是如何设置的，我们想在下一节课中看看这是如何实现的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/020_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p20 024 Run Glue ETL Jobs (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


所以我们在这节课中将要做的是
我们将设置一个非常简单的 etl 流程在 glue 中移动数据
我们希望从源头提取它
在我们的情况下这将是我们的桶
我们已经在这里添加了 csv 文件
所以我可以再次按创建日期排序
在这里我们有这个
我们的第一个桶
在这里这个未保存实际上是来自 athena
所以这是为查询创建的
我们不关心这个
在这里和这里有 csv 文件
这就是我们想把它移动到另一个位置的
所以我们可以做的是
我们可以添加一个另一个文件夹或创建一个另一个桶
在我的情况下我将实际上创建一个另一个文件夹
例如这是一个目标桶
也许一些数据湖的部分 这是我们的目标
在这里我们可以说
例如客户
像这样的客户
我们可以创建这个文件夹
现在我们想看看我们如何使用 etl 作业来移动数据
我们可以连接到一个源头
然后我们想把它加载到一个目的地
当然在这之间这就是这个名字所建议的
etl 提取、转换和加载
所以在中间我们也可以做一些转换
但现在让我们直接跳到 glue 看一下我们是怎么做的
所以我正在搜索 glue
我将只打开一个新标签
然后在这里我们要去 etl 作业
然后我们要设置一个新的 etl 作业
你可以看到我已经设置过一个作业
我将创建一个新的只要点击这里视觉 etl
这是非常方便的创建一个 drop
我们已经听说我们也可以使用这些笔记本来交互式开发 etl 代码
但在我们的情况下我们只想使用这个非常简单的视觉 etl 编辑器
在这里我们可以看到
我们可以添加那些节点
我们现在在这个未命名的 drop 中
我们可以给它起一个名字
也许这是我们的第一个 etl drop
在这里用这个加号
我们可以添加那些节点
它们本质上只是一些我们可以连接的源头
以及转换
然后也有我们的目标
所以ETL
提取 转换和加载
这就是它的工作方式
在我们这个案例中，我们希望选择三个
这将是我们数据的来源
我可以选择这个，当我选择它时
我可以进行配置
所以我可以给它一个不同的名称
但是S三
我可以说例如
S三来源
这仅用于描述目的
然后我们理解这里做了什么
所以这里如果我想让一些东西帮助我理解这个
以更描述性的方式
我们可以这样做
然后我们有来源是S三
现在我们可以指定一个路径
所以我也可以再次使用
这个小的浏览窗口
我可以在这里搜索特定的桶
也许我们的第一个桶
我可以再次在这里钻取到我想要使用的文件夹
所以我可以选择例如这个整个文件夹
然后我选择它
现在我尝试推断模式
它会自动识别非常类似于爬虫
实际上这也是在这里做的事情
所以模式也可以在这里推断
所以数据表和数据目录中的表也会创建
所以这是很有趣的注意
没有显式运行爬虫在这里
数据也会加载到这里
元数据也会在数据目录中收集
好的 所以现在我们已经完成了这个
我们再次可以这里推断模式以获取所有这个元数据
我们看到这成功了
我们得到这个复选框非常好
所以现在我们可以在这里做点什么
我们可以添加转换
例如
我可以做一些SQL查询
我可以评估数据质量
检测 敏感数据聚合
做一些种转换
但我们现在还不关心这个
但是在这个课程的这一部分
我们只关心数据的摄取
因此我们将前往目标
在这里我们又可以选择不同类型的目标
在我这个案例中，就像我们提到的，我将再次加载到S3中
好的 当然，我们需要现在连接它
我们将使用这个的端点作为目标的起点
再次我们可以选择并配置它
现在非常重要的是格式
在这里你可以看到默认实际上是parquet
所以你可以看到数据会自动转换为更准确的说法
所以它被转换为parquet
parquet是一种更适合分析目的的文件格式
这是一种列式格式
在这里，当读取数据时，数据非常有效
因此这里默认设置为parquet
稍后我们会更详细地讨论不同的数据格式
但现在你可以记住，这些格式如avro或rc parquet
它们对于读取数据也很好
所以parquet是一种非常强大的格式
这是默认设置
我们将在这里也使用它
现在我们也必须选择目标目的地
再次我们可以浏览
我可以搜索我想要加载到的特定桶
所以我再次选择相同的桶
在这里我选择目标
所以我像这样选择它
然后我们就已经完成了
在这里我们有更新数据目录的选项
这是我几分钟前提到的
在这里我们有这些选项
我可以做默认的
不要更新数据目录
但我也可以做
我可以在数据目录中创建一张表
在随后的运行中
这是如何工作的
我们也想要更新模式并添加新的分区
这意味着当我们有分区时
我们也会谈到分区
很快就会提到
本质上当我们有额外的文件夹时
例如
让我们说这是按天分区的 所以按日期我们有可能每个文件夹每天一个
然后这将作为一个分区添加
在读取数据时更有效率
因为当我们过滤到特定日期时
然后我们自动知道我们只需要查看这个分区
所以只需查看特定日期
所以当添加额外文件夹时
所以添加额外分区
我们希望更新模式并添加这些分区
或者，
我们也可以说我们想要保留现有模式并添加新分区
仍然如此，我们希望不修改现有模式
所以这些都是我们可以做的事情
在我们这个案例中，我们希望创建一个表
我们也可以更新模式并添加分区
所以当模式有更改时我们可以更新它
当然，在这种情况下，我们也需要选择一个数据库
让我们再次选择
客户，我们可以在这里输入表名
让我们称其为
客户 这样我们知道这是我们的表
就是这样
所以这是配置
我们可以在这里看到所有配置都是正确的，在我们运行它之前
我们需要保存这个作业
这就是我们需要做的
然后只有在那时我们才能运行它
让我快速 因此保存
顺便说一下，这是某事
让我们转到详细信息
我们还需要选择一个具有执行权限的角色
所以我们需要读取数据
写入数据
因此我们需要在这里选择一个角色
我们已经创建了先前的角色
但我也可以使用我们上次创建的那个
这就是那个
这是我们之前为爬虫创建的那个
然后我们会看到这样是否起作用
或者我们是否需要做一些调整
在这里我们可以看到该角色需要访问三个源的权限
所以在这个角色中需要这些权限
我们已经提到过这一点
我们在这种情况下只提到了spark版本
我们也快速提到了这里我们可以留下默认值
也在默认值上
然后我们有工作类型
在这里我们提到了我们想要使用
在这种情况下我们可以使用
这个默认工作类型
我们希望减少工人数量
所以GPU数量
所以我们已经提到了默认值是十
但我们想使用更低的数字
而且我们不能实际使用一
所以我想这不会保存它
因为这里我们可以看到工人数量的最小值是二
所以这里我们需要使用TPU的最小数量
在这种情况下是二
但这很好
只是为了确保我们不以不必要的方式增加成本
所以这是我们可以在这里配置的东西
并且书签也在帮助我们进行增量加载
但这也是我们稍后会讨论的事情
所以现在我们将尝试再次保存
然后一旦这已经完成
让我们现在继续运行这个作业以查看是否正在运行
我们可以进入这个特定掉落的运行详细信息
然后我们可以看到这个掉落目前正在运行
我们可以看到实际上这现在已经失败了
并且这失败的原因是实际上我们可以在这里看到有三个拒绝访问
所以我们没有足够的权限
因此我们不能读取或向我们的桶写入数据
因此我们需要为这个角色授予权限
以便这可以正确执行
因此我们可以做以下几种操作
实际上我们可以进入
我直接进入并且我们可以在这里搜索我们为这个掉落分配的角色
并且我们可以修改这个角色
所以我可以进入角色
并且我在这里搜索
是的 我使用了类似的东西
所以这就是线索服务角色
并且这里我们想要添加一个策略
以便我们有足够的权限
以便这可以执行
好的 我们可以在这里看到这里我们只有这里的权限
线索服务角色
但这不足以读取数据和查看数据
所以 因此我们需要添加一个额外的权限
我们可以通过点击这里添加权限并附加策略来完成
这样我们就可以添加一些预构建的策略
为了简单起见
我们可以在这里搜索S3并为其提供完整的S3访问权限
所以 当然在生产环境中
我们可能会在这里更加精细
但在我们的情况下这样做是完全可以的
然后我们就可以添加一个权限
然后我们现在也能将我们的数据写回到我们的桶中
这是Glue任务中需要的
这样数据也能被移动
好的 一旦我们更新了这个
实际上我们已经完成了，我们可以重新运行这个任务
让我先回到Glue中的任务
我将回到特定的运行
我们可以再次使用相同的配置重新运行
现在应该能成功
再次之后
大约两分钟后，我们可以看到它已经成功了
现在我们可以检查一下是否真的起作用了
正如预期的那样
我们可以回到S3到我们的桶中
我们可以现在转到目标客户
我们可以看到parquet文件是可用的
此外，我们还可以查看
实际上我们的数据目录
看看表是否已经创建
也被创建了
所以我进入数据库客户
现在我们也有目的地客户
这是一个parquet文件
我们也可以看到列名
在下一讲中，我们将看看如何安排这些运行
以及爬虫 这就是我们在下一讲中要做的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/021_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p21 025 Scheduling crawlers & ETL jobs (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在上一堂课中，我们设置了我们的etl工作
如果我们看一下，
我们可以看到这已经被创建了
如果我们看一下详细信息，
我们也可以导航到运行
在这里，我们可以看到这已经被运行
现在我们可以再次运行这个
但我们可能也不想只在需要时运行
而是按计划运行
这也很重要，了解如何做到这一点，所以为了这样做，
我们可以只是进入这些掉落详情
在这里，我们可以导航到日程
在这里，我们可以为我们的etl工作设置日程
我们可以只是来到这里创建日程
例如，我可以说这应该被做
我想提到，我们会在后续删除这个
因为我们不想为此承担成本
所以因此我们会只是创建它
然后在后续再次删除它
好的 所以名称可能是每月日程像这样
让我们使用日程像这样
然后我们可以选择频率
例如每天，在这里我们可以指定开始时间和小时分钟
例如，让我们说在三点半
这可以运行
我们也可以设置每月
然后我们可以指定月份的日期
所以这可以在
例如月份的第一天，然后三点半
所以这里我们可以只是指定频率，然后
当然如果我们想要，我们也可以添加一个描述
然后我们可以创建一个日程
然后这会被激活
我们也可以添加多个日程
所以也许每周一次，然后也许也每月一次不同的
如果我们想要这可能
这不太有用，或者我不能想象一个有用的情况
所以通常我们实际上只有一个日程
但在某些情况下可能会有使用多个日程的选项
所以如果我们有一些非常定制的日程，
这也可以做到
所以现在对于这些家伙如所说，
我们想要再次删除这个
所以我选择它然后在操作下，
我想要说， 我想要删除这个日程
那么这会被删除
然后我们刷新后，
这会被删除
所以我们可以快速看一下
这可能只需要几秒钟
好的 现在我们想对爬虫做同样的事情
在这里如果我们回到爬虫概览
然后我们跳转到我们设置好的爬虫
我们也可以在这里设置一个日程
这也可以在这里的日程导航选项中进行
我们可以用同样的方式做
我们已经按需设置了这个
但我们也可以编辑这个爬虫
爬虫的属性
在这里我们可以转到调度部分
这是步骤四
在这里我们也可以更改这个频率
例如 我可以说这个应该每天再次运行
在这里我们有同样的选项来定义日程
在我们这个案例中我们不想在这里设置日程
但我们只是想演示我们可以设置这些频率
无论是对爬虫还是对etl工作 希望对你有帮助，下次课见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/022_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p22 026 Stateful vs Stateless.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈有状态和无状态的系统
同样在这个方面，有状态和无状态数据摄取
那么在本讲座中
这也是当然与考试相关的
我们将涵盖这一理论
然后，我们也将通过实际例子来应用状态全与状态无的区别
首先
什么是有状态的系统
在有状态的系统中
系统只是保持与用户或客户端每次交互的状态或上下文
这意味着
例如，当我访问一个网站并登录我的账户时
我做了一些更改
也许在我的用户偏好设置中
然后我再次访问网站
我将保留过去的交互记录
这就是为什么它是有状态的
在这里，我们当然有优点，这有助于提高用户体验
因为我的先前交互被记住
对于后来的请求，它们将被继续使用
这与无状态的系统形成对比
在这里，每个新请求总是独立处理
所有的先前交互将完全忽略
例如，RESTful API
我们必须为每个请求提供所有我们需要的信息
因为以前的交互没有被存储
没有会话数据或存储的东西，以便我们可以记住我们以前插入的内容
所以每个新请求总是独立运行
这也可以与数据摄取相关
当然，在有状态的数据摄取中
系统只是记住在摄取事件中已经完成的内容
例如，我们可以使用时间戳或偏移量
或者也可能使用处理状态来
例如，记住以前已经加载的内容
然后，我们只能加载尚未加载的数据 或者尚未处理的数据
然后我们有
当然，也有无状态的摄取
这基本上也是重新加载一切 但这并不总是坏事
但有时我们不想存储任何东西
当然这可能比存储任何东西都更简单
在前往AWS Lambda时，我们可以执行一些代码，仅在发生某些事件时执行
然后，我们可以处理与该事件相关的对象
这意味着我们不需要记住任何先前的加载信息
然后，我们可以加载新的对象，该对象是由触发器提供的
这意味着我们不需要记住任何先前的加载信息
然后，我们可以加载新的对象，该对象是由触发器提供的
这意味着我们不需要记住任何先前的加载信息
然后，我们可以加载新的对象，该对象是由触发器提供的
但是也许通过
例如aws glue
如果我们在一个yeah中存储多个文件，并且出现了额外的文件
也许在某种形式的s三文件夹中，并且额外的文件出现
那么我们可能想使用状态全量加载
这意味着我们希望使用之前的信息，之前的加载
因此我们想要使它有时也状态全
那么在aws中这是如何工作的
我们在aws中有这两种选项
例如，在亚马逊kinesis中我们可以
当然 当我们想要流化数据时
我们需要在实时中存储数据
因此我们需要维护状态
否则我们不能跟踪已经处理的数据
所以我们需要处理数据序列号，哪些数据已经加载
然后我们能够恢复摄入，我们从哪里开始
也许有一次有某种形式的失败
然后我们需要知道我们上次停止的地方
所以这里支持了亚马逊kinesis数据流
并且 例如
当我们使用aws数据管道时
我们可以使用状态全量和状态数据摄入
并且在glue中
我们可以使用书签配置etl作业
以便我们可以启用一些增量加载
这样我们可以跟踪进程和之前加载的数据
这样我们只能处理新出现的数据，还没有处理的
所以这是通过使用书签我们可以做到的
这也是我们在下一讲中想要讨论的，以便演示我们如何实现aws glue中的状态全量数据摄入
这就是我们在下一讲中想做的 所以那是我们在下一讲中想做的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/023_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p23 027 Stateless Data Ingestion in Glue (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在让我们看看如何应用这个状态全和状态不足的概念。
AWS数据摄取
实际上我们已经从前缀文件夹中加载了一些数据。
或者将文档文件夹移动到目标文件夹中
并且我们为这个使用了一个etl工作
现在可能会发生我们有额外的文件
所以例如
如果我看到这种情况，在这种情况下我们可能会有额外的文件
我可以直接上传这个
所以我去上传
因此，让我们假设这个附加文件会出现
现在，如果再次在aws中加载数据会发生什么
如果我们去我们的etl工作并再次运行这个
那么这将再次加载所有数据
我们可以快速演示这一点
我再次运行这个工作
所以在我的文件夹中
所以它已经开始是的
所以在我的文件夹中
如果我去目标
我们可以在我的桶下看到目标
里面只有一个文件
这里有2.4千字节
这里有一个在3月份加载的文件
实际上我们可以
甚至使用athena
所以我可以进入表格
我们记得当我们加载这个的时候
我们创建了一个表格
在这里我们可以看到这些parquet文件
这就是我们通过ETL作业加载的数据
我可以直接来这里查看表格数据
这样我们就可以快速查看数据如何
再次我们没有输出位置
这是我们需要设置的东西
也许我只需要再运行一次
然后它就应该起作用
现在我们可以看到这个数据在这里我们有这十个记录
如果我现在加载这个附加文件
就应该有
这是客户的
总共理想情况下有两个记录
所以我们在第一个中有11个
然后这里有10个更多
好的 这就是我们在这种情况下想要的
但在我们看见这个运行在我们的etl drop中被完成之后
所以让我回到我们的etls
我们不幸地发现所有数据都被重新加载
所以让我们再次查看这份工作
并且看到这次运行完成后，我们可以看到这次运行已经完成，现在我们如果我们重复这个查询
所以我们再运行它
我们会看到
如果我们现在改变
当然，限制
当然它限制在十 所以我们只看到十个
但现在如果我再运行它
我们看到三十二
这意味着第一次运行中的十一个数据又被加载了
所以如果我再次排序，可以看到
让我或许排序
让我或许按id排序
所以我只添加order by
然后我说id
然后我看到id从一到十一的数据都被加载了两次
然后当然我们从十二到二十一的数据都被加载了
这基本上是新的文件
这是一次加载的
当然，我们希望总共只有两十一个
所以不重复加载第一次文件的十一个记录
这就是我们想要实现的，所以
因此我们需要实现增量加载
在aws glue中可以使用书签来实现这一点
这是如何实现的 现在我们想要在接下来的讲座中更详细地了解
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/024_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p24 029 Stateful Ingestion with Bookmarks (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们让我们更仔细地看看，我们可以如何使用书签来创建一个状态
全数据摄入
使用aws glue来做这个
我们可以导航到我们使用的工作
在这里，工作详情下
我们可以配置它以启用书签
这样我们就可以记住我们做的那些设置
这里也有书签选项
所以这里说，指定aws glue如何处理书签，当作业运行时
所以这里发生的是，当它启用时
它会记住之前处理的数据
当它被设置为暂停时
它会更新状态信息
所以我们记住的状态基本上是信息的
在这种情况下，已经加载的和尚未加载的
并且意识到即使有一个现有文件中的修改
例如 在一个现有文件中添加一些行或数据被更改
这也会被书签处理
所以这可以非常有用
当然，如果设置为禁用，那么状态信息将被忽略
因此，如果我们想再次设置它
我们需要再次加载第一个文件
当它被启用时
这样状态信息就会被保存
这样我们就会记得第一个文件已经被加载过
然后我们可以添加第二个文件
这样我们就可以看到只有额外的文件会被加载
然后总共我们应该只有两个记录在这里
所以我们快速做这件事
因此我们将在这里清空
这个桶
这是我们的目标桶
我们记得我们设置这个为目标目的地
我可以选择所有这些
然后我可以删除这个
所以我只需确认我想永久删除这些对象
然后一旦这完成了
我也快速想要更新来源
所以在文档中我们记得我们有那两个文件
首先我们想要如我们所提到的
只加载第一个文件
这样状态信息会被更新
我们已经加载了这个特定的文件
然后在第二次运行中我们应该只加载
然后是我们将在第二次etl运行中加载或上传的第二个文件
所以我也会继续删除这个
所以我又可以说永久删除
所以现在我们记得我们只有这个一个文件
因为我们刚刚更新了
实际上我们还需要以启用状态保存它
因为我们刚刚启用了那些书签
现在它们还没有存储任何信息
但只有在第一次运行后
这意味着 即使我们已启用它们
我们将当然再次加载此文件
因为我们之前的运行中未启用它们
因此无法存储任何信息
因此现在让我们开始我们的第一次运行
在我们保存后
所以我们可以看到这已被更新
现在我们可以继续运行
我们只记得此文件
也在目标目的地
我们不应该有任何东西可用
所以我可以搜索这个
我们有我们的桶
这里什么也没有
在此任务完成后
我们可以来这里查看运行详情
然后我们会回来查看
如果我们上传
然后第二个文件
然后看看有多少记录可用
并且看看此增量加载是否起作用
大约两分钟后完成
现在我们运行同一查询
我们应该看到这前11行
现在我们想要
当然演示增量加载
如果这确实起作用
当然我们在目标桶中看到
如果我们刷新
这里有这个power k文件
现在我们想在源中添加额外文件
当然我们想要测试 看看这是否起作用
让我转到源
这里现在添加此额外文件
所以现在我们可以拖放它
我将上传它
现在我将再次运行同一ETL
现在我们将期望只有那些我认为21条记录
所以 而不是32条
因为这第一个文件将再次加载
以获得那些21条记录
所以让我们继续运行此drop again
所以我们大约一分钟后再次看到
现在已被成功
现在我们再次运行相同的查询，没有限制
当然 现在我们想看看是否现在有那两个记录
确实，这种情况发生了
所以这做得很好
我们也可以实际上删除这个order by
然后我们会看到，这正是我们所希望的
我们已经成功实现了增量加载 所以像这样，我们已经创建了一个状态全的数据摄入
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/025_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p25 030 Glue Transformations (ETL).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看我们有哪些重要的转换方法可以使用
我们再次回顾我们的etl过程
在glue中，重点关注我们可用的重要转换能力
当然，我们有一个初始的发现阶段
这意味着我们可以连接到数据源
并从那里提取数据
这里有不同的服务
不同类型的数据源可用
然后我们可以使用提取的数据来转换它
然后将其加载到我们的目标存储
当然，如果我们将我们的数据重新加载到目标中
我们也可以决定我们希望更新我们的数据目录
在这里，我们可以选择数据如何更新到我们的数据目录
例如，我们希望为这个数据创建一个新的表
我们也可以选择我们在数据目录中的数据库
数据应该存储在哪里
然后，一旦我们提取了数据
我们可以 当然，我们可以决定我们希望转换数据
这意味着我们可以进行基本的操作，如过滤数据
加入它 或者进行不同类型的聚合
此外我们还可以拥有更多的复杂或更高级的选项
例如我们有一个名为查找指标的转换能力
这允许你识别重复数据或匹配记录在不同的数据集
例如
如果你有两个数据集，数据不完全相同
也许有一些拼写错误
或者它只是具有不同的结构
例如你有一个产品目录，你有自己的产品目录
也许一个竞争对手有一个类似的目录
你可能想找到匹配项
但结构不同
也许你没有匹配的字段，比如标识符或主键
那么你可以使用查找匹配转换来找到这些匹配的记录
或者你可能想以另一种方式清理你的数据集
查找一些重复项
因为也许有一些拼写错误或信息不完整
那么你可以使用这个基于机器学习的转换
用机器学习来找到这些匹配项或重复项
所以这可能是一个相当有用的选项
如果我们想做一些像这样的事情
此外 当然，数据隐私也非常重要
所以当我们处理敏感信息时
我们可以使用检测PI
这代表个人身份信息
因此，这可以让你识别和管理那些敏感信息
所以这可能是
当然，当我们必须处理隐私法规时，如GDPR等
在这里我们可以使用这个来检测敏感信息
所以这将扫描数据以查找潜在的敏感信息
在这里我们可以有不同的选项，如姓名和数字，这可能是个人可识别信息
也许是社会保障号码
也许是信用卡详细信息等等
一旦检测到这些信息
我们可以配置它以对其进行各种操作以保护数据
所以，我们可以在这里同时使用它来检测和管理应用，以保护数据
例如，我们可以对其进行令牌化
或者我们可以说我们想要完全删除它
所以我们这里有不同的可用选项来管理敏感信息
并将其转换为一种方式，以便这种敏感的个人可识别信息得到保护
所以这是我们可用的另一种选项
当然，当然也有可能只是
实际上这也是最常见的用例之一，同时也是改变文件格式的一种方式
在这里 例如
我们在数据源中有一个CSV文件
但我们希望将其用于分析目的进行优化
因此，Parquet可能更好
这是一个基于列的存储
所以这更优化了分析目的
当然，任何其他常见文件格式的转换也可以做
所以我们可以处理json等等
这也可以被转换
这也是一个非常常见的用例
然后我们做完了转换
我们希望将数据加载到我们的目标数据存储中
所以这里我们可以
例如 将其加载回数据库或红移数据仓库中
或者简单地将其保存在我们的s三桶中
如果我们想在我们的s三桶中进行操作
我们也可以使用像athena这样的服务查询数据
我们不需要移动数据
所以我们可以将其保留在s三中
我们可以使用athena运行查询
甚至使用亚马逊红移数据仓库在这些s三桶中运行的查询 所以这就是关于aws glue中转换能力的内容
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/026_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p26 031 Glue Data Quality (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看Glue中的数据质量功能
这有助于我们提高数据的完整性和可靠性
我们想在实践中动手操作，以了解这是如何工作的
在这里，我们可以使用一组质量规则，以确保数据符合我们的期望
我们可以自定义这些规则，这些规则然后具体到我们的数据
我们还可以使用机器学习算法来识别我们数据中的不寻常模式
这与传统的基于规则的方法有所不同
如果某些东西被遗漏
这可以捕捉到这些错误
我们可以在Rest中应用Glue数据质量
那就是我们的S3桶中的数据
例如 我们也可以在ETL管道中使用它来处理数据传输
这就是我们现在要看的，以了解这是如何工作的概念
所以，现在我们想打开我们之前创建的ETL作业
这是一个非常简单的作业
我们只是连接到一个S3桶
然后将数据加载到另一个目标桶中
现在我们想做的是添加一个新节点
这就是数据质量评估
这是用了这个功能
所以我正在搜索数据质量
我已经可以看到评估数据质量和完整性
我们选择它 然后这会添加到我们的屏幕上
我们也可以关闭这个
现在我们想做的是
我们将数据源连接到这个
我们可以配置它
在这里我们也可以实际上直接删除这个
所以我们稍后再连接到它
现在让我们配置这个评估数据质量
我们可以看到非法参数
这里没有提供任何规则
当然我们现在要设置这个
所以让我们看看规则集编辑器
这里我们又有异常检测
这将自动扫描我们的数据并生成我们对数据的观察
但我们想使用传统的规则集编辑器
在这里我们可以配置基本条件
我们可以说
例如，列数应该大于10
或者我们可以说完整性在0.4到0.8之间
如果你觉得复杂
你还有这里的助手
所以你可以说
例如，我想说列数
如果我在这里规则中
我可以从这些助手中添加这些规则
例如，我可以说，我想添加这个
所以 当然，如果我们现在查看这一点
所以这将为我们准备一个预览
我们可以看到这条规则会失败
在这里我们可以看到这些数据集有七列
当然，它不符合我们的约束条件
列数等于十
因为我们的数据源中只有这些七列
好的 这就是它应该的样子
因此，我们将其更改为七
这在我们的情况下是正确的
然后如果我们等一会儿
这又在加载预览
现在通过了
现在我们可以添加其他规则
我们可以说我们想要另一个
我说只是逗号
现在我可能想要检查数据类型
在这里我也可以编辑
顺便说一下，实际上我们也可以以更动态的方式使用它
所以这些只是示例
例如，我们可以使用像平均过去十天这样的东西
这样你也可以创建动态规则
这些只是示例
你可以在这里指定条件
也可以使用代码
或者你可以检查是否存在列
所以您可以使用这些不同类型的操作
现在我说我想要使用列数据类型
所以这里再次
当然，第一列不存在
这可能会失败
因此我们只是使用
例如名称
当然，这并不符合日期数据类型
所以现在这也应该失败
所以它又在加载
当然，0%的行超过了阈值
所以它失败了
像这样，我们现在可以说我们要做什么
所以让我们向下滚动看看有什么选项
现在我们
是的 我们想做什么
因此，让我们滚动到底部看看有什么选项
现在我们
是的
我们可以说，我们还想输出原始数据
所以这里有原始数据输出
在这种情况下我们可以
当然，我们也可以将我们的桶连接起来
也许我们可以再有一个桶
这个桶也会使用我们的质量规则输出
我们也可以决定输出应该怎么处理
如果我们
比如失败了
在这种情况下我们再次选择主音符
在这里我们可以说在规则集失败时
我们应该可能失败我们的工作而不加载目标数据
所以我们可以配置这样使得工作停止
如果失败了
我们不会继续
这样我们可以节省容量并将它整合到我们的工作流程中
所以在这个课程的另一部分我们会讨论工作流程
这样它就会失败我们可以触发另一个操作
这就是我们可以这样使用的
以便管理和确保我们数据质量 好的 希望对你有帮助，下次见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/027_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p27 033 Glue Workflows.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈Glue的编排选项
这是Glue工作流
在这里我们可以管理我们的爬虫和我们的工作的执行
这是一个理想的选项来管理我们的Glue操作
在这里我们可以在Glue内部管理操作
这意味着我们有爬虫
例如，这里有一个爬虫
然后我们也可以有掉落
也许我们有多个工作
然后在中间或开始我们可以有触发器
所以这里我们可以有不同的类型的条件
基本上，所以例如我们有一些东西被触发
当Glue掉落成功完成时，爬虫
或者也许我们可以有另一个触发器
有条件在前一个中有一些错误
这将触发 然后另一个工作
所以这里我们可以在Glue内部管理不同的组件
所以Glue掉落
爬虫
以及在之间也有一些条件
所以这些都是触发器
基本上，如果我们想要一个更复杂的编排工具
它也使用其他服务
那么也可以使用Step Functions而不是Glue工作流
所以这在这种情况下会更好
所以这主要是我们的Glue组件
除了那些例外
所以这里你可以创建一个工作流在视觉方式
这意味着你可以使用视觉界面来理解不同任务的流程
你可以配置不同的组件
也许在之间不同触发器
并且像这样监控
也整个工作流程的执行并组合不同的组件
彼此
你也可以从一个Glue蓝图创建一个工作流
或者你也可以使用这个界面手动构建它
或者也使用AWS Glue API
这也是可能的
所以这里我们已经谈论了触发器
这在这里是非常重要的
我们可以有不同的条件包括
并且这些触发器可以启动工作或爬虫
所以基本上没有区别
并且在掉落或之前的爬虫完成时激活
所以我们可以配置这是当爬虫完成时触发
或者也当它失败
所以这里有不同的可用配置选项
并且除了这个每个工作流都由一个主要触发器启动
这是一个启动触发器
而且这还有不同的类型
因此我们可以安排触发器
这些触发器可以在固定时间间隔启动工作流
我们可以在时间表中定义它们
例如每天
每周或每月
或者也可以设置任何其他自定义周期我们可以使用
例如 使用Cron表达式设置此类自定义周期
我们也可以选择按需
这将允许您从Glue控制台手动启动工作流
像这样，您可以在Event Bridge中设置工作流规则
或者配置其为某些事件的响应
所以您可以使用 然后使用Event Bridge从外部管理
同样，在这里，您可能又有更多复杂的选项可用
这使得它与按需触发器结合使用更加灵活
例如 您也可以设置一个Lambda函数来调用此工作流
像这样 这些是不同的选项
而且一般而言，从Glue内部
您也可以使用Event Bridge事件
这将在工作流响应特定由Event Bridge捕获的事件时启动工作流
像这样，您可以设置实时事件驱动架构
这很有用
当然 如果您想对一些实时事件做出响应
例如将文件上传到桶中
这可以在Glue工作流内部管理
这些都是不同的类型
再次，按需触发器可以与其他服务结合使用
例如
Lambda函数
或者我们可以使用Event Bridge规则来管理此触发器
这也可以在Glue之外管理
所以这里我们可以连接它
所以这就是Glue工作流在理论上 现在让我们也跳到实践中
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/028_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p28 034 Glue Workflows - (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在让我们看看在实际中如何使用Glue工作流
这是如何工作的
因此，让我们搜索Glue
在这里，我们可以在Glue中使用工作流
这样我们就可以编排我们的不同元素
包括这个包括爬虫
还有Glue drop
我们可以使用触发器来连接这些不同组件
当然也可以
设置日程或实施基于事件的ETL
例如
所以让我们看看这是如何工作的，我们首先在在这里选择工作流
通过添加一个新的工作流
我们将基本添加一个空白工作流
然后我们将使用图形界面来配置它
这是一个名称
我们可以说也许工作流
让我们说一些什么
只是非常简单
我们也可以添加其他属性
例如设置最大并发性以设置并发运行的最大数量
或者我们也可以添加标签等等
但这基本上就是这样设置我们的空白工作流
因此我们转到创建工作流
如果我们要实际配置它
我们必须选择使用此图形界面
在这里我们从一个触发器开始我们的工作流
我们总是要先添加一个触发器
如果我们选择它
我们可以现在选择一个现有的来使用它在这里，或者我们也可以添加一个新的
例如 我将说这是一个触发器
可能是按需
像这样
在这里我们有这些不同的触发器类型
所以我们可以说应该按计划运行
所以我们说应该每天或每周运行一次
或者我们可以使用这里的自定义频率使用cron表达式
或者我们可以在这里设置
这个下拉菜单
这是可能的
我们也可以使用事件
然后它只是基本使用前一个
让我们说组件的输出
这是接下来我们会看到的
我们也可以按需使用
如果我们选择这种类型
我们也可以使用其他服务，例如
事件桥 我们可以设置一个事件桥规则，以便也从事件桥触发此工作流
或者我们可能使用lambda函数来调用此工作流
这些都是与该触发类型可能实现的事情
或者我们也可以在这里使用事件桥事件
然后我们也可以在这里管理它
但在我们的情况下，我们将使用如前所述的按需选项
所以我们会添加它
然后它就会开始
所以现在我们可以像这样手动从以这里运行此工作流
现在我们如果想要添加其他组件
例如，我们希望运行我们的工作
那么我们可以在这里添加额外的节点
所以现在我可以在这里添加要么掉落物还是爬虫
如我们所提到的
我只选择我们的第一个etl掉落物
所以我会在这里添加它
现在我们如果想要添加如我所提到的几秒钟前所说的其他组件
我们将不得不添加额外的触发器
如果我选择这个etl掉落物
我现在可以配置一个新的触发器，使用此工作的事件
所以无论这是否失败还是已成功完成
我现在可以添加一个新的触发器
所以我会这样做
在这里我选择了事件类型的触发器
触发逻辑可以是
例如任何监视事件
或者所有监视事件
如果我们有多个事件连接到这个
所以如果我可能有五个不同的etl工作
然后所有他们都已成功完成
我可以触发此触发器
在我这并不重要
我只有一个 所以我会留下任何
我会添加它
当然让我们也添加一个名字
让我们说这是第二个触发器
也许像这样 我们可以添加它
然后之后我们可以
如我们所提到的
添加额外的
是的掉落物
这样我们就可以在这里集成额外的节点
我们也可以添加在之后应该发生的事情
所以这触发器实际上应该触发什么
所以我可以添加 例如现在爬虫
我想要 完成这项工作后
我想添加一些爬虫
在这里我们可以选择任何爬虫
没关系 我将添加它
然后这样就设置好了
如果我想获得更多信息
我可以选择它，在这里我可以看到这是触发逻辑
所以任何被监视的事件后
在这个案例中，它只是监视之前的工作
并且当它成功时触发
如果我想更改
我也可以说我想选择这个
在这里我可以说编辑被监视的事件
在这里我可以说这可能应该更改为失败
如果我这样做
我将基本更新这个触发器
所以这是应该发送的事件，在这里
如果我现在选中了这个触发器
我可以看到这将监视失败的输出
所以在这种情况下，我可以在这里修改它
我将其更改回成功
所以我转到被监视的事件并更改为成功，它已更新
然后它也会更新到我们的触发器中
当然我也可以添加其他东西
所以我可以说我想添加其他工作来触发
然后这将在这里添加一个节点
或者我可以说，我想添加其他掉落或爬虫来监视
然后这将在这里添加一个节点
或者我可以使用视觉界面
在这里我也可以添加一个新的爬虫，像这样
然后它也会在这里添加
这就是它的工作方式
然后从这里我可以按需运行它
所以我可以说我想运行工作流
或者使用事件桥规则来触发这个
或者我们可以使用完全不同的触发器
所以我们可以按计划运行它
例如 所以我可以删除这个触发器
然后也许我想添加一些其他触发器
所以我想说，也许我想添加一个新的
然后这将在每天运行
是的
这将在每天运行
我添加了 当然，我会给它起一个名字
让我们说测试触发器
我将添加它
然后这将在开始时添加
那么这就是它
现在我们想连接这个，所以这意味着我选择它
我去到操作 我说添加工作触发
我选择这个
在这里我有工作
所以我会选择这个
然后它将其连接到我们的工作
那就是已经属于这个工作流的工作
有时候在这个可视化界面有点混淆
但基本上这非常简单，我们可以像这样
设置我们的工作流
在我这个案例中我将继续并实际上删除这个
因为我已经按照时间表设置
我不想按照时间表运行
这样我们确保我们不会产生任何费用
我在这里将删除这个工作流 我希望这个实践演示对你们有帮助，下次再见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/029_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p29 035 Glue Job Types.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈我们的胶水滴的不同选项
在这里，我们可以使用不同类型的底层引擎
我指的是不同的工作类型
当我们创建我们的胶水工作
我们在这里使用胶水的不同选项
所以我们可以使用视觉界面
在这里，我们可以只是拖放
如果我们这样做
在引擎下自动为我们选择的引擎将被选中
所以我们不能更改它
并且这将会根据我们选择的数据源类型自动设置
所以通常这就是spark
所以这里这就是底层使用的
通常对于glue
但我们也可以使用一个笔记本
所以我们可以进行一些数据处理
也许交互式地
也许只是使用
这里也是一个笔记本
并且对于这一点我们也有不同的类型可用
所以这里我们可以选择它
同样的，当我们自己编写脚本时也是真实的
所以当然当我们以视觉方式创建一个任务时
那么在幕后
也会有为我们生成的脚本
但如果我们想要做的完全从零开始
那么在这种情况下我们也可以有更多的控制来选择
例如不同的引擎
也许我们有一些轻量级的任务只需要使用python shell
所以这可能是一种成本节省的方法
在这里我们也有新的ray
我们也可以使用spark
所以让我们快速讨论一下它们，先从spark开始
所以spark是默认的
通常这是用于不同类型的大规模数据处理工作流的常见方法
在这里我们只是使用Apache Spark在幕后
在这里我们使用spark的分布式计算能力
当然这非常适合处理大数据工作负载
在这里我们可以选择使用两到一百个GPU
所以这很多，并且默认这里是十个GPU
实际上已经是很多
但这仍然是针对大规模数据处理和单个dpu
顺便说一下，这代表数据处理单元
这仅仅代表处理能力
在这里我们使用它来执行我们的etl任务
在这里每个dpu包括四个vcpus和十六GB的内存
我们是基于dpu小时构建的
所以如果我们想要缩小规模
我们不能少于两个
我们也有spark streaming etls
所以，这个东西可以用来实时分析和处理数据
在这里，我们也可以对事件做出反应，当他们发生时
所以我们是实时做的
在这里，我们可以与流数据源一起工作，如kines's数据流或kafka
在这里，我们有相同的dpu选项
就像在spark drops中一样
再次，默认这里是10
此外，正如提到的
我们也有python shell drops
所以，我们用它们来做更简单的或更小的数据处理任务
当我们不需要这种分布式计算的能力时
我们可以使用Python shell
在这里我们可以在受管理的环境中运行Python
用于非常轻量级的任务
这是因为我们可以使用更少的资源
与那些大规模的Spark ETL相比
因为我们可以从零开始配置
从零点零六到五点五tpus到一dpu
所以你可以看到，即使是最大值也低于Spark ETL的最小值
对于轻量级的任务
在这里我们可以节省很多成本
当我们不需要这么大的电力时，spark ETL工作将不再需要
因此，这将是一个很好的选择
最后，我们还有ray工作
这是刚添加的
在这里，我们可以使用ray框架
这是一个开源框架
我们使用python在一个多节点的环境中在分布式集群中运行它
所以，本质上，ray是python原生的
如果你想使用python
你对这个很熟悉
也许用于机器学习或数据科学
然后你可以使用你在python中熟悉的技能
所有这些框架和库
并将它们用于连接和扩展你的大数据集
你不需要修改太多代码，因为它是python原生的
所以你有这个选项
所以很多时候
当你想使用你在python中熟悉的东西时，这是有用的
也许用于机器学习
或者你可能有一些并行化
所以你可能想并行应用多个转换
或者你想在同一个Python工作负载上运行数百个数据源
或者可能在机器学习上进行大规模的操作
摄取 在你的数据上进行并行批处理推理
所以这是对机器学习的特定情况
例如 你可以现在访问这个ray引擎并在glue中运行它
在这里这些ray掉落
它们运行在新的基于引力子的易于工作的类型上
所以这与Spark有点不同
所以那些新的基于引力子的EC two工作类型
它们仅适用于radrops
再次
这同样适用于大规模并行处理任务
然后我们也有执行类型
所以我们有 当然标准执行类型
这只是默认的
但我们现在也添加了新的flex执行
所以这种类型只是成本有效的替代品
如果我们有较少时间敏感的etl
有时执行时间并不重要
在这种情况下我们可以节省成本
在这种情况下，drops只是酷的
它们在资源可用时运行
因此可能会有一些延迟
但它们可以因此节省成本
例如 我们可以说我们想要一个每周的数据转换drop
因此我们不需要特定的时间
因此我们可以在这种情况下使用flex执行来节省一些成本
所以这些都是不同的特性 以及我们可以如何以不同的方式配置我们的glue drops
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/030_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p30 036 Glue Job Types (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们快速在实践中演示一下
所以我们只想简要回顾一下这在实践中会是什么样子
因此，让我们导航到我们现有的etl工作
我们可以看到，我们可以在这个视觉编辑器中创建一个etl
在这里，我们记得，引擎将自动为我们设置
根据这个etl的任务和数据源
所以我们记得我们之前创建了这个drop
在这里，如果我们看一下
我们可以在drop详情下看到，这里我们看到类型
并且这里会自动为我们设置，我们仍然可以
当然可以做一些更改比如
例如调整使用的语言或者工人类型我们可以增加
例如或者自动调整工人数量
这里我们记得我们有工人的数量
这基本上是GPU的数量
这就是我们构建的基础
所以基本上我们构建在GPU的小时数上
我们也记得对于这种引擎我们不能使用少于两个DPU
所以如果我想将此更改为1
如果我尝试保存
我得到了一条错误信息，在这里
允许的最小工人数量是2
在这种情况下 因此我们只能使用2
如果我们想使用更少
我们必须使用不同类型
我们可以使用 例如Python shell工作
在这种情况下这将是一个替代方案
此外我们还记得
我们可以使用弹性执行来降低那些时间敏感性较低的工作的成本
那么在这种情况下，它可能会稍微晚一点被执行。
那么我们在我们日程中定义的那样
但是因此我们可以节省一些成本
所以我们也可以在这里检查这个柔性执行
并且如果我们想手动选择这个
如果我们想把这个写出来
我们也自己编写代码
因为我们记得，如果我们在这个视觉编辑器中这样做
当然在引擎盖下
这个脚本也会为我们自动生成
因此它也会使用脚本
但我们只是在这里以视觉方式定义它
在这个视觉编辑器中
因此如果我们想要
我们也可以使用脚本编辑器
所以我可以在这里自己定义引擎
我可以说 例如
我想创建一个射线任务
然后从这里重新开始写剧本 所以这只是一个快速演示，展示在实际中这是怎么样的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/031_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p31 037 Partitioning.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们也快速谈谈胶的性能方面
分区特别适用于查询数据
一旦数据处理已完成
当我们有大型数据集分布在不同位置
例如我们的s3桶
这对查询很重要
这也适用于胶
因此，我们可以通过设置分区来优化数据处理
我们在这些分区设置文件位置
这样我们可以根据某些条件组织数据
所以这是可以的
在日期中这是非常常见的
我们有一个文件夹，年份
例如，这是1个桶
这里有一个位置
一个子文件夹
数据 另一个子文件夹只是年份
如果我们将数据组织成这样的分区
然后我们有一个
也许常见的查询条件
例如，我们希望获取2023年的数据
所以我们有where year = 2023
我们可以跳过所有其他分区
我们只需要扫描与我们相关的数据
这样我们可以减少查询执行期间读取的数据量
这样选择性扫描减少了iops操作
这样我们可以加快查询速度
这样我们可以提高响应时间
对于etl操作，当我们将数据组织在这些分区时
胶也可以独立处理每个分区
这样当我们有不同的分区时，我们也有优势
我们可以确保数据更容易管理，像这样
当然 由于我们有更好的性能
需要更少的时间
我们也有更好的成本
当然，对于后来的查询
这更好 因为那些服务，如athena
它们基于扫描的数据量收费
这样分区也会直接导致成本节省
通过减少扫描的数据量
当你使用aws胶时
你通常在设置你的etl作业时定义那些分区
你也可以直接在胶中做
你可以设置爬虫
它们会为你做这项工作
所以胶可以自动识别并处理这些分区
如果数据被正确组织
并且在第三部分
分区通常在目录结构中指定
所以我们在这里使用一个与我们在这里看到的类似的方案 所以这种组织数据的分区策略非常普遍
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/032_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p32 039 AWS Glue DataBrew.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在让我们更详细地看一下aws数据酿造
这是数据工程中我们肯定需要了解的东西
当然也是为了考试
这很重要
所以数据酿造是一般的东西，我们可以用于数据准备
我们可以以视觉方式做这件事
所以我们这里有一个非常简单的视觉界面
我们很快就会看到这种界面的样子
但如果你已经熟悉Excel中的Power Query或在Power BI中
这非常类似于这个
所以这里的目的是以视觉方式使用此工具
这样我们就可以看到我们的数据预览
我们可以应用数据转换步骤
所以这主要用于清洁和重构我们的数据
所以数据预处理这可能是我们是的一部分
我们的etl数据转换
但也当然常用于机器学习
例如，仅在数据准备和数据预处理方面
所以这是AWS Glue套件的更大一部分
所以这一般是为了大数据分析而设计的
数据酿造拥有两百五十多种预建的转换功能
它们可以帮助你非常容易地修改和转换数据
你可以在这里做很多不同种类的转换
我们很快就会进入这些内容
因为它们中的一些也很重要
总的来说，每个人都可以做到这一点
你不需要成为编程专家
你可以用这种无代码的方式去做
就像这样
在这个图形界面中，你可以自动化重复任务
因为你还可以安排你的数据释放
像这样，你可以节省时间，并确保数据准备得当
数据一致，并且你已经检查了数据质量
然后当然，一旦数据准备好
你也可以将其与其他aws服务集成
所以你可以将清理后的数据存储在s3和目标存储桶中的服务
或者你也可以将其移动到红移
例如 以供进一步分析
你也可以将其与aws湖形成集成
也许用于安全和治理
并且与aws合作
我负责管理权限
所以它与其他服务广泛且容易集成
这就是界面看起来的样子
所以我们在这里可以看到一些概念
在这里我们可以看到 数据的预览
我们还可以看到有一些列概要
在这里我们可以看到一些统计数据
有多少个不同的值
数据是如何分布的
一些统计数据，如均值、模式等
我们可以对数据有一个概述
我们可以通过视觉方式很容易地识别
数据中的一些不一致性
数据中的一些问题
这就是我们在这里的项目
在这里我们做的事情是，我们可以使用此界面添加步骤
这些步骤是转换步骤
我们有不同的东西可用
我们稍后会讨论这些
例如
我们可以分割一列
或者我们可以对数值进行四舍五入
或者我们可以将文本值转换为大写
或者我们可以分割一列
也许在一列中有城市和邮政编码
我们希望根据特定字符分割这些列
或者我们可以合并和分组
像这样，我们可以将多个步骤组合成一个食谱
所以让我们谈谈这些重要概念
稍后我们也会在实践中深入探讨这一点
首先我们有一个项目
在项目中 这基本上是我们配置我们的转换任务的地方
这意味着我们可以添加一些步骤
步骤就是我们想要应用的转换
我们可以看到我们的数据集的预览
然后这些步骤将组合成一个我们之前见过的食谱
我们可以看到 例如
如果他们正在排序一个特定的列
然后我们可能想要删除一些重复项
所有这些然后将组合成我们的食谱
所以这基本上只是我们对数据集应用了转换步骤的组合
我们也可以轻松地保存这些食谱
然后在其他项目中再次使用它们
用于其他数据集，因此它们可以重用
然后一个掉落基本上就是食谱在一个数据集上的执行
在这里我们可以指定一个特定的输出路径
例如我们希望将结果存储到s3桶中
这也可以安排
所以我们可以在特定的时间间隔上运行它
因此我们可以自动化这些转换工作并使其非常容易
进行这些种类的数据转换
数据准备
只需自动化并按计划运行它们
我们也在前面提到过我们可以看到不同类型的数据概要
所以我们可以在整个数据集上运行这个
然后它会分析我们数据集中的所有内容
我们也可以看到列统计的预览
所以当我们选择一个特定的列时
我们可以看到，就像这里看到的
根据数据类型
我们有不同类型的统计信息和可视化
所以你可以大致了解
值是如何分布的
也许有很多缺失值你可以看到
是否有很多唯一值或者很多不同的值
所以你可以在这里看到概览，以便对数据有一个理解
这也许能识别出数据中的一些问题
这就是概览
现在我们想深入探讨一下
谈谈一些重要的数据转换，这些转换对于重塑数据很重要，以便我们能从另一个角度审视数据
这可能对于准备数据很重要
例如，用于数据可视化
或者也可能用于机器学习模型 例如 数据可视化
或者也可能用于机器学习模型 那么我们在下一节课中就来做这件事
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/033_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p33 040 AWS Glue DataBrew - Transformations.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在在我们深入实践之前
我们想讨论一些重要的转换，这些转换在数据工程中普遍重要
当我们谈论数据转换时
数据准备在数据工程中
再次 当然
这是非常重要的准备数据
可能用于报告
这些是非常常用的函数
了解它们实际上在做什么非常重要
因此我们来讨论它们
首先 我们需要将多个列组合成一个列，其中包含这个映射
或者这个包含键值对的字典
这意味着键总是列名
然后值将是这个特定列的值
然后这里 例如
三十是年龄
三十城市是纽约等等
如果我们有另一行
我们也会在这里添加一行
这就是它如何工作的
我们可以基本实现这种json格式的格式
我们拥有这个包含键值对的字典
这种类似但不创建映射
但数组是嵌套到数组函数
所以这里我们有这种转换类型，它只是组合这些到一个数组
这里我们没有键值对
但我们只有这个包含值的数组
这就是只创建这个数组
然后我们也有另一个函数和转换，它与嵌套到映射非常相似
但我们可以非常确定
百分之百确定它将保留确切的数据类型和确切的顺序
当我们需要确保数据类型被保留，并且顺序也被保留时
那么我们可以使用嵌套到stru而不是as到map
但那之外它们是相同的，当然这也很重要
反之亦然 因此我们有解嵌套函数
这里作为一个例子
我们可以展示一个嵌套数组
这将使用给定的数组并将其展开为多个列
我们选择这列
然后它将此展开为多个列
这将结果为这些分开的列
所以我们有埃利斯三十和纽约
所有这些值都已拆分为多个列
解嵌套map也适用于此
这将使用给定的映射键值对
并将其展开为多个列
所以这将以完全相同的方式工作
然后我们有一些额外的非常常用的转换
我们想要讨论的第一个是 pivot
所以 pivot 实际上在做什么
以及它是如何工作的
所以这里我们将数据从行旋转到列
这意味着我们选择一个 pivot 列
例如这里是季度
然后 pivot 值所以这里我们可以
例如选择销售
然后将其 pivot 到列中
所以这意味着我们的 pivot 列是
然后这是季度将进入单独的列
然后行将使用那些值将其拆分为那些列
所以你可以看到 例如
Q1 的产品
A 在这里
是150 所以这里会进入
所以这将创建一个更报告式的表格
并且这直接可见在一个更接近报告的感觉中
但是当然如果我们用于数据可视化作为报告的基础
通常这将实际上会是更好的结构
所以因此我们通常得到这种结构
然后我们想要 unpivot 它
因为像这样当我们有一个季度在一个给定的列中时，更容易可视化数据
而不是分散到多个列中
因为那样我们可以只使用这里的列作为季度
并且这更易于数据可视化
通常所以这里我们会做相反的事情
所以我们需要选择我们要 unpivot 的列
然后我们将属性放在一个列中
例如列名并将值放在其他列中
所以例如 在这个例子中我们有属性所以这里
实际上已经处于良好的格式
但是如果我们将其像这样
我们可以 unpivot q1 和 q2 列
这意味着它们将进入我们的属性列
这将意味着这是我们的季度列
所以这将再次有 q1 和 q2
这将只是 pivot 的反向操作
如果我们 unpivot q1 和 q2
但是在这个例子中以查看这里如何工作
我们想要 unpivot 的列
他们将进入属性
并且那些列的实际值
都将进入值再次
这就是 unpivot 和 pivot 的工作方式
最后，我们也想谈谈转置
这将只切换列和行
所以这实际上是在视觉上旋转表格
像这样，我们可以看到名称
年龄和城市将进入行
所以所有列都进入行
在这里，我们有 然后是这个属性列
所以这是对玫瑰名称的
然后我们当然有爱丽丝和弗兰克
在这种情况下 它们进入列
所以这基本上只是改变列和行
在这里，我们转置它
你可以视觉上认为只是旋转你的表格，改变行到列
这就是转置
现在，最后，让我们也简要讨论一些非常常见的变换，总的来说
我们有当然有一个连接
这是通过连接列连接两个数据集
有一个常用列被使用
例如 它可以是一些关键列
像这样，我们可以结合两个数据集
我们也可以根据分隔符将一列拆分为多列
例如，我们可能有邮政编码和城市只在一列中
它们只是分开
可能是由空格或连字符
我们可以使用这个分隔符将其拆分为多列
因为我们也可以过滤一个数据集
所以我们想要过滤掉
也许我们不想保留的一些值
它们无关紧要 或者它们可能有一些缺失值
像这样，我们可以过滤掉
有时也可能是一个很好的中间步骤来排序它
但一般来说
是的 排序对于数据可视化准备可能不是那么必要
但它可以是一个中间步骤，以便我们可以在这个基础上进行额外的变换
或者只是看看值可能看起来如何
当然，我们有不同类型的转换
我们可以将字符串转换为日期时间
或者我们也可以将其转换为数字
这样我们就可以确保数据类型是正确的
这也是重要的
然后我们有不同的计数类型
不同类型的统计，我们可以简单地汇总数据
并且这些只是一些我们应该知道的重要变换的选集 现在，让我们进入实践，看看glue数据酿造
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/034_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p34 041 AWS Glue DataBrew (Hands-On).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在让我们也看一下数据酿造在实际中的应用
所以我们在这里搜索数据酿造
然后我们就可以看到可视化数据准备，用于清理和分析数据
这也包括例如
为报告准备数据
这里也包括机器学习
准备和预处理数据也很重要
所以是的 我们可以清理和归一化数据
应用转换
我们可以以交互的方式做到这一点
所以对于这一点 在我们开始我们的样例项目之前，让我们快速谈谈价格。
因此我们可以使用交互式会话
这就是我们将要做的，我们将只使用这些转换并将它们作为步骤添加
我们看到数据的预览
这样我们每场会建立一美元
但是，第一次会有前四十次的课程是免费的
数据酿造的用户
所以我们会尝试这个
并且一次会话是三十分钟
然后我们在使用数据酿造时
所以我们想在生产中使用它
并且安排它并将其放在一个特定的输出路径
那么它将按节点和每小时构建
我们也可以配置这一点
有多少节点 我们可能想要使用的最大节点数
这将按分钟构建
当然它将每小时构建
但是到最后，它会在两分钟内被分解
所以这将是计费单位
或者，是的 如果你使用
例如 半小时
它将只是半小时
这就是它如何构建的
当然，在这里我们已经看到我们重要的组件
比如食谱项目
数据集
工作 我们现在只是想将其付诸实践
因此，让我们现在开始创建一个样本项目
你可以选择你最感兴趣的任何数据集
我就选择这些著名的棋局
因为我喜欢下棋
你可以在这里选择数据集
以便在这里工作并尝试
我们有样本项目可用，我们也必须选择一个角色
我以前创建过一些角色
你也可以直接创建一个新角色
然后这个角色将为你设置好
在这里你有这个前缀
然后我们可以说
例如测试可能变成像这样
然后这个角色也会为我们创建
现在我们可以继续创建项目
然后交互式会话将被启动
所有事情都将为我们准备就绪
所有的计算资源
以及数据集也将提供
所以这可能只需要一分钟左右
然后当我们一切准备就绪时，我们将回来
大约一分钟后，一切准备就绪，一切都已就绪
让我们看看这是如何工作的，我们可以看到
当然在中间 这是我们数据的预览
在我们应用步骤后的样子
我们可以在这里构建我们的食谱
以及我们可以使用所有不同类型的转换来构建
当然，目前我们还没有应用到任何步骤
但我们实际上可以做两种或多种不同的方式
但一切都是交互式的
所以，我们可以
例如，我们可以在这里看到列的数据类型
在这种情况下，它是一个字符串
我们可以说，例如，我们希望将其转换为
让我们说一个日期 当然，在我们这种情况下这不起作用
但我们可以说，例如，我们希望将其转换为一些字符串
但当然，实际上这里一切都已经准备好得很好
但我们可以做其他转换
所以，我们可以
我们可以选择这列
然后我们可以从这里应用它
所以现在你可以选择一些转换
例如，我可以过滤掉缺失值
或者我可以应用函数
例如，我想做的是
我想 是的
使用一些文本函数
因为这是一个字符串
我想将其转换为大写
这样也会将所有字符转换为大写
在我们这样做之前
你会看到在这里你有列详细信息
所以你会得到一些信息
关于数据质量和其他统计信息
你可以在这里的列概要中看到一些信息
在这里的列中
此外，你也可以在这里的概要中查看
然后，你可以选择运行数据概要
这将创建摘要
一些关于你数据的统计信息
但我们不想这样做
但你可以在这里看到一些概述
如果你去方框
是的 这样你可以获得更多关于
也许你数据的质量
有多少无效值
数据是如何分布的等等
所以你可以很容易地在这里看到
这样你就可以了解你的数据
并可能额外识别一些问题
如我们所说 我们要做的是
我们要应用这些步骤
正如我们所说，我们可以通过仅仅选择一个列来交互式地做
或者我们也可以从这里构建它
例如，我可以说
我想删除一个特定的列
这可能不需要
我们希望它准备好
以便更容易使用
所以我可以说
也许让我们说创建日期列是令人困惑的
我们不需要它
我们希望删除它
所以我可以选择一个列，这将应用于
我总是可以说预览更改
那么在这种情况下，这将很容易
所以我们只是删除这个列
但这将有助于在我们实际应用它之前看到我们的数据将受到影响
所以我们看到 是的
这就是我们想要的，我们现在应用它
这样它将将这个步骤添加到我们的食谱中
在这个食谱条带或这个面板中
我们现在可以看到
这个应用的步骤 在这里我们看到
删除列是第一个步骤
我们也可以编辑它并在之后删除它
或者我们说我们要应用更多步骤
例如
我想做
也许 我想创建一个
也许可以从那个数据中提取一些统计数据
我想应用某种数学函数
我并不是说这有意义
例如，我们可以说，我想应用加法函数，也许可以加
让我们说，我不知道一个具体的数字
所以我可以再次选择不同的配置
在这里我选择了加法函数
然后我可以添加一个特定的值
让我们说，我将添加值为
然后我说，我想再次预览更改到
确保我没有做我不想做的事情
然后当我对这一切都满意时
我可以看到，是的
这实际上是我正在寻找的
我可以说应用步骤
好的 像这样
在这种情况下，我们创建了一个额外的列
并且它现在也是可见的
所以我们可以看到这里的步骤是可用的
而现在我们可以做的是
我们可以要么发布这个食谱
以便可以重复使用
但这不是我们想做的
我们也可以额外创建一个掉落
在这里我们可以创建一个新的掉落
在这里我们可以说
例如 这是我们的机器学习掉落一
例如
在这里我们可以定义一个输出
在这里我们可以说，这将是一个特定的as三桶
我们有一个特定的文件格式
然后当然如果我们选择csv
我们有一个特定的分隔符
我们也可以添加一个压缩等
在这里我们可以指定位置
当然，我们也可以做
我们可以配置我们想要的容量
在这里我们可以说，也许我们不想有这么多的单位可用
所以我们只把这个降低一点
这样我们可以节省更多的成本
然后我们也可以给它添加一个日程
这样我们可以按计划运行它
如果我们还没有创建过日程
我们也可以在这里创建日程
所以我们提供一个日程名称
让我们说这是每日的
在这里我们说，也许我们有
是的 也许只有特定的日子
所以我们可以说这可能只限周二和周三
在这里我们有这个每x小时
所以也许每20
让我们说也许每8小时我们希望这样做
然后我们可以看到这里这些发生
我们也可以说也许像这样的两三
然后它将像这样运行
所以这里我们可以设置我们希望这样
然后我们可以说我们要添加这个到我们的工作
然后一旦我们有了这个日程
我们也必须使用一个角色
所以我只是在使用我们之前创建的这里
然后我们可以说创建和运行drop
所以这就是我们要做的
在这里我们也有我们的食谱
正如我们所说 所以这里我们可以发布食谱
这样我们可以在特定的数据集给定的项目中重用它们
所以这里你可以看到我已经包括了一些数据集
和一些项目
在我们停止之前
我想也实际上删除所有的项目
这样我们确保我们不会不必要的产生费用
我正在通过选择删除这样做
然后我将删除那些项目
然后之后我们已经删除了所有东西
这样我们确保我们不会不必要的产生费用
这是glue数据扫帚的快速实际演示 希望对你有帮助，下次见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/035_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p35 043 AWS Lambda.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看aws lambda函数
这有助于我们的数据摄取
也在数据处理中发挥作用
我们理解aws lambda在这个摄取和处理过程中的作用
通过理解aws lambda实际上在做什么
本质上 aws lambda允许我们运行代码
而不必在后台管理服务器及基础设施
这就是aws lambda在做的
让我们运行代码
我们不必在后台管理服务器
这意味着这是一个无服务器计算服务
我们只需专注于编写代码，专注于我们要做的事情
然后所有的基础设施都会在后台为我们管理
而且这是基于需求自动扩展的
当我们有大量工作负载时
这会自动扩展
我们又不必担心在后台管理或更改任何事情
因为这会自动为我们扩展
我们可以使用各种编程语言编写代码
这里我们有可用的语言
这非常灵活
对我们来说
这的使用场景
在我们这个案例中 当然我们专注于数据处理任务
我们可以将数据存储在s3中
像这样，这也是一个非常常见的场景
或者配置一些事件驱动的摄取
这意味着对某个事件做出响应
某事正在发生
例如，一个文件上传到s3桶中
这可以配置为触发器
然后代码将被执行
所以新上传的文件将被处理
所以我们可以执行一些代码来做一些事情
比如分析数据或修改数据
清理数据
或传输数据
也许到另一个桶中
在这里，我们可以处理实时事件
例如 如我所说
上传文件
但总的来说 当然我们不仅局限于数据处理
但任何我们希望在事件响应中执行代码的情况
所以这里我们可以通过触发lambda函数来自动化任务
工作流
对事件做出响应
这就是AWS Lambda在做的事情
这样做的优势是它非常可扩展
我们已经提到过我们不用担心计算能力不足
这是我们需要管理的事情
但这会自动为我们进行扩展
这非常经济实惠
因为我们只需为我们需要的付费
所以我们只需为将要使用的计算能力付费
再次强调这是无服务器功能
所以它非常简单
我们不关心在后台设置任何东西
顺便说一下，这也是无状态摄入的示例
函数的每一次调用
所以教练的每一次执行都是独立的
它不会存储任何状态或任何先前执行的信息
当然，我们可以将其连接到其他服务
然后在某种方式上也可以调整它并使其保持完整
但通常这是无状态的
所以代码的每一次执行都是独立的
现在我们来看看一些更实用的东西
那么这些用例将如何有用
在实际操作中这将如何看起来
当然，我们也会在几秒钟内亲自实现这一点
所以让我们想象一下，我们希望执行一段代码
也许我们希望在文件到达S3桶时处理某种类型的数据
所以我们上传了一个文件
或者这个文件可能从其他应用程序或另一个服务到达
然后，我们希望一旦这个文件到达，基于这个事件，立即执行Lambda函数
所以执行Lambda函数，基本上就是执行代码
所以执行代码，基本上就是执行代码
我们可以通过使用S3的通知来实现这一点
这是基于存储桶中发生的事件发送的
例如，新文件正在上传
这将从存储桶向我们的Lambda函数发送S3通知
在Lambda函数中
我们可以配置不同类型的触发器和S3通知
存储桶中发生了某事，我们已配置，如文件
上传 这将触发
然后执行代码
当然，这段代码可以做各种事情
例如，将文件传输到另一个存储桶或处理数据
无论代码在做什么
另一个例子是实时处理来自kinesis数据流的数据
假设我们有一个数据流
可能一些物联网设备持续生成数据
别担心 稍后我们也会谈到kinesis数据流
但现在我们只是捕获一个数据流
现在我们可能也想处理这些数据
也许我们可以做些什么
把它存储在某个地方 我们也可以连续使用这个lambda函数
我们可以通过设置kinesis事件源来设置触发器
我们将其映射到事件源
然后以连续的方式执行代码
例如，我们可以设置一些批次
假设我们有一百条记录
并且每100条记录都在这个数据流中连续到来
对于每100个批次
我们正在执行每一百条记录的代码
然后这可能再次被处理并发送到另一个桶
所以这也是可以在aws lambda中配置的东西
所以这里我说的这些一百条记录
这是默认设置
再次如果我们想要扩大规模
因为可能我们有大量的记录以非常高的速度流入
并且可能我们有复杂的处理任务
这也会再次自动扩展
有时候数据流入的波动很大
所以，自动扩展这一事实在这种情况下也非常有用和实用
这是关于aws lambda和一些用例
并且 当然，现在让我们跳到aws并实现这个案例 我们在下一讲中见过的案例
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/036_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p36 044 Event-Driven Ingestion with AWS Lambda (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看事件驱动的数据摄取的实际实现方式
在我们的用例中使用aws lambda函数
我们要做的就是我们要使用这个文件
它应该上传到一个桶中
然后一旦这个文件出现在桶中
aws lambda函数应该自动触发，基于文件的出现
所以任何新上传的文件
然后应该自动将这个文件移动到另一个桶中
我们将设置一个源桶，我们将文件上传到其中
然后当这个文件出现时
这应该触发aws lambda函数将文件移动到我们的桶中
这是我们之前已经创建的
所以这是我们的第一个桶
因此我将首先创建这个新桶
我们将其命名为源桶
假设可能是数字的附加组合
然后我们可以将所有设置保留原样并继续创建这个桶
现在我们有两个桶
一旦文件出现在这个桶中
我们将手动上传它到这里
这将触发函数
然后将文件移动到这个桶中
这就是我们想做的
要做这个 我们将设置aws lambda函数
因此我们导航到服务lambda
这样我们就可以在不考虑服务器的情况下运行代码
这就是我们想做的
在我这种情况下 我已经有一个函数创建了
但我们可以重新开始
所以我们选择第一个选项
从头开始创建作者
我已经提前设置好了这段代码
所以你可以看到 这只是一些简单的Python代码
当然，我们必须在这里更改目标存储桶的名称
在这里我们也有源存储桶
但这将基于事件
所以我们根据我们设置的触发器
所以这会自动处理
然后我们很快就会看到
这是如何工作的
所以我们将在我们的函数中使用这段Python代码
好的 首先我们将设置函数的基础
我们可以选择一个函数名
让我们说这个是我的事件函数
然后我们可以选择运行时
在我们这个案例中我们说我们使用Python
然后我们几乎有了所有需要的东西
让我检查一下，是的
这些都很好，更改默认执行角色
在这里，我们将默认创建一个具有所有基本lambda权限的新角色
然后我们也可以使用另一个角色
或者我们可以向这个角色添加一些额外的权限
这是我们需要做的事情
以便我们能够实际将文件写入到另一个s3桶中
但我们首先会创建这个基本的角色
然后我们会修改这个角色的权限
所以这些都是这个函数的基本内容
我们将继续创建像这样的函数
所以你可以看到它非常基础
设置 现在我们可以在下一步添加代码
然后也是函数的触发
所以步骤一我们有这里
我们已经准备好了这个代码
这只是一些非常基础的代码
甚至可以使用cgpt创建这个
正如所说，在这个情况下，我们需要修改目标存储桶的名称
我们可以再次打开这里的名称
在我们的情况下，我们希望使用第一个存储桶作为目标
这就是它应该被移动的地方
让我像这样粘贴，不带破折号
像这样，这是目标存储桶的名称
正如所说，源存储桶将来自我们的触发器
我们将在给定的存储桶上设置事件通知
然后，我们将自动将存储桶名称传输到这段代码中
然后在这段代码中，我们将能够使用存储桶名称
所以我们知道桶在哪里
然后我们可以将其转移到这里的指定桶中
这就是所有需要做的
当然，我们还可以做一些额外的事情，比如
例如 删除文件
如果我们有权限
所以，如果这个lambda函数的执行角色有这些权限
这些都可以做到
当然，还可以实现额外的数据处理
但我们这里只想保持简单，只移动这个函数
然后我们完成这一步后
我们可以转到配置
因为在这里我们会在权限下看到
我们有一个角色
这就是创建的角色
正在执行这段代码
如果我们想修改这个
我们可以点击这里，这会打开一个新标签页
我们将被导航到
我在这里可以管理角色并给予额外的权限
我们可以看到已经附加了策略
这只是为aws lambda函数提供的基本权限
但我们可以添加额外的权限
所以我可以附加一个额外的策略
并且为了简单起见
我将添加一项对s3的全面访问权限
当然，我们可以提供更精细的权限
如果这个是生产环境，我们可以这样做
但在这里，这已经足够了
所以我们添加 s three 全权限
然后我们完成这个之后
我们看到这个现在出现
也在这个角色中
然后我们可以再次关闭这个窗口
所以现在我们有这个角色具有正确的权限
但我们也需要现在设置触发器
我们已经设置了代码
但我们还需要设置触发器
这样 Lambda 函数也会被触发
基于这个将文件上传到桶的事件
源桶
这就是我们想做的
所以我们在这里点击添加触发器
在这里我们可以选择来源
我们提到了 我们可以使用广泛的不同服务作为来源
在我们这个案例中我们可以看到
有这么多我们可以甚至使它搜索服务更容易一点
所以这里我们使用s3桶
现在我们可以选择一个桶的名称
我可以从我的列表中选择
我也可以再次搜索
所以这个名称在我的情况下是
这个只需选择你的桶
现在我可以选择事件类型
这里我默认已经设置了所有对象创建事件
但我也可以使用仅put
所以每当一个文件被添加
这就是是的
在这里触发函数这里你看
在我们这个案例中我们有各种各样的不同的事件
我们只是使用所有对象的创建事件
这样我们确保当我们上传文件时这会触发
我们也可以进一步限制这个通知
或者通知的发送范围
或者通过添加前缀来限制函数应该被触发
所以我们可以 例如
说 如果我们有一些子文件夹，我们可以将其作为前缀添加
或者可选地，我们也可以在特定类型的对象上添加后缀来限制
例如
CSV文件等
但在我们的情况下，我们将保留这些基本设置
然后我们完成这些后
我们可以点击
添加 我们必须在这里确认这个消息
然后我们可以点击添加
然后我们基本上已经设置好了所有事情
所以让我们快速回顾一下
我们配置了触发器
在配置下我们可以看到触发器已经设置在这里
我们可以甚至添加多个触发器
我们也可以在这里看到触发器
在这个函数概览中
在这里我们可以看到这是函数的触发器
此外当我们导航到代码时
我们可以看到我们也设置了代码
现在我们需要做的是
我们需要部署函数
这样它实际上就能运行
然后几秒钟后这就完成了
现在函数已经部署完成
现在我们可以测试函数
我会通过上传文件来测试
这将是上传的文件
我会把它拖到这里
我会上传它
然后过了几秒钟，让我们给它一到两分钟
我们应该看到函数被触发
我们可以在这里的监控中看到实际上函数被调用
我们可以看到错误计数
以及持续时间
所有这些监控细节
在这里我们可以看到调用
也在目标桶中
当然我们应该看到桶或文件出现
所以让我导航到这个桶
这是我们的目标
然后我会在几秒钟后回来
或者可能一两分钟来检查
看看这是否起作用
在我这种情况下，大约一分钟后，我可以看到这份文件已经到达
实际上它非常快
我认为它不到一分钟，我们可以在这里看到
这是最新的文件
这是我们的客户文件
现在已经到达我们的第一个桶
我们可以跳转到我们的lambda函数，进入监控部分
如果我在这里刷新
我应该看到右侧有一个点出现
我们可以看到函数已被调用
也可以看到它花了多长时间，仅仅几毫秒
实际上不到一秒
还有错误计数
我们有零个错误和一个成功
你可以看到 这就是如何设置一个aws lambda函数，该函数基于s3通知触发 我希望对你有帮助，下次课程见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/037_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p37 045 Lambda Layers.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在lambda中，我们可以使用所谓的lambda层
这是一种管理代码和依赖与实际lambda函数分开的方式
因此，我们可以集中管理
例如，跨多个lambda函数的一些共享代码和依赖项
这样管理起来会更容易
也许当有多个函数需要更新时
那么，lambda层到底是什么
所以，lambda层本质上就是一个包含不同内容的zip文件，比如额外的代码
一些库
自定义运行时
也许一些其他依赖或配置文件
它们与主函数分开
但在运行时，它们属于函数的环境
因此，这个函数可以使用这些依赖
我们可以将这些外包到一个lambda层中
然后多个函数可以访问这个层
为了更好地理解这一点
让我们看看没有层的情况
然后我们比较一下当我们有层时的情况
假设我们有多个函数
例如这两个函数
所有的代码依赖
自定义运行时 等等
这些都是实际函数的一部分
它们并没有真正分开
这只是我们给定的函数的一部分
现在假设我们需要进行一些更新
或者我们需要将一些代码共享到多个函数中
这将非常繁琐
而且
是的 函数的大小也会增长
所以我们有一些缺点
因此，我们可以通过将依赖项打包到层中来受益
让我们看看这将如何工作
在这种情况下我们该怎么做
首先，我们打包层内容
在这里，我们将创建一个zip文件
这个zip文件包含依赖项，类似于准备部署包
然后，我们将创建实际的lambda层
在这里 我们只是上传zip文件到lambda并注册其为新层
当然，我们还需要在函数中添加这个层
因此，我们需要在函数配置中指定在执行过程中包括这个层
这就是它如何工作的
然后，函数在运行时可以访问层内容
因此，函数需要的内容将从lambda层获取
这就是它如何工作的
像这样
如果我们想在这个lambda层中做一些更改
也许吧 有些事情正在改变
我们可以简单地进行更改并从这个lambda层中集中控制它
现在我认为你应该对好处有了一些想法
让我们简要总结一下
首先，我们可以将依赖项共享到多个函数中
因为我们一旦创建了一个层
它可以在任何账户中的任何数量的函数中使用
所以像这样
我们可以减少冗余并简化我们的更新
这尤其有用于将核心逻辑与依赖项分开
在这里，我们可以独立管理函数代码和依赖项
这种分离使更新和维护更加容易
我们有分离的功能逻辑和依赖项
这使得它再次在多个函数中更加易于管理
当我们有一些更新时
例如
当然 此外
它还可以减少包大小
通过将库和依赖项移动到层中
我们也可以最小化lambda函数的部署包的大小
这些都是主要的好处和lambda层的工作方式 我希望这有所帮助，下次再见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/038_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p38 047 Replayability.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们想讨论在aws中的数据流处理
在我们这样做之前
我们想讨论几个非常重要的概念
让我们首先从可重放性的概念开始
这到底意味着什么
可重放性是重新处理数据的能力，在我们的情况下
重新处理已经处理的数据
由于某种原因，它需要再次处理
这就像有一个第二次机会来正确处理数据
尤其是当第一次尝试没有按计划进行时
这在我们需要纠正错误时可能非常重要
或者我们可能想适应某些变化
我们需要确保随着时间的推移一切都可靠
并且我们能够很好地处理这些变化或错误
让我们更详细地看一下为什么这实际上很重要
首先
我们有时需要处理错误
某事没有按预期工作
例如 如果你发送一条短信并且信号不好
那么这可能无法发送
然后这也需要重试
所以第二次尝试或某些尝试
希望会成功
就像在数据摄取中，这也非常重要
这对于纠正错误非常重要
如果我们有这种能力，可以在遇到错误时重试过程
也许有些数据丢失了
或者
不是所有数据都被正确处理
那么重试这一事实就非常重要
然后第二次尝试再次尝试
并将此也放在内容一致性的事实中
我们需要确保所有数据都是同步和正确的
在数据流中，有时这可能有点复杂
可重放性有助于同步和纠正数据
以确保所有数据统一且准确
此外，我们也有时会有一些变化
也许某种数据类型正在改变
我们需要适应模式中的一些变化
可重放性也以非常高效的方式帮助适应这些变化
这使得它成为一个非常耐用的解决方案
这也是为什么它很重要
有时我们也想测试和开发一些东西
并且像那样
开发者有能力仅测试新功能
或者使用真实数据修复一些错误
而不会危及实时数据的完整性
因为它可以重放并重新处理
因为可以重放并重新处理
因此，这是一个非常重要的事情
但现在问题是这如何从技术上实现
所以，我们有不同的方法来实现这一点
第一种方法是所谓的项目潜在操作
这基本上是一种华丽的说法，意思是做同样的事情多次，应该与一次做它有相同的结果
例如，如果你多次点击喜欢按钮
并且在某个社交媒体平台上发布内容 那么这将只会产生一个喜欢
我们不希望这多次发生
或者如果你点击提交按钮，可能是提交某个审核
那么你不希望这个审核多次出现
所以，项目潜在操作帮助我们做到这一点
当某事多次发生时
我们只想要一次结果
正如我们所见，有时这是非常重要的
此外，我们还可以做日志和审计
我们应该记录发生了什么
以及这何时发生
这对于跟踪谁在你的数据中做了什么非常重要
这样，当某事出错时
我们可以确定何时事情开始偏离计划
我们可以轻松跟踪并修复它
另一个概念是检查点
这意味着我们可以在数据处理旅程中有特定的标记点，例如
例如，书籍中的书签
这样我们可以返回到那里
所以我们有这个特定的标记点 如果出现问题
我们可以总是
是的
只是回到这个特定点
我们不必从开始
但我们可以只从这个最后检查点开始 然后我们还有一个所谓的回填机制
有时你需要用新信息更新旧数据，回填就是这个选项
所以我们可以回到我们的数据，当我们有问题时
这样我们就可以确保历史数据仍然是正确的和完整的
所以这只是一个非常简短的
概念性概述，不同策略如何实现重放性，为什么这也是重要的
当然，在AWS的服务中，尤其是我们将讨论亚马逊Kinesis
我们有重放性的这个选项
所以这是一个满足的要求
因为这是数据流中一个非常重要的需求，特别是
所以这就是快速总结
重放性在数据处理中是非常重要的
所以这就是快速总结
重放性在数据处理中是非常重要的
所以这就是快速总结
重放性在数据处理中是非常重要的
所以这就是快速总结，重放性在数据处理中是非常重要的
这样我们就可以确保一切都很坚固
一切都准确无误
这就像一个安全网
当出现问题时
我们可以回放并
并且 这将高效地处理
好的 所以这只是对这个主题的快速介绍
现在 当然，我们希望以更实际的方式应用这个，并看看它是如何工作的 AWS中的数据流
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/039_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p39 048 Amazon Kinesis for Streaming Data.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈数据流
这就引出了亚马逊Kinesis的话题
Kinesis是一组处理数据流不同方面的服务
所以我说这是一个服务集合
我们首先讨论的服务是Kinesis数据流
这主要涉及摄取流数据
这里我们可以讨论大量数据
因此这是非常动态和可扩展的，能够捕捉流数据中的峰值
所以这里我们首先讨论的是Kinesis数据流
这是为了捕获数据并摄取它
我们将在下次讲座中详细讨论这一点
此外，我们还有Kinesis的火车站
这是一个完全管理的服务，也是将流数据发送到不同目的地的一种选择
可以将其发送到S3、Amazon Redshift
这是我们想要永久存储数据并做不同事情的地方
我们可以进行分析等
此外，我们还有管理的Apache Flink
以前称为Kinesis数据分析
这是用于实时分析流数据的
我们可以使用标准SQL查询实时分析这些数据
所以每当我们有这些需求时，
需要在实时中进行一些复杂的分析，
我们也可以使用这个服务，
这个数据可以从数据流中发送，
也可以从kines's firehouse发送，
我们会详细讨论这些不同的服务，
来看看这是如何一步一步工作的，
在这里，我们只是想先得到一个概述，在我们深入研究这些个服务之前，
让我们也快速讨论一些常见的用例，正如我们所提到的，
这所有的一切都关于流数据
所以，这里有各种实时数据处理的用例
所以，当我们想要进行某种涉及流数据的实时分析时
我们可以使用亚马逊Kinesis
这可能是一个很好的用例
这是一个例子，我们拥有物联网设备
它们以实时方式生成数据
我们希望捕获这些数据并做一些事情
也许我们希望存储它并进行处理
或者我们希望进行分析，另一个用例
这是一个非常典型的用例，也是为了安全原因或欺诈检测
因为从这个案例来看
我们在处理数据时非常注重时效性
因为我们希望立即发现一些异常
一些安全问题和交易
这需要高度注重时效性，以便我们能够立即做出反应
但是 例如
客户行为
比如在网站上，我们希望尽快
对客户行为做出反应
在所有这些用例中
我们能够通过使用流数据实时处理这些数据
在这里我们可以使用这些数据流
亚马逊Kinesis可以处理这些流中的不同方面
现在 当然，正如我们所提到的
我们想更深入地逐步探讨这一点
从 当然
使用Kinesis数据流开始 这就是我们在下一节课要做的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/040_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p40 049 Amazon Kinesis Data Streams.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 现在我们来谈谈第一个组件或服务
这总是开始的地方
这是kinesis数据流
我们可以用这个捕获和摄取数据
现在我们来更详细地看看kinesis数据流
以处理数据流的摄取
我们想要现在更详细地看看这是如何工作的
沿着这条路的第一步总是所谓的生产者
所以生产者基本上就像数据源
所以这可以是任何生成数据流的设备或应用程序
然后将其写入到我们的kinesis数据流
这就是我们所说的生产者
在这里，我们可以使用aws以非常自定义的方式创建这些生产者
例如 当你有一些需要向kinesis数据流发送数据的自定义应用程序时
那么aws sdk非常灵活
你可以真正自定义所有不同的选择
就像所有的低级细节一样
就像批处理是如何工作的
错误处理是如何进行的
所有这些都可以使用wsdk进行自定义
Kinesis生产库稍微更直观
当你有一个非常高吞吐量的应用程序时
并且你需要高效地发送大量数据到Kinesis时
这也是一个很好的选择
在这里一切都被优化了
我们有一些额外的性能
这也处理了一些更复杂的任务
例如批处理和重试
这将需要我们使用aws sdk进行定制
当然，在这种情况下，sdk会更灵活一些
此外，当我们有一些日志数据需要导入服务器时
我们不想编写自定义代码
那么我们也可以使用kines's agent
这是一个预构建的应用程序，我们需要进行配置
在这里，我们不需要编写任何代码
所以这稍微更省事一些
现在我们也
当然，一旦我们有了生产者
他们是将数据写入数据流的人
在数据流中，我们有这些不同的分片
所以他们是处理数据的人
我们稍后会讨论这些分片
但我们首先想谈谈这是如何实际写入的
所以，在这一过程中发生了什么
当生产者将其写入数据流时
生产者将数据流格式化为数据记录
一个数据记录只是一个数据单元
它可以是结构化数据
未结构化数据
例如
JSON对象
就像我说的，这些单位
它们可以包含高达1MB的数据
除了数据本身
所以对于值
数据记录还包括一个所谓的分区键
而这个分区键
它有一种分区键算法
它将每个记录分配或关联到一个分区
所以这将由这个分区键分配
这样我们就可以非常高效地并行处理数据
现在我们想要更详细地了解分区是如何工作的
所以什么是分区
一旦数据发送到我们的数据流
所以让我们更详细地看一下
我们曾说过这些分区，它们本质上是处理能力
所以每个分区都包含特定的固定处理能力
所以每个分区可以处理每秒1MB的吞吐量
或者每秒最多1000条记录
所以这二者中哪个先达到就是限制
这基本上是一个单独分区的极限，而对于吞吐量
因为我们也想将数据发送给消费者
所以我们也有这个出吞吐量
在这个例子中，每个分区的出吞吐量为2MB/秒
有了这些分区，有了这些分区的数量
我们可以有一个或多个这些分区
通过这种方式，我们确定了数据流的摄入和处理的能力
当然，吞吐量越多
我们需要的分区就越多
这里 这决定了我们流的容量
我们已经提到过，它可以并行处理数据
这是通过使用分区键来实现的
这通常用于并行处理，非常高效
我们可以配置数据流
数据应该保留多久
因为这是一个流
我们通常希望从流中消费数据
因此，默认情况下，数据在那里存活2小时
这也是最小值
但我们也可以配置保留期
高达365天
也是为了耐用性原因
数据跨多个可用区进行复制
这样，如果出了什么问题，无法处理
我们有这些多个可用区
这使得它更加健壮
而且，我也想知道数据是不可变的
这意味着一旦数据写入到数据流中
就无法删除
所以它是不可变的，无法更改
这是数据流的一个特性
当然，在保留期过后，数据将被删除
所以这就是数据流本身
现在我们也想添加一些关于流的信息
它是如何扩展的
这非常重要
因为这里实际上有两个不同的选择
所以第一种是我们可以手动添加和删除这些分片
我们可以动态地添加额外的分片，当需要更多的处理能力时
但也有自动扩展的选项
然后根据需求
这可以弹性地扩展和缩小
根据之前的工作负载
这可以自动扩展和缩小
然后这就引导我们到两种不同的容量模式
因为我们如果想手动操作
我们使用预留模式
在这里我们必须手动指定分片的数量
这可以
当然增加和减少
并且我们需要支付每小时的费用
根据我们使用的分片数量
在按需模式下我们自动扩展
我之前说过
根据过去30天的吞吐量峰值
并且我们以4MB/秒或4000条记录的默认值开始
然后我们根据实际的吞吐量支付
这是关于数据流
然后我们已经提到过
现在我们也想将此发送给消费者
以便他们可以从数据流中读取并进行进一步处理
这就是消费者在做的
他们从数据流中获取记录
并进行进一步处理
这里我们也有之前见过的这些选项
在我们可以构建自己的定制应用程序以消费数据流之前
在这种情况下，我们可以使用AWS SDK
但我们也可以使用Kinesis
我们也可以类似地使用Kinesis生产库
也可以使用Kinesis客户端库
这是一种非常有效的构建应用程序的方式，具有很好的性能
当需要高可扩展性和高吞吐量时，这是一个很好的选择
这也是一个好选择
此外，我们还可以使用其他服务进行连接，所以
例如 如果我们想将流数据加载到不同的位置
我们可以使用数据管道
这是非常高效的方式，非常稳定且可扩展
在这里我们可以做到这一点
然后我们还有管理的Apache Flink
这就是是的
之前被称为Kinesis数据分析
当我们想要做一些更复杂的分析时
比如运行SQL查询或构建Apache Flink应用程序
这也可以在流上完成
当然我们也有其他可以连接的，比如AWS Lambda
当我们只想在响应某事时运行一些非常轻量级的自定义代码
一些被摄入的数据流
我们希望这样做
当然，我们希望以一种不需要管理服务器的方式
我们以这种方式进行
无服务器方式 我们可以使用AWS Lambda
这也是我们将要展示的东西
这是非常轻量级的
自动扩展以处理数据量的方式
这也是我们即将展示的内容
所以这就是介绍
用更概念性的方式理解kinesis数据流是如何工作的
当然，现在我们想要深入探讨并实现它 当然，在实践中
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/041_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p41 050 Throughput and Latency.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈数据流中的两个非常重要的因素
这些都是可以在不同场景中非常重要测量
我们先来谈谈吞吐量
吞吐量只是数据被摄入到我们的数据流中的体积
或者也从我们的数据流中检索
通常以每秒兆字节或记录来衡量
就是单位给定的时间间隔
通常每秒
重要的是也要理解这是一个现实世界的测量
基本上数据流中的实际吞吐量是多少
这是一个实际的速率
这包括了所有可能的因素
因此在现实中我们看到的
有多少数据被处理
这是一个重要的因素
在aws中可以通过 shards的数量来扩展
这是成正比的
因为每个shards都增加了特定的流处理能力
因此这直接关系到shards的数量
有时候 当然
当我们有大量数据时
我们希望增加吞吐量
因此我们可能想增加shards的数量
这样我们可以提高在给定时间处理更多数据的能力
我已经将这与带宽进行了对比
带宽更像是一个最大限制
因此它是吞吐量的一个因素
但它更多是一个理论上的上限
而在吞吐量中还有其他因素起作用
第二个重要概念是延迟
这是延迟
通常在广义上从过程开始的时间到结果的可用性
在aws kinesis中我们有传播延迟
这是我们这里关心的
这是特定地从记录写入到我们的流
直到被我们的消费应用程序消费
直到被我们的消费应用程序读取
所以基本上 这是从数据记录被摄入到我们的流
到它在消费应用程序中可用进行处理的时间
这主要受消费应用程序检查新数据的频率影响
这就是我们所说的轮询间隔
消费应用程序检查新数据的频率
aws建议每次消费检查流中的每个shard一次
这是aws的建议
在kinesis客户端库中
默认是每秒一次
这符合推荐的做法
这通常确保平均传播延迟低于一秒
有时候，虽然我们有一些非常具体的需求
所以我们可能需要一个应用程序以极低的延迟访问流
在这种情况下，我们可以修改默认设置来增加轮询频率
这样我们可以改善传播延迟
这样我们就可以降低延迟
这意味着我们设置
例如
轮询频率设置为小于一秒的值
这样我们就可以每秒多次检查新记录
这样我们就可以更快地获取和处理数据
当然，这可能是一个相对特殊的情况
在这种情况下，我们也需要小心
是的 很好地管理这些调整，以确保数据流也能正确处理
并且我们不会触及任何速率限制 所以，关于数据流，这是两个非常重要的概念
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/042_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p42 051 Creating a Data Stream (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看在实际中这是如何工作的
我们现在想先设置一个数据流
然后我们想放一些记录
然后当然我们也想看到这是如何被消费的
然后我们也想用lambda函数直接处理这些
所以第一步
我们现在想在kinesis中设置数据流
因此我们只是搜索服务kinesis
所以我们看到实时流数据处理
我们只是打开这个
现在我们有了这个概览
我看到我已经创建了一个流
如果我只是跳回去
我可以看到这是整个菜单
你可能也会看到这样的东西
在这里我们可以看到这些服务
例如 我们已经听说过这些
Kinesis数据流我们可以用我们的流收集流数据
然后我们也有额外的服务
当我们进行这个过程时
或者分析流数据
这是我们即将要做的事情
在我们这样做之前，我们还有关于定价的信息
我们看到每个碎片每小时大约1.5美分
然后还有每100万单位的存储单元
大约每单位1美分
所以你看到我们的情况不会太贵
我们将手动插入单个记录
让我们点击创建数据流
这里我们可以给一个名字
所以我会叫它我的第一个数据流
随便什么名字
你可以看到你可以使用大写字母
小写字母，数字和下划线
所以我也可以在这里
使用下划线或破折号
例如
现在我们已经谈论了容量，我们可以选择按需
这是当我们根据需求自动扩展的时候
这就是非常好和有用的
当流速有点不可预测，变得有点不稳定时
当我们确切地知道有多少记录正在流入时
那么我们也可以使用预分配模式
并且只需分配恰好多少分区是合理的
以及我们为总容量所需的分区数量
实际上在我们这种情况下，我们也会使用它
因此我们将使用两个分区
在这里你也可以使用分区估算器
来计算推荐的分区数量
因此，根据记录的数量
例如，我们有10条记录，假设你有
这是平均记录大小
在这种情况下的建议是使用两个分区
在我们的情况下，我们将无论如何使用两个分区
只是为了演示不同分区如何处理我们的数据
好的 一旦我们做了这件事
我们可以在这里看到设置
你可以看到，大多数这些设置
或者实际上所有的都可以在创建后编辑
这也包括我们之前提到的保留期
以及分区的数量
是的 容量模式也可以在创建流后编辑
所以你可以看到设置非常简单
我们现在就这样点击创建流
我们可以看到这是流的概览
让我们现在快速看一下这是如何设置的
是的 设置如下
这看起来如何
现在我们可以看到流是活动的
这意味着我们现在已经开始为流付费
我们还可以看到这里有生产者，我们已经讨论过他们
这是我们生成数据的地方
或者将数据发送到我们的流
然后我们也有消费者
在这里我们也有那些不同的选择，我们将会探索
稍后再说 至少我们使用亚马逊数据火炬
在这里我们可能看不到任何监控信息
在这里我们可以看到是否有任何记录进来
但这可能需要一段时间才能在这里反映出来
然后我们也可以
如我们所提到的 配置它
所以如果我们想编辑容量模式或编辑分区数量
我们可以在这里做
即使这已经创建了，我们也可以在这里编辑
在这里我们可以看到写容量
读容量
以及我们的容量模式
如我们所提到的，这也可以在这里编辑
在这里我们也可以配置增强型扇出
这是我们稍后将讨论的内容
目前我们将使用标准模式
这将非常简单
这就是设置情况
这就是它看起来的样子 当然，现在我们想看看这是如何处理数据的
我们如何能在这里写入数据 我们还想在实践中演示这一点
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/043_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p43 052 Enhanced Fan-Out for Kinesis Consumers.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈我们可以使用的增强型扇出
也在我们的亚马逊Kinesis数据流中
在我们这样做之前
我们首先想快速讨论一下实际上意味着什么
扇出和扇入，扇出只是指的是我们有一个单一的来源
在我们的情况下，它是一个数据流
我们希望将这个分发到多个
在我们的情况下，消费者
所以到多个目的地
这是我们想在我们的数据流中实现的
所以我们有一个数据流，它是
是的 包含数据
现在我们可以让多个消费者消费它
这可能是不同的应用程序，需要从中消费数据
然后我们也有扇入的概念，这只是使用多个来源
并且基本汇总或组合数据到一个目的地
这可能是有用的
例如，当我们有多个数据流时
或者我们有多个
也许设备
我们希望汇总数据
也许我们想聚合它并且从不同的来源组合它
一个例子可能是我们有多个
让我们说，传感器在不同的
例如房间来捕捉温度
然后我们想把这些都发送到一个目的地
这样我们就可以汇总它
另一方面
我们记得我们有扇出
现在将数据分发到多个应用程序
因为我们想在那些不同的应用程序中使用数据流中的数据
由那些不同的消费者
传统上，这在某种程度上是个问题
因为我们可能会有一些瓶颈
这是因为我们有共享的吞吐量
并且这不太可扩展
因为现在我们如果有多个消费者
这可能会导致瓶颈
因为他们共享相同的吞吐量
并且我们记得我们有大约两兆字节每秒的吞吐量
现在如果我们有很多消费者
这可能会造成一些问题
并且这不太可扩展
因此我们有这个解决方案
所以这是增强型扇出的解决方案
这是我们可以在流中配置的新功能
对于我们的消费者
数据通过HTTP推送
这样做的好处是，用这种方法
每个消费者都能获得2兆字节/秒的专用读取吞吐量
现在每位消费者都有专用的读取吞吐量
不再共享
这是每位消费者的专用读取吞吐量
无论我们添加多少消费者
我们可以添加多达20个消费者
每个消费者最多可以获得2兆字节/秒的吞吐量
这是非常重要的，因为这是一个专用的吞吐量
这样非常容易扩展
现在我们添加额外的消费者
这不再成为瓶颈
因为我们现在有那些
是的 每位消费者的专用吞吐量
例如 如果我们有10个消费者
我们最多可以获得20兆字节/秒的总吞吐量
而不是使用标准消费者共享吞吐量时的2兆字节
这样做的好处是
当然现在我们有更高的吞吐量
因为每个消费者都有自己的专用吞吐量
所以我们处理速度更快
这也当然更易于扩展
因为我们可以更有效地处理更多的并发消费者，具有更高的性能
此外，这种方法还可以减少延迟
我们这里的延迟大约为70毫秒，是的
与标准消费者相比，我们大约有200毫秒
如果我们需要更低的延迟
这也可以是使用增强型扇出的一个原因
总的来说，这使开发应用程序更加简单
因为我们不需要自己管理这个
但当我们有许多消费者时，这会自动管理
当然，这就是我们想要使用增强型吞吐量的情况
当我们有多个消费者时，比如更高的消费者数量
让我们说有5个，6个，7个消费者
那么，是的
如果我们需要更高的吞吐量，更高的性能，仍然可能比较复杂 我们可以使用增强型扇出
当然，这也会带来更高的成本
如果我们能承受这个成本，或者这对我们来说是值得的
为了支付一点额外的钱，是的
这意味着我们有很多消费者
我们希望使其更易于扩展，也许我们还想减少延迟
这些都是我们可能考虑配置增强型扇出的用例
当我们有很高的消费者数量时
我们想以最简单的方式使其更易于扩展
也许我们还想减少延迟
这些都是可能的用例
当我们考虑配置增强型扇出时 这些是所有使用案例
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/044_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p44 053 Pull and Consume Data From Stream (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看我们如何与流一起工作，我们希望
当然，现在测试它
所以第一步是
我们想要放入一些记录
当然我们没有自动化的生产者
在我们这种情况下，这就是为什么只是为了理解流的机制
我们希望手动放入我们的个人记录
我们可以使用cli来做到这一点
这在我们的情况下非常有用，只是为了理解我们的流的机制
因此，您可以下载此文件
所以你也可以使用这些相同的命令
当然，我们必须做的是，我们必须将流名替换为你的名字
所以，在我这种情况下，我只是使用这个名字，我们必须指定
让我删除它
所以我将输入流的名称
所以我们必须指定
如前所述，流名
然后也要指定分区键
所以，我们可以指定一个给定的键
我们将测试它
当我们使用不同的关键时，会发生什么
记住，在我们的情况下我们有两个镜头
我们也记得，分区键负责将记录分配给个人镜头
所以，同一个键总是与同一个镜头一起
这就是我们现在要测试的
所以第一步就是现在只放一个记录
这将是记录
它将是数据输入一，请记住，这是用六十四进制编码的
以便它能够高效工作
是的 一切都会高效传输
因此在这种情况下，它以64进制编码
现在我们只想复制并粘贴这个命令
所以我会像这样输入它
在这里我们可以看到
这就是分片ID
所以在我的情况下它是1号
在你情况下可能不同
然后我们也有序列号
这对于每个记录都是唯一的
现在我们想继续进行第二个记录
再次我们使用类似的命令
我可以再次使用实际上这个相同的命令
所以我只是在做同样的事情
我们再次看到
这指向相同的分片ID
并且我们当然有一个不同的序列号
但是现在如果我们想使用可能使用不同的分区键会发生什么
我们使用相同的我们可以看到这次发生了什么错误
我们没有使用流名称
那么让我来纠正这个问题
现在让我们再试一次
现在我们使用正确的流名
然后我们完成这一步
我们可以看到它仍然使用相同的分区ID
所以这当然因为我们的记录不多
因此也可能发生不同的分区键导致相同的shid
所以这是我们知道的当我们使用相同的分区键
它也会转到相同的shard id
但当我们使用不同的分区键
它也可能转到不同的chart
让我们在这里尝试最后一个
我们也会进入它
然后我们看到是的再次
我们忘记了更改名称
所以让我再次输入正确的流名
现在我们已经将这些记录输入
让我们用最后一个试一下
在这里我们看到它转到我们的基本第一个shard
所以这是shard id zero zero
现在我们再次尝试相同的事情
我们可以甚至更改记录中的数据
这将是四
这将肯定会转到相同的shard id
让我再快速演示一下
我们可以看到它再次转到这里
如果我们再次使用第一个中的一个
也许我们也可以更改名称
让我们说这是五
当然我们可以多次输入相同的数据
这不是问题
但我们会只是修改它
所以现在我们可以看到一些差异
所以现在我再次使用
如我所说第一个分区键
我们将看到这将转到我们的第一个chart
这就是我们在这里看到的
但现在当然我们想做的
我们想要消费这个数据
为此我们使用shiterator
所以shiterator基本上是一个指针
我们可以看到哪个部分的流我们想要消费
所以我们使用shard iterator
你可以想象这是一个指针
它指向流的特定部分
所以我们可以从那里消费数据
在我们情况下我们首先获取这个shard iterator
这样我们知道我们从哪里消费数据
所以我们有不同的方法做这件事
首先我们可以使用我们的shard id
所以我只修改了这个为一
这里我们使用最新的图表迭代器
但要注意，这仍然有一个有限的生命周期，只有五分钟
所以它使用最新的一个
但是，如果没发生任何事情，这五个分钟过去了，这仍然可能会过期，所以
因此，我只会使用最新的一个
这样我们就可以看到，一旦我们添加了更多的数据
那么我们就可以消费这个迭代器之后的所有数据
所以现在这就是指针
这就是最新的指针
然后，在这个迭代器之后的任何数据我们都可以消费
这给我们提供了最新的图表迭代器
因此，我们现在也想快速演示一下
所以我们只使用API调用图表迭代器
当然，我们必须再次修改流名
所以我快速调整一下
现在我们再试一次
所以我只删除所有这些
再输入一遍
这里我们得到了图表迭代器
这是一个非常长的序列
所以我只复制它并粘贴在这里
我必须删除最后一个括号
现在我可以复制这个图表迭代器
当然，首先我们应该输入更多的记录
我们记得我们有像这样的东西
所以我们可以使用six
所以这里 我使用图表ID1
所以这是与这个分区键一起使用的
所以我可以在这里输入一些更多的值
例如，我可以输入这个
然后我再这样做一次
然后再次
是的
这些都是我们输入的记录
现在我想获取提到的数据
如前所述
我们想使用我们刚刚收到的图表迭代器
所以我只是继续并复制这个命令
我们想要获取指定图表迭代器之后的记录
所以我只复制了这个命令，带有我们刚刚收到的shiterator
我也再次粘贴
现在我们应该能看到几个数据记录，我可以在这里看到
这些都是我们现在的记录
我们可以看到与毫秒的延迟
以及下一个图表迭代器
有时这可能是空的
我们想看到之前的数据
那么我们就可以使用这一系列的下一个图表迭代器
所以我们也可以实际上复制并测试它
但在我们做这个之前
让我先可能粘贴它
在这里我们看到 这是一个不同的
现在我们想看到这个结果
你可以看到我们有这些记录
这实际上是以基六十四格式编码的
我们还想解码这个
这是默认的
是的 所以我们必须解码它
我们可以这样做
例如解码器
是的 像这样
我们可以粘贴在这里
所以 如果我是在这个
当然碎片ID
让我就使来自流的这个
例如我们期望这是数据输入
也许六或什么
我会只复制这个
粘贴在这里并解码它
我们看到这是数据输入六
这是为了仅数据可以高效传输和传输
并且是的这是正确的编码
我们也可以解码它
当然在任何时间
现在我们已经看到
如果我们 是的
我们也可以得到也许这个
这可能是同一个或之前的一个
让我回去
嗯 是的 实际上是我们拥有的同一个
正如我所见
嗯 或者像我们之前看到的
我们已经多次输入这个
因此它也可以多次可用
但现在我们也可能想使用一点不同的方法
或者实际上让我们快速也演示我们也可以在这之前获取数据
我们使用仅那个旁边的shiterator
这是我们现在要的
第二个我们也想在这里粘贴
我们需要使用这些引号
现在我们使用这个
我们应该 如果这五分钟还没有过去
这仍然有效
我们应该能够从那里获取这些记录
所以我只要在键盘上按q
这样我们就可以从这里退出
然后我会把这个粘贴过来
现在我们也应该能看到一些东西
在这种情况下，确实它是空的
我们可能还能再用下一个
所以我试试这个，也可以粘贴那个
在这里我们可以看到它是不同的
现在我们可能能从这里看到一些东西
好的 这个也是空的
我们还可以尝试另一个
然后我们可能从那里停止
因为我们已经看到可以从那里获取记录
但有时这里可能什么都没有
或者这次喷水器
当然，这可能会发生
当然，在一个生产环境中
通常情况不是这样
因为我们有大量的值通常以恒定的速度流入
所以我再试一次
但我假设这就是全部
所以我们在上一次有了这些记录
是的 所以在这种情况下，让我们尝试使用另一个方法，我们使用最旧的迭代器
我们可以通过使用剪切地平线来实现这一点
如果我们这样做
当然，我们还必须再次使用我们的流名称
我将其粘贴在这里
我可以再次获取分片迭代器
在这种情况下，请记住我现在使用了零零
因此，我们的第一个分片
在这里，我们现在也想要获取这个迭代器
所以我使用了这个命令
如果我这样做
我看到这里的迭代器
这是输出
我可以再次复制并粘贴
这里到我们的获取记录
在分片迭代器之后
我再次可以看到
这已经改变了
现在我只使用这个命令
希望我们现在能够在这里看到一些记录
确实我们看到那两个记录
这可能再次成为可能
在你这种情况下，你可能需要回去
或者你可能需要再次尝试
因为这种喷射器只有五分钟的寿命
是的
因此，你可能想在稍快一些的时候再做一次
现在我们又想要解码它
所以我们可以复制数据并在这里解码
我们可以看到这是数据输入和之前的那些
我想这是我们在这个碎片中仅有的
所以这是数据输入三和数据输入四
所以我们可以看到我们可以使用这个流来存储数据或捕获数据
并且了解它是如何通过不同的分区键分配到不同的喊ID的
我们还快速看到了我们如何使用这些shiterators从流中消费数据
所以这只是一个简短的演示，只是为了演示我们的流实际上正在运行
现在，当然，我们希望深入研究一下，也让它更具实用性
因为当然，在实践中我们不会使用这个shell 但在我们的情况下，我们要做的是创建一个lambda函数
它实时从我们的流中消费数据
然后也想要对这个数据做些什么
例如，将其写入一个s three存储桶，或者以不同的方式处理数据
这就是我们想在下一讲中设置的
这就是我想要在下一讲中设置的
例如 将数据写入S3存储桶或以不同的方式处理数据 这就是我们想在下一讲中设置的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/045_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p45 054 Calling a Lambda function from Amazon Kinesis (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看我们如何将我们的流与lambda函数连接起来
这意味着确切地
在我们的lambda函数中，我们将设置一个来自kinesis数据流的触发器
因此，每当出现新记录时
我们也想在lambda中处理它们，lambda
我们记得这是无状态的数据处理方式
这非常简单，我们可以很快地开发它
这也是无服务器功能
因此，根据工作负载
它会自动调整规模
所以，是的
对于简单的任务，我们希望处理数据并快速开发
我们也可能有一些零星的使用
因此，数据有时才出现
然后我们想处理它
这不太复杂
在这些情况下，使用lambda函数非常直接
这就是我们现在要学习的
因此，让我们转到lambda
我们已经设置过一个lambda函数
所以我们知道基本的操作方式
在这种情况下，我们还需要创建一个额外的角色
以便函数可以执行并访问我们正在使用的服务
并执行它需要执行的操作
首先，我们需要访问数据流
以便我们可以读取数据
然后，我们还需要访问s3存储桶
我们需要权限将数据写入那里
所以我们需要将数据放在那里
因此，在第一步
我们想要设置这个角色
以便我们有所有必要的权限，为此
我们将转到
我 所以我们可以在新标签页中打开它
搜索iam
然后，我们想要创建一个新角色
我们转到角色
在这里，我们要创建一个新角色
在这里，我们将选择服务lambda
这是我们想要用它的地方
然后，我们可以转到下一步，在这里，我们可以添加权限策略
当然，我们也可以创建我们自己的
但我们想要保持简单
因此，我只是搜索kinesis
在这里，我们有amazon kinesis只读访问权限
这就是我们想要的
然后，我们也可以使用s3
在这里，我们可以自定义以仅对个别存储桶有访问权限
但在我们的情况下，我们希望保持简单，并展示lambda函数的工作方式
我们如何设置触发器
因此我们保持简单，使用s3的全局访问
然后我们可以点击下一步
在这里我们又有概览
我们需要给一个名字
那么我们就叫这个lambda kinesis角色
是的 随便起个名字
在这里我们可以看到策略
现在我们可以继续创建这个角色
当然这是我们现在要在lambda函数中使用的角色
这样这个函数就有执行任务所需的所有权限
那么我们现在设置函数本身
我们回到lambda并到这里创建函数
我们可以再次从零开始编写
我们叫它my
函数随便起个名字
然后运行时我们再次选择python
然后我们也可以看一下角色
更改默认执行角色
我们使用现有角色
我们刚刚设置的角色
我们再次搜索kinesis
在这里我可以看到lambda kinesis角色
我们创建的角色
然后我们就完成了
我们可以继续创建这个函数
现在我们创建了函数
当然我们还需要做两件事
首先我们要添加触发器
然后我们也想编辑函数的代码
所以步骤一
我们要添加触发器
我们可以这样做
从这里从函数概览添加触发器
在这里我们可以选择来源
我们也可以搜索kinesis并添加kinesis作为来源
在这里我们可以选择我们之前创建的流
这是我们之前创建的流
在这里我们不需要放任何东西
但是我们也可以实际上留下批处理大小
但是我们也可以修改这里
如果我们每秒有很多记录
我们也可以配置批处理大小
然后我们也有起始位置
这是我们刚刚设置的
是的 或者我们之前看到的
我们也有起始位置修剪水平面
这是旧的shard迭代器
然后我们还有最新的
这是最新的
所以在这种情况下我们可以只保留
是的 实际上我们可以只保留最新的
然后我们会手动添加额外的记录
这样我们就可以看到这将实际完成工作并处理数据
当我们输入额外记录时
好的 所以当我们完成这个
我们可以添加一个触发器
现在我们只想输入我们的代码
现在我们可以在配置下的触发器这里看到
在这里我们可以看到这触发器
我们的kinesis数据流
现在我们想转到代码
你可以从资源的资源中下载代码
请注意，您还需要更改桶的名称
如果我去s 3
您可以找到自己的桶名称
在我这个案例中，这只是我们的第一个桶
因此，我已经使用它并将其输入到这里
所以这是桶的名称
在这里，我们希望从触发器获取记录
从kinesis，我们希望
当然也要直接解码它
然后我们根据时间戳构建一个文件名
这将被写入我们上面定义的桶名
这是一个处理数据的非常简单的方法
然后将其写入我们的s three桶
因此，让我们继续复制这段代码
当然，在你调整了桶名之后
然后我们想把它输入到我们的lambda函数中
现在我们可以继续部署这个函数
所以我们点击部署
现在我们当然想测试它
所以现在一切都应该准备好了
我们在这个函数中有了权限
我们也有了代码
我们还设置了触发器
所以现在我们期望，当我们进入或向我们的数据流添加更多记录时
这些记录应该被写入到我们的s3桶中
我们应该能够很快观察到这一点
因为这应该实时处理
因为我们这里有这个触发器连接到我们的函数
所以让我们通过再次
只是打开cli来演示这一点
在这里我们应该能够再次输入一些记录
所以我们可以再次使用以前做过的事情
所以这里我们使用了类似于put record的东西
所以我们可以添加一个额外的记录
所以我再这样做一次
然后我们在这里看到 shid 一次，让我们再这样做一次
现在我们有两个记录，也许再试一次
现在我们有三个记录
所以我们添加一些基本的记录
现在实际上相对较快
我预计数据很快就会出现在我们的桶中
所以我们去看看那里
确实我们可以看到数据正在出现
如果我想要
我也可以继续下载这个
所以我可以检查这是否确实如我们所期望的那样
我已经下载了这个
我可以在这里看到
这就是记录
像这样 我们非常容易地设置了我们的 lambda 函数
从 kinesis 触发捕获流中的数据
进一步处理并将其存储用于长期使用
因为我们记得在我们的流中
默认设置是流中的数据仅存储四个小时
所以我们可能想得到这些数据并做一些事情
以便我们可以稍后分析数据
希望这个演示对理解我们如何与 lambda 一起工作有所帮助
了解我们如何与 kinesis 一起工作 希望这个演示对理解我们如何与 lambda 一起工作有所帮助，了解我们如何与 kinesis 一起工作
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/046_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p46 055 Common Issues & Troubleshooting.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们快速看一下一些常见的问题，以及这些问题如何影响性能，以及如何解决这些问题。这将有点理论化，但请坚持下去，这对考试也很重要。
我们将讨论一些常见的问题及其解决方案，这些有时与性能相关。
我们将探讨一些常见的问题及其解决方案，这将有点理论化，但请坚持下去，这对考试也很重要。
请坚持下去，这将对我们考试很重要。
我们将探讨一些常见的问题及其解决方案，这对考试也很重要。
请坚持下去，我们将开始探讨一些常见的问题及其解决方案。
我们将开始探讨一些常见的问题及其解决方案，这些问题可能影响性能。
我们将探讨一些常见的问题及其解决方案，这些问题可能影响性能。
只是为了确保数据均匀分布在那些分片上
我们也可以监控分片级别的指标，以便识别那些热点分片
看看是否有一些热点分片普遍存在
我们也可以使用批处理策略
当我们通过量大时
那么直接一个一个处理可能不那么高效
因为会有很多API调用
这在一般情况下并不高效
所以在这种情况下，我们可以批量处理记录
这样一次批处理可以处理多个记录
基本上就是这样
这也是一个好策略 同样，我们也可能遇到消费者方面的问题
我们也可能遇到读取速度慢的问题
同样，这里有一些分片限制
解决方法是增加分片数量
这与读取速度慢的问题类似
我们也可能遇到获取记录数量的最大值问题
这是配置问题
如果我们设置获取记录的最大值过低
这可能会导致最大数量的问题
那么我们应该恢复系统默认设置
这通常是推荐的，这样可以避免此类问题
但我们也可能遇到处理逻辑问题
如果我们遇到这个问题，可以通过使用空记录进行测试
也许可以改进代码逻辑以提高性能
现在我们要讨论一些常见问题
第一个问题是获取记录可能返回一个空数组
我们在演示中也看到过这种情况
这并不是一个大问题，但我们也想快速了解为什么会发生
首先，每次调用获取记录时，我们都会得到分片迭代器值
我们在前面提到过，在下一次调用中必须再次使用此值
这是之前调用返回的值
通常我们会创建一个循环
只有在分片被关闭时，分片迭代器才会失效
通常我们会得到这样一个循环
在某些特定点上，数据可能没有值
也就是说，没有记录
这通常不是个大问题，因为通常情况下会自动处理，这并不是一个错误
但这就是实际情况，这种情况通常有两种可能
第一种情况就是我们分片上没有记录
这通常不是个大问题，因为通常情况下会自动处理，这并不是一个错误
但这就是实际情况，这种情况通常有两种可能 第一种情况就是我们分片上没有记录
这通常不是个大问题，因为通常情况下会自动处理，这并不是一个错误
但这就是实际情况，这种情况通常有两种可能
第一种情况就是我们分片上没有记录
这通常不是个大问题，因为通常情况下会自动处理，这并不是一个错误
但这就是实际情况，这种情况通常有两种可能
第一种情况就是我们分片上没有记录
也许保留期已经结束
或者里面没有写入任何内容
或者在分片器指向的特定位置
在这个迭代器旁边没有找到数据
这也可能是这种情况
这是有一点区别
那么我们就可以继续进行下一个调用
然后在下一个地方
我们期望的数据就在这里
如果我们使用kinesis客户端库
这也会自动为我们处理
所以我们不需要担心这样的事情
所以这更多是一个通常自动处理的机制
所以这里 这至少是使用kinesis客户端库的情况
当然，如果我们使用aws sdk手动编写代码
那么可能会稍微复杂一些
好的 这通常不是一个大问题
我们有时也会遇到一些记录被跳过的问题
这通常发生在一些异常没有被处理时
我们应该检查处理记录时的异常
以确保我们处理了所有的异常
我们也可能遇到分片器过期的问题
我们在演示中通常也看到过这种情况
这通常不是一个问题，也可能自然发生
我们看到，当它没有被调用超过五分钟时，会过期
所以这个分片器在五分钟后会过期
有时这可能指向一些问题
因为在生产环境中
这通常不应该发生
但如果我们有大量的分片
那么我们可能没有足够的容量来存储所有数据
这可能是kinesis使用的dynamo db表的问题
这个表没有足够的容量来存储数据
那么我们就应该增加容量
这可能有时也是分片器意外过期的原因
然后我们还有两个问题
第一个问题是消费者落后
这意味着数据无法以我们希望的速度读取
因此我们落后了
第一步是增加保留期
以确保数据最终会被处理
我们不会永久丢失数据
这就是第一步
然后我们应该尝试监控我们实际上落后了多少
我们可以通过使用迭代器h毫秒或毫秒后最晚
我们可以查看这些指标
我们也可以看看潜在的原因是什么
如果我们看到这种情况以尖峰方式增加
所以我们看到一些尖峰
这可能是暂时性问题
也许与一些API失败有关，这些问题随后会自行解决
我们可以忽略它们
但如果我们看到有一些稳定的增加
并且我们越来越落后
也许我们的代码有问题
我们的处理逻辑不够高效
或者我们没有足够的资源在需要的时间内处理这些
那么我们也应该解决这个问题
增加扇区数量
或者修改处理逻辑
然后我们也有这个问题
有时KMS主密钥权限错误
这通常发生在我们从加密流中读取或写入数据时
我们没有必要的权限
那么当然我们如何解决它
我们只需确保从KMS获得足够的权限
以便我们能够使用正确的密钥
并且实际上可以访问并解密数据
或者在IAM策略中，我们也许也想
是的 调整那里的权限
当然，这是我们需要解决的问题
只是分配必要的权限
这些都是一些最常见的问题
我知道这有点理论化
不管怎样，这些都是一些最常见的问题 我希望这有所帮助
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/047_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p47 057 Kinesis Firehose.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈亚马逊Kinesis数据管道
在某种程度上，这与数据流有些相似
它同样帮助我们进行实时数据处理和数据加载
但首先，为什么它不同
那么它有什么不同
一个关键的区别是，这是一个完全管理的服务
这使得近实时的数据处理和加载变得简单
它使我们能够轻松地捕获
并转换数据流，将它们加载到不同的存储中
这里有两件事
首先，这些完全是为了
是的 更注重实践，更注重易于设置
易于维护
与传统的数据处理系统不同
这不需要这种复杂的设置
我们需要手动管理底层基础设施
例如分片的设置
但在这种情况下，这些都为我们做好了
所以我们这里采用了一种手把手的方法
我们定义了数据源和目的地
甚至在整个过程中进行一些非常动态的数据转换
在加载数据之前
然后我们让服务处理剩下的部分
此外，这也包括自动扩展
因此，当传入的数据量增加时
我们也会自动增加资源
这样我们就可以确保数据处理能够满足需求
我们不需要手动干预
所以这一切都会为我们处理
但是现在让我们更详细地看看这是如何具体工作的
所以这里数据是如何流动的
以及这是如何工作的
首先再次 我们有数据生产者
所以这是旅程的开始
但是现在这里要简单得多
然后是数据流
因为这里我们通常不做任何自定义编码
没有生产者和消费者需要手动编码或复杂设置
但这将以非常直观的方式完成
因此，我们可以使用各种不同的应用程序和aws服务
例如，可能是最受欢迎的kinesis数据流
这里可以消费数据
或者 我们可以直接将这个数据流连接到
此外，我们还可以使用aws云监控日志等服务
如果我们想要监控一些资源
我们可以将日志转发到kdf进行进一步处理和分析
此外，我们还可以通过aws iot连接设备
然后将数据直接发布到kdf，以便我们可以
是的 只需进行额外的
也许从这里进行数据聚合或存储数据进行进一步分析
然后我们也有云监控
我们也可以将一些更详细的事件数据路由到kdf
对这些数据进行实时分析
这也可以
当然然后可以发送到其他地方存储
所以这里我们有这些不同的产生者
从那里kdf实际上缓冲数据
但在我们做这个之前
我们可以看到，我们有几个内置选项
我们可以使用这些特定的消费者
当我们使用这些消费者之一
这是用kinesis数据火烈鸟的一个很直接的方式
所以这里我们有
当然亚马逊s3是我们主要的选项之一
然后也间接地通过首先一个s3存储桶
我们首先将数据放入
然后使用复制命令，以便我们也可以将其移动到红移
此外，我们还可以将其发送到open search
以及第三方服务，如splunk或mongo db
在数据发送到之前
实际上首先缓冲
所以我们这里有这个缓冲机制
这与kinesis数据流不同
这就是我为什么说
总是接近实时
由于这个缓冲机制
数据首先使用一些时间间隔或文件大小限制收集这些批
一旦这些限制达到，数据
那些记录被拼凑在一起
然后从那些消费者消费
这样做的好处是这更有效率
因为这里我们不总是需要对数据进行实时处理
但接近实时足够了
然后我们不需要做那么多api调用
这更有效率
我们不需要使用那么多资源
这也更有成本效益
就像我说的，这个缓冲
要么是按文件大小
这可以配置
高达128兆字节的文件大小
或者使用一些时间间隔
我们也可以配置这些缓冲机制的时间间隔
然后它会批量发送
这将更有效率
此外，我们还可以包括一些数据转换
这意味着
数据在流式传输过程中可以进行重新格式化
我们可以对其进行过滤或进行其他修改
如果我们对数据存储方式有特定要求
或者在某些分析工具中使用方式
我们可以使用aws lambda在实时进行这些转换
这可以很容易地与这里集成
这也是一个非常有用的功能
我们可以在接近实时的时间内进行转换
在数据加载到目标系统之前
此外
这非常依赖于
如果处理数据有任何问题
我们可以将数据路由到一个备用s3存储桶
这实际上也可以很容易地设置
这是一个可用的功能
甚至可以总是发送数据
也许
出于归档目的
将原始记录发送到这个s3存储桶
并且为了可靠性
当我们有问题时
也许处理有问题
有些事情没有工作
我们可以重试
这就是为什么我们可以使用这里的重试机制
因为数据在这个备用s3存储桶中
因此如果有任何问题
我们可以确保数据保持完整性和可靠性
当然当这很重要时
并且当预期没有工作时
这就是它的工作方式
典型的用例包括实时分析
我们有日志数据或事件数据捕获
我们可以在这里收集大量数据
并且我们对接近实时分析没有问题
这是大批量处理的，并且更有效率，并且很容易设置
当然当我们需要应用一些转换时
也许我们需要加密数据
并进行一些小的转换或压缩数据
所有这些都是可能的
因此压缩也基本上是现成的
这非常直接
当我们有那些数据源或那些数据目的地和那些源时
并且我们希望有一个直接的
成本高效的方式，它可以自动扩展并且一切都由我们管理
所以让我们快速总结一下关键特性
首先，接近实时可能是非常有力的
因为我们可以缓冲
因此我们可以非常高效地进行数据处理
同时需要处理所有数据至少接近实时
如果这符合要求
这将是一个很好的解决方案
此外我们还有许多数据格式
这包括像Apache Hadoop这样的结构化格式
Apache ORC
我们在压缩和格式转换方面有很多灵活性
正如我们所提到的
可以集成AWS Lambda
这样我们就可以应用一些自定义的数据转换
这样它允许很多自定义和进一步的数据处理
这非常有用
再次我们有压缩能力和加密也可以在这里实现
基本上出厂设置
这非常有用
现在我们已经谈论了一些这些场景
我们有快速分析
日志数据
事件数据
关于定价方面
这主要是基于消费的定价
这直接与处理的数据量有关
这也相当直接
现在你可能会问
亚马逊数据流和kinesis火烈鸟有什么区别
所以第一个区别是KD需要更手动和全面的设置
我们还有更多的手动干预，如分片
但是因此即使我们需要定制和编码
一些消费者和生产者
我们有实时处理
延迟在两百毫秒或七十毫秒以内
在另一侧使用肯尼的消防站进行增强型扇出
巨大的好处是这完全由我们管理
在这里我们不需要担心
任何广泛的设置调整
它会自动为我们进行缩放
在这里我们的重点是高效的交付
这在设置方面非常高效，非常直接
我们也有资源的高效利用
由于这种缓冲机制和记录的批量处理
此外，我们这里说这接近实时
并且它很容易设置和使用
再次，我们在kinesis数据流中有实时数据
与经典消费者相比，我们有200毫秒的延迟
与增强型扇出相比
我们有70毫秒的延迟
所以这里我们也需要记住，kinesis火炬流
没有任何等价数据存储
所以这里的数据没有存储
但它直接送达目的地
而在kinesis数据流中我们有
数据的保留期
可以配置从两到四个小时到三百六十五天
默认值是两到四个小时
火候的优点主要在于其简单性
并且这是完全管理的
这使得我们能够快速轻松地设置近实时数据处理
包括一些转换和加载而不需要复杂的基础设施
我们不需要管理任何事情
它也是集成的
我们看到它与aws lambda集成
我们可以应用转换
以及加密和压缩
这也是非常直接的
它也会自动为我们扩展
这使得它是一个非常容易使用的工具，用于非常直接的设置 希望对你有帮助，下次课见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/048_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p48 058 Creating Data Firehose Stream (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看我们如何实际操作亚马逊数据火烈鸟
在实际操作中
以及我们如何设置一个数据火烈鸟流
首先我们可以从这个菜单中选择数据火烈鸟
然后我们可以创建一个火烈鸟流，请注意下面
在这里我们也可以进行成本计算
所以如果我选择亚马逊数据火烈鸟
我可以根据被摄取的数据量看到那些价格
所以根据我摄取的数据量，我们有这些价格
这里我们有几个级别
所以对于每月前500太字节
我们大约支付每千兆字节3美分
这也可能取决于地区
所以在这种情况下，我们看到在我们这个案例中我们只会发送很少的数据
所以这里我们选择数据火烈鸟并创建一个火烈鸟流
看看这如何设置工作
所以我们记得这是一个非常直接的设置
我们只选择来源和目的地
然后我们可以检查几个框
我们要应用的一些转换等
所以让我们看看我们这个案例如何工作
我们首先选择来源
如前所述 最常见的选项是亚马逊kinesis数据流
我们已经在之前的讲座中设置了这个数据流
因此我们使用这个作为我们的来源
然后我们可以选择我们见过的任何目的地
我们有open search
我们有亚马逊红移
当然亚马逊s3和一些其他服务，如
例如 Snowflake或Splunk
这也可以添加为目的地
在我们这个案例中我们选择亚马逊s3
我们保持简单
并且我们可以检查这是否工作
并且我们可以找到数据到达s3存储桶
现在我们因为我们选择了来源类型
我们只需要选择我们确切要使用的数据流
我将使用我们之前创建的那个
从这个列表中选择
然后它会被添加
现在我们可以给它起一个名字
这个流会有一个名字
当然 在我们这个案例中我们选择一个名字
也许选择kd测试
然后也许
一些数字
这将是我们的流名
正如我们所提到的，我们有一个非常简单且内置的方法来转换记录
转换格式并压缩记录
在这里，我们可以勾选数据转换的复选框
使用AWS Lambda
我们可以转换记录格式，同时也解压缩源记录
在我们这个案例中，我们不会使用这些选项
我们将稍后讨论这一点
也许稍后
然后我们有目的地设置
在这里，我们可以选择一个S3桶作为目的地
所以我们可以再次选择我们要选择的桶
所以我会找到我们之前创建的那个
但我们这个案例我们会选择这个桶
所以我选择了
现在再次我们有几个更多的选项
例如我们可以启用动态分区
这是我们可以直接使用的一些选项
我们可以添加一些前缀
或者也一些错误输出前缀
当然我们也要根据时区选择前缀
当我们想要使用一些前缀
包含时区
但现在我们只想保留所有默认设置
在这里我们也想看看缓冲
所以当我去这个缓冲提示
我们可以定义缓冲大小
所以我们已经提到
这可以从1MB到128MB
我们将在这里也留下默认值
但这里我们也可以根据秒来设置
从零秒到九百秒
然后，在这两个限制中
无论是大小还是时间间隔
首先会成为决定因素
此外，我们也可以
我说过 压缩数据记录
所以我们可以很容易地启用压缩
此外，我们也可以加密数据
但我们不会这样做
在我们这个案例中 所以这里我们也可以启用服务器端加密
但我们不想那样做
当然，就像往常一样，我们也可以添加文本
所以我们现在将保留这些默认设置
我们将使用这里设置的设置
因此，我们将继续并创建这个火炬流
然后一旦我们创建了这个
我们可以看看这是如何配置的
当然，现在我们也想测试它
所以我们可以看到，仅仅一到两秒后，这个就被创建了
现在我们可以看到
当然，当这正在运行并且在生产中时，我们也可以在这里看到数据进来
可以看到读取了多少字节等信息
但在我们的情况下，我们也可以在这里看到配置已经设置好了
我们也可以之后编辑这个
所以我们可以添加一些更多的转换
这些我们也会在这次讲座之后做
所以现在我们可以看到，这已经被成功设置好了
我们也可以用一些示范数据来测试这个
所以我们第一步要做的就是快速检查这是否起作用
这将看起来像这样
让我们从发送一些演示数据开始
现在正在发送，我们可以等待，等待，等待
过一会儿，我们也可以停止发送演示数据，以便我们可以
是的 测试它，看看这是否起作用，所以
几秒钟后，我会在这里点击停止
然后我会打开桶
所以我已经把这个添加到我的收藏夹里了
我们现在要导航到这个桶
我们已经将这个添加到我们的火力管道流中
这就是那个桶
在这里，我现在可以看到，我已经直接在这里做了
它直接到达这个桶，没有额外的文件夹
因为我们也没有启用任何分区
这将像这里一样直接添加
我们现在可以查看这个数据
我可以例如查看这个文件
我可以继续下载它
然后之后我也可以打开它并看到数据在这里到达
是的 这些文件
现在我们可以回去
我们也可以看到基本功能正在运行
现在我们想要在此基础上进行构建
并添加一些更多的转换和一些更多的选项使其更有趣 这就是我们在下一节课要做的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/049_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p49 059 Data Firehose - Transformations with Lambda (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看我们如何在线应用转换
使用aws lambda到我们的火爆流中
我们可以向下滚动
当我们选择了我们的流
例如，我正在使用这个，在这里这些配置选项下
如果我导航到配置
我可以看到这项转换和记录转换
我们也有
当然在这里是目标设置
所以所有这些也可以在这里添加
所以这也可以改变，除了某些东西，如数据分区
如果我在这里编辑
我可以看到一旦我们创建了流
它不能
是的 我们不能在后续添加动态分区
是的 这可以是一个非常有用的功能
这样数据不会分散
但它被分区到子文件夹
这也可以提高消费效率和性能
所以当我们想要动态分区时
这也可以在开始时设置时启用
在我们这个案例中，我们要滚动
现在我们要使用转换
因此，我们在这里编辑
在这里我们看到我们可以转换记录格式
我们可以启用这个
如果我勾选这个框
我们可以指定输出格式等等
但我们想应用一些数据转换
因此我们勾选这个框
现在我们可以选择一个aws lambda函数
所以我们可以通过浏览或输入来选择函数
也可以直接输入arn
在我们这个案例中，我们要浏览
所以我可以 哦
实际上我们也可以创建一个新函数
这更容易
即使这样，让我们继续创建一个新函数
现在我们可以选择一些预构建蓝图
它们是模板
它只是给我们函数的设置
我们可以
例如，我们说我们要做一些通用的火爆处理
我们可以返回处理状态
然后我们可以添加一些额外的自定义
当然在代码中
所以让我们选择这个默认蓝图
然后使用蓝图
在这里我们可以选择这个蓝图的名称
我们可以给它起一个名字
这将创建一个真实的lambda函数
我们可以说这个是消防栓函数
可能是测试
我们可以选择一个不同的运行时
当我们从这里选择其他内容时
所以我们可以说在这种情况下搜索
也许搜索消防栓
在这里我们可以看到一些选项
我们可以看到处理记录
将记录发送到数据消防栓流
在这里我们可以看到这也是一些我们有的选择
我们可以看到python是可用的
如果你更熟悉python
你也可以
我们可以选择这些选项
当然我们也可以手动更改代码
甚至从头开始创建
但在我们的情况下我们将选择这个蓝图
现在我们必须等待
就像任何aws lambda函数一样
我们必须选择一个执行角色
我们可以选择默认创建的角色
或者选择一个现有角色
所以我们之前已经设置好了
我有这个之前使用的角色
这样它就有写入我们桶的所有权限
我选择这个角色
然后这里我们有代码
在这里我们可以看到这当然又是先解码记录
然后输出再次编码
这是在这里完成的
这里也预先构建了，除了这些之外，没有其他太多事情
这里没有其他太多事情发生，除了这些之外
这里成功处理了特定数量的记录信息
然后我们将使用非常简单的默认值
当然我们可以在这里输入我们的自定义处理
也许进行一些转换等
但我们将保持这里为默认值
现在我们想要创建这个函数
现在我们可以看到这里使用的代码
我们可以看到它已经部署了
当然现在我们想在数据消防栓中使用这个函数来转换记录
当然现在我们可以在这里启用数据转换
现在我们可以选择这个函数
所以我们可以在这里浏览，请注意
这可能需要一些时间才能在这里显示
你可能想为我刷新
这也花了一点时间才出现
但现在我可以看到这种消防站功能
我可以选择它
我可以说我想选择这个功能
当然我还可以调整缓冲区大小
这里也可以调整缓冲间隔
所以这种情况下是60秒
我们也可以额外转换记录格式，如果我们想要的话
或者当然我们也可以在代码中实现这一点，在我们的lambda函数中
在这种情况下我们不会做任何额外的更改
我们只选择这些转换函数
我们可以做一些自定义转换
然后我说我现在保存这些更改
当然现在我们保存后
我们可以看到我们的消防水龙头流状态是活跃的
现在我们当然想测试
这是否真的有效，为此我们会去云外壳
这可能需要一点时间
然后我们就可以再次使用
我们的之前的
是的 Put record commands
所以我们可以再次使用相同的命令
确保你也包括你的数据流在这里
因为我们是从我们的流发送的
我们将这些记录发送到我们的数据
消防水龙头流通过kinesis流
这是我们的来源
所以因此我们将这些记录发送到我们的kinesis数据流
然后我们将使用相同的命令，就像我们之前所看到的
我将发送第一个记录
实际上我可以多次发送
这不会有太大影响
让我再选择另一个吧
所以我也可以选择这个
当然确保我们使用正确的流名
所以我将发送这个
这样我们也有数据条目2
所以另一个记录，也许再来一个记录
然后我们就完成了
所以现在可能还需要一点时间
因为这不是实时的
但它是接近实时的
所以请记住这可能需要一点时间
所以在这段时间内
不管怎样我们可以回到我们的桶
我们可以快速刷新
然后我们会看到什么都没有出现，所以再次
等几分钟
直到这里有东西出现
然后过一会儿，我可以再次刷新
现在我应该在文件夹下看到它
现在按年份对它进行分区
然后按月份和日期进行分区
在这里我可以看到最新的一个
我刚刚试过了
所以你在这里可以看到你的文件
你可以勾选这个框
然后你可以说你想要下载这个
我现在在我的情况下保存它
然后查看我下载的文件
这样我就可以在这里看到它，我也可以用文本编辑器打开它
这样我就可以看到我们的记录在这里
当然，正如我们所说，我们可以设置这个lambda函数来进行一些自定义处理
当然，在我们这个案例中我们没有做太多
也要注意，这个lambda函数的目的是将我们流中的数据转换为我们需要的形式
这样数据会自动发送到我们的目的地，也就是firehouse stream
所以这里不需要特别设置
特别是因为这个函数的目的就是处理和转换数据
具体来说，它的目的是处理和转换数据
然后当然在这里在我们的流中，我们已经将桶配置为目的地
因此它将进入我们的桶
当然，在你完成所有事情之后
也不要忘记删除你的数据流
你的火焰流和kinesis数据流
这样它就不会永远停留在那里
你也会在之后清理资源
因此我将转到火炬流
我将选择这个流
我想删除这个
所以我可以直接复制并粘贴这个名字在这里
然后我会删除它
此外，我还会导航到Kinesis
所以我也会删除我们在那里创建的数据流
所以你可以看到，在这种情况下，我有这个流可用
我会选择它，并在操作中继续并删除它
在这里，我们又必须确认，只要输入删除即可
然后，之后这也会被删除
所以过一段时间后，我们可以看到这已经被删除
在Firehouse中，我们也可以看到这也应该被删除
所以我们导航回到消防站
我们可以看到这种情况下，我们所有的消防流都被删除了 好的 希望对你有帮助，下次课见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/050_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p50 060 Amazon Managed Service for Apache Flink.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈Apache Flink的托管服务
这又是一个我们可以用来查询和处理数据流的完全托管服务
这允许我们创建一些实时分析
我们在一个非常可扩展和高效的方式下处理数据流
在底层
我们只是使用Apache Flink，而亚马逊为我们管理底层资源
无缝集成所有这些服务
这样我们不需要自定义编码
因此我们可以专注于构建我们想要构建的应用程序
一些用例是
当然，实际上，任何我们需要实时处理数据流的地方
或者我们需要进行某种实时分析
例如
我们希望计算一些指标
或者我们对数据流进行聚合
只是为了监控目的
我们可以创建这些实时指标或任何其他实时分析
例如，我们希望实时监控网站流量
并只看到用户的活动
我们也可以使用Flink来实现这一点，这是一个非常普遍的用例
这也是一个流处理etl
因此当我们想要计算某事或处理etl中的数据时
所以某些流数据
那么我们也可以使用s flick构建这个
为此我们可以使用数据查询
例如sql
或者我们也可以使用python构建我们的应用程序
Scala和java
然后这将为我们处理数据
而现在这里再次说明这是如何工作的
它只是使用开源流处理框架Apache Flink
所以这实际上是一个现在由AWS管理的开源流处理框架
因此我们不必担心底层资源
扩展它们和整合不同服务我们自己
所以它帮助我们构建一个非常可靠和高性能的解决方案
而不需要关注底层基础设施
这意味着它是一个无服务器功能或无服务器服务
并且它会根据需求自动扩展
根据实际工作负载
现在让我们更详细地看看这是如何工作的
那么这是怎么工作的
我们如何构建这个
首先 当然我们有flink源
这些可以是某种类型的流
当然，这可以通过使用少量代码，如我所提到的，进行集成
因此，我们不需要设置一个需要自定义代码解决方案的庞大系统
但我们可以使用一些内置服务
并且可以非常无缝地在flink中集成它们
当然，我们可以使用kinesis数据流
这就是我们之前提到过的
我们也可以使用亚马逊管理的Apache Kafka流处理
或者简而言之
亚马逊MSK
所以这也是一个为Apache Kafka管理的服务
这也使得构建和运行使用Kafka处理流数据应用的过程更加简单
当然我们也能使用亚马逊S3
或者我们也可以包括一些通过Apache Link、连接器或API访问的自定义数据源
然后一旦数据被摄入
然后我们就可以进行处理
我们可以实时处理数据流
这包括过滤数据
汇总数据
或者在系统中丰富数据
这是通过链接完成的
非常快速和可靠的流引擎
这可以处理数据在
是的 非常非常
几乎实时
所以这接近实时，延迟非常低
而这就是这款引擎的设计方式
以便我们能实现接近实时的分析
Flink 在这方面也表现出色
这意味着它可以根据传入的数据维护和更新状态
这当然可以 当然这对于例如异常检测等用例非常重要
以及启用检查点和快照也很重要
以便我们能确保这是的
容错解决方案
我们保持数据完整性
这定期进行
因此，无论何时出现数据处理失败
某种形式的问题
我们可以恢复到之前的状态
从那里我们可以确保一切顺利进行
此外 如我所述
我们还可以启用异常检测
这样用户就可以在应用程序中实现这一点
只需实现一些算法
这些算法只是检测一些奇怪的模式或数据流中的一些偏差
以便我们可以实时触发一些警报或自动响应
这就是 当然
在 例如
欺诈检测
或者也许银行业
一些交易在这里
这可能是一个非常有用的场景
我们还可以选择启用一些事件驱动的操作
因此，我们也可以在我们的应用程序中定义这些操作
基于在数据流中处理的数据
此外，我们还可以集成其他AWS服务，例如
AWS Lambda
因此，我们也可以执行一些自定义代码
这使得我们可以进行几乎所有类型的数据处理
我们称之为Flink的目的地是什么
这就是数据处理流程，然后将数据发送到这里
这可能包括
当然 分为三个桶或其他目的地，如亚马逊kinesis
数据流我们又有了
亚马逊msk
或者也有其他自定义数据源
或一些分析工具
数据在这里进行分析或可视化
所以这里
这也可以用在称为flink sincs的目的地
这就是ms flink的工作方式
现在让我们快速看一下定价结构，以便了解这是如何工作的
您只需为实际使用的资源支付费用
没有前期成本
我们使用按需消费定价模型
我们为所谓的kp use支付费用
这些是提供计算能力的处理单元
我们根据这个数量收费
每个kpu有一个v cpu和4GB的内存
这就是我们定价的单元
并且我们为每个应用程序要求编排目的
一个额外的kpu
这就是我们被收取的费用
此外 如果我们的应用程序使用某种数据存储
我们也会根据每月的数据量进行收费
这是每月按GB计费的
我们已经提到过我们会自动扩展
kpus的数量会根据应用程序的需要自动扩展
并且我们也可以手动配置所需的kpus数量
当我们非常确定
或者我们希望有更多的控制
我们也可以手动配置它们
我们还可以选择使用交互模式
我们可以使用笔记本进行此操作
这是flink studio
在这里我们只需额外支付两台kpus的费用
如果我们想以一种更交互的方式开发
在这里我们也有相同的存储成本 这就是我们需要了解的关于电影的事情
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/051_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p51 061 Amazon MSK.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们也来谈谈亚马逊管理的Apache Kafka流处理服务
或者简称 亚马逊MSK
所以这在某种程度上也是Kinesis的替代方案
因为它也能帮助我们处理实时数据流，我们在这里使用
所以这也是一个管理的服务
但它帮助我们与Apache Kafka一起工作
所以这是一个相当复杂但非常强大的平台来处理流数据
但这样我们做起来就容易多了
因为我们的一切都被管理了
所以Kafka也被管理
我们不用担心运行
例如Kafka集群的任何操作挑战
但这都被管理了
因此MSK让我们可以专注于构建应用程序
我们不用担心那些挑战
所以在基础设施第一
让我们理解这一点
我们有两个关键组件
第一个是Kafka代理
代理是存储和处理我们流数据的服务器
然后我们还有Zookeeper
它们是操作的大脑
它们帮助管理集群的状态
它们执行不同的操作任务，如代理之间的领导者选举
而集群本质上是一组代理节点
它们一起处理数据流
它们提供了计算能力
所以它们是存储和移动数据的主要力量
MSK将这些代理分布在多个可用区
这样做可以确保数据始终可用
即使网络某部分在某点宕机，数据也能保持可用
数据存储在Amazon EBS卷上
这为我们提供了消息持久且安全的存储
这再次跨不同代理和可用区进行复制
所以
我们不仅保护数据免失
这不仅更可靠，也更持久
而且当有故障发生时
我们可以迅速恢复
这样我们就可以确保流平稳运行，一切都能正常运作
我们有一些限制
这有一些配置上的区别
与Kinesis流相比，消息大小可以增大
我们有更高的限制
这可以配置到10MB
而在Kinesis中，我们只有1MB的限制
所以我们能处理更大的消息
总的来说
我们可以
在这里调整更多内容以适应我们应用的具体需求
以便我们可以 例如
处理更大的消息或调整Kafka集群的其他方面
以便我们可以直接优化性能
在这里我们也有生产者和消费者
所以再次
生产者是数据流的来源
例如应用程序或传感器
他们将数据发送到MSK
然后我们还有消费者
它们是终端或应用程序，然后将读取和处理数据，所以在这里
Mk 这也使我们更容易
所以数据从生产者流向消费者
我们可以像这样进行实时数据处理
现在让我们比较亚马逊Kinesis和MSK之间的差异
所以这里我们已经讨论了一个关键差异是定制化与便利性
我们在配置方面有更多的选项
当我们有一些更复杂的设置时
当我们需要对事情进行更精细的调整时
这比kinesis更受欢迎
然而，它也可能更方便
使用kinesis更容易
还有消息限制
让我们详细看一下
我们已经说过与msk有关
我们有更精细的控制成本
这可以实现更复杂的优化
当我们有更复杂的场景，有大型消息时
这可以配置得更详细
所以这里我们已经在kinesis中提到了
设置起来要简单得多
它更直接
总的来说，体验更完善
而这
是的 它只是管理更多的配置
而在msk中我们有这种控制
但这也意味着我们有一个更复杂的设置
而kinesis的关键区别之一是消息限制
我们有这个1兆字节的限制
所以每当我们想要
是的 克服这个限制
那么我们有一个选项mk
以及它是如何组织的
当然，组织方式有点不同
在kinesis中，这些被组织成流和表
我们可以调整吞吐量
所以我们可以拆分或合并分区
这就是我们如何处理它的方式
而在msk或kafka中
我们使用主题和分区来管理数据
因此，扩展在这里涉及添加分区
但一旦添加，它们就不能被删除
因此，这与kinesis相比要更僵硬一些
并且也更复杂一些
是的
更多 这也需要更多的管理
所以，你可以记住，这里组织方式不同
现在，让我们也快速谈谈访问控制
在这里，我们有一些安全措施，这也有点不同
一些事情是相似的
但然后，也有一些差异
所以，无论是kinesis数据流还是亚马逊mk
它们都提供飞行中TLS加密
在msk中
我们也有明文的选项
这也是可能的
两者都支持使用kms加密
这适用于kinesis和msk的访问控制
在msk中 我们有三种不同的模式
我们有用于与kafka进行身份验证的互 TLS身份验证
所以，我们可以有一些主题级别的授权
此外，我们还可以使用用户名和密码组合机制进行身份验证
这依赖于kafka acs
然后我们也有iam访问控制
这也集成了，我们可以管理身份验证和授权
在iam中，所以这里管理访问管理就简化了一些
所以，我们可以使用我们aws生态系统中的东西
而在kinesis中，这更直接
所以，在这里，我们也使用iam策略，无论是身份验证还是授权
所以这也简化了访问控制
所以现在问题是
我们在什么情况下使用这些
所以 当然，我们在加密和访问控制方面有一些差异
但总的来说，如果我们想使用kinesis
这就是说我们可能需要一个更直接的设置
并且管理应该更容易
并且
是的 当消息--即数据负载--在1兆字节限制内
那么我们就有一个更易于使用且更易于集成的平台
并且我们有这个简单的设置
所以当我们在这个限制内时
这就是更直接的
但然后，有时我们就需要更精细的控制配置
也许也
当然我们对消息的大小有更高的要求
所以每当这超出了1MB的限制
那么这可能意味着我们使用msk会更好
在这里我们有更多的灵活性 但这也更复杂，需要更多的管理
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/052_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p52 062 MSK Connect & MSK Serverless.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈MSK中的两个具体服务，详细讨论一下
首先我们有MSK Connect
这也是一个完全管理的服务
这使得构建和运行Kafka Connect连接更加容易
这完全在亚马逊内
MSK和Kafka Connect是Apache Kafka的一个开源组件
这总体上简化了Kafka与其他系统之间的集成
例如 数据库或搜索索引
或其他文件系统
因此这允许我们将数据流进流出kafka
没有使用一些自定义代码来集成这个
所以在这里我们只有这个管理的
因此，msk正在为我们提供完全管理的服务
就像这样 正在使其再次稍微更加流畅。
部署和扩展那些kafka连接器
所以使用kafka connect
我们能够 例如
在不需要所有复杂的情况下扩展我们的数据集成
是的 管理kafka connect基础设施
所以这完全由我们管理
所以这里可以做到
这里我们也有一系列的连接器可供选择
这允许我们在可能更复杂的场景中进行不同的操作
例如，也许我们希望将数据移动到其他源
也许亚马逊S3或Apache Flink
这里都可以使用MSK Connect简化操作
然后我们还有msk无服务器
这是亚马逊mk中的一种更具创新性的集群类型
这是为了简化kafka操作而设计的
此外 这消除了现在需要管理集群容量的需要
因为这现在是根据工作负载要求自动分配和扩展资源
因此，这可以减少对这需要为非常详细的配置的需求
这是一个无服务器选项
并且，这使得它在数据量波动的使用场景中完美无缺
因为，这将根据所经历的负载进行调整
因此它会动态调整计算和存储
在这里我们也确保我们只支付我们实际需要的费用
这样我们就可以保留kafka的强大功能
这会让我们轻松一些
因此这会让我们容易一些 我们仍然可以使用kafka的强大功能
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/053_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p53 064 Importance of Partitioning.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们开始将数据分区到s three中
这是非常重要的事情
分区意味着我们设置一个基本的文件夹结构
在我们的s three桶中建立一个目录结构
这可以基于不同类型的属性
通常这是基于时间
例如我们有一个文件夹表示年份
然后在年中我们有多个文件夹
例如月份等等
像这样我们可以根据
例如时间对数据进行分区
或者可能基于其他属性例如
地区等等
这取决于数据通常的过滤方式
即数据通常的访问方式
像这样我们可以提高查询数据的性能
因为这样我们可以减少需要扫描的数据量
像这样我们可以提高athena和glue的性能
在这里我们不需要查看所有不同桶和文件夹
但我们只注册这些分区
我们有这个文件夹结构
然后在我们的数据目录中注册
我们需要这些元数据来了解数据是如何分区的
这在一般上也有助于数据管理
例如我们可以更轻松地设置一些生命周期规则等等
像这样我们基本上提高了查询性能
但也在数据管理方面有一些好处
我们将这样做
我们将基本组织数据到文件夹和子文件夹中
根据数据应如何分区
根据数据如何频繁访问
然后我们使用glue爬虫自动创建那些分区键
并存储这些元数据
在这里我们也可以使用glue和glue爬虫来自动化这个过程
最常见的分区示例是基于时间的分区
例如我们可以这样做
我们有一个文件夹表示年份
然后我们有一些子文件夹表示月份
日期等等 这里我们使用常见的键值方法
然后这些元数据存储在我们的数据目录中
即使我们有这个文件夹结构
这些元数据仍需存储和管理在glue数据目录中
然后我们有更高的查询性能
因为athena可以更有效地查询数据
然后它只需要从实际需要的分区中检索数据
其他则可以忽略
像这样我们可以提高性能并实现更好的查询性能
这里取决于数据通常的访问方式
通常的过滤选项 所以这里 我们应该决定我们划分数据的方式，通常是过滤或聚合
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/054_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p54 065 Partitioning with Glue (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看如何在AWS中创建那些分区
为此目的我们将再次使用一些文件，而这些文件
当然，你也可以在资源中找到
因此，在这种情况下，我们使用这些销售文件
首先，我们将为这设置一个存储桶
所以让我们假设我们有这些不同的CSV文件
我们希望
当然，将它们包含在我们的数据目录中
以便我们能够查询它们
我们希望了解创建分区的过程以及其后果
那么这是如何工作的呢
让我们从设置我们的存储桶开始
所以我们转到S3
在这里我们可以简单地设置一个新的存储桶
快速设置 所以我们可以之后再次删除
我将使用我的名字
也许一些数字
然后可能是测试
然后希望这通常会工作，所以
当然，必须在全球命名空间中唯一
但这应该没问题
所以让我们创建这个存储桶
然后我们可以使用这个存储桶
所以我在这里有它
现在我们只是想创建一个文件夹结构
而这将用于自动创建我们的分区
让我们看看这是如何工作的
首先，我们有我们的文件夹数据
这是一个伦敦
我们将把伦敦数据放在这里，我们将创建这个文件夹
然后是第二个，我们也称之为纽约
让我们创建它
我们点击创建文件夹
现在我们有了这两个文件夹
如果我们继续将这些数据上传到这些文件夹中
在我们在数据目录中设置表之后，我们将能够查询数据
所以让我们看看这是如何工作的
首先，我们将使用销售数据伦敦
我们将只使用这个单个文件
我将上传它
然后我们可以关闭这个
然后我们也可以继续将数据上传到另一个文件夹中
这有点太多了
让我们回去，进入纽约
在这里，我们也上传销售数据
纽约，然后我们将进行测试
所以我们将运行我们的爬虫
首先，我们将设置一个爬虫
我们将首先设置一个爬虫
这就是在捕捉结构
它还会自动创建分区
所有数据都将被收集
以便我们能够使用athena查询这些文件
让我们通过导航到clue来做
在这里我们将快速创建一个爬虫
让我们导航到爬虫
创建一个新的
我们将其命名为
在这种情况下，分区可能只是像这样的测试
然后我们可以点击下一步
在这里我们说数据尚未映射
到glue表
所以我们可以简单地从头开始创建
我们也可以这样做
但我们稍后会讨论
所以首先创建表
我们说尚未
我们必须 当然要添加一个数据源
在这种情况下我们可以浏览文件夹
我将使用
当然我的文件夹，在这里我将实际使用这两个
所以我将简单地选择整个桶
这样我们就有多个子文件夹
然后我们将根据这些子文件夹自动创建分区
看看这是如何工作的
我将此作为数据源添加
然后我们将转到下一步
我们必须 当然选择一个角色
这是我们之前使用过的一个
在这里我们可以转到下一步并保留所有默认设置
当然我们必须选择我们的数据库
我们可以选择表名前缀，所以我们可以做这个
但在我们的情况下，实际上没问题
所以我们将保持在线
然后我们可以手动启动这个
所以我现在去创建爬虫
现在快速手动运行这个爬虫
现在约一分钟后已完成
我们可以看到一张表和一张分区表发生了变化
因为我们有那些子文件夹
让我们看看
如果我们选择它
我们可以看到这些详细信息
现在我们回到数据目录
我们去表
我们可以在这里看到一些新表
如果我们刷新
我们看到这是这个桶的名称在这个例子中
因此因此这就是如何创建此表的
所以我会选择它
当然我们现在可以查看此
所以我们看到列名已创建
并且现在也有这一分区名称
所以这里这就是这个分区键
所以这看起来怎么样
我们希望看到这种行动
因此让我们去athena并看看这个
首先我们前往athena
现在应该可以看到此表
所以现在如果我们选择我们的来源
我们有数据库
我们可以看到此是此表
所以我们看到此实际上分区了
所以我们可以查看此预览
然后我们将看到此将返回的数据
我们有数据和位置
并且也将返回分区作为列
所以我们看到此情况下如果我们查看所有数据
让我们不限制到10
但让我们看看50并再次运行
现在我们将看到来自纽约和伦敦的数据
所以我们有这些上传的文件
但现在如果我们
例如 这就是问题为什么拥有此分区是有益的
因为它提高了查询性能
因此查询不需要扫描所有文件
但它只需要扫描相关的分区
对于分析目的
我们通常希望限制我们的结果到几个
例如位置或特定日期
通过拥有分区
我们可以只说看看此文件夹
只需查看该文件夹
因此不需要搜索所有文件以查看所有数据
让我们快速演示
所以 例如
我可以在此添加位置等于纽约
然后我运行它
我将看到数据将过滤为纽约
这就是扫描的所有数据
这基本上扫描了所有文件和所有分区
因为我仅按位置过滤
但如果我使用分区列
所以分区零
让我复制此
在这个例子中，我将其粘贴
让我像这样删除它
现在我们有了这个
当然没有下划线
现在让我重新运行它
在这种情况下，我们实际上扫描的数据要少得多
因为它的50%
因为我们只需要查看此特定分区
所以我们可以 当然，如果我们想查询所有单个列
并根据新约克分区进行过滤
但现在如果我们添加额外的文件会发生什么
例如
让我仅在新标签中打开它
现在我想在我们的s3桶中添加额外的文件
首先，让我们看看
如果我在现有分区中添加文件会发生什么
然后，如果我添加新分区会发生什么
所以我会进入我们的桶，这是一个错误的
让我们进入正确的一个，并在伦敦
现在我将在此添加额外的文件
我们有伦敦2
这也会上传
一旦这可用
让我们再次查看
之前我们有20条记录
让我们看看现在我们能找到什么
在这种情况下，我们看到有5行被添加
现在处于这个位置
如果我们再次查看仅伦敦
那么我们将看到现在有15个文件
让我们看看伦敦
如果我们运行它
现在我们可以看到这些15条记录
我们也可以按顺序查看
这些是我们的15条记录
但现在如果我添加一个新分区会发生什么
让我们也看看这一点
在这种情况下，我将创建一个新文件夹
这在我们的情况下是东京
我将添加这个文件夹名
我也会上传文件
让我们进入这个文件夹并上传这些数据
让我们上传它
关闭
现在运行我们的查询
当然没有筛选
我们希望看看新数据是否可用
我们看到这不起作用
我们可以看到这没有工作
因为分区还没有创建
这就是为什么我们需要更新我们的元数据
这可以有两种方式完成
第一种方式是手动完成
所以我可以 例如
可以使用这种脚本
让我来这样做
我们的表名为这个案例中的表
我们可以说，我们希望创建一个新的分区
这将是我们的东京分区
我们必须指定确切的位置
在这种情况下，它将是这个位置
我们有 如果我们转到属性
这个确切位置
我们可以复制它
我们可以在这里粘贴
让我在这里粘贴
我们还可以使用这个单引号
然后我们可以看到这个
我们还有这个 msc k 修复表命令
这将自动完成
但我们必须有一个非常具体的密钥对命名约定
所以桶和文件夹必须在密钥对命名约定中命名
所以必须是年份等于2022
例如 这样它将起作用
但在我们的情况下，我们希望手动完成
所以我们创建了这个新的分区
让我们快速运行这个
我想我们犯了一个错误
所以我们的分区列实际上是称为分区零
让我们快速修复那个分区
下划线零
让我们重新运行
然后它完成了
让我们看看这在我们的目录中看起来如何
在这种情况下
但首先我们实际上可以查询它，以查看这是否实际上起作用
所以让我们再次查询所有内容
现在我们看到
我们有现在三五
所以现在东京也包含在内
如果我们查看我们的目录
我们去胶水
我们可以看到这种情况下在我们的表中
如果我们进入这个表
在我们这个案例中 当我们转到分区时，这些三个分区现在可用
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/055_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p55 066 Lifecycle Management & Storage Classes.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈s三中的数据湖生命周期管理
这是非常重要的事情
因为我们有时会有不同的使用模式
不同类型的用例
这可能也取决于数据的年龄
因此，我们需要将这些数据在不同的
所谓的存储类别之间移动，我们将要讨论这些存储类别
这些不同的存储类别它们只提供了
当然一个好的成本和然后也不同的要求之间的平衡
有时我们需要对数据有非常快速的访问
数据需要高度可用
当然这会稍微贵一些
有时候我们只需要存档数据
因此根据使用场景
我们应该选择最合适的数据存储类别
我们还需要管理这个数据生命周期
这意味着当我们摄取数据时
我们通常首先选择一个特定的存储类别
那就是S3标准存储类别
然后我们想要对数据进行分析
我们需要频繁访问数据
它应该高度可用
我们希望在这个初始阶段有好的性能
但后来可能
在某个时候，我们可能希望将数据转移到一些不太频繁访问的数据
在某个时候，我们可能希望删除它
或者只是移动到一些长期归档
我们通常不再访问它
但可能出于合规原因，我们需要保留它
所有这些都可以通过不同存储类别来管理
并且还有生命周期规则，这些规则会自动根据数据的年龄转换数据
因此，让我们首先谈谈不同的存储类
这些是不同的类别，我们将逐步讨论
首先，我们有标准s3标准
这是用于存储频繁访问数据的默认选项
因为这里延迟非常低
这适用于我们经常需要访问数据时的通用用途
这里我们有标准的用例
当数据需要频繁访问时
然后我们也有选项
这并不是真正的存储类别
但这基本上是一个选项
它也是一个存储类别
但需要注意的是，这会自动将数据在不同层级之间移动
这意味着当我们不确定访问模式时
或者访问模式可能会改变
那么我们可以自动管理
在这里我们需要支付一些监控费用
但这可以是一个非常经济有效的解决方案，因为我们可以
在这种情况下，当访问模式频繁且不断变化时
我们可以自动在这些不同层级之间移动数据
在这里，我们可以选择将其放在最常访问的这里
然后 根据实际的访问模式
数据也可以被移动
当它连续30天未被访问时，数据可以被移动到不太常访问的这里
然后在90天没有访问后
它会被移动到归档即时访问这里
这将使数据仍然可以即时访问
然后我们也有一些可选的异步归档访问层级
在这里我们有归档访问和深度归档访问
异步归档的意思就是数据不是即时可用的
但它是异步的
这意味着我们需要等待几个小时，数据才能实际使用
所以数据需要解冻
然后我们可以使用它
所以我们首先需要恢复它
然后我们也可以使用它
这些都是智能分层选项
然后我们也有快速一区域
这是一个高性能
但单可用性区域
这是专门为提供一致性的单位数据访问而设计的
最常访问的数据
对于非常敏感的应用程序
在这里我们可以通过提高访问速度最多提高10倍
并且与s3相比，请求成本降低50%
但我们只有
当然一个可用性区域
在这里 缺点是我们数据稍微不那么可用
也许稍微不那么耐用
所以这里与这个快速区域
数据也存储在一个不同类型的桶中
这是一个亚马逊s3目录桶
它支持每秒数万次请求
这也是一个非常高性能的选择
然后我们也有如果我们想要节省一些成本的s3标准不太常访问
在这种情况下，对于不太常访问的数据是有用的
但我们仍然需要快速访问当它需要的时候
在这种情况下我们只需要稍微低一点的存储成本
但当我们支付访问时
这可能会稍微贵一点
所以这里这更多地是一个成本的问题
但我们也需要选择正确的用例
这就是我们需要选择的
所以不太常访问
但我们需要快速访问
所以这里对于长期存储是有用的
然后我们也需要毫秒级的访问
非常相似的东西
基本上相同
但只有一区频繁访问
但这里我们仍然有较少的可用性
所以这里这仅在一个单一的可用区
因此这使数据稍微不那么高可用
因此我们应该只使用可重建的数据
所以我们可能无论如何在其他地方存储它
因此我们不太担心这仅在一个单一的可用区
所以这里这可能是一个次要备份
或者
某事我们可能也不访问数据太多 我们也有一些其他副本
在这个情况下我们可以使用一区的不频繁访问
然后我们也有冰川存储类 所以这里我们也有三种类型
所以我们的第一个是冰川即时检索
然后我们有灵活检索然后有深度归档
所以如果我们谈论这些
我们也有关于数据如何频繁访问的差异
所以这种情况下我们有即时检索
这实际上是我们快速检索的地方
然后检索数据也会稍微贵一点
所以这里我们有即时检索
甚至在毫秒内甚至提供最快的访问
当我们只看归档存储时
所以这里当我们需要立即访问归档数据时
这是一个选项
然后我们有介于两者之间的灵活检索
所以这是有用的当我们需要每年一次或两次访问数据
但我们异步检索它
所以这里我们可以从一分钟到十二小时恢复
但这对于我们需要每年一次或两次访问数据非常有用
数据被检索
所以这是有用的
例如
对于长期备份我们拥有这个检索选项
我们也可以在这里配置这些检索时间 所以这是我们可以在灵活方式中做的
然后最后我们也有冰川深度归档
所以这将是最便宜的存储定价
但当然当我们需要访问它时
这将使访问稍微贵一点
所以我们需要解冻它
这可能需要更多时间
这适合如果我们需要每年一次或两次访问数据
然后我们可以在十二小时内恢复它
所以这里这可能是一个次要备份
或者
某事我们可能也不访问数据太多
所以这里是所有这些不同类别的概述
这些基本上是非冰川类别
我们有标准，通用，频繁访问的类别
你也看到了价格差异
这就是基于位置美国东部的存储价格
所以是北弗吉尼亚
是的 你不需要知道确切的价格
但了解它们的用途是好的
然后我们有 当然，对于长寿命和毫秒级访问的稀疏访问
但稀疏访问的数据
在这里，我们有一个区域版本或变体，我们处理可重建的数据
因此，数据可能不那么可用
然后在这个在三个可用区中的稀疏访问中
然后我们也有智能分层
它帮助我们在不同分层之间进行自动切换，以应对访问模式的变化或不确定性
然后我们也有特殊类别
这是一个新类别
S3 Express 一区
因此，对于高性能存储，最常访问的数据
但只在一个可用性区域
然后我们有三个冰川选项，也包括智能分层
我们也有归档访问和深度归档访问
对于这一智能分层
所以这也是一些可用的选项
然后是即时检索
这是为每季度访问一次左右的数据
然后我们有灵活检索
这仅给我们提供了从一分钟到十二小时的检索选项
然后我们有深度归档用于长期存档的数据
在这里这适合一年访问一次或两次的数据
并且可以在12小时内恢复
所以这些都是不同类型的存储类别 现在我们想谈谈生命周期规则
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/056_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p56 067 Using Lifecycle Rules.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看生命周期规则
我们已经听说过，我们有不同类型的存储类，用于不同的使用案例
和不同的访问模式
当然，我们已经学过我们有一个数据生命周期
通常这意味着数据开始时可能有不同的访问模式
我们需要更频繁地访问它
然后，我们希望将其移动到另一个存储类
这可能更经济实惠
因此，我们希望自动化这一点并定义一些规则
所以，我们可以这样做
所以 我们使用这些生命周期规则来应用一组规则到我们的对象上
在S3桶中定义一些操作
这里有两种类型的操作
第一种是我们转换数据
这意味着我们希望将其转换到不同的存储类
可能在一定时间后
例如，30天后，我们希望将其转换到更便宜的存储类
我们也可以删除对象
我们可以有删除操作
过期对象将由我们删除
当然，我们也有转换
这样，我们就可以将对象转换到不同的存储类
这可以以不同的方式进行配置
当然，我们可以根据数据的年龄再次定义这一点
我们可以定义特定转换何时发生 希望对你有帮助，下次再见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/057_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p57 068 Storage Classes (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看这些存储类
在实际操作中
我们将通过上传一个文件并设置特定的存储类来实现这一点
所以我们想看看这是如何工作的
当然，之后我们也会看看这如何自动化
因为这次我们将手动进行
然后，我们也会看看
是的 一些自动化这个过程的策略是什么
首先 为什么这实际上相关
如果我们去看S3定价
我们看到根据存储类别
我们有不同的成本
例如
对于这里的雷霆
这里是通用目的存储
我们有它用于频繁访问的数据
这是存储定价
但我们可能有其他类型的存储
也许不经常访问
这更便宜
然后我们也有也许像冰川一样的东西
这甚至更便宜
但是为什么我们不总是使用这个呢
当然这里我们看到这需要恢复
实际上 所以我们不能直接访问数据
但是它需要先冻结
我们也有
如果我们要去 例如去访问
在这种情况下 假设是不经常访问
数据的访问成本也可能不同
所以这里存储成本可能更便宜
但我们可以针对数据访问设置不同的价格
所以你可以看到对于标准
这是每1000次请求选择数据的成本
如果我们向下滚动
我们可以看到 例如，在不频繁访问的情况下，这会更昂贵
即使使用冰川存储
我们需要首先检索数据
所以解冻这个数据
这会产生一些额外的费用
因此我们想要
是的 当然 找到正确的数据存储方式
正确的存储类别
当然，数据生命周期中这可能会发生变化
这就是为什么之后
我们也想看看不同的
是的 我们可以设置的策略或规则
首先
让我们去我们的一个存储桶，在这里
假设我们使用这个存储桶
我们可能想上传一些文件
如果我们这样做
例如 我正在使用一个PDF
在这种情况下，如果我们没有更改任何东西
如果我们查看属性，默认情况下将是标准存储类别
但我们也可以使用
例如智能存储，数据将
是的 具有一些变化的需求或未知的访问模式
然后我们也有之前讨论过的其他存储类别
例如单一区域频繁访问
数据存储在一个可用性区域
它是不频繁访问的
但它需要 是的
它需要在毫秒内访问
在这种情况下我们可以访问数据
所以我们有这些不同的选择
假设我想使用单一区域频繁访问
我们可以看到数据只存储在一个可用性区域
现在我们转到上传
我们可以看到如果我们想要访问数据
我们可以立即这样做
所以这种情况下让我关闭它
然后查看这一个
我们可以首先看到
如果我们滚动下来
存储类别在这里定义
如果我们想看看数据
我们可以立即打开它
像这样访问数据
这是不同的 然而，当我们想把这存档时
这是不同的 然而
当我们使用例如
冰川时
如果我们想更改存储类别
我们也可以在属性中这样做
所以在这种情况下我们可以转到编辑
然后我们改变它
也许在这种情况下改为冰川灵活检索
在这种情况下如果我们改变这个
假设我们现在也想访问这个数据
我们可以看到直接访问这个数据是不可能的
首先我们需要恢复这个数据
我们看到这个对象存储在冰川中
为了访问它
你必须首先恢复它
在这里我们可以发起恢复
这将花费一些时间
然后我们只能在之后访问数据
这就是不同存储类如何工作的
我们可以在开始时指定它们
但默认总是s3标准
但现在在实际中这是如何工作的
因为我们不想手动做这件事
但我们想做一些特定的事情以便这能自动化
当然也包括智能听
我们可以使用一些生命周期规则自动将数据在不同类之间切换
这是如何工作的 现在我们想在下一节课看一下
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/058_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p58 069 Lifecycle Rules (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


假设我们有一些特定的使用场景，我们希望频繁地访问数据
刚开始 这是非常常见的
也许前30天，之后我们希望将数据移动到不同的存储类别
只是为了节省成本并优化我们的生命周期像这样做
我们可以创建生命周期规则
所以如果我们在我们的桶中
我们可以转到管理
在这里我们发现生命周期规则
这里 我已经创建了一个
我们也可以创建一个新的生命周期规则
在这种情况下我们可以称其为
在这种情况下也许过渡到归档
例如我们可以现在也定义范围
所以这应该适用于我们桶中的所有对象
还是我们应该只过滤
所以我可以以三种不同的方式过滤
但第一种是通过使用前缀
在这里我可以 例如也可以添加文件夹路径
例如 csv
在这种情况下它将只适用于我的子文件夹
Csv 我们可以在这里也指定路径
然后一个非常高效的方式也是使用标签
所以我们可以给我们的数据或桶添加标签
因此我们可以具体定义不同类型的数据不同的生命周期策略
因此标签也可以非常有用
并且还可以基于对象大小
所以当 例如有一些大数据量的数据
但重要的是我们有那些生命周期规则操作
所以我们可以说首先我们希望移动数据
当前版本
在我们的情况下我们不使用任何版本
所以这将只是数据本身在不同类别之间的移动
所以可以说刚开始它应该立即
也许移动到智能存储
所以这里我可以输入一个数字
或者我甚至可以说在这种情况下零
这意味着它将立即移动到智能存储
这可以是一个很好的默认值
以及我们说过数据会自动移动到不同的层次
如不经常访问或频繁访问
所以这可以是一个很好的选择
但在我们的情况下我们只想也将其移动到冰川即时检索
所以假设180天后
我们希望将其移动到这个存储类别
然后我们可以添加另一个
所以我们可以说 例如，我们希望将其移动到另一个位置
因此，也许深度归档也可以这样做
然后我们还可以额外地说，我们希望在
也许我们说两年后删除数据
所以我们可以说过期当前版本的对象
这意味着如果我们没有版本控制，数据将被删除
因此，在这种情况下，我们可以使用
如果我们向下滚动这里
我们看到启用版本控制时
它会添加一个删除标记
并且当前版本的对象将被保留为非当前版本
所以我们稍后会讨论这些版本
对于非版本控制桶
这将永久删除对象
所以这里我们也可以指定天数
例如 正如我们所说
让我们说 例如
七百二十天，现在我们这里也有可视化
所以开始时，对象上传后
一百八十天后，它将被移动到冰川即时检索
然后另一百八十天后，它将被移动到冰川深度归档
然后之后，对象过期
在我们这种情况下，这意味着数据将被删除
所以现在我们可以继续创建这个规则
像这样，我们还需要确认这可能是一次生命周期请求
每个对象的请求成本
这可能是有关的
如果我们有很多小对象，在我们的情况下
我们当然勾选这个框
然后我们可以说创建这个规则
所以现在我们看到这是我们的规则
它们也可以结合使用
例如，我可以有一个规则
我定义一些关于删除数据的政策
然后我可以有另一个是的
只定义数据如何移动到不同存储类
如果我们想要删除一些政策
我们可以选择它
然后我们可以去删除
删除它 所以我说
删除 然后过一会儿这也会被删除
这就是如何设置不同的生命周期规则来自动管理它 希望对你有帮助，下次见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/059_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p59 070 Intelligent Tiering (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


让我们看一下智能听的稍微具体一点的存储类别
当我们自动使用它时
数据将在不同的级别之间移动
所以像这样，它是非常自动化的，我们不需要手动做
所以也许在某些情况下，数据并不总是可预测的
它是如何被访问的
并且它可能会随着时间的推移变化很多
因此，在这种情况下，我们也可以将其用作一个很好的默认设置
这将自动进行，只需支付很少的费用
在这种情况下，我们有三个不同的级别，我们不需要做任何事情
所以自动地
开始时的数据将存储在频繁访问区
这是默认设置
当数据30天未被使用时
它将自动移动到不频繁访问区
每当再次使用时
它将移动回频繁访问区
然后我们还有归档区
归档即时访问
这将在90天后进行
其中数据未被使用，像这样自动
数据在不同层次之间移动
这是默认设置
通常这很好
但我们也可以通过在桶中更改来更改归档选项
一般来说，这是不错的默认设置
但如果我们想要节省更多成本
我们可以稍微调整一下
所以让我们快速演示一下
我们希望去我们的桶
现在我们假设在我们的智能听存储类中有几个数据文件
现在我们想要转到属性，在属性下我们看到了这个智能听归档配置选项
所以我们看到这对于归档是有关的
现在我们可以快速做一个例子
所以我们创建一个新的配置
我们称它为测试，再次
正如我们所见，我们现在可以限制范围
我们可以说这应该适用于桶中的所有对象
或者可能通过指定特定的前缀来限制
也许一个文件夹路径
或者一些特定的文本
在我们这种情况下 让我们用它来处理所有对象
正如我们所提到的，我们可以使用不同的归档规则操作
我们可以说，如果勾选这个框，那么在180天后
例如 假设是180天
数据将被移动到归档访问
我们默认情况下
在90天后
数据将被移动到归档即时访问
因此，这将使成本增加10%，或者更准确地说
这里的归档访问大约降低了10%
但我们只剩下一分钟
两个小时的检索时间
所以，可能在180天后
我们对这个数据的限制感到满意
这个数据的归档和检索
然后我们可以
是的 在180天后继续使用这个
然后我们可以将其进一步移动到深度归档访问这里
现在，不仅需要5小时，而且可能需要12小时来解冻数据
因此，在这里检索数据
我们可以再次说
也许让我们说360或65天 像这样
我们可以创建一个规则，仅改变数据的归档方式 总的来说
这是一个很好的存储类，用于智能分层 我们可以使用智能分层
正如我们所见之前 通过使用生命周期规则自动进行
将我们的生命周期包括在内
也许在零天
我们自动希望将此转换为智能分层类
然后，我们可以继续修改它
也许在删除后
让我们说 3年或什么 像这样 我们可以将一切结合起来，在aws中管理我们的数据生命周期
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/060_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p60 071 Versioning in S3.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们讨论数据湖中的文件版本控制
这将帮助我们基本管理文件变化
我们通过保留文件的多个版本来实现这一点
随着时间推移，文件状态会发生变化
每当我们做出更改时
会生成一个新版本
这个版本不能更改
但如果我们对文件进行更改
我们会得到一个新版本
这样我们就可以得到所有不同的版本
基本上就是变更的历史
这样做的目的是为了能够回滚到之前的状态
以防万一有误删除的情况
或者一些不想要的修改
这样它就可以成为我们灾难恢复策略的一部分
这样我们就可以在必要时将数据恢复到特定的时间点
此外
一旦创建了一个版本
它是不可变的
这意味着它不能更改
这将有助于我们确保数据的完整性
这样我们就可以确保数据的一致性和完整性，随着时间的推移
因为一旦有了一个版本
它就不能更改
我们可以获得数据的变更历史
最后，有时某些行业如金融或医疗保健
版本控制往往是一个监管要求
因为这使我们能够追踪数据的变更
在这些行业中，这可能是一个必须遵守的要求
因此在某些特定情况下，这可能也是必要的
但是现在这是如何工作的
我们已经提到过，每当对文件或文档进行更改时
会创建一个新版本
每个版本都有一个唯一的版本号或唯一代码来识别
一旦创建了新版本
它通常是不可变的
这意味着它不能被更改
所以每当有新的更改时
只会创建一个新版本
这样我们就保存了更改的历史记录
这也引导我们面临一个挑战
因为我们知道在数据湖中我们存储大量的数据
通常是PB级别的数据
这可能会使版本控制变得更加昂贵，因为存储成本增加
这并不是增量版本控制
而是会保存一个全新的版本
如果我们在AWS S3桶中讨论的文件，整个文件都会被保存
因此存储成本增加
当然这会带来额外的成本
在某些情况下这也会变得更加复杂
因为我们现在也维护这些版本
所以这会带来成本
因此我们需要在保持必要版本的同时找到平衡
享受这些好处
同时也要管理额外的存储成本
这就是数据生命周期政策的用武之地
我们在上一节课讨论过的数据生命周期政策在这里发挥作用
因为我们可以定义之前的版本应该保留多久
以及何时归档或删除
这样我们就可以管理非当前版本应该保留多久
以及何时归档或删除
我们可以为当前版本制定不同的策略
这样我们就可以具体定义
但在我们的aws环境中这是如何工作的
我们可以非常简单地通过启用s3桶来实现
这样s3会为对象保留多个版本
每当我们进行更改
替换文件
甚至删除文件
我们总是可以回滚到之前的版本
我们会很快在实操中看到这一点
问题是我们应该如何构建策略
我们知道这会带来一些额外成本
也许我们不应该对所有数据这样做
这是我们应该记住的
我们不需要为所有数据默认启用这一点
我们可以只针对一些数据
我们可以为某些关键或敏感数据集启用它
我们只需要这个额外的好处
需要恢复和跟踪这些更改
当然 这取决于业务本身和我们正在查看的数据
但我们可以记住这三点
数据有多关键
如果数据非常关键
启用它可能更有意义
为我们的整体策略提供一个额外的安全网
当然我们也需要考虑额外的存储成本
因为我们需要维护这些多个副本
并且
意识到这些额外成本有助于我们评估 这是否真的有意义
当然最后
我们知道在某些行业
有关于数据保留和审计目的的严格规定
如果数据符合这些规定
当然版本控制就是必要的
这就是它的工作原理
当然我们也应该记住
每当我们做某事
而且这同样适用于版本控制
我们应该为我们的策略保持清晰的文档
所以哪些数据确实被版本化了
为什么这些数据被版本化
像这样 如果我们记录它
这将非常有用
现在我们有了这个理论 让我们现在在aws中实践这个
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/061_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p61 072 Versioning (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们将演示在实际中这是如何工作的
首先我们想要启用这个版本控制
然后看看这是如何具体工作的
那么行为是什么
每当我们更新一个文件
或者当我们删除一个文件
然后我们也想看看这如何影响我们的生命周期规则
因为它实际上确实如此
首先让我们看看如何启用它
如果我们去s三
然后选择我们要启用的桶
我们可以通过转到属性来做到这一点
现在我们在我们的桶下，在属性下，我们发现了这个桶版本控制
在这里，我们可以简单地编辑它，然后简单地启用它
这就是它的工作方式
然后我们已经看到了这个警告消息，这个桶有一个生命周期规则
启用这个桶版本控制会影响这些生命周期规则的执行方式
所以我们很快就会看到这一点
也因为我们记得这些生命周期规则的行为是不同的
无论是否启用了版本控制
那么我们来看看这个
现在只需一秒钟，我们要启用这个桶的版本控制
我们保存更改
然后这基本上就启用了
但现在让我们演示这是如何工作的
让我们快速创建一个新文件夹
让我说我想创建一个名为
我们叫它版本控制
然后我创建这个
我想上传一个新文件
所以如果我这样做
我会进入这个文件夹
这个错了
让我在这里找版本
现在我们想上传一个文件
在这里我已经有了这两个文件
基本上版本一和版本二
它们有完全相同的名字
所以每当我将其上传到同一个文件夹
它将会基本替换它
然后我们当然，由于我们启用了版本控制
它会保留之前的版本
因为我们做了一些更改
这会保持之前的版本
让我来把版本1拖入到我们的桶中
我们可以说在这种情况下上传
然后不到一秒钟它就会被上传
然后不到一秒钟，这个就被上传了，我们看到我们的文件现在在那里
但是现在如果我们正在添加另一个版本会发生什么
所以基本上，让我来到版本2
这现在已经被修改了
我已经添加了一个名为版本二的标题
现在我们想用不同的版本替换这个文件
所以我们想对我们的文件进行一些修改
让我们替换现有的文件
我做到这一点，只需将文件上传到相同的位置，文件名完全相同
然后我这样做
我看到这将在几秒钟内被替换
我可以在这里关闭
我看到仍然只有一个文件
但实际上，幕后
也保存了之前的版本
因为我们启用了 版本控制
但我们如何实际查看这些
有两种方法可以做到这一点
第一种方法是直接转到这个文件
现在我们来看看
让我来打开它
实际上我们可以看到这是第二个版本
所以这就是一个我们可以看到版本号二的地方
这是我添加的标题
与原始形式相比
我们可以看到现在只有新版本可用
这是我们最近上传的一个
但现在如果我们在这个文件中
我们也可以转到版本
在这里我们可以看到当前版本
是我们刚刚打开的一个
在这里我们也有上一个版本
所以我可以选择它
然后我可以再次转到打开
在这里我看到这现在是原始版本
如果我将它与版本2进行比较
我们看到这里有标题
我们看到在这个之前的版本中这个标题不可用
所以我们可以看到我们保留了这两个版本的副本
现在让我再回到版本
我们也可以看到这些版本存储有它们特定的版本id
但更简单和实用的查看那些不同版本的方式
就是要回到我们的文件夹
所以我们的位置在这里，这里有个小按钮
我们可以切换显示版本
在这里我们也可以看到版本的历史，我们看到
这是当前的版本
我们可以打开它
我们基本上会看到，这是主版本
这就是我们在这里看到的，然后在这种情况下，这是最新版本
然后我们回去看，再看版本
我们可以看到
还有维护之前的版本
这是我之前上传的版本
但现在会发生什么
如果我删除这个对象
这有点不同
我是删除特定版本
还是删除对象
让我们演示这两种情况
我先禁用显示版本
现在让我们在正常视图中
基本上删除或删除这个对象
这实际上添加了一个删除标记
所以它没有永久删除
但我们会看到它如何工作
现在我去删除
我已经看到它不再问我永久删除
这已经表明它实际上没有永久删除
但这里我们也看到做了什么
这里我们看到 这将为它们添加一个删除标记
如果我们需要撤销删除操作
我们可以删除删除标记
让我们再看一下这个
我们说删除对象
然后我们关闭它
我们看到它不再可见
所以它看起来被删除
但当我们启用显示版本
我们可以实际上看到之前的版本仍然可用
但删除标记只是作为另一个版本添加
基本上 它目前显示为
它是当前版本
所以当前版本使
这是删除标记
使那些之前的版本不可见
但我们仍然要为存储支付费用
它们仍然在那里
但你也可能会问
为什么这样做有意义
因为我们的整体策略可能适当
因为我们记得
当我们回到生命周期规则时
我们可以有不同行为对于非当前版本与当前版本
所以我们可以说 例如
非当前版本应该总是删除后
让我们说30天
一旦这些版本被删除
它们将永久删除
实际上这也是如果我们手动删除这些版本时会发生的情况
但首先
现在我们如何撤销删除
是的 如果版本启用
我们可以通过删除这个删除标记来做到这一点
所以如果我们说删除
现在我们必须永久删除它
因为我们正在删除一个特定的版本
然后这个文件将再次可见
所以我可以说关闭
现在我看看即使我关闭显示版本
文件再次出现
但这是当我们没有启用版本时，会有不同的行为
这将永久删除文件
或者如果我们删除一个特定版本
所以无论我想删除这个版本还是这个版本
这将永久删除这个版本
这就是点击删除时会发生的情况
现在我们再次看到这实际上是永久删除
所以现在我们必须输入永久删除来删除这个特定版本
所以如果我们点击关闭
当然文件仍然在那里
但我们可以看到实际上只有一个版本可用
因为另一个已经完全删除
这就是不同选项的工作方式
我们可以删除一个对象
我们可以删除一个版本
我们也可以更新对象
因此，正如我所提到的，这影响了生命周期规则
所以让我们也快速看一下
只是为了再次提醒这一点
如果我们在我们的桶中
我们知道在管理下
我们有不同的生命周期规则
让我们只是转到一个现有的
在这里我们记得
如果我们编辑这个
我们可以根据当前版本与否有不同的操作
我们可以移动当前版本
我们可以以不同的方式移动非当前版本
然后我们还有这个选项过期当前版本
如果我们看一下这做了什么
这将有不同的行为
取决于我们是否启用了版本控制
所以我们记得对于启用版本的桶
这就是我们刚刚做的
它添加了一个删除标记
并且对象的当前版本作为非当前版本保留
当然，当我们没有版本的桶时，情况会不同
这将永久删除这个对象
如果我们勾选这个框
现在我们也可以删除非当前版本
我们可以说，也许30天后所有非当前版本将被删除
像这样
我们也会完全删除这个文件
因此，我们可以先设置一个删除标记
然后，也许30天后，所有版本将被完全删除
像这样 我们可以通过使用不同版本定义我们的整个策略
只需为某些重要桶启用版本控制
然后为我们的数据湖中的不同版本定义生命周期规则 希望对你有帮助，下次课程见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/062_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p62 073 Replication.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们来谈谈跨区域复制
这意味着我们将数据复制和同步到多个区域
例如，我们有一个源存储桶在一个区域
现在我们在完全另一个区域有一个另一个存储桶
所有的文件
源存储桶中的所有内容都被复制
基本上同步到另一个存储桶
这有助于或可以作为灾难恢复
它可以帮助提高数据访问性能
因为我们有较低的延迟
例如 大多数用户在一个特定区域
那么如果数据也位于这个区域，这将有所帮助
并且我们还由于这种额外的冗余而增加了可用性和耐久性
所以每当有
也许有些临时中断
我们可以仍然访问数据
我们仍然可以访问这些数据
所以为了快速展示这个的好处
让我们看两个用例
首先我们要谈谈灾难恢复
让我们想象我们有一个医疗保健公司
他们有一些关键数据在一个区域
让我们说在悉尼区域
现在他们启用跨区域复制到东京区域的另一个存储桶
现在假设有些故障
某种区域中断
也许在这个整个区域中的数据中心正在关闭
他们存储数据的地方
但现在由于这种跨区域复制
他们还有这种额外的冗余
所以所有数据仍然在另一个存储桶中
并且由于这种额外的冗余层，数据得到了保护
我们避免了这种失败
因为数据仍然在那里可用
此外我们还有延迟减少的好处
让我们想象有一个游戏公司，可能是一个游戏公司
他们位于美国
但他们有大多数用户可能在欧洲
所以现在他们也可以将数据复制到S3存储桶
例如在欧洲
现在他们可以像这样减少这些欧洲玩家的数据访问时间
因为当然，这减少了
然后延迟
数据更接近用户
这现在只是改善了
用户体验，当然与版本控制类似
这种复制也带来了额外的存储成本
并且请注意，这也有额外的数据传输成本
因为我们将数据从一个区域复制到另一个区域
因此我们需要记住我们如何决定哪些数据需要复制
我们不应该复制所有数据
但我们应该非常谨慎
我们可以看看哪些组件在这个情况下很重要
首先
数据在灾难恢复方面的重要性如何
我们可以确定哪些数据是最重要的
我们依赖于这些数据
然后我们也可以为这些数据启用恢复
当然我们也需要确定哪些应用程序可能需要非常快的响应时间
也许某些用户在不同地区频繁访问哪些数据，
这合理地意味着也应该将这些数据复制到这些地区，
如果我们有一些不同的地点，
不同的用户在那里可用，那么将数据复制到这些不同地区就合理了，
此外， 如果我们有一些特定的要求，
也许数据需要高度可用，
例如，需要随时访问的媒体内容，
确保它们始终可访问非常重要，
那么我们也可以增加一层额外的
是的 只是复制数据到另一个区域以提高可用性
当我们将其复制到另一个区域时，这提高了可用性
当然，考虑到所有这些
我们需要评估和权衡成本与收益
如果文件很大或者数据量很多
那么当然这会越来越贵
所以我们需要以某种方式平衡这一点
当然
此外，我们还有时需要满足一些合规性和监管要求
所以例如，数据有时需要复制以符合这些标准
是的 治理标准
所以这 当然在金融和医疗保健等行业更为重要和相关
所以有时这只是强制性的，我们必须这样做以符合合规性
但现在在aws中这是如何具体工作的
例如 我们可以根据一些复制规则和策略在我们的s3桶中设置这
所以这里我们可以启用它
我们做完这些后，AWS会为我们复制数据
例如
每当向一个存储桶添加新对象时
源存储桶
它会根据这个规则自动复制它
所以我们只需要记住数据只会单向复制
它只适用于我们在源存储桶添加文件
如果我们将其添加到复制的目标或目的地存储桶
它将不会复制回我们的源存储桶
并且默认情况下
当我们在源存储桶中删除一个对象时
它不会从目标存储桶中删除
因此，删除时这并不起作用，因为默认情况下这只是一种预防措施
因此，它可以作为某种灾难恢复
因此，它被复制并作为意外删除的灾难恢复服务
最后
当我们想要复制数据时，需要启用版本控制
源存储桶以及目标存储桶都需要启用
现在 有了这个理论 让我们在实际中实现它
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/063_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p63 074 Replication (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看如何在不同地区实现此复制
在我们的数据湖中
在AWS中这样做
首先，我们要创建一个新的桶
这将作为我们的源桶
因此，我们在给定的地域中选择
例如 我选择us-east-2
我将其命名为我的复制源
我们需要确保这是一个唯一的名称
所以我会添加一些随机的数字集合
然后我们可以保留所有默认设置
如前所述，我们需要启用桶版本控制
这样我们就可以在创建时启用它
否则复制将无法工作
所以我会在这里启用它
我说创建桶
然后我们现在可以设置目标桶
我们可以在同一地区或另一个地区设置它
当它在另一个地区时
它将只是放置
给我们提供一个额外的冗余层
这将进一步增加可用性
因此我们也会这样做
在这种情况下我们只使用类似的名称
当然你必须使用不同的名称
但我将创建一个新的存储桶
我将选择一个不同的区域
例如 us east one
在这里我将使用类似的存储桶名称
我只在这个案例中使用目标
同时，我也会在这个案例中禁用桶版本
只是为了能看到这行不通
然后我们必须启用它，以便能看到它实际上起作用
所以让我们也创建它
正如在源桶中提到的
我们希望设置这个复制
因此，让我们首先转到源桶
我们在这里有 源桶，在这里如果我们转到管理
我们看到那些复制规则
要创建一个
我们只需点击创建复制规则
我们可以给它起一个名字
例如我们说这是复制测试
默认它是启用的
这没问题 这是我们的源存储桶
我们可以通过使用一些过滤器来限制它到特定的范围
作为前缀或某种文本
在我们这种情况下，我们将对所有对象进行操作
然后，我们必须使用目标存储桶
这是我们可以随便浏览的东西
所以现在记住，我们正在使用目标存储桶
我说选择路径
在这里我们可以看到这不会起作用
复制需要版本控制已启用
从这里开始
幸运的是 我们也可以轻松地启用它
然后它就会起作用
否则它将不会起作用
然后在下一步中，我们也可以指定一个角色
在这里我们可以选择一个现有的角色
或者为了这个目的创建一个新的角色
然后，我们可以保留所有默认设置
在这里我们也可以添加此复制时间控制
这将仅会增加复制速度
但在我们的案例中会有一些额外的费用
在我们这种情况下一切都很好
然后我们就这样保存这个规则
所以现在我们来试试这个规则
在这种情况下，我们可以看到可以复制现有对象
因为现有对象默认情况下不会被复制
因此，我们在这里有一个选项可以复制现有对象
在我们这种情况下，我们没有任何对象
所以我们会说不
不要复制现有对象
我说提交
在这里我们可以看到我们在源存储桶中的复制规则
现在我们去源存储桶，在这里上传一些文件
在这里我有一些传感器数据
我想把它添加到这个存储桶中
我已经上传了
现在很短的时间内
几乎是实时的
实际上这会复制到我们的其他存储桶中
当然，这取决于文件的大小
但我们的文件很小
因此，如果我们查看其他存储桶，它将几乎立即可用
所以让我们去目标存储桶
让我们快速刷新
然后我们看到几秒钟后这个文件在这里可用，这不起作用
如我所说，在其他方向上
如果我在这个目标存储桶中上传一个文件
它将不会复制回去
所以我们也可以快速演示一下
如果我上传
也许这个文件在这里
它不会复制回去
它将不会复制回另一个桶
所以我们回到我们最初的源桶
我们会看到数据只会在这里显示
当然原始的数据，所以如果我快速刷新这里
它将不会显示
即使我等很长时间
这仅在一种方向上起作用
现在删除数据的同样情况也是真的
所以如果我只是删除这里的数据
它不会删除在目标桶
所以我只是删除
然后删除这个对象
如果我关闭然后回到我们的目标桶
它总是仍然可用
所以即使我们刷新一段时间后
这不会被移动或从目标桶删除
这实际上是因为我们设置的特定设置
所以当我们创建了这个复制规则
让我们回到我们的源并回到我们的复制规则
所以在管理下我们看到这是规则并且我们总是
当然可以编辑这个规则
所以在这种情况下我可以说操作
然后我可以禁用它或删除它
但我只想要编辑它
现在我滚动下来
我看到在这个情况下在额外的复制选项下
如果我们检查这个选项
这将删除目标桶的删除标记
或者从我们的目标桶
它也会将它们添加到我们的目标桶，在我们的情况下
因为我们希望用它作为灾难恢复
并且为此额外的保护
我们不会启用这个
所以我们可以只是去取消
这就是如何将复制规则应用于我们的桶 以启用跨区域复制
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/064_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p64 075 Security in S3.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈s三中的加密
所以数据可以
当然可以通过加密进行保护
这既适用于在传输中的数据
这意味着在数据在s三和s三之间的传输过程中，以及在静止状态下
所以当数据存储在s三数据中心的硬盘上时
所以让我们首先看看传输中的加密
在传输中的加密情况下
数据通过使用安全套接层或传输层安全保护
所以是ssl或tls
所以这种方法
SSL或TLS将加密客户端传输到三处的对象，安全传输
然后在那里解密
三可能会再次加密对象
然后在服务器端
如果服务器端加密配置为如此
此外 当然我们也有静态加密
因此数据将被加密并保存
这可以在服务器端或客户端级别进行
所有桶都默认启用了静态加密
配置默认启用
所以这里亚马逊s3在保存数据对象之前会加密这些对象
然后在您下载它们时解密这些对象
这是一项自动发生的操作
我们这里有不同的方式来使用服务器端加密
这里我们有不同的方法
我们有使用s3管理密钥的服务器端加密
使用kms的服务器端加密
这给了我们更多的控制权
我们接下来会谈论一下这些差异
然后我们为了获得更多的安全
我们还有双层服务器端加密，再次使用KMS密钥
所以这将是额外的一层安全，使用第二套密钥
然后我们还有服务器端加密，使用客户提供的密钥
当然这会更加复杂
然后我们也可以使用我们自己的密钥
让我们快速看一下这些不同的方法
首先我们有S3管理的服务器端加密
这是每个桶的默认加密配置
我们有这些s三管理的密钥
并且有这个选项
s三仅使用s三管理的密钥进行服务器端加密
所以s s e s三
这只是每个桶的基础加密级别
在s三中，您只需上传常规数据
然后s三会为您处理加密
一旦到达他们的服务器
您上传的每个文件都使用唯一密钥加密
然后亚马逊处理创建
存储和管理这些密钥
这样我们就不需要处理这些问题
当你需要访问数据时
aws将自动使用适当的密钥
然后 当然，解密数据以便您可以实际使用它
然后我们也有服务器端加密wk m s
所以sse km s
基本区别在于，而不是使用亚马逊管理的密钥
我们也可以现在集成kms
这样允许您创建和管理自己的加密密钥
这些密钥可以用来加密数据解密
您对这些密钥的控制就多了很多
这也包括设置策略的能力
您可以定义谁实际上可以使用这些密钥以及它们如何管理
这给了我们更多的控制选项来控制
例如 谁可以
例如，使用这些密钥来访问数据的解密
这里您对密钥有控制权
您还可以旋转它们，禁用它们并删除它们，需要时
这可能出于合规原因
或者出于审计目的，有必要与kms集成
此外，我们还有双层服务器端加密
也使用kms密钥
在这种方法中
您的数据实际上会被加密两次
每次使用不同的kms密钥进行加密
所以这里 我们将有一个加密层，它将在数据到达s三时发生
然后我们还会有一个加密层被应用
这里我们又使用kms，但现在是一个单独的kms密钥
这种双重加密确保即使一层被破坏
第二层仍然会保护我们的数据
这种方法再次使用kms来管理加密集
在某些情况下，可能需要这种加密类型
我们还有客户提供的密钥服务器端加密
与aws管理的其他s三选项不同
在这种方法中，我们必须为我们上传的每个对象提供自己的密钥
这意味着我们将完全负责生成密钥，存储和管理
所有事情都在我们的手中
这将完全由我们管理
当你上传这里
文件到s三时
这完全由我们管理
这将完全由我们管理
用这种方法
你也必须通过安全的连接发送加密密钥
所以这里必须使用https
在这个级别 我们可以获得更高的安全性
在某些领域可能需要这种级别的安全
也许这些领域受到更严格的监管，因为你管理加密密钥
而aws从不存储它们
这也可以减少未授权访问的风险
但这里你需要负责在传输和静止状态下保护密钥的安全
你还必须定期轮换密钥
并且只使用行业标准的加密算法生成这些密钥
这意味着更多的责任
好的 这就是加密 希望对你有帮助，下次讲座见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/065_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p65 076 Security (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在让我们快速看一下我们在桶中的加密
所以如果我们想创建一个新桶
我们可以给它起一个名字
我们叫它加密测试和数字组合
这里我们有所有所有默认设置
我们也有默认加密
所以这里我们只能
当然可以配置服务器端加密
在这里我们可以处理这三个选项
这基本上是桶的默认设置
每当我们将新对象上传到这个桶中
我们也可以为单个对象覆盖这些设置
要么当我们上传这些对象到桶中
所以在上传过程中
或者之后我们可以修改对象以覆盖这些默认设置
所以在这种情况下我们使用默认设置
如我们所述
亚马逊s3管理密钥
所以在这种情况下我们可以像这样创建桶
然后我们可以继续上传文件
所以在这种情况下我正在上传这个客户文件到这个桶中
在这里如果我展开属性
我可以看到加密选项
在这里我们看到服务器端加密
不要指定加密密钥
所以在这种情况下我们将使用默认加密
所以在这种情况下将是s3管理密钥
所以让我们快速上传并使用这个设置
然后看看这是否实际上有效
我们有这个文件可用
现在我们可以选择它
在这里在属性下
我们可以看到现在正在使用这种s3管理密钥
所以s e s three
我们可以 尽管我们可以在上传文件时编辑这一点
或者在我们已经上传的情况下
所以这里我们也可以修改已经上传的对象
所以在这种情况下 我们可以更改为覆盖桶设置以使用默认加密
并且我们可以 例如
更改为sse kms
所以在这种情况下我们将使用aws密钥管理服务加密
所以使用aws密钥管理服务管理的密钥
所以这里我们有更多的控制
例如 如果我们想要控制对密钥的访问
谁可以使用这个密钥
例如
所以如果我们需要对访问有更多的控制
例如或者对密钥本身
我们可以使用这个选项
在这种情况下我们可以从aws kms密钥中选择
在这里你只有默认密钥可用
所以我们选择这个默认密钥
这就是 基本上已经在kms那里，在这里我们可以更改它
所以我们可以像这样保存更改
然后之后我们就更改了它
所以现在如果我们想再次回到属性
我们可以现在看到在服务器端加密设置下
这里显示现在正在使用AWS密钥管理服务密钥的服务器端加密
在这里我们可以看到加密密钥ARN
现在我们可以打开这个
我们可以基本导航到KMS并查看这些密钥
在这里我们可以进行配置等
在这里我们可以使用这个KMS密钥对本文件进行加密
所以我们基本上覆盖了默认设置，好的
就是这样 当然，一如既往，最好删除我们刚刚创建的桶
这样我们就可以清理资源了
即使我在视频中没有这样做
这样做总是好的
不管怎样 记住要做这件事
我必须通过永久复制来删除它
删除空桶
然后我也可以继续删除这个桶，所以再次
不要忘记这件事
现在我们已经把一切都清理干净了 希望对你有帮助，下次课程再见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/066_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p66 077 Bucket Policies.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈桶策略
所以，桶策略基本上是一组规则
它定义了谁能访问我们的S3桶中的物体
以及他们可以对这些物体执行哪些操作
我们在桶中指定这些
我们不使用IAM规则来专门为此目的
但这是在桶中定义的
我们可以设置这个
这是一个JSON文档
所以我们使用这个JSON格式
在这里我们定义谁能访问桶
所以是哪些用户或服务
然后我们也定义他们在桶及其物体上可以执行的操作
桶策略的基本内容，我们可以在这里右侧看到
所以这里有版本
这是策略语言版本
然后我们有陈述
所以这包含一个或多个单独的规则
然后我们也有效果，可以是允许允许执行操作
像我们的情况一样
它也可以是拒绝
那样会拒绝那些操作
然后我们也有原则
允许或拒绝访问的人
这可能是AWS账户
当我们使用IAM时，也可能是用户或AWS服务
然后我们有
当然有操作，所以我们定义允许或拒绝的操作 例如我们有S3获取物体
这将允许下载文件，这可以使用
然后我们可以指定这应该在哪些资源上执行
我们也可以添加一个条件
所以这里我们可以定义一个条件，必须满足
以便这个陈述生效
例如
在我们这种情况下，我们可以有这个条件，只允许来自这个IP范围的访问 然后我们有这个条件，这将要求连接使用HTTPS
所以这是可能在我们的桶级别设置的
我们不使用特定的IAM角色来控制访问
但我们可以根据资源定义这个
所以基于桶级别
我们可以在给定的桶中转到权限选项卡
并在桶内部设置这个
我们不使用特定的IAM角色来控制访问 但我们可以根据资源定义这个
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/067_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p67 078 Access Points in S3.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们也来谈谈S3的三个访问点
通常当你使用S3时
你将你的数据存储在桶中
但随着你存储越来越多的数据
你也可能想与不同用户或应用程序共享这些桶
也许与不同的用户或应用程序共享这些桶，管理谁可以访问数据可能会变得非常复杂
所以这就是访问点可以发挥作用的地方
通过让你创建可定制的入口点来简化这个过程，入口点可以指向一个桶
每个入口点都可以有自己的一套独特的访问控制策略
基本上它们是
是的 可定制的访问点到你的桶
每个访问点总是有一个策略
取决于谁需要访问数据以及他们如何访问数据
所以每个访问点总是有自己的DNS名称
所以这可以是互联网或VPC的起源
当然 互联网起源就是从互联网
而VPC起源是从VPC访问S3桶
当然也有一个访问点策略
所以这是用来定义
当然 权限，有了这个，我们可以设置可定制的权限
所以每个访问点可以只有
正如我们提到的，自己的一组权限
这意味着你可以为需要读取数据的团队创建一个访问点
也许另一个团队需要同时读取和写入数据
所以所有这些都可以使用相同的底层存储桶完成
你不需要更改存储桶的任何结构
这对于可扩展性也很有用
随着你的项目增长
管理访问权限可能会变得更加复杂
这样我们可以更有效地管理访问权限
例如 你可以为不同的部门或外部合作伙伴设置不同的访问点
再次 你不需要改变你如何组织你的桶
此外 这也可以增强安全性
因为这可以帮助你限制数据如何被访问以及由谁访问
当然 降低敏感数据意外泄露的风险，我们不希望泄露这些数据
所以这些都是访问点 希望对你有帮助，下次课程见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/068_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p68 079 Object Lambda.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在让我们看一下对象lambda
对象lambda是s三中的一个特性，它允许你转换数据
正如从s三中检索出来
这意味着您可以使用lambda函数在实时修改数据
你不需要创建或存储数据的单独副本
如果你可能需要在不同的版本中拥有数据的可用性
例如 你以不同的形式拥有它
或者你有一个特定的使用案例，你需要纠正敏感信息
所以这些可以是有用的事情
例如，过滤掉敏感信息
也许包括个人身份识别信息
或者调整图像大小
或者更改数据格式
或者也许也通过添加额外信息来增强数据
想象你有
例如 存储在s三中的文件
也许一些客户数据
并且这里 你现在想使用对象lambda来添加一些代码来处理这个数据
以便从这个数据中移除这些敏感信息
所以你用对象lambda来做这件事
现在我们想看看这是如何工作的
所以基本上
我们已经学过我们可以向处理数据的代码
当数据在请求中被检索时添加
我们不需要复制数据来创建单独的副本
但我们可以在请求这个数据时添加这个逻辑
所以通过对象lambda访问点
那么让我们看看这如何运作
让我们说 一个用户或应用程序正在发起请求，以访问存储在此处的数据
三个桶
这个请求是针对对象的lambda访问点而不是直接针对桶
这个接入点像这样充当一个中介，会对数据进行转换
在实际上归还给用户之前
并且对象Lambda访问点是连接到Lambda函数的
因此这将触发lambda函数
然后数据基本上被处理了
所以请求的数据由这个lambda函数处理
这也可以通过标准访问点访问数据
在这里，在lambda函数中
我们有数据转换正在发生
这可能是减少敏感信息
或者可能转换数据格式或任何自定义处理逻辑
实际上在我们的lambda函数中定义
然后一旦lambda函数处理了数据
它将转换后的数据返回给对象lambda访问点
然后从这里对象lambda访问点
然后将转换后的数据发回请求该初始请求的用户或应用程序
有很多使用场景我们可以使用对象lambda来简化这种方法
例如
我们已经提到过 我们可以用它来对个人身份信息进行去标识化以用于分析
例如
我们也可以将不同类型的数据格式进行转换
例如
将XML转换为JSON
我们也可以将数据与其他服务或数据库的信息进行补充
这就是对象lambda 希望对你有帮助，下次课程再见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/069_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p69 080 S3 Event Notifications.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看事件通知的三种情况
事件本质上是我们存储桶中发生的任何事情
我们可以使用这些事件通知对这些事件采取措施
我们可以接收通知并对其进行处理
例如，事件可以是文件上传到一个存储桶
然后我们想根据这个事件做点什么
例如，我们希望使用lambda函数处理此文件
因此，此事件通知将触发我们的lambda函数
这是一个基本的例子
事件可以是创建对象
删除对象
恢复或复制它们
以及与生命周期相关的其他事项
例如 生命周期过期
或转换不同存储类别
例如 这些都可以是事件通知
例如，智能分层
自动归档
这也可以是事件，甚至是对象
我们还有对象ACL PUT事件
这些都是可以使用的不同类型的事件通知
然后，我们可以使用这些事件通知将它们发送给
例如，发送消息到SNS或SQS
我们也可以使用它来触发lambda函数
例如
这是一个很常见的场景
例如，文件上传到存储桶
现在我们可能想对这个文件做点什么
也许继续处理它
或者存储一些有关此文件的元数据
这可以全部使用lambda函数完成
因此，这个将被特定事件通知触发
我们还可以将这些事件路由到事件桥接器
例如 然后在那里对这些事件进行更复杂的处理
这也是可能的
如果我们仔细看一下不同类型的事件通知
我们将看到一些示例，说明这如何工作以及如何过滤它们
这里有一些通知示例
例如 对象创建
然后使用特定方法PUT
这将是上传至存储桶的对象的事件
使用PUT方法
这里使用POST方法
您可以看到，创建对象可以有不同类型的事件
我们只有不同的方法
当然，这也包括当对象被删除时
在这里我们也可以使用通配符来捕获
例如所有对象创建事件
所以这里我们只需使用这些星号
这将独立于将要使用的方法
对于这些事件通知，我们也可以过滤到特定目录
或者可能也是特定文件格式
这样我们只想触发
也许我们的lambda函数基于csv文件
例如 或者当这发生在我们桶的某个特定目录时
因此我们可以使用前缀和后缀过滤器
所以前缀过滤器将过滤给定目录中的对象
例如，只有当我们有
嗯 一些文件上传到这个图像目录或文件夹
然后只有这会触发事件
所以这可以像这样设置
后缀过滤器的一个例子是在这个例子中当我们使用
例如点jpeg作为文件类型
所以这里我们只使用后缀jpeg
然后只有当文件是jpeg文件时才会触发这些事件
因为这就是文件的结尾方式
所以这里像这样
我们可以使用这些前缀和后缀作为过滤器
例如
让我们假设我们想对文件上传到桶时采取一些行动
但只使用特定put方法
所以这种情况下
当然我们也可以使用后缀和前缀过滤器
但假设 在这种情况下，整个桶中当使用put方法上传文件时
这会触发lambda函数将此文件存储
也许将其存储在rds数据库中
或者可能存储有关此操作的一些元数据到数据库中 因此我们可以使用这些事件通知来设置类似这样的东西
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/070_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p70 081 S3 Event Notifications (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看事件通知的设置
我们想对这些事件通知做些什么
我们将在一个新创建的桶中配置这些事件通知
我们想要上传一个文件
然后这会现在向一个s主题发送事件通知
正如我们在课程后面提到的那样，我们可以直接访问s和s
所以这是一个通知服务
在这里我们可以设置一个主题
我可以在这里展开
这是我们将要设置的导航，我们设置一个主题
然后我们可以订阅这个主题
这意味着 例如，我们可以通过订阅从这个主题收到电子邮件
所以这是我们需要在s和s中设置的两件事
因此，步骤1
直接访问s简单通知服务
所以步骤1是，我们必须创建一个主题
然后通过订阅订阅这个主题
好的 首先设置我们的桶
这是我们想要上传文件的桶
然后配置通知，好的
所以我们直接去创建一个桶
在这里我们称之为事件通知测试
然后加上一些数字使其唯一
其余的我们可以保留为默认设置
这些都很好
我们可以继续创建这个桶
让我们整理一下，看看这个桶
所以这里我们可以打开这个桶
在这里，我们在属性下找到资源名称
我们有arn
我们需要使用这个来配置这个主题
所以这个主题实际上能够接收通知
我们需要在这里设置策略
因为默认情况下，这个主题不允许任何东西接收它
所以我们需要特别授权接收来自这个桶的通知
现在我们在哪里设置这些通知在我们的桶中
我们做的就是在这里属性下，我们再次滚动
在这里我们找到事件通知
我们也有选项启用将通知发送到亚马逊事件桥接
在这里，我们可以在事件桥接中设置一个稍微复杂的设置
这也是可能的
所以我们将转到编辑并启用此功能
在我们的情况下，我们希望设置从我们的桶设置事件通知
然后将其发送到我们将要设置的这个主题，好的
所以这里我们可以设置这个
我们将会在下一秒钟设置这个
所以这里我们可以设置这个s和s主题
然后从这里选择这个主题
所以我们可以说我们要记录的事件类型是什么
前缀或后缀是什么
如果我们想要并且像这样
我们可以设置这个事件
因此，在这种情况下，我们当然需要先创建这个主题
这样我们就可以在这里选择它
因此，让我们回到s和s，现在开始创建这个主题
我们可以选择标准
这是用于通用目的的
再说s和s，我们将稍后在这门课程中讨论
那么我们就叫它上传
也许叫桶什么的像这样
在这里我们也可以可能指定一个显示名称
但是我们也可以留下所有其他默认设置，只有访问政策
所以在这里我们需要配置谁能访问这个主题
所以事件通知将发送到此主题
所以我们需要在主题中允许这一点
因此我们在高级这里，我已经准备好了这个策略
所以你也可以在资源中找到它
你可以下载它并复制和粘贴
此外，在这里和这里，我们只需要更改两件事
首先，我们需要更改来源
在这里，我们需要配置来源
这是我们的桶
因此，让我们首先访问我们的桶
在这里，我们可以在新标签页中实际打开它
然后在属性下，我们可以找到arn
我们可以再次从这里复制它，并将其现在粘贴到这个来源arn中
此外，我们也需要源账户
我们也可以复制这个
那么我们也把它粘贴到这里
然后之后我们就可以保存它了
所以我们现在基本上像这样创建这个主题
让我们继续创建主题
主题已创建
现在我们还需要订阅这个主题
以便我们可以发送
例如 一个特定的电子邮件
当主题中发生任何事情
发送到我们的电子邮件地址，好的
所以我们这样做，要么在这里选择订阅，要么从这里创建订阅
在这里，我们可以选择我们要订阅的主题
在我们的情况下，这是上传桶
然后我们可以选择一个协议
所以我们应该做什么，在我们的情况下，我们希望发送电子邮件
在这里，我们可以指定我们的电子邮件地址
然后，我们也必须确认这个订阅
所以我们需要去我们的电子邮件地址
然后也确认这个
所以我将快速完成这一项
所以我已经收到了这封电子邮件
我可以继续确认这个子订阅
所以现在我订阅了
这已确认 现在我们可以继续
因为我们已经在S和S中设置好了所有内容
继续在我们在S3中配置事件通知
所以首先我们给它起一个名字
实际上上传事件前缀和后缀我们不需要
但我们想要检查对象创建
事件类型
所以你可以看到对象创建
然后这个星号
这意味着所有不同的创建对象的方法都会在这里
记录并发送到我们的主题
当然我们可以让它更精细
通过选择特定的方法在这里
但我们将保持所有对象
创建事件使用这个星号
然后我们还可以选择其他类型如删除
恢复等等所有这些都可以在这里选择
可能我们需要快速刷新
让我快速设置一下
上传事件
然后也是对象创建就是这样
现在我们应该在S主题中选择这个桶
然后继续保存这些更改
现在我们收到这个错误消息
所以让我们仔细看看
实际上再次 在这里我们看到无法验证目标配置
这意味着主题访问策略可能存在问题
所以让我们快速编辑这个
所以在访问策略中
我们看看是否有什么问题
我想我们忘记添加主题的资源了
所以我们仍然需要添加这个
所以在这种情况下让我们回到主题并打开一个新标签页
在这里我们也能找到这个ARN
所以我会复制它并在这里粘贴
所以我会在这里粘贴它
所以现在应该能工作
所以现在让我们再次保存
现在成功了
所以现在我们还想测试它
所以我们想访问我们的桶
让我重新打开这个桶
所以我会添加一个json文件
所以这就是一个桶策略，并将其上传到这里
然后当我上传这个，希望很快
我现在应该收到一封电子邮件
让我快速回到我的电子邮件，看看我是否收到了电子邮件
果然我现在收到了这封电子邮件
所以这就是我现在收到的消息
这是我想要得到的通知
有人在这个桶中上传了东西
好的 这就是如何在S3中配置事件通知的方法 我希望这对你有帮助，下次讲座见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/071_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p71 082 S3 Select & Glacier Select.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈s three select和glacier select
那么我们为什么要这样做
所以s three select通常允许我们执行基本qq查询
对我们的s three对象进行非常基本的操作
这有一个特定的目的和用例，那就是过滤对象
所以我们对象的内容
例如，我们有一个csv文件，里面有
让我们说三十或四十列
而我们只想实际获取数据的一个子集
也许我们只想有一个列或两列
因此我们可以使用s three select来通过安全的查询过滤数据
我们可能只选择一列或两列
这样我们不需要访问整个文件
这可能有时更快也更经济
因为我们不需要访问所有数据
所以我们只需要传输这些非常小的数据子集
这就是s three select的基本思想
这是如何工作的，我们以请求的形式发送我们的SQL语句
然后s three select过滤数据
然后只返回过滤的结果
所以像这样，数据传输被最小化
同时也最小化了延迟
因此，这优化了成本，同时也减少了延迟
所以，这是一个非常简单的获取数据子集的方法
这种方法适用于存储在csv、json中的对象
也适用于apache parquet
它还适用于压缩数据和加密对象
然后我们这样做
我们也可以指定输出格式，比如我们是否想要它
例如，作为csv或json
我们可以确定结果中记录的界限
这在请求中也是可能的
在这里，我们有一个限制，我们每次请求只能查询一个对象
所以它非常基础
想法就是基本过滤文件
例如，在我们实际下载并访问它之前
所以这也可能已经回答了这个问题
这与athena有什么不同
为什么在某些情况下我们不使用athena
为什么我们会使用s three select
所以区别在于
例如，使用s3
正如我们所说，使用场景是我们只想获取数据的子集
所以我们可能有一个文件
我们不想下载整个文件
这可能非常大
但我们只想要数据的子集
所以我们使用s3 select
在这里我们可以查询单个对象
而athena适合更复杂的分析
对于更复杂的查询
在这里我们可以跨多个对象和多个桶进行查询
并且可以进行更复杂的查询
而使用S3 Select
它只允许我们做一些非常基本的查询
在这里我们可以使用from、where和limit
基本上就是这样
所以这里我们有一种基本的方法来获取数据的子集
我们不能做任何连接或其他操作
而使用Athena这更适用于分析和分析查询
现在相同的事情也适用于Glacier Select
所以这基本上是相同的事情
在这里我们也可以直接对我们的冰川对象进行过滤
使用那些SQL语句
所以我们会将SQL语句发送到请求中
像这样我们可以访问存档数据并只获取数据的子集
这样我们不必将数据恢复到更热的级别
如S3标准 但我们也可以使用Glacier Select直接访问它
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/072_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p72 083 S3 Select (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在让我们也展示如何在实践中使用s three select
我们要做的是我们要创建一个新的桶
在这个桶中上传一个文件
然后使用s three select通过运行一个sql查询
来获取这个数据文件的一个子集
然后我们就可以去下载这个文件
让我们通过创建一个新的桶来开始
我们可以给它一个随机的名称
让我们说，我们称之为select或s three select
然后加上一些数字，使其唯一
我们可以这样创建这个桶
现在我们想要打开它
现在我们想要添加一个文件
实际上我们这里有两个文件
因为我们想演示这仅对单个对象有效
如果我尝试上传这两个文件
我会在几秒钟内看到
这将被上传
我可以关闭这个
现在我想使用S3选择
我可以选择我想要使用的文件
然后在操作下我有这个选项查询与s三选择
如果我有多个文件被选中
你会看到现这个选项不可用
所以这不适用于多个文件，好的
所以现在让我们去选择一个
然后我们想要查询与s三选择，所以这里有我们现在的输入设置
在我们的情况下我们可以看看实际上这个文件
所以如果我打开这个
我可以只许打开它记事本
我会确保这仅仅是一个带有几行的文件，好的
我们也会在下一秒看到
在这里，我们需要指定输入设置
在我们的情况下，它是csv
它是逗号分隔的
这也将适用于压缩文件
我们需要在这里指定
我们也可以排除csv文件的第一行
如果这 例如，有一个标题
在我们这种情况下，这里有一个标题
我们可以在这里看到
在我们这种情况下，包括这一点是好的
我们现在可以排除这个数据的第一行
然后我们也可以指定输出设置
我们可以选择也许不同的格式
或者也许也使用不同的分隔符
在这里我们有qq查询
所以我们可以做
我们现在可以运行这个
所以这就是从我们的对象中选择一个星号
然后我们有别名s
所以我现在可以继续运行这作为查询
然后我会看到这是结果并格式化
它看起来像这样
所以这就是它如何工作
现在我也可以进行过滤
例如 所以我可以说s点
这就是我的对象
现在我可能想要获取列
例如名称
所以我可以说双引号名称
这将使其对大小写敏感
然后像这样运行查询
然后之后我看到我得到输出像这样
或者我也可以说我只想要第一个列
然后第二个列
这将通过引用列的数字来工作
所以这里是第一个列
然后我们可能也想要包括第二个列
所以这里我们得到前两列
所以这种情况下如果我们包括第一列将不会起作用
所以如果我取消选择这个
我可以像这样引用数字
所以这种情况下我得到输出像这样
所以这种情况下我们想看不到那些标题
所以我只是删除这个
然后我通过名称调用它
所以这种情况下我们可能想要查看id和名称
所以我可以使用使其对大小写敏感的to_lower
然后是名称
在这种情况下我会从csv数据中排除第一列
然后我可以说我要运行这作为查询
然后我得到结果
我也可以将其格式化为这样
像这样并且我们也可以在行上过滤
如果我们想要 当然这里有一些限制
这不允许我们编写非常复杂的查询
但通常一些基本的
你也可以看到这种情况下处理大文件
所以没有那些限制
或者可能更复杂的查询
我们更愿意使用athena
所以这更多用于过滤
也许只是从中获取数据的子集
我们可以使用s3
选择 运行查询
然后像这样下载结果
这就是它的工作方式
希望对你有帮助 下次课再见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/073_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p73 084 Data Mesh.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈数据网的架构概念
这不是AWS中的特定服务
但这仍然是理解这个架构概念的重要部分
什么是数据网
数据网本质上是一个概念
正如我们所说 这是将数据所有权和管理去中心化的概念
这是一个战略性的方法来组织和管理公司内部的数据
再次强调 这不是一个服务
这可以通过使用不同的aws服务来实现，以达到完全去中心化的数据所有权
同时也提高了数据质量和可扩展的数据治理
这些都是非常重要的事情
当然非常重要
那么这是如何工作的呢
它将数据视为产品，并分配领域导向的责任
因此，一个组织的每个领域都拥有自己的数据
并且对其治理、质量和可访问性负责
数据质量
例如 你有一个营销部门或销售团队
他们处理自己的数据作为产品
所以再次 虽然数据织物只是一个概念
但它可以使用不同的aws服务来实现
我们也需要了解这一点
这些服务有助于实现这种去中心化的数据架构
这些服务是s三为实现可扩展存储解决方案
然后我们有a clue进行数据目录
并且ETL过程用于数据仓库和分析的红移
然后当然还有AWS湖形成
我们会讨论这些
当然在其他的课程和部分中
因为这些当然是非常重要的服务
所以因此我们有湖形成用于安全治理
以及数据湖的一般设置
当然还有Tina用于无服务器查询和Hoqueries我们的数据
然后API网关用于将数据产品作为API公开
所以让我们现在快速地也覆盖那些
是的 数据织网核心原则
我们已经说过，数据织网主张按领域划分数据所有权
这里 数据织网将数据所有权分配给最接近数据的领域
例如，销售数据或营销数据
这样我们可以更好地管理数据
我们基本上有这个数据所有权或监护权
数据被视为一种产品
这里也如此
因此每个领域将其数据视为产品，并专注于质量、可发现性和可用性
因为他们现在是负责这些数据的人
因此，责任非常重要
他们也是实际使用数据的人
因此，他们真的最了解数据
因此，他们应该是这里的实际所有者
我们也有自助式数据基础设施
这意味着我们希望赋予领域权力
以管理和维护其数据的工具和平台
因此，他们应该是实际使用数据的人
所以他们也应该有权力这样做
我们还有一个联邦数据治理
这确保了组织内的合规性和安全性
同时仍然允许各个领域保持自主权 这就是全部了，希望对你们有帮助，下次讲座再见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/074_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p74 085 Data Exchange.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈aws数据交换
这是一个集中的数据目录，允许客户基本上查找
订阅并使用第三方数据在云中
所以这就是它
因此我们有第三方数据源
并且这个数据交换只是简化
与那些第三方数据集或那些第三方数据源安全工作的过程
那么使用数据交换的一些常见用例是什么
您可以使用数据交换以不同的原因使用数据产品
这可能是金融服务
您可以获得实时市场数据
一些历史财务数据
或者一些其他经济指数
以获得更多的了解
也许一些交易策略或一些风险管理
这可能对金融服务有用
当然，医疗保健组织也可以访问
例如 关于医学研究的数据
一些可能患者的数据
一些第三方数据提供商提供的
我们可以轻松地访问这一点
我们也可以利用地理空间数据
例如
卫星图像或天气数据
或者一些基于位置的数据用于城市规划和如此
并且零售公司
他们可以使用这些数据来获取有关客户行为的数据
也许一些销售趋势等等
此外，aws数据交换也正在整合其他aws服务
让我们也看一下这种集成
数据集可以作为aws市场上发布的产品
因此，数据提供商需要作为aws市场上注册的卖家
数据交换还支持aws湖形成数据权限数据集
因此，订阅者可以访问存储在提供商湖
形成数据湖中的数据
他们可以查询
转换并共享对这些数据的访问权
最后，作为第三方数据提供商，他们可以将数据文件导入并存储在他们的s3桶中
订阅者然后可以将这些文件导出到他们自己的s3桶中
这就是aws数据交换 希望对你有帮助，下次见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/075_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p75 087 Amazon Elastic Block Store (EBS).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在让我们来谈谈亚马逊EBS，它代表弹性块存储
所以我们想要理解目的是什么
有哪些功能
主要的特点是什么
因此，EBS卷的主要目的是
它们是我们e的耐用且可扩展的块存储
C 两个实例
所以eb 当然指的是弹性块存储
正如我们所提到的，我们已经专门为EC two设计了这个功能
EC two 两个实例
在这里，我们可以为这些实例获得一个持久的数据存储
基本上，你可以将它们视为
你可以将它们插入到你的EC two实例中
两个实例 然后，它的目的是
当然，是为了获得不同类型数据文件的持久存储
例如操作系统文件
应用程序数据
或者数据库
因此，这些ebs卷它们是为高可用性和耐久性而设计的
数据在可用区中进行复制。
为了确保我们在硬件故障中得到保护
正如你所看到的，在这里有一个e
C 两个实例 我们可以创建一个卷
然后这可以基本上连接到一个给定的e c
两个实例 我们只能对一个实例挂载一个存储卷
我们不能同时将存储卷挂载到多个实例
当然如果我们有一个实例
我们可以有多个存储卷，就像你可以在一个机器上有多个硬盘一样
所以这是可能的
当然可以
现在让我们快速探索一些最重要的功能
这里我们有
当然这非常可扩展
像这样 我们可以增加我们的存储容量
以便我们可以满足不同的工作负载要求
当我们的需求发生变化时
那么我们也可以动态增加或减少容量，无需停机
此外，它也非常耐用
因此，冗余已经内置
我们确保了高可用性
当然，我们也保护数据免受损失
更具体地说
我 O 两快表达
它们提供99.99%的耐用性
年故障率为
在这种情况下 0.01%
所以其他存储类型
它们提供至少99.8%到99.9%的耐用性
所以这里的年故障率仅为0.1%到0.2%
重要的是，EBS正如其名所示，它在块级上运行
这为我们提供了对数据存储和访问的更精细控制
这使得它非常有用
对于E
C 2实例
我们可以非常具体地定制一切
我们还可以获得持久存储
这样数据会被保留
即使相关的E
C 2实例被停止或终止
这样我们可以确保数据仍然可用
这样我们可以确保数据的完整性和连续性
你还可以使用所谓的EBS快照来备份你的数据
你存储在卷上的数据
你可以使用那些快照来立即恢复卷
或者你可能也想将它们迁移到其他账户或区域
因为你无法轻易地将其插入到其他地方
在另一个区域 但你必须通过快照来做到这一点
所以你只需创建一个快照
这样你就可以在另一个区域恢复它
另一个可用区
甚至另一个AWS账户
此外，我们还有高性能
例如，通用目的SSD或预留IOPS
SSD或吞吐量优化
ADHD
EBS为我们提供了非常高的性能
特别针对我们的特定工作负载
此外，你还可以通过加密来保护你的数据
无论是静止数据还是在实例之间传输的数据
当然这非常重要
在安全性和数据保护方面
此外，我们还有按需付费定价
我们有不同的选择来根据我们的特定需求优化成本
我们已经说过，这与E
C 2集成 所以这就是一些重要的事情
当然这非常重要，因为它们那些卷可以很容易地附加和拆卸
我们可以管理它们并提供像这样一个灵活且易于使用的存储我们的e
C Two实例
这就是我们的弹性块存储
现在我们想快速看看这确实如何工作
所以我们有一个ebs卷我们创建
我们可以以特定大小或特定类型创建它
例如通用目的ssd
然后我们可以将其附加到一个easy to实例
所以我们指定类型
然后我们可以说现在这应该附加到我们的实例
然后我们一旦有这个附加
这个e C
Two实例可以访问ebs卷，就像它是物理硬盘一样
所以你可以想象这个
当然这不是一个物理硬盘
但在你的e c
Two实例或在你的虚拟机中，它看起来像一个块驱动器
所以它看起来像一个块驱动器
并且它可以被格式化为文件系统
所以你可以将其挂载到目录
并且像这样，你将得到一个实例文件系统直接附加到你的实例
ebs卷与特定的可用区绑定
我们已经提到过
所以你可以创建快照
如果你想将其移动到不同的可用区或不同的地区
所以你不能将它们拆卸
然后附加到另一个可用区的实例
所以它们总是在
在一个可用区内
你可以将其附加并拆卸到不同的实例
但不跨可用区
除非你创建一个快照
所以这也是重要的事情要知道
现在我们想深入探讨那些实例的配置 我们将在下一节做
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/076_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p76 088 EBS Provisioning.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在最后一堂课 我们看到了EBS是如何工作的
我们也看到了我们如何创建它
然后一旦我们配置并创建了它，我们就可以将其附加到特定的实例
我们还看到，这与特定的可用区绑定在一起
因此，如果我们想在另一个可用区中附加它，那么我们必须这样做
通过使用所谓的快照
所以让我们现在深入探讨一下快照
然后谈谈一些其他重要的细节
这些备份基本上捕获了特定实例的特定状态
在我们处理的大量数据中
现在我们想讨论一些重要的技术细节
这些备份
它们是增量备份
这意味着只有自上次快照以来发生变化的块才保存
这有助于减少存储成本
并且提高了效率
这些EBS快照也在块级运行
这意味着它们捕获所有数据块
元数据 并且与ebs卷相关的配置设置
这是在块级
在我们创建快照之前
aws确保数据一致
因此他们暂时暂停i/o
在卷上的操作
以创建一个时间点图像
以便这反映了数据的一致视图
这是由aws管理和自动完成的
此外，一旦创建了ebs快照
它存储在s三中
这可以通过在给定区域中的多个可用性区中复制数据来提供耐用性
所以这存储在s三中
此外，快照也自动压缩
这减少了存储成本
它们还在休息时加密
当然，我们也可以使用aws密钥管理服务
所以如果我们想自己管理
并增强与生命周期管理相关的安全性
aws允许您为快照定义生命周期策略
你可以启用快照管理的自动化功能。
例如保留
删除或复制到其他区域
所以，在这里我们有关于数据恢复的生命周期管理选项可用
你也可以从快照创建新的EBS卷
因此，像这样，您就可以将卷恢复到之前的状态。
或者你也将快照复制到另一个区域
所以像这样你可以创建这种冗余
如果你需要将其连接到另一个地区的e上
C 两个实例 然后你可以通过将快照复制到另一个区域来完成这一点
总的来说
关于成本
它们对于长期数据保留来说非常经济实惠
当然你也应该监控快照的使用情况
你也应该使用生命周期管理
这样你就可以设置保留政策以优化成本
这当然基于存储在三中的数据量
这就是它的工作方式
现在我们也来谈谈容量规划
这包括指定卷的大小
以及配置其他特性，如IOPS等
对于某些卷类型
我们有不同的选择
因此让我们快速分解容量规划的不同方面
首先 当然
当我们创建卷时
我们指定卷的期望大小，以GB为单位
这将确定我们卷的总容量
我们有不同种类的卷可供选择
它们针对不同的工作负载进行了优化
例如我们有通用目的SSD
例如 GP2或GP3
如果你已经使用GP2卷
你可以直接将其升级到GP3
因为它们实际上是新一代
它们实际上更好
这样你可以独立指定IOPS和吞吐量
这样你不需要增加存储大小
但你可以获得更低的价格和更高的性能
因此你可以仅指定较小的卷，仍能以更低的成本保持高性能
因此升级似乎是一个不错的选择
这可以无缝进行
你不需要做任何事情
只需更改设置
设置所需的吞吐量
设置所需的IOPS
然后你可以直接无缝升级
我们还有带预留IOPS的SSD
我们还有吞吐量优化的HDD
这是ST1
例如
然后我们有代码HDD，例如C1
然后我们有稍微低成本的
例如我们有磁性存储等
根据我们的工作负载要求，我们可以选择不同类型的存储
我们选择合适的存储类型
所以，例如，你可以使用这些通用目的gp来执行通用操作
如果你想在价格和性能之间保持平衡
这非常适合大多数工作负载
这就是为什么它们被称为通用目的
你也可以使用那些配置的IOPS
SSD 如果你有一些I/O密集型应用程序，需要这种一致的性能
然后也 当然
如果你有一些不经常访问的数据
所以你不需要有非常高的性能
然后你可以有一个预算版本的磁性类型，用于那些i/o一种类型
所以这些是提供iops的卷
你可以根据你的具体工作负载要求提供所需的iops数量
它们提供一致的可预测的性能
所以它们可以非常适合我们有一些延迟敏感的应用程序
当然，你也可以根据你的应用程序调整这些提供的iops
读取和写入要求
所以这可以进行调整
但这可能取决于应用程序的要求，此外在我们的ebs卷中
我们也可以像之前所说的那样增加容量
这可以间接地提高性能
尤其是对于通用目的
所以那些ssd
例如 gp two
这可以提高性能
因此，更大的容量通常具有稍微高一些的基础性能
它们还具有更高的吞吐量
这使得它们能够维持更高的性能
我对此长期持积极态度
这可能更有益，因为我们可以长期维持这一点
是的 总的来说
当然我们也应该监控这一点
在这里，我们可以使用云观察来监控不同的指标
例如，容量
读取 写入操作
延迟
队列长度 等等
这也可以在云观察中完成
在这里我们也应该
当然要监控和分析这些以识别潜在的瓶颈
同时也用于优化我们的资源
最后 我们也想检查删除时的终止属性
这里有一些与这相关的考试问题
ebs中的删除时终止属性
指的是一个设置，决定是否应自动删除音量
当相关的EC
实例被终止时
想象一下你终止了你的实例
然后你可以检查这个属性
这将节省成本，因为相关的EBS卷也会被删除
当你使用那些EC
2实例时，这很有用 这些实例连接到EBS卷用于存储
你想要确保一旦终止实例，EBS卷也会被自动删除
一旦这个实例被终止，存储卷将被删除
如前所述
可以启用这个功能，当它被启用时
它将自动删除
然后，关联的EC two实例终止后，存储卷也会被删除
当它被禁用时
存储卷不会被删除
所以
即使EC two实例已连接 也不会被删除
两个实例已被终止
即使EC two实例被终止，我们的数据卷仍然独立存在
当你启动一个EC two实例时
实例 你可以选择是否在实例被终止时删除挂载的数据卷
这通常通过控制台进行配置
对于现有的EC two实例 C 二实例 这也可以被修改
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/077_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p77 089 EBS Volumes (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看我们可以为我们的实例创建的ebs卷
要做到这一点
我可以只是搜索卷
然后我在e c two特性卷中找到了这个选项
在这里我们看到
这是一个e c two特性
它是专门为我们的e c two实例设计的
我还没有创建任何卷
因此我将继续首先创建e c two实例
我转到e c two仪表板
在那里我可以
然后也可以将此卷附加到我们的实例
因此我将快速设置一个基本实例
我可以说基本示例实例
我将在这里使用ubuntu作为操作系统
我在这里只使用t到微
因为这是免费层的一部分
此外，我们还必须设置一个密钥对
其余配置我们将保持默认
但这里有一个选项
在这里我们看到我们的根卷
我们可以添加一个额外的卷
我可以说 例如
这是一个特定类型的卷
例如，我们这里使用通用目的to
例如，我们有4gb的卷大小
这就是我们可以设置作为额外卷的方式
就是这样，所以让我们继续启动这个实例
当然，我们还必须选择这里的密钥
我已经在之前创建了一个
但你也可以继续创建一个新的
这总是必要的当你设置实例时
当你启动这个实例时
这样你就可以从本地设备连接到它
对我来说，我将选择这个
然后继续启动这个实例
在这里，我们会看到这可能只需要几秒钟
让我回到实例
在这里，我们会看到这可能需要一点时间
也许大约一分钟，直到一切都准备好
然后我们会看到我们可以配置这个实例
我们也可以添加额外的卷
一段时间后，我们看到这正在运行
现在我们也可以继续创建额外的卷
例如，在这里卷
我看到目前这个区域没有卷
这具体取决于区域
并且更具体地说
甚至针对可用区的特定需求
因此我们不能直接将一个存在于特定位置的卷简单地转移到另一个位置
就像我们不能直接将其附加到一个EC two实例
实例上 另一个可用区的实例上 我们必须首先创建一个卷的快照
快照类似于备份
然后可以将其复制到另一个区域
并从那里恢复并附加到另一个
EC two实例上
区域或可用区 所以让我继续创建一个新的卷
我们可以再次从类型中选择
例如，我可以再次选择
请注意，我们必须选择可用区
我们必须选择与我们的EC two实例相同的可用区
否则它将无法使用 因此让我们选择一A
实际上我们可以回到我们之前的实例
所以让我搜索EC two实例
我们应该能够在这里看到可用区
所以这是us east 1A 因此这是正确的
所以这是我们的卷
我也使用了一A
所以像这样
我们现在可以使用它
然后滚动到下方
现在创建这个卷
它将在短时间内创建 我们可以看到
实际上已经有一些卷被创建
它们是与EC two实例一起创建的
所以这种情况下我们可以看到它们都在这个可用区
所以现在如果我想将其附加到一个EC two实例 我可以在这里说
在这种情况下，让我们使用未被使用的一个
那就是刚刚创建的 因为这些实际上正在使用中
所以这种情况下我们可以看到它们都在这个可用区
所以现在如果我想将其附加到一个EC two实例
我可以在这里说
在这种情况下，让我们使用未被使用的一个
那就是刚刚创建的
因为这些实际上正在使用中
所以我们可以继续解离这个卷
或者创建快照等等
但在我们的情况下，我们希望使用新创建的一个
然后我们想将其附加到我们的实例
我们必须等待几秒钟，直到它实际创建
现在我们看到它可用了
但这一个还没有被使用，所以如果我们继续并想将其附加
我们在这里只是附加卷
在这里我们可以选择我们要将其附加到的实例，所以
当然，因为我们在同一可用区创建了这个实例
我们能够使用它
但是另一个不行，因为这里我们看到实际上还有一个
实际上它已经终止了
但这不会在这个列表中显示
因为它在不同的可用区
因此，让我们使用这个，我们也可以选择设备名
所以我正在使用这个标准并附加这个卷
所以现在我们看到它正在使用中
我们也记得在我们设置这个时在我们的实例中有这个选项
在终止时删除它
这仅在创建实例时可以配置
但在实例创建后无法修改
所以例如
如果我们查看刚刚创建的实例
然后转到存储
我们可以看到我们的卷
在这里我们有三个卷，其中一个是根卷
在这个例子中我们可以看到有三个，其中一个是根卷
在这里我们有这个选项
根卷总是默认的
终止的领导是谁
所以这将被删除
所以那些其他项不会被删除
所以让我们快速演示一下
所以我们可以说我们要去
让我们回到主视图
在这里让我们选择这个
我们要去终止这个
所以让我们去终止这个实例
所以这里我们可以看到在ebs支持的实例
默认操作是根
要删除的EBS卷
当实例被终止时
这就是我们刚刚提到的
所以根将被删除
但其他的一些仍然会存在
那么我们继续并也终止这一个
因此它将被关闭，然后终止
如果我们看一下我们的体积
我们也可以刷新
这将会更新
在这里，经过一秒后
让我快速等一会儿
我们应该看到这些反映
现在我们看到这已经被终止
如果我们刷新这些卷
我们看到它们现在已经消失了
至少根卷不见了
因为这已经被删除
而其他的还是可用的
所以我们仍然需要照顾它们
如果我们想删除它们
我们必须手动做
所以我必须输入
在这种情况下删除
也删除这两个附加资源
所以它们真的被清除了，再次
如果我们去我们的实例
让我快速启动一个新的来快速演示
如果我去这些卷
我们有这个选项
这里有高级设置
在这里我们可以添加一个卷
我们可以在这里更详细地配置所有细节
我有这个选项
如果我去高级那里
我也可以更改
终止时删除
这里 它是终止时删除
这里是根
默认是yes
在这里，如果我们去第二个
默认是知道的
我们也可以更改
我们可以再次指定卷类型和指定iops
还有加密等
这就是关于连接到我们e c 二实例的卷
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/078_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p78 090 Amazon Elastic File System (EFS).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈亚马逊FS，它代表弹性文件系统
这提供了无服务器、完全弹性的文件存储
所以，我们可以共享文件
我们不需要规划或管理任何底层存储容量或性能
因为这将由我们管理
高可用性通过在给定区域中的多个可用性区复制数据来实现
当然 也通过在给定区域中的多个可用性区复制数据来实现
这也非常可扩展
因此，取决于需求和工作负载
我们可以根据需要扩大或缩小规模
这种存储的主要用途是用作共享文件系统
这与EBS的主要区别在于，EBS卷只能连接到一个实例上
它们只能基本插到一个实例上
C 两个实例
因此我们不能同时使用多个实例
但这与弹性文件系统不同
现在可以在同一时间使用多个实例
因此可以在多个实例之间共享
C 两个实例
就像这样 我们可以启用协作，同时在不同的设备上使用文件
C 两个实例
这使得协作和使用变得更加容易
C 两个实例 这是一个共享的文件系统
这是弹性文件系统的一个关键用途
这就是它的用途
而且这不仅可扩展，而且具有弹性
这意味着它会根据工作负载自动缩放和缩放
所以这对我们有用
当然可以节省成本
因为我们不再需要任何东西时
这也会缩放
所以在成本方面这非常好
这也提供了广泛的兼容性
因为它使用了NFS v4.1协议
这样我们就可以实现广泛的兼容性
此外，我们还有两种不同类型的性能模式
我们有通用目的
适用于广泛的用例
然后我们也有Max I/O
这对于高吞吐量和高IOPS更有用
我们也会在稍后快速讨论这一点
关于定价
这就是我们习惯的
没有前期费用
但我们只根据需要存储的数据量和数据传输量来付费
所以这里只是根据消费定价来考虑安全问题
当然，这里数据也是加密的
因此，我们也可以使用KMS来创建和管理我们自己的加密密钥
此外，我们不仅可以创建密钥
我们还可以选择旋转密钥并自动删除它们
这非常有用，因为它是集成的
此外，它还完全符合POSIX标准构建
在这里，我们有一套API调用和文件操作
这使它与各种基于Unix的操作系统兼容
所以我们有广泛的不同api调用和文件操作
这使得它更简单，更容易将应用程序移植到其他基于Unix的操作系统
所以像这样，我们不必担心不同的操作系统
但这可以很容易地集成
也关于性能方面
我们说这自动扩展
所以这里我们有多达1000个nfs客户端支持
这样我们得到一个灵活的吞吐量
所以像这样，我们得到灵活性
我们这里有不同的选项可供选择
这样我们就可以根据文件系统的大小和具体活动来适应
此外，我们还有两种不同的附加存储类
当然，我们有标准的出站存储类
然后我们也有低频访问
所以IA或者也是一区低频访问
所以数据不会跨多个可用性区复制
但它只是在一个可用性区内
所以如果我们不需要高可用性的要求
或者当我们访问数据更不频繁
然后我们在这里也有这些基本的成本节省选项
我们也已经讨论过这个了
在这里我们有通用模式
这适用于大多数应用或工作负载
在这里我们只有一些基本的通用用途
正如名字所暗示的
然后我们有最大I/O性能模式
在这里我们有一些用例，我们需要更高的聚合吞吐量和IOPS
这也可以是一个好选择
所以在这里再次有通用用途
这是为所有用例最好的选择
或者不是所有用例
但让我们说大多数用例
在这里我们有低延迟
良好的吞吐量 所以这在大多数情况下足够了
但有时我们有更高的要求
所以更高的iops和吞吐量
所以在这些情况下，当我们有一些非常敏感的延迟时
或者i O
密集工作负载
这种额外模式或更强模式可能是更好的选择
这里我们有更高的吞吐量和iops
再次，这是大多数用例的一般用途
所以，网页服务内容管理
这是一个很好的用例，可以使用这种通用模式
但有时我们有这些需要非常低延迟的应用程序
在这些情况下，或者在我们有高
我 工作负载
然后我们可以使用更多的例子
这将是大数据分析
一些特定的处理
一些数据库工作负载
在这些情况下，我们有这种特殊模式
我们达到最大
我 O 我们必须注意，在这种通用模式中
这自动根据我们所学的进行缩放，因此取决于我们存储的数据量
但这在max模式下是不可能的
I o模式中，这不会自动向数据大小缩放
但我们需要手动调整
因此我们需要手动配置吞吐量容量
此外我们还有两种不同吞吐量模式
我们有突发吞吐量模式
这是为具有不可预测或突然的访问模式的工作负载设计的
然后我们有配置的吞吐量模式
这更适合可预测或更恒定的吞吐量
因为随着爆发性通量的增加，我们会获得那些爆发性信用，并且在特定的时间段内，我们可以利用这些信用获得极高的性能。
所以对于某些情况，
假设在5分钟内我们有非常高的工作负载，
因此，我们可以通过一个爆发性工作负载实现极高的性能，并且我们可以利用这些信用。
而提供通量时，
我们只需要为我们的文件系统特别提供指定金额的通量。
这意味着爆发性通量更适合，
当我们有更多变化且具有尖峰使用模式或一些周期性的数据处理任务时。
而提供通量则更适合，
当然更适合持续性的数据处理。
或者任何我们可以持续通过的地方，这样更易于计算
更可预测 这就是关于ev的，希望对你有帮助，下次课见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/079_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p79 091 AWS Backup.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看aws备份
所以aws备份允许您从一个单一的中央位置配置和管理备份
这意味着您可以创建一个备份策略
然后您可以将它们应用于不同的aws服务
如EC two实例
e c 二
亚马逊
更多
这一切都可以通过aws备份中心化方式完成
此外，你还可以通过设置备份计划来自动化备份过程
这些计划定义了备份应该多久进行一次
例如，每天
每周或每月
还规定了备份应该保留多久
这种自动化确保了备份在固定时间进行
无需手动设置和干预
aws还允许你将备份复制到不同的aws区域
并且也适用于不同的aws账户
所以这是一个重要的功能
例如，用于灾难恢复，以便有这个选项可用
aws备份还提供了一个直接恢复备份的过程
当然 这是我们需要的，当需要恢复备份时
所以你可以恢复单个文件
也可以恢复目录甚至整个系统，取决于
当然，服务的类型和备份的性质
所以这当然取决于我们创建备份的服务
该服务还遵循合规要求，允许您为保留设置规则
我们谈论过这一点
并且也备份的生命周期
此外，AWS备份还与IAM集成
这样您可以控制谁可以访问和管理备份
并且您可以监控备份的状态
并通过AWS备份可用的仪表板查看详细报告
所以这可能有助于您理解
所以这就是帮助您理解的东西
确保备份确实成功
它可以让你在备份过程中遇到问题时解决问题
您通常会定义一个备份计划
您可以在这里设置备份的频率和备份计划的保留期
然后设置您想要包括在备份中的资源
然后您可以通过管理控制台监控这些备份
然后之后
AWS备份将数据存储在一个内部S3桶中
该桶专门为AWS备份分配
现在让我们看一下AWS备份的保险库锁定功能
所以在我们深入研究之前
了解备份库的实际含义非常重要
在aws备份中
备份库是一个安全的容器，用于存储备份
您可以根据不同的标准（如部门、应用程序或特定的合规需求）创建多个备份库来组织备份
例如，部门
应用程序 或特定的合规需求
现在让我们回到aws备份库锁定
这可以帮助您增强备份的安全性和合规性 因为，您可以在备份库中应用不可变和合规保护
就像这样
这可能对
例如 某些需要数据不可变性的监管要求非常有用
以保护免受意外或恶意删除或修改的影响
这可以通过备份库锁定来实现
一旦您为备份库应用锁定
您设置的策略将变为不可变，持续时间可指定
这意味着一旦锁定生效
在锁定期限届满之前，没有人可以修改或删除存储在库中的重置点
甚至root用户也不行
所以这是非常安全的
就像这样，锁定可以帮助您遵守需要数据以不变状态存储的法规
这有时是必要的
持续时间可指定
当然
在金融或医疗等行业
这种数据
完整性和安全性是必须的
这是一个必须且非常重要的功能 这里我们有两种不同的库锁定模式
第一种是合规模式
当您在库策略上启用合规模式时
策略在锁定期间无法更改或删除
这确保了合规要求得到严格遵循
然后是第二种模式
在这种模式下，指定的管理员角色可以管理和更新库策略
但仍然保护了备份点
这就是两种不同的类型
这就是如何使用aws备份
库锁定
的方式
所以这就是两种不同的类型 这就是如何使用aws备份
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/080_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p80 092 AWS Backup Hands-On.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看aws备份的实际应用
所以我们将导航到服务
在这里我们可以看到
这是用来集中和自动化我们的备份
在这里我们可以看到在左侧
我们可以看到一些选项，我们在上一节课已经讨论过
所以我们可以立即创建一个备份计划
我们很快就会做
这是我们在这里的主要任务，创建这些备份计划
但是我们已经提到过
我们还有一个仪表板
我们可以在这里查看一些成功的备份等信息，它们已经完成
或者有一些问题等，这里也可以查看
在这里我们可以创建一个新的备份计划
我们将在几秒钟内这样做
然后，我们还有备份库
如果我导航到这个
我可以在这里看到 我们已经设置了一个默认库
现在我们想要创建
当然，我们也可以在这里创建一个新的
如果我们愿意 我们可以选择一个特定的加密密钥，选择一个新的备份名称
但在我们的情况下，我们想做的是
我们现在想开始创建一个新的备份计划
所以你可以从很多地方做这件事
实际上，无论是从这里还是你直接导航到备份计划
然后你说创建备份计划
现在我们将要创建一个计划来创建一些备份
然后我们也会看到如何为它分配资源
那应该属于这个备份计划
所以这些资源应该被备份
这是基于这个备份计划
让我们看看如何做
我们可以使用模板
在这种情况下我们可以从一些预定义的规则中选择例如
每日每月
一年保留期等等
所以我们有不同的配置
然后我们也可以完全从头开始一个新的计划
所以这就是
是的 基本上同样的事情
但我们可以使用一些预定义的规则
所以它已经预配置了一点
但我们仍然有可以更改的选项，所以因此我们将使用这个模板
你也可以根据json表达式做同样的事情
这就是完全相同的事情
只是json格式
这也是可能的事情
在我们这个案例中 我们选择模板
这会快一点
让我们选择 例如
这里每日每周
每月留存五年
你可以看到这种情况下你只设置了三种不同规则
这是因为我们选择了这个
这里我们有每日备份
这意味着如果我们选择它
我们可以看到为这个规则我们设置的备份频率，记住
我们有三个规则
但这一个是每日频率
例如每日频率存储到我们的默认备份保险库
我们也有窗口
我们可以选择默认
或者我们也可以修改它来找到一些离峰时间的窗口
也许在晚上某个时间进行备份
我们可以设置窗口
然后我们也可以管理生命周期
我们可以说这个应该从暖存储移动到冷存储
在一段时间后
因为那时我们可能不需要这个备份
我们可以节省一些成本
此外，在这里也可以设置总保留期
我们也可以说它应该永远保留
实际上在我们这个案例中，我们选择35天
然后我们也可以选择区域
因此，我们可以配置一个区域，备份应该复制到那里
例如，我们可以选择
让我们说加拿大
我们可以选择这个区域
然后我们可以说它应该复制到另一个账户
Va这也可以做到
在这里，我们也有目标备份保险库
当然，我们也可以进一步自定义生命周期
一旦我们做完这些
我们可以保存这个备份规则
现在我们对这个每日备份规则进行了修改
我们还有一个可能是每周或每月的备份规则
因此，我们可以看到它有更长的保留期
但这只是每月一次
这样我们就可以组合多个规则
这样我们就可以制定出最佳策略来组合所有这些不同规则
在我们这个案例中，我们可以保留默认设置
不管怎样，我们将删除这个
这对我们来说不是那么重要
当然，我们不应该忘记名称
让我们称其为备份计划测试
然后我们像这样可以设置我们的计划
所以我可以前往创建计划
现在我们需要做的是定义哪些资源属于这个计划
在这里我们可以创建一个所谓的资源分配
例如 让我们称其为分配备份一在这里我们可以使用默认角色
如果我们没有创建任何内容
它将为我们创建
或者我们也可以选择
是的 我们设置的特定角色
我们将使其简单并使用默认角色
此外我们现在可以说我们希望包括所有资源类型
或者我们希望只使用特定类型的资源
如果我们这样做
我们可以选择具体类型
例如我可以说这应该是s3
在这里我们可以再次指定特定桶
不是所有桶
但我们可能希望选择特定桶
让我们说例如我想要选择这个作为示例
在这里我们也可以排除特定资源类型
所以我也可以
说一些不是
我可以做 反之亦然
基本上，我们也可以使用文本过滤
如果我说
例如我想使用文本这样做
我可以说只使用那些桶
如果我有 例如
具有特定标签的桶可以被包括在内
例如我可以说部门
让我们说这个是我早已设置好的s3桶标签
我也可以说
例如 这里有特定的条件
我可以说那是相等的
然后也许我有营销
然后只有那些桶
让我们说 或者有一些是的
让我们说这里有
所有桶或多个桶创建
或者我们有一些资源在桶中有不同的标签
然后我们只有营销部门的过滤
像这样我们可以继续将我们的资源分配给备份计划
像这样我们已经设置好了
所以你可以看到在这里我们有这个备份计划测试
这是我们的备用计划
我们有规则，当这种情况发生时
生命循环规则是什么
备份窗口是什么
然后是资源分配
在这里我们可以为多个资源分配
我可以做额外的资源分配
在这里我们指定应包括在备份计划中的内容
在这里我们可以查看备份运行时间
在这里我们可以查看概览
这是我们在备份中设置备份计划的方式
在我们的情况下，我们也要确保删除它
我选择资源分配并删除它
我只是复制并粘贴名称
然后我也想删除整个备份计划
我也可以复制并粘贴备份名称
在这种情况下，备份计划测试
然后我们删除这个计划 希望对你有帮助，下次课程见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/081_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p81 094 DynamoDB Overview.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 现在我们来谈谈dynamo db
这是aws中的无sql数据库
这意味着它与rds不同
这意味着它不是关系数据库
但它被称为无sql数据库
那么，无sql数据库是什么
首先，人们有不同的说法方式来说这一点
有些人说它不是只有sql
或者你也可以说它是一个非关系数据库
它基本上意味着两者都相同
但现在这意味着什么
它与rds等关系数据库有什么不同
例如
让我们更详细地看一下 在关系数据库中，我们总是需要一个固定的模式
这是预先定义的
这意味着我们将有列
因此，我们将有基本的表，它们由列组成
这是我们预定义的模式，然后我们将有那些行
这是关系数据模型
在无sql数据库中，我们可以存储不仅仅是结构化数据
还可以存储半结构化和非结构化数据
因此，当处理来自不同类型来源的更多变化数据时，这将非常有用
所以这里每个记录都可以有所不同
我们不需要非常厚的模式
这就是无sql数据库设计的目的
我们不需要模式
因此，我们也称之为无模式
这与关系数据库非常不同
其中表中每一行的列总是相同的
但在无sql数据库中，您可以在运行时向数据添加额外字段
因此，一个记录或一个项目可以与下一个不同
例如
在文档数据库中，这可能也是一个无sql数据库
您可以使用类似于JSON的格式来表示更复杂的分层结构 它可以嵌套
这与关系数据库非常不同
因此，我们可以看到两种数据模型的主要区别在于数据如何建模
此外，我们还可以看到用例也是不同的
无sql数据库是为了快速扩展而设计的
因此，它可以很容易地水平扩展
它是一个分布式数据库
这意味着我们可以有多台机器
当我们需要更多的处理能力
更高的性能，或者请求更多时
我们可以轻松地添加更多的机器
以获得更好的性能
或者当数据变得更复杂时
我们可以轻松地添加更多的机器
这意味着我们可以很容易地获取强大的力量，我们不会干扰任何事情
而在关系型数据库中这将更加复杂
所以在关系型数据库中，我们必须垂直扩展
这意味着我们必须扩展
这意味着我们可能需要更改RAM
更改CPU并修改我们已经拥有的机器
所以实际上动态扩展这将会更加困难
因此，NoSQL数据库的使用案例是
如果我们有大量流量
如果我们有大量数据
所以这更优化了大批量和高性能
以及灵活性
因为我们可以用这种模式更灵活地存储数据
所以这在大数据或实时分析中非常普遍
然而，如果我们有一些用例
我们需要使用一些连接
所以我们可能会
是的 继续使用SQL
我们知道我们可以用连接和聚合来写复杂的查询
这不是那么直接
或者更精确地说
连接和聚合在SQL数据库中是不可能的
实际上在NoSQL数据库中
因此，根据SQL数据库使用的类型和数据模型
我们有不同的数据查询方式
但我们不能在NoSQL中执行这些复杂的查询
例如连接和聚合
所以这就是
当我们需要这些复杂的查询时
需要的东西
我们不能在nosql数据库中完成
就像我说的，使用案例有点不同
所以，一个sql数据库
只是 它们它们在复杂的查询和连接方面表现优异
因为我们有结构化的特性
但是在nosql数据库中
我们的读取和写入操作通常具有更快的性能
特别是对于大数据集
当我们有大量流量时
这就是nosql数据库大显身手的时候
这意味着在这种情况下
nosql数据库更适合用于用例
当我们需要处理大数据或实时应用时
因为这些是它们真正发光的场景
因此，模式可以随时间演变
当我们需要快速扩展时
然后nosql数据库更有用
然而 另一边
续集数据库在有些复杂查询需求的地方非常有用
也许一些事务完整性
一些关系型数据建模
这在报告时常常需要
也许一些财务系统
所以任何数据一致性和可靠性都非常重要的地方
比如在报告场景中，数据结构也更为严格
这就是关系型数据库和非关系型数据库的区别
现在我们深入探讨一下DynamoDB
这里使用的数据模型
如我们所说，这是一个非关系型数据库
我们不使用关系型数据模型
但我们可以使用键值模型或文档数据模型
重要的是，这是一个完全管理的分布式数据库
这意味着分布式数据库的所有管理负担
是的 我们不需要担心
所有设置 配置
一些如复制的东西
这也当然很重要，以确保可用性
所有集群的补丁
扩展 所有这些都由AWS管理
它非常高性能
正如我所说
特别是在高流量高工作负载下
当然处理所有级别的请求量
我们可以轻松地扩展表
这也可以自动完成
DynamoDB会自动分散数据
如我们所说，这是一个分布式数据库
所以数据会自动分散
我们所有数据都存储在SSD上
它也会自动复制
这就是我们实现高可用性的方式
它自动在给定区域中的多个可用区中复制
这样我们就有了基本的高可用性和数据耐久性
内置到这个数据库中
我们还有存储加密
我们可以轻松地保护敏感数据
如果必要的话
我们已经说过
我们有非常高的性能
我们可以轻松实现毫秒级延迟
这结果就是一个良好的用户体验和快速的响应
无论是写入还是读取操作
现在我们快速看一下它是如何构建的
DynamoDB的核心组件是什么 这就是我们在下一堂课想看的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/082_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p82 095 Create a Table (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好吧，现在 让我们在实际中看看这一点
为了使这一点更容易理解和消化
在这节课中，你将看到dynamodb的许多不同设置
开始时这会看起来很多
但我们将一步一步地在接下来的讲座中详细分解所有内容
所以这里我们将只做一个快速的概述
然后在接下来的讲座中，我们将深入探讨
因此，让我们把它付诸实践
我们将在这里仅搜索服务
Dynamo db
我们已经可以看到这是一个管理的nosql数据库
这就是我们已经学到的
让我们选择它
然后，在这里，我们看到我们的仪表板
现在我们可以深入那些表和项
我们将在下一节课中更详细地了解这一点
我们现在想做的是
我们想创建一个新表
这就是我们在dynamo db中创建的
因此，我在这里点击创建表
有趣的是，我们直接看到必须插入一个表名
因此，在dynamo db中
这是一个没有模式的数据库，它只需要一个表名
以及一个主键
我们将在后面详细讨论这一点
但这意味着这是一个没有模式的数据库
它将由我们管理
我们不在这里创建数据库
但这里只有表
我们创建表
我们可以在这些表中放不同的东西
但我们不在这里创建数据库
例如，我可能创建一个关于书的书
好的 我们选择表名
书籍 然后，如前所述
我们需要总是一个主键
我们将在后面详细讨论这一点
这可能是 例如
书id 这是唯一标识我们数据库中每个项的东西
我们将在下一节课中讨论项
在这里，我们没有像关系数据库那样的行
但我们将在下一节课中讨论项
你可以把它们看作是行
这是等效的
关系数据库中的行
所以这种分区键的作用是，因为这是一个分布式数据库
它基本上负责将数据分配到不同的宿主
这就是这个ID的作用
因此它必须也是唯一的
通常情况下，除非我们有所谓的排序键
当我们有排序键时
我们可以将其作为二级主键的一部分
然后例如
我们可以有多个相同的书籍ID，如果他们在排序键中不同
但我们将详细讨论主键
剑钥匙和分区钥匙在下一讲中
不管怎样 在我们这种情况下，我们只是想快速发现它
所以我们留下这个空白
我们只需要指定我们的分区键
然后我们有表格设置，我们可以选择这里的默认设置
然后我们将获得这些设置
但我们也可以自己精确地定制这些设置
这样我们就可以看到这是如何工作的
首先我们有所谓的表格类在这里
我们可以选择默认选项
这是一个通用目的的表格类
这就是我们通常使用的
但如果我们想要节省一些成本
那么在数据访问非常不经常的情况下，我们有特定的用例
我们也有标准a的不经常访问
在这里我们可以节省一些成本
因为这种方式是为不经常访问优化的
在我们的情况下，我们只是使用标准输出
因为这实际上是免费层的一部分
所以我们现在有这个可用的
是的 这就是我们为什么想使用它的，我们也能现在
这是我们必须指定我们需要多少功率的
我们需要多大的容量
我们必须指定这个
所以我们可以选择这个预留模式
我们必须指定我们需要多少读写
别担心
我们也会在接下来的讲座中讨论这个
但基本上我们可以指定一个固定的金额
因此，在这种情况下，我们需要准确管理并规划这一点
或者如果我们的工作负载非常不可预测
那么我们也可以选择按需模式
这可能会稍微贵一些
当然，这是额外的费用
因为它更灵活，所有事情都会被管理
但是，如果是非常不可预测的情况
这也许也是
更好的选择
再次我们详细讨论这一点，所以选择提供模式
因为这实际上是免费层的一部分
在这里对于提供模式
我们需要指定容量
在这里我们可以说我们需要指定
如果我们不使用此自动扩展
例如为读取提供五容量单位
我们稍后详细讨论这些
但这仅适用于表的吞吐量容量
在这里我们可以选择自动扩展
这适用于读写
所以我们可以指定一个最小值
一个最大值和一个目标利用率
在我们的情况下我们保持关闭以保持简单
我们为读写提供五容量单位
再次我们稍后详细讨论
同样如次级索引
我们也稍后讨论
这只是一般情况
如果我们有一些特定的访问模式
例如 我们更频繁地从一个区域或一个类别检索数据
当我们有一些与分区键不同的特定访问模式时
就像我们提到的那样
例如我们想看特定的书籍ID
那么我们可以通过定义额外的次级索引来优化性能和成本
我们也稍后讨论
现在我们不需要担心这一点
现在也这里我们看到这是我们的容量
我们可以定义加密
我们也保持默认
然后我们可以继续
当然我们也可以添加一些文本
但现在我们可以像这样创建此表
所以你看到我们已经选择了设置此表所需的内容
其中包括分区键
然后我们设置一个表
现在我们想要更详细地看一下
这个表是如何构建的
所以这是我们表的组件
首先 我们在幻灯片上可以看到
所以这是如何工作的 这些是组件
然后我们也想在实践中应用 这就是我们在下一节课中要做的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/083_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p83 096 Core Components.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在让我们快速谈谈这个Dynamo DB是如何构建的
以及它的核心组件是什么
所以这总是由表组成
一个表是
是的 就像在其他数据库系统中一样
数据存储在这些表中
所以基本上这只是一个数据集合
并且我们很快就会看到一个例子
所以当我们看到每一张表的结构时
我们在每一张表中总是可以有
有时候我们也有零项
但基本上
嗯
这是表的构建块 所以我们可以说这些是行
在其他数据库系统中
这叫做行
但在NoSQL数据库中，Dynamo DB
这叫做项目
你可以将它们视为单个记录
例如
如果你有一个书籍数据库或一个书籍表
那么每个单独的书籍
例如
可以是一个项目 然后每个项目都由不同的属性组成
所以属性只是基本数据元素的一种
例如
一个项目可以由不同的属性分类
例如 一个单独的书籍可以有特定的作者
它可以有特定的页数
它可以有特定的价格
所有这些都被称为属性
我们记得这是一个模式
所以每个项目可能有三个属性
而另一个项目可能有完全不同的属性
所以这些都是构建这个的核心基础
现在我们将更详细地看看这是如何工作的
这是如何构建的 顺便说一下
所有的表都需要有一个主键
我们将更详细地看看主键
但就像我说的
让我们把这个变得更实际
以便我们理解这是如何构建的
就像我们提到的那样
这可能是一个表
这是书籍表
例如 现在我们有一个项目
这是第1个项目
这可能是一本书，具有特定的书ID
这将是在这种情况下我们可以看到唯一的标识符
这是我们的主键
这可能是1个项目
我们曾说过1个项目总是可以有多个属性
所以出版日期
有哪些类型
价格是多少
等等，所有这些都是属性
你也可以看到，这个表是无模式的
这意味着在所有的项目中，所有的属性都可以不同
这不是事先定义的
它只是也许也在随时间变化
这并不是一个问题
如果你考虑一个数据库
这是一个关系数据库
这将更加复杂
在这里这将需要添加一列或删除一列或更改列名
然后，如果某事没有精确的schema
就像我们的表
那么我们将有一些问题，它将不会工作
在这里这更加灵活
并且你可以看到，大多数的属性
它们是我们称之为标量
标量意味着这不是一个数字
但这只有1个值
例如 作者或标题只有1个值
并且书ID
在这种情况下是我们的主键
这总是必须是标量的
并且数据类型可以是字符串或数字
这些都可以是不同的
所以这是我们在这里关于组件和如何构建这些表的情况
并且现在，每个表
正如我们所说
总是有一个主键
现在我们在下一讲
我们想要更深入地理解主键
并且可能有一些次要键
这是一个非常重要的概念 这就是我们在下一讲想要了解的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/084_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p84 097 Creating Items (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看在dynamodb中表是如何构建的
我们已经提前创建了这个书籍表
现在我们可以打开它
在这里我们可以创建更多的项目
现在别担心 你会看到很多不同的东西
在接下来的讲座中我们将逐步探索它们
但现在我们首先想看看表的样子
所以我们想把这个表填满数据
我们知道这是通过添加项目来完成的
这些项目可以有不同的特性
一个项目可以有不同于下一个项目的特性
所以现在我们来看看这是如何工作的
我们现在选择了这个表
在这里我们有一个概览
然后我们可以打开这个表
现在我们可以更改不同的东西
但我们也可以创建一个新的项目
在我们的情况下是第一个
所以我们看到项目计数为零
因此我们将创建我们的第一个项目
我们知道我们总是必须指定一个分区键
在下一讲中我们将更深入地探讨这些分区键
在我们的情况下它将是主键
因此我们现在将直接指定这一点
我们必须这样做
例如我可以说这是b1
例如 在这种情况下它是字符串类型
我们已经指定了这一点
当我们创建表时
类型是什么 一旦主键被创建后就不能更改
所以这种情况下这个分区键已经创建
我们不能更改它
所以这是被固定的
当我们创建表时
我们必须提供值，我们现在正在做这一点
现在我们可以添加一个新属性
基本上实时
所以我可以说它应该是数字
在这里我们可以说例如页数
有多少页 所以我可以说这可能是200
然后我可以添加一个新的属性
所以我可以说这可能是字符串
所以我可以说例如标题
所以现在可以说这是书一
像这样
现在我们可以创建我们的项目
这是我们的第一个项目，具有这些属性
现在，这可以使用这个简单的表单创建
或者也可以使用json视图
这就是它的样子
然后从这个json角度来看
所以这里我们看到
这是我们的项目，这些属性
我们将回到表单
这更容易理解
我们将创建这个项目
现在我们可以看到
这需要一段时间才能看到
现在我们的表格有一个项目
你可以认为这是一行
如果你认为这是一个表格
现在我们可以继续并创建另一个项目
例如
如果我回到这个表格
我可以说我想要创建一个新的项目
创建项目
现在我们可以自由创建
当然，我们总是需要提供书籍ID
这就是分区键
但我们可以自由添加其他属性
所以这里让我们说这是B2
现在我们想要创建一个新属性
可能是一个数字 例如可以是页数
可以是240页
但现在我们可以添加其他东西
例如现在我可以说我想要一个字符串
这可能是作者
我们不必在这里指定
标题 我们可以自由选择每个项目的属性
这就是我们所说的schema is
例如，这可能是作者
可能是一
然后，我们可以继续创建这个项目
现在我们的表格中有数据
我们有这两个项目，让我快速刷新
现在我刷新后可以看到
我们可以看到我们正在消耗那些读容量单位
稍后我们会详细讨论
但我们可以看到这些项目已经被返回
我们可以在这个表格视图中看到
当然，这样我们更容易理解
但重要的是，你可以看到，这个项目
例如，没有作者的值
所以这没有必要
然后这里就会被填充为这个null
基本上这里也是
这也可以是不需要对所有项的东西
所以你可以看到这更灵活
这就是它的力量
当然这也使它稍微不那么僵硬
所以如果有些错误
也许有不同列名或什么的问题
那么我们就没有结构
这在这种情况下也可能是一个缺点
所以它当然有好有坏
现在如果我看看这个项
例如 我也可以在json视图中看到它
这是我的一项
好的 这就是带有项和属性的表格看起来的样子
现在我们想更详细地谈谈主键
我们的分区键
然后我们也会看看排序键
这都是如何工作的 现在我们想更深入地探讨
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/085_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p85 098 Primary Keys.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的 现在我们深入探讨一下dynamo db
我们已经学习过，每张表必须拥有一个主键
每个主键属性我们也已经看到，它必须是标量
这意味着如果我们回头看看它是如何看起来的标量
这意味着它不必是数字
但它意味着它只有一个单一的值
那些主键允许的数据类型
可以是一个字符串
一个数字 或二进制
所以对于非主键属性如
例如 价格或类型
我们不受数据类型限制的
但对于主键
它只能字符串
数字或二进制
并且必须标量
这意味着它只有一个值
这就是现在用于给表中每一项唯一标识的东西
这在分布式数据库如dynamo db中非常重要
因为数据可能分布在多个服务器上
而主键的唯一性很重要，以便知道数据在哪里
这样我们就可以检索数据和操作数据
如果必要的话，以一致的方式
所以主键是dynamo db架构中非常基础的部分
这就是用于存储数据，访问数据和管理数据的
所以在dynamo db中这是一个非常重要的事情
这就是用于存储数据，访问数据和管理数据的
我们已经讨论过，一个项目不能有相同的键
这意味着它是唯一的
我们不能有一个项有一个键
然后下一个项有相同的键
这通常不可能
我们也说过它必须是标量，并且必须具有三种数据类型之一
主键在创建表时指定
然后它不能更改
这就是在表设计时固定的东西
我们也说过这是一个非常重要的概念，当我们创建数据库工作时
数据是如何组织和检索的
现在 这些主键是什么
我们有两种不同类型的主键
第一种是分区键
然后我们还有一种称为复合键
这不复杂
所以分区键也称为是的
基本上简单的主键
我们也称之为有时哈希属性
所以这就是一个基本属性来区分项目
我们在我们的书籍示例中已经看到这一点
书籍ID 它就是主键
所以它只是一个单一的键
然后我们这里有一个复合键
我们用两个键来区分项目
这里有两种类型
所以它们构建了这个复合键的分区键
这是第1个
然后排序键
有时也称为范围属性
所以我们快速看一下它们并快速比较
我们看到分区键
我们提到这是哈希属性
然后复合键也称为哈希
这是我们的分区键
然后范围属性
这将是
我们已经在这里说过，这是排序键
我们快速看一下一个例子，以便使这更实际一些
所以重要的是要知道，如果我们快速比较它们
所以分区键只能有一个单一的键
这意味着我们只有那些分区键
因此它必须是唯一的
因此不能有任何具有相同一分区键的项目
而与复合键，我们有选择分区键
然后也有一个排序键
并且只有这两个的组合
它将是唯一的
所以这里我们可以有不同的分区项，但它们的分区键是相同的
例如，你可以说书籍类别或书籍作者
像这样，作者可以作为分区键
但我们还有一个排序键
例如，它可以是标题
然后结合这些
它将是唯一的
所以这里我们在结构化键方面有更多的选择
并且我们将看到为什么这很重要
不久我们就会看到
并且在什么情况下这确实有意义
所以让我们现在更仔细地看一下
在这种情况下，我们有一个简单的分区键的表
在这种情况下，如果我们看学生
例如
我们有学生ID
当然，如果我们有一个记录
每个学生一个项
这可能是主键
因为在这种情况下，它可以使每个项目唯一地与学生号码相关联
然后，这里属性的内容可以是年龄或学位
或者任何可用的其他信息
这里我们以表格形式表示
但它可以有不同的结构
我们可以有不同的属性
对于某个学生，学位的信息可能缺失
或者对于另一个学生，年龄可能不可用
这就是这种非常灵活的模式
所以这里我们说这是我们的分区键
在这种情况下，这是唯一的
因此，这可以是分区键
然而，当我们看复合键时
我们有时可能会有信息分布得更广
例如，我们有两个不同的学生编号
因为每年我们都会给出一个学生编号，这可能是从前一年再次获得的
但在这种情况下，这是我们的主键
这必须由组合组成
在这种情况下，我们可以按学生编号分区
但在这种情况下，我们将有一个排序键的毕业
这将使其变得唯一
这就是我们在键方面有两个不同的选择
现在我们有了主键
所以为什么它们实际上需要
我们看到我们总是有一个主键
但现在的问题是
为什么我们实际上需要它们
所以重要的是这一点
由于这是一个分布式数据库，用于高效地检索数据
这个分区键
这就是为什么它被称为分区键
决定了数据物理上存储的位置
但现在的问题是，如果你要查询
通常这是我们检索数据的方式
我们指定
我们要查看的项目
例如 我们要查看的书
但现在如果我们有不同的访问模式
这意味着我们要根据不同的属性查询数据
例如，我们要查看特定作者的所有书籍
或者我们要查看特定类别的所有产品
在这种情况下 我们需要谈谈二级键
它们用于更有效地访问数据
如果我们有不同的访问模式，这些模式不同于通过分区键访问数据
通过主键
具体如何工作 我们现在想在下一节课看看
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/086_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p86 099 Working with Primary Keys (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


在我们深入下一个概念之前
我们首先快速演示主键在这里是如何工作的
我们已经学习到的一些概念
我们已经看到，我们指定这本书的ID作为我们的分区键
这作为我们的主键
并且我们在创建表格时只能指定这个
所以我们创建表格后不能更改它
然后我们也看到，我们不能有主键的重复
如果我们不使用复合键和排序键
那么这些分区键都需要是唯一的
例如，如果我创建相同的ID
让我复制它并创建一个新的项目
例如，让我回到表格选择
然后我们创建一个新的项目
我们不能使用相同的
所以如果我现在试图再次使用相同的值
我可能会添加一些额外的属性，比如页数
所以我说这可能是一百
然后我们会在一秒内看到这不会起作用
我试图创建它
我看到条件请求失败
您提供的主键项已经存在
这个分区键是我们的主键
我们看到我们不能创建具有相同分区键的同一项目
或者相同的主键
所以我们必须选择一个不同的
如果我使用三
然后它将再次起作用，我们可以轻松地创建它
然后，我们也看到，我们可以添加一个额外的排序键
我们不仅将我们的分区键作为主键
它是一个复合键，由分区键和排序键组成
这将帮助我们更有效地在分区中排序数据
例如
我们希望将页数作为我们的主键的一部分
我们将其作为排序键添加
然后我们只能在开始时这样做
当我们创建表格时
您可以看到我们可以执行一系列操作，如果回到我们的表格
所以我可以在我的表格中做不同的事情
编辑容量
更新表格类等
但我们不能创建表格后更改主键
因此，我们必须创建一个新表格
所以让我回到表格，在这里我可以创建一个新表格，在这里
例如，如果我有像产品或
或者让我们说书二
然后我们可能有
书ID
现在我们可以选择这个排序键
例如，我可以说一些
也许我们也想按作者排序
然后我也可以说作者ID
也许这将是排序键
这样我们就可以更有效地排序或搜索所有共享分区键的项目
这将使其更有效
在这种情况下
这将是一个复合键
如果我们这样做
我可以再次保留所有这些设置
所以我选择标准表类，这里是配置的无自动扩展
我们再次使用这五个单位
顺便说一下，分区键的数据类型再次使用
当我们创建表格时，我们必须指定
你看到，我们为项目有更多的数据类型
但在这种情况下，我们只为分区键只有三种数据类型
这与排序键相同
我们将只选择在这种情况下的字符串
所以现在让我们继续使用这种容量创建它
其余的将只是默认设置
现在我们在这个表中创建一个项目
我们还必须指定现在的排序键
所以现在它不需要在整个分区键中唯一
例如，如果我们快速刷新
一旦这被创建
我们可以打开这个表，我们可以说我们想要再次创建一个新项目
现在你可以看到我们必须指定这些值
所以这种情况下如果我说让我们假设这是b1
我想要创建这个项目
我们可以看到这不会起作用
现在我们必须也指定排序键
因为这是我们主键的一部分
所以这不能是一个空值
当然这不是我们要提供的
所以我可以说这可能是作者一
所以现在我们可以创建这个项目
现在我可以说我想创建一个新的项目
所以让我再回到这个表并创建一个新项目
现在我说这又是相同的分区键
所以再次b one
但现在如果我们当然选择a one
我认为我选择了它，价值正好像这样
让我创建它 啊，不
它有点不同 显然，我们可以看到我选择了相同的
如果我快速刷新
我们将看到这将在这里可见
然后我们看到
是的 它是作者一
但是你看到这本书的ID
所以我们的分区键
负责将数据分布在不同的分区中
我们看到它们可以现在是相同的
但现在我们有不同的
这是必要的排序键
所以现在我们可以有这个复合键
是的
这需要不重复地识别每个项目
这就是关于实践中的主键
现在我们想要探索的概念是次级键
当我们有一些不同的访问模式时
所以我们想要良好的性能
但我们不想只依赖于这个分区键
也许我们想看作者
或者也许我们想看
我的意思是 作者实际上是这里的一部分键
但让我们说类别并且这经常被访问
我们需要谈论次级索引 我们在下一节课中这样做
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/087_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p87 100 Secondary Indexes.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们已经听说过主键
但有时我们希望以稍微不同的方式访问数据
为了提高效率
我们可以使用二级键
这就是我们在这节课中想要讨论的
这是一种替代方法，使我们能够创建非常高效的查询
在这里，我们可以使用与主键不同的替代键来访问数据
这里有两种类型的键
这里有不同类型的
第一种是本地二级键
第二个是全局二级索引
现在我们快速比较它们，找出它们之间的不同
我们有一个局部二级索引
这必须与基础表具有相同的分区键
我们可以有不同的排序键
但这必须具有相同的分区键
所以我们在这里 基本上
是的 固定或绑定于此
我们不能改变这一点
所以这有点不太灵活
并且需要在创建表格的同时创建
我们不能修改它，所以它在这里更有效率
但这在这里有点不太灵活
所以当我们需要一个与原始分区键不同的东西作为基本表时
那么我们就不能使用地区次键
每张表格最多可以有五个地区次键
所以你可以看到你可以根据你访问模式的类型添加多个这些
是的 在你申请或用例中看到
但现在我们看到这可能更有效率
但实际上这也是
当然不那么灵活
所以我们看到它必须被创建
它不能被修改
它必须与基表有相同的分区
而现在所有这些都不是使用全局二级索引所必需的
这里有一个带有分区键和排序键的索引
所以你可以看到这是一个复合键
但它可能与基础表不同
所以分区键可以是
例如 基础表中不是索引的东西
所以它是不同的
它的分区键与基础表完全不同
而且它可以在后续被修改
这使得它更加灵活
这里我们可以有更多的全局二级索引
这就是GSI
或者这里我们称之为本地二级索引lsi
这在这种情况下与全局二级索引相比更加灵活
我们必须意识到
基表和全局二级索引之间存在一种关系
这意味着当你执行写操作时
例如 在基表上进行插入或更新操作
与之相关的所有gsi都需要反映这些变化
这意味着正确的操作不仅涉及对基表的写入
还涉及对gsi的写入
但是也骑向GSI
所以这是我们在这里必须考虑的事情
这与容量有关
但是现在问题可能是
我们在什么时候选择那些键
所以我们什么时候选择一个LSI
什么时候我们选择GSI
所以你可以使用LSI
如果你需要维护相同的分区键
但是你只需要一些额外的排序或一些额外的查询能力在这个相同的分区
所以当一致性非常强的时候，这可能会有用
是的 这是非常必要的，也是非常重要的
但是另一方面
你可以使用GSI
如果你需要一个更灵活的访问模式
当你只需要查询能力时
在基于不同属性的访问上具有快速性能
所以当你需要在之后添加或修改一些东西
所以这被创建之后
你需要这种灵活性
然后你也可以使用GSI
因为它可以在之后进行修改
所以这些都是你可以选择它们的使用案例
现在让我们快速地看一下一个真实的例子
以便以更实际的方式理解这一点
所以想象一下，这里有一个Dynamo DB的产品表
也许这是一个电子商务平台
这里有不同的产品，它们由主键识别
这就是我们的分区键
你看它们是独特的
但现在也许你有一个需要这些数据的应用
你需要有不同的访问模式
例如，你可能需要根据给定类别列出所有产品
或者你需要为特定制造商查找所有产品
解决这个问题的方法是使用二级键
你可以 当然可以使用类别索引
这将是一个全局二级索引
所以是一个全局二级索引
在这里你可以说，是的
现在您想按类别高效地列出所有产品
在这种情况下，类别将是我们的分区键
然后，是的
我们可以对制造商做同样的事情
所以在这个全局二级索引中
类别索引将是我们的分区键
然后，从这个基础表中
在这个二级索引中，分区键将仅作为排序键
结合这些，我们就能形成复合键
所以，GSI通常是一个
复合键
所以我们有类别或制造商
这些是我们的分区键
在这个例子中我们有两个GSI
然后我们有排序键来使其唯一
当然，这非常有用
以便我们可以启用额外的访问模式
我们可以避免全表扫描
这样我们就可以更有效地查询这些特定访问模式
对于用户来说，这意味着更快的数据检索和更好的性能
所以这非常有用
现在，它是如何构建的
二级键通常使用投影属性构建
实际上，它的构建过程并不复杂
在下一讲中，我们将快速看一下
在构建过程中
它并不很复杂 我们想要快速看一下
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/088_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p88 101 Projecting Attributes.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


我们已经看到，我们可以添加额外的二级键
它们允许我们有良好的性能以不同的方式访问数据
我们通过投影属性来做到这一点
这意味着
属性将从基表复制到索引
然后我们像这样构建二级键
我们可以指定应将哪些属性复制到索引中
当然 根据我们看到的访问模式
我们是否想根据类别查询数据
那么我们投影这个属性
这是我们可以指定的
这里有不同的选择
所以我们基本上有三种选择
总共我们可以在每个索引中投影20个属性
这里是这三种选择
首先
我们有所有选项
如果你的应用程序访问模式不可预测
并且你需要很多不能预测的不同属性
或者当你只是想
也许一般 确保所有项目都可以直接从索引访问
这样你就可以为所有不可预测的读取操作获得最快的性能
那么你可以选择所有选项
当性能非常重要并且能够直接从索引访问任何属性
那么你可以选择这个选项
你有 当然 缺点是存储成本会增加
这可以使它更加灵活
这可以简化设计
你不需要
是的 思考它应该如何
你不需要计划
应该属性是什么
但你以额外存储为代价获得了更多灵活性
然后你也有使用仅键选项
当您的查询主要需要访问项目键属性时
这是存储成本最低的解决方案
因为你只在索引中存储键
当您用于这些数据的应用程序
是的 只需快速查找某些项目键
然后你可以根据那些键获取完整的项目
这可以是 然后这是存储成本最低的解决方案
然后你也有包括选项
当你频繁访问不是索引的特定属性时
那么你可以只包括对你应用程序必要的属性
是的 这非常好
当你有非常可预测的访问模式时
你知道确切需要哪些属性
如果你也有
是的 一些需要查询但不是基于主键的访问模式
但你可能有
我说过类别和制造商
你知道这是可预测的
那么你在这里就有性能和高效存储之间的良好平衡
这些都是三个选项
现在我们理解了这三个选项
我们也想谈谈
所谓的DynamoDB流
这是我们用来跟踪表格中变化的工具 让我们在下一节课看看
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/089_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p89 102 DynamoDB Streams.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看 dynamo db streams
这是我们可以用来跟踪表内更改的工具
所以 streams 会捕获我们表内发生的所有更改
这样我们就可以启用一系列实时用例
因为我们会收到这些更改
它们会被发送到 streams 中的记录
我们可以实时处理它们
它们按时间顺序排列
这样它们可以很容易地被消费
让我们看看这在实际中会是什么样子
假设我们有一个表
在这里我们启用了 streams
这意味着我们需要启用它
它并不是默认启用的
每当发生更改时
它们现在会被 streams 捕获
所有正确的操作都会被捕获
这意味着所有更改包括在内
当然 删除
更新和插入
再次，这也只发生在我们启用 streams 之后
因此，如果我们没有激活 streams，我们不能回顾性地查看发生了什么更改
首先我们需要激活它
只有之后在出现的所有更改
它们会被捕获
现在我们看到它们现在以记录的形式组织
每个记录代表一个正确的操作的一个更改
如果我们在一个正确的操作中有多个更改
也许我们更新了多个值
那么所有这些更改都会被捕获到这个正确的操作中
所以每个记录是一个单一的正确的操作
现在可以在接近实时中访问
因此它可以被其他服务处理
我们可以消费 streams
我们会在几秒钟内看到一些例子注意
同样，这个 streams 也与 kinesis data streams 类似
以 shards 的形式组织
并且它们决定了容量
但这次我们不会关注这些 shards 的管理
但这会被 aws s 自动管理
现在让我们再看一些重要点
总结 这是一个可选功能
我们需要启用它
如果我们想使用它
我们需要在一个给定的表上启用它
如果我们不这样做
那么我们将无法使用它
我们将无法捕获这些更改
再次，我们不能事后处理
因此，我们可以从过去捕捉一些更改，现在同样如此
我们说一个记录包含一个操作期间发生的所有更改
所以是一个正确的操作
如果多个属性发生了更改
那么所有更改都将记录在这个记录中
现在我们可以决定应该将什么记录到流中
因此我们有几个选项可供选择
我们将在几秒钟内查看这些选项
重要的是再次说明，如果我们没有激活流，它将不会被记录
我们已经提到过这一点
然后数据
所有那些更改
这将在流中保留24小时
现在让我们看看这些流选项
它们允许您定义应该记录哪些更改
即我们的流中应该看起来什么样子
在这里我们有三个不同或更准确地说
有四个不同的选项可供选择
因为选项一个是两个选项的组合
但我们将看到第一个
这是仅包含键的选项，这里每个记录仅包含已修改项的键属性
这意味着当您只需要知道项已修改时，这将是有用的
但我们不需要知道修改的具体细节
我们只捕获已修改的键属性
然后，我们也有新图像选项
这里记录包括所有已修改的属性，以及如何在修改后它们看起来
所以图像基本上在修改后
当然，当您想要知道项在更改后的情况时，这是有帮助的
即更改后的样子
更改后，项的状态
插入或更新后
然后以类似的方式，我们也有旧图像
在这里，我们也看到更改前项的样子
更改前，项的样子
然后，我们也有新旧图像的组合
在这里，我们可以看到新旧图像
我们可以看到所有属性
更改前后的样子
这将是更改的最全面的视图
我们将能够确切地知道更改了什么以及它是如何更改的
即更改前的样子以及更改后的样子
您将如何选择最佳选项
嗯 这取决于 当然
您正在构建的应用程序的要求
即您的确切用例
例如 如果你想审计一些更改，用于审计或日志目的
并且你需要这个完整的大局观
那么你可以使用新和旧图像
如果你只是想用aws lambda构建一些东西
你只想处理已更改的内容
那么你也可以只使用新图像
所以这取决于你要做的事情
所以这些是选项
让我们快速看一下我们可以用这些条带做什么
我们如何处理数据
因此，使用案例通常涉及使用 AWS Lambda 的事件驱动架构
在这里，我们可以在发生某些变化后响应我们的自定义代码
我们也可以有数据流
所以这些变化被发送到
例如 像 S3 或 Amazon Redshift 这样的东西
我们可以利用
在这种情况下，使用 Kinesis 数据火烈鸟以良好的方式分发
我们也可以使用 Kinesis 数据流捕获它
因此，我们在本课程的另一部分更详细地了解 Kinesis 数据流
此外 我们也可以使用elasticsearch集群
来同步我们的dynamo dp数据
这可能对搜索或分析有用
我们也可以构建自己的应用程序来处理流数据以满足各种需求
例如 我们可以在不同系统之间同步数据
或者进行一些更自定义和更复杂的数据处理任务
在这里我们可以利用kcl
所以kines's client library
我们已经在kinesis数据流中看到过这一点
这就是简化从是的流处理过程
不同的源头 因此，例如
来自Kinesis数据流
或者，在这个情况下，也是为了 Dynamo DB 流
此外，我们也可以利用胶水
所以这里我们也可以只是
是的 需要有某种ETL过程来处理这些更改
我们也可以使用流数据来复制表格在不同地区的更改
因此，我们可能需要确保数据在其他地区一致且可用
这样，我们也可以将这些更改带到另一个地区并同步它们
这些都是一些用例
现在，让我们快速看一下与lambda的特定示例
我们如何设置这一点
所以第一步
我们需要在我们的表格上启用这一点
这需要启用
我们需要选择我们想要的流类型，如仅键、新的图像等
然后我们需要
当然 创建lambda函数
这包含在检测到更改时应该执行的代码
然后我们也需要在这个函数中配置触发器
在这里我们需要配置流作为我们函数的触发器
在这里我们可以指定
是的 确实我们可以使用这个事件源映射来只说
每当出现新记录时
我们应该触发我们的函数
这就是在lambda中我们可以做到的 所以这是对dynamo db streams的简要介绍
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/090_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p90 103 DynamoDB Streams (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看 dynamo db 中的流功能
我们可以像之前所学的那样启用这些流，以捕获我们对表的更改
这是我们可以启用的功能
然后所有的更改都会被记录
它们会在我们的流中存储四个小时
然后我们可以做些事情来处理这些更改
例如，我们可以将一个 lambda 函数连接到那里
这就是我们现在要做的
为此，如果我们有一个选定的表
例如，我们可以导航到导出和流
在这里我们有这个选项
dynamo db 流详细信息，所以默认情况下它没有被启用
但我们可以通过在这里进入这些详细信息来启用它
在这里我们可以说打开
然后当我们这样做时
我们可以看到这里有我们之前听说过的四个选项
在我们的情况下，我们将使用新和旧图像
以便我们在流中有最详细的视图
让我在这里点击这个按钮来启用它
现在，对这个表所做的所有更改都会被记录
让我们快速演示一下
所以如果我现在做一些更改
例如，我说探索表项
在这种情况下，我想创建新项
在这里我可以去创建项
然后我可以说这可能是一个
也许让我们在2024年创建项
现在让我们说，我们要创建一个新项
所以我们可以说这可能是二
这可能是2020年
现在我们想要添加一个新属性
可能是一个字符串
然后我们可以说在这种情况下，也许我们想要
让我们说作者
也许这是作者一
我们可以创建这个项目
好的 所以现在这将记录在我们的流中
但我们实际上如何看到它
所以现在如果我们再仔细看一下我们的表
让我们回去，转到表详细信息
我们可以再次在流下看到，现在这是活动的
但我们目前没有对流做任何事情
所以现在我们想要
例如
添加一个 lambda 函数来做些事情
也许我们想要导出它
当然，我们可以在监控下看到一些更改
所以在流下，我们将能够看到它
但我们目前没有对它做任何事情
因此，让我们回到流
现在，让我们在这里连接一个lambda函数
我们可以通过添加触发器来做到这一点
同时，我们也会创建一个函数
让我们在这里创建触发器
因为我们没有
我们可以选择一个现有的lambda函数，我们将其与该触发器连接
但我们将直接创建一个新函数
这将帮助我们拥有预定义的代码
所以我们选择从零开始
我们称之为流dynamo db，在这里我们可以选择python运行时
我们可以保留默认的执行角色
我们将创建一个基本的lambda角色
我们将需要添加一些额外的权限
以便此函数实际上可以从流中获取信息并访问流
我们将在几秒钟内完成这一点
除了这些，我们可以保留默认设置
然后我们说创建函数
现在我们想在代码中快速添加一些内容
一旦创建了这个
我们将看到该触发器很快就会出现
所以我们将首先进行这些更改
所以我可以说
例如 打印事件
我们将此事件作为触发器添加
因此，我现在能够
让我保存这个函数
所以让我部署它
现在，我们应该能够在这里找到这个函数
所以我可以看到，如果我刷新一下，这会在这里可用
现在我也可以选择批处理大小
即应该触发函数的记录数量
我们希望每次记录都触发函数
因此，每次记录都会自动触发函数
因此，我们保持为1并创建触发器
我看到这当然不会起作用
因为角色无法执行这些操作且无法访问流
这就是我们所说的我们需要更新角色
以便我们可以执行这些操作，即获取记录
获取分片迭代器，描述流和列表流操作
需要在角色中调整这些
让我们进入我们的函数并在配置下
然后导航到权限
我们将看到使用的角色
这只是创建的基本角色
我们可以快速打开它
从这里，我们可以在这里添加必要的权限
这意味着我们可以附加另一个策略
所以我们可以 当然
以更精细的方式进行操作，只允许对特定资源执行特定操作
但我们也可以让它变得更简单
为我们的使用案例做出更广泛的设定，使其简单化
那么我们继续，设定触摸政策
在这里我们可以搜索 dynamo db
我们可以在此处给予 dynamo db 完全的访问权限
并添加此为权限
然后这就被添加在这里
现在我们应该能够将其作为触发器添加
那么我们点击这里创建
我们看到这已经成功了
现在我们看到这已经启用
我们在这里添加了一个触发器
现在我们在表格中更改一些内容
也许我们想要添加一个项目
所以我去
在这种情况下创建一个另一个项目
也许这是第四本书
年份是2023年
也许还有一个属性
让我们说页面
我们可能有200个创建项目
也许我们想改变一些东西
所以让我们说，我们要去这个
也许两个
所以 例如
我们要在这里将作者更改为作者2
我说保存并关闭
现在Lambda函数应该被调用
作为我们的功能
让我回到代码
这里只是打印我们的事件
它现在应该显示这个
现在我们可以转到监控
在这里我们应该现在可以看到在日志中
现在我们应该看到它正在显示什么被更改了
让我打开云日志
在这里我们应该看到这次冲刺，好的
所以这实际上花了很长时间
直到现在这种情况才被反映
我也做了一些改变
我在表格中又做了一些改变
如果你看到一些问题
你也可以再次去表格项进行更改
确保一切都能正常工作
然后一旦你做了这些，在函数的监控下
你现在应该能够导航到云监控
现在我们在这里
如果我们导航到日志流
我们可以看到日志流，在这里如果我们查看这里
现在我们可以看到记录
已经被修改
所以让我们更仔细地看一下
在这里我们看到事件类型
基本上就是事件名称
在这个案例中是修改
所以我们应该能够看到旧事件
然后也是新事件
因为我们在流中设置好了
像这样，所以我们看到新和旧图像
如果我们看新图像
这就是已经做出的更改
这是新图像
在这里我们也有旧图像
这就是我们得到的输出
我假设我必须做出另一个更改
因为Lambda函数还没有活跃
如果你也看到了这一点
然后您应该能够看到
现在我们得到了这个记录
当然，如果我们想要
我们也可以定义我们的函数，以便我们做些别的
在这里我们只是打印了这个触发器事件
但我们也可以做别的事情
如果我们刷新
我也应该能够看到触发器出现在这里
这就是如何连接我们的流
我们如何创建它并将其连接到我们的Lambda函数 希望对你们有帮助，下次再见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/091_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p91 105 APIs.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


考试中可能会出现的是Dynamo DB的基本API
它们主要用于程序化地与Dynamo DB交互
您可以执行如创建表等操作
插入项
或查询数据
使用这些API，通常它们用于
是的 所有这些基本的操作，如创建
读取 更新 删除
在我们的表中进行所有这些操作都是可能的
这是为与那些数据库进行一些简单直接的交互而设计的
以便我们可以使应用程序的开发更加直观
这就是我们使用这些API的原因
我们有四种高级操作
首先我们有控制平面
这类操作涉及管理Dynamo DB服务本身，而不是处理数据
这包括创建或管理表等任务
设置流
或者配置一些辅助索引
此外我们还有数据平面
这些是直接处理表中数据的操作
例如插入 获取更新
直接删除我们的项目
然后我们还有Dynamo DB流API
这是一个用于访问和管理与Dynamo DB表相关的流数据的API集
我们可以使用它
我们
例如 这使能在表格上流数据
访问流记录
或实时处理数据变化
最后我们也有交易API
这支持交易，允许你将多个操作分组成一个
所有或无的操作
所以基本上一个交易
并且 这在我们需要确保多个项目数据完整性的情况下非常有用
所以，比如一些行业金融交易，这可能更重要
是的
我们需要能够做到这一点
所以现在让我们更深入地了解那些单个的API
首先我们有控制平面
这些，正如我们之前所提到的 是我们能够管理服务本身的管理操作
所以这里一些重要的API操作包括创建表
我们可以用这创建一个新表
所以
描述表
我们可以检索信息
例如配置或状态关于表
所以只是一些一般信息
然后我们也可以使用列表表
这可以帮助你管理你创建的表
然后我们也可以使用更新表
所以这里我们可以修改表的设置
例如提供通过量设置
或者一些全局二级索引也可以在这里管理
此外我们还有删除表
这是领导
整个表
和所有相关的数据
所以这些都是对设置和持续管理我们的数据库环境的重要操作
然后我们也有数据平面
所以 这涉及到允许你直接与存储在我们表中的数据进行交互的操作
然后我们使用party ql api
party ql 是一个qq兼容的查询语言
我们可以在dynamo db中使用
所以我们可以像这样
执行基本的操作
所以创建读取
更新 和删除
并且这种相似性使我们容易
如果我们已经熟悉sql
然后我们也可以使用party ql
然后我们也有dynamo db经典粗略
粗略api
所以这里我们有
是的 我们也会在这里看一下
我们可以在这里也执行粗略操作
所以让我们深入探讨一下
首先我们有party ql 我们有执行语句
我们可以读取多个项目
或对一个项目进行写入或更新
使用这个ql语法
这又是安全的兼容
所以它非常相关
然后我们也有批量执行语句
我们可以在这里执行批量操作
我们可以做写更新
读取多个项目
然后如我们所述我们也有经典粗略api
重要的是我们必须提到主键
这必须在操作中指定
所以这里我们可以做很多事情
首先我们可以创建数据
所以对于这一点 如果我们想将单个项目写入到表中
我们将使用put item api action
所以这里我们需要再次指定主键
这在批处理中也同样重要
但我们可以只写最多25个项目
然后我们也可以读取数据
我们可以从表中获取单个项目
使用主键
使用get item action
此外，使用批get item，我们也可以以类似的方式做到这一点
现在我们可以从一个或多个表中获取最多100个项目
然后我们也有查询
在这里，我们可以检索所有匹配特定分区键的项目
并且可选地，也可以使用排序键条件
最后，我们还有扫描
在这里，我们可以读取表中的所有项目或二级索引中的项目
最后，我们也有更新数据
我们有update item，用于修改一个或多个属性
然后是delete item，用于从表中删除项目
然后，同样地，使用批处理
现在我们可以删除最多25个项目
让我们也看一下dynamo db streams的操作
它们只与streams有关
我们可以捕获对项目所做的更改
在这里，我们有以下行动
我们有list streams
我们可以获取与表关联的流的列表
然后我们也有scribe stream
我们可以检索有关特定流的信息
然后我们也有get shard iterator
我们可以获取允许您从特定位置读取流的记录的迭代器
这稍微具体一些
最后，我们还有get records
在这里，我们可以使用分片迭代器检索流记录
这可以包括更改，如更新
插入和删除
这些是dynamo db streams的操作
最后，我们还有事务操作
现在我们可以执行原子事务
它们可以打包多个操作
在这里，我们可以使用party ql apis
这样我们就可以使用party tql进行事务操作
如果我们熟悉sql
这可能会更容易，更直观
在这里我们有execute transaction
这是一个批处理操作
我们可以在多个crud操作和不同表中进行事务操作
再次我们使用ql
如果我们熟悉sql
这可能会更直观
然后我们也有dynamo db的经典事务api
在这里我们使用事务正确项
这再次原子地执行一些写操作
这包括
当然 插入更新
删除 我们可以在这多个项目上跨一个或多个表进行操作
然后最后我们也有事务获取项目
我们可以从一个或多个表中检索多个项目作为单个原子事务的一部分
所以这些事务确保所包含的操作要么成功
要么以原子方式失败
所以要么所有成功要么全部失败
这样我们就可以维护数据完整性
总的来说这些api允许我们与dynamo db交互
所以我们可以设置表
我们可以管理表
但我们也可以看到我们可以操作
更新数据
检索数据
我们也可以以原子方式这样做
这样我们就可以确保数据完整性和数据一致性，使用这些事务api 希望对你有帮助，下次课见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/092_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p92 106 DynamoDB Accelerator (DAX).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈dynamo db加速器
这是一个用于dynamo db表的内存缓存服务
这是通过减少读取延迟来提供性能提升而设计的
这可以减少到毫秒级
这样我们也可以提高吞吐量
那么我们来分解一下
这意味着我们之前说过
这是一个内存缓存
因此还需要设置一个dynamo db加速器集群
因此需要配置并设置以启用此功能
所以鸭群的集群只是节点的集合或服务器
其中一台节点作为主要节点
然后我们可以添加额外的节点来扩展集群
并且像这样提高缓存容量
所以这里再次在内存缓存中
这意味着我们可以减少读取时间
所以从表中的项目的读取延迟
这又
然后依次
与直接从dynamo db读取相比，结果性能提高最多十倍
当我们使用一些直接读操作时
因此，与鸭子通信的应用程序
它们通过与集群关联的端点进行操作
它们不会直接与表本身通信
但我们有一个更快的读操作
因为数据不直接从表中检索
而是从内存缓存中检索
因此，我们可以选择哪些数据经常被使用
这样我们就可以从内存缓存中减少数据检索的延迟
然而，如果请求容量超过阈值
因此吞吐量容量
然后鸭子可能会限制那些多余的请求
这意味着它将暂时拒绝额外的请求
就像这样 这将向客户端发出信号，它正在超过当前允许的请求
正确，在这里我们可能想要
然后也实现一些重试逻辑来处理这样的异常
因此这将通过将限流异常传递给我们来指示
因此我们在这种情况下有几种不同的操作
首先 我们有读取操作
因此，当应用程序请求数据时
鸭子首先检查其缓存
如果数据
所以 例如 这个特定的项目在缓存中
所以它是缓存命中
然后鸭子会将数据立即返回给应用程序
如果数据不在缓存中
所以这将是一个缓存
鸭子错过
然后直接从dynamo db获取数据，所以从表中
然后返回给应用程序
同时也会更新缓存以供未来读取
所以像这样，我们通常可以获得
希望频繁访问的数据在这里可用，我们支持几项API调用
所以我们有批处理
获取项目
然后也有查询和扫描
这些都是读取操作
现在我们也有写入操作
当数据被写入时
这可以通过使用这些API调用完成
我们当然可以插入项目更新
项目 删除 等等
在这里
当数据被写入时
它将写入到dynamo db表中
然后这些更改将反映在鸭子集群中
所以像这样，我们确保缓存与表中的数据保持一致
所以总的来说，总结一下
这个鸭子 对于需要读操作具有高吞吐量和低延迟的应用程序非常有用
来自dynamo db
这使我们能够
或者这是通过分频访问数据启用的
像这样我们可以最小化响应时间和减少读取延迟 希望对你有帮助，下次再见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/093_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p93 107 Capacity Modes.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好吧，现在
让我们快速谈谈成本和性能
以及我们如何配置 DynamoDB
在这里我们需要讨论几件事
首先一个非常重要的事情是我们需要理解 Dynamo DB 可以配置的不同容量模式
这将决定成本
同时也会影响性能
它们适用于不同的用例
因此让我们谈谈这两种非常重要的容量模式
第一个是按需容量模式
所以这里
重要的是您需要按请求为基础构建读取和写入吞吐量 因此不需要指定吞吐量容量
我们将仅在此基本按需进行
因此这非常灵活
并且当不知道工作负载时非常有用
有时我们也可能会有更零散和突发的流量
而当我们有这些更大的峰值时
这更难预测
因此在这种情况下使用按需容量模式可能更合适
所以这里我们总是可以获得一个非常一致的
在规模上的极低延迟性能
即使我们有这些峰值
并且重要的是这里永远不会阻塞
所以这里请求永远不会阻塞
我们总是可以确保性能
当然这也很有用因为我们不需要担心任何 overhead
所以所有事情都将由我们处理并且我们总是可以确保性能
当然这会带来一个溢价
因此当我们有一个非常可预测的工作负载时
那么使用我们接下来将要讨论的另一种容量模式会更便宜
所以这会更贵
因此如果我们有一个可预测的工作负载
那么我们就不理想地使用按需
在这种情况下我们应该使用预留模式
所以这是为一个非常可预测的工作负载设计的
因为就在这里它是如何工作的
是我们需要指定每秒的读取和写入数量
一个所谓的读取能力单位
所以 rcu
以及写入能力单位 w cu 我们在稍后会讨论它们
所以我们将讨论这具体是什么意思
但这只是一个每秒读取和写入的容量测量
所以这就是更经济的模式
所以这是为更可预测的工作负载设计的
因为这里我们只支付指定通量的金额
这里我们也有一些特性可以使用以自动调整容量
以响应实际发生的流量模式
这可能有助于稍微优化成本
尽管这仍然没有按需模式那样灵活和全面管理
所以我们将要讨论的风险是稍后提到的
当我们没有足够的吞吐量预留时
实际吞吐量将超过我们预留的
那么我们将得到那些被限制的额外请求
这需要更仔细的容量规划
也需要监控这一点，以便我们可以避免这些性能问题
当然，这也需要更多的管理
所以这只是需要持续的监控，可能需要一些手动的缩放调整
如果我们不使用自动缩放
所以这需要更多的额外工作和更多的规划
但它可以节省成本，适用于更可预测的工作负载
现在我们也想谈谈与预留模式对齐的一些功能和可能性
例如，我们有预留容量
这基本上是一种我们可以用于预留模式的计费选项
所以，我们可以同意一个特定的容量量
这意味着s和w s在一年或三年的时间范围内
并且这样，为了换取这个承诺
我们基本上可以得到一个与按需或预留容量价格相比的折扣
所以，我们可以承诺一个价格
这使aws更容易规划
然后我们得到一个折扣 所以我们基本上
是的
预先支付
然后我们得到这些折扣 所以，这可以当然适合我们有非常稳定的应用
非常可预测的流量
并且我们可以确实可靠地长期预测容量
所以，这对于长期和非常稳定的工作负载来说，可以是一个非常经济的选择
现在，让我们快速讨论一下这个自动缩放功能
这又是可以在与预留容量模式结合的
所以，它就是一个动态的机制
根据一些指定的速率调整表的读写容量
所以，这有助于我们
当然
也优化成本
在这里，我们需要设置一些最小值 最大值和一些目标值
这仍然需要一些设置
所以我们可以控制这个
但最终，我们也可以优化成本
正如我之前所说
它不是像按需模式那样灵活
因为这里可能会有对流量变化的响应延迟
所以，这将需要基于历史进行调整
因此，这可能仍然导致限速
但是的确是
但是，是的
但是
相对较快
但这里也可能会有一点延迟
所以这是两种容量模式
现在 我们想更详细地谈谈在配置容量模式下我们需要指定的那些写入容量单位
和读取容量单位
所以我们在下一节课中详细讨论它们以更好地理解它们 因为这对于后续也很重要
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/094_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p94 108 Capacity modes & Auto-Scaling (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看不同的能力模式
简要说明 也以实际方式说明
当我们创建我们的表时
每个表都会以特定的能力模式创建
如果我们查看所有表
我们可以看到能力模式
在这个例子中，预留
所以读取能力模式是预留的
我们稍后会讨论这些单位
所以我们会谈论这些读取能力和写入能力单位
我们将在下次讲座中讨论这些
我们可以看到这是预留类型
在这里我们有这些单位
我们也可以修改这一点
例如，如果我
选择这个表
我可以转到操作并转到更新设置
我现在有这个概览
在这里我可以更改此模式
如果我转到附加设置
我可以在这里看到设置
在这里我看到这是预留模式
我可以更改它
所以我可以通过转到编辑来做到这一点
您可以看到，能力模式可以在表创建后随时更改
在这里我们也可以看到预留
管理并优化您的成本，通过提前分配特定的能力
然后我们只需为我们实际使用的支付费用
但这将支付一个相当高的价格
所以这将是预留模式的大约两倍价格
所以但是我们只需支付
当然，我们实际需要的费用
因此，如果我们有非常强烈的需求和工作量波动
那么我们也可以使用按需模式
如果我们无法很好地计划
否则，如果它更稳定且可计算
那么预留模式将更便宜
这里我们也有自动缩放功能
我们可以启用此功能
在这里我们可以指定最小数量和最大数量
这适用于读取和写入
我们也有这一目标利用率
这是自动缩放的指南
这是我们希望达到的目标
这是我们可以使用的
我们还有这个选项，用于对所有全局二级索引使用相同的读取能力设置
因为这些全局二级索引，它们也会有一个默认设置
默认情况下，这将使用至少相同的能力单位
这也将为全局索引可用
所以这已经创建了
他们也会有自己的专用容量
但它将使用相同的值，现在默认使用
同样的，我们也有关于右侧容量的东西
我们可以启用自动缩放并设置指定数量的
是的 最大值，最小值和目标值
或者我们只是分配我们想要的那些单位
在这里我们可以看到这一点
因为我们已经有这个全局索引
我们现在的读容量也比这里看到的要多
所以现在如果我更新这个
我也会看到这些变化
所以现在让我们使用这个读容量的自动缩放模式
让我快速保存这些更改
然后我们应该能看到这些变化
所以现在仍然在附加设置中
我可以在这里看到这张表的容量
读容量现在正在使用自动缩放
而对于读操作我们不使用自动缩放
所以我们可以看到这些值实际上在我的情况下，我在一和十之间
所以我们可以使用一个更低的
但这是可以接受的，所以70%
现在我们在这里看一下
我们可以看到这是为索引容量
在这里我们可以看到这些自动扩展活动
所以如果我更新这个或刷新这个
我应该很快就会有一些变化
因为默认值会在这里
设置得非常高
然后我们会缩小这个规模
因为我们目前没有使用任何东西
很快我们应该在这里看到一些活动
一些事件表明这个正在缩小
在我等了一会儿后只有几秒钟
实际上我可以看到这一事件正在发生
这现在表明在读取中
这是我们启用了自动缩放
我们将读取容量设置为1
因为我们不需要多于1的容量
所以1是最小值
因为我们目前没有读取任何东西
它会自动缩放到1
这就是自动缩放活动 希望对你有帮助，下次课见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/095_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p95 109 WCUs & RCUs.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好的，现在我们讨论了不同的能力模式
我们记得在配置能力模式下
我们需要指定写入能力和读取能力的单位
那么这是如何工作的
这实际上意味着什么
现在我们深入探讨
因为这对考试也很重要
首先，什么是写入能力单位
这只是代表每秒可以执行的写入吞吐量
在Dynamo DB
这是针对一个物品至一兆位大小的情况
所以1 wcu意味着每秒对一个至一兆位的物品进行一次写入
例如 如果你有一个0.8兆位的物品
那么这也会被向上舍入到1 wu
当然，如果我们有一个物品是
假设是3千兆位
那么这将代表3个w
所以这将被乘以
当然，如果我们有一个3.5兆位的物品
将被四舍五入到最接近的数字
所以3.5个项目将代表4个
所以我们只需要四舍五入
现在我们必须
在容量模式下根据预期的满载数量分配us的数量
所以这里可以
当然可以稍微过度配置
以便我们也能处理意外的峰值或一些不可预见的情况
是的 也许只是一些可能意外发生的情况
这样我们就可以避免限速
而这样当然就不会那么好
在这里我们可以选择超额配置
我们也可以使用自动扩展
这样我们就可以在这里更加灵活
当然这在成本方面很重要
因为我们直接为配置的w的数量支付费用
无论我们是否使用它
因此，容量规划对成本有直接的影响
再次 如果我们配置不足
这可能会导致限速
如果我们的操作正确，超出了配置的数量
我们的Dynamo DB将仅限速
额外的写入请求
我们不想这样
这将导致配置吞吐量超过的异常
因此，理解这一点非常重要
并且尝试预测我们的流量模式
如果我们做不到
那么也许更好使用按需容量
当然这会带来一点额外的费用
这样它也可以自动扩展，更容易
这是写入容量单位
现在让我们也谈谈读取容量单位
这相当相似
但这里当然我们测量读取容量
所以这里一个读取容量单位代表每秒一个强一致性读取
或者每秒两个最终一致性读取，适用于大小不超过4千字节的项目
但等一下
什么是强一致性读取，什么是最终一致性读取
所以让我们快速理解这是什么意思
当然，再次
如果我们有大项
那么我们就按照同样的方式乘以它
就像与正确的容量单位一样
但现在让我们理解这些强一致性和最终一致性的含义
这基本上决定了数据是多新
以及通过我们的读取操作检索的数据是多同步
因为我们知道这可能会有所不同
我们在谈论
当然关于分布式数据库
我们需要确保所有客户端看到相同的数据
这就是我们所说的读一致性
这里有两种不同类型的读取
第一种如前所述是最终一致性
这意味着你可能在某些地方得到稍微过时的数据
这可能不会反映在写操作中的所有更改
但当然这里有更好的性能
所以在写操作后立即可能发生
最终一致性读取可能不会反映这个更改
所以我们在某个地方有一个写操作
然后如果我们立即之后做一个最终一致性读取
这个更改可能不会反映
但我们有一个好处，就是更快
而且这里也可以同时处理更多的读取
因为它不需要等待所有数据更改的反映
所以这在数据不需要绝对最新时非常合适
例如我们可以等待几秒延迟查看最新更新
但我们可能
是的 我们可能想要更多的
是的 更好的表现
而且这也更便宜
因为我们只支付一半的价格
然后我们还有这些强一致性读取
这意味着你将总是得到最新的数据
这反映了所最近的更改
所以这里 我们保证读取会反映所有已成功的权利
在读取被发起之前
所以这里我们基本上只获取数据的最新版本
但这也有一定的缺点，因为它可能会影响性能
并且可能导致稍微高一点的延迟
并且我们可能通量会降低
因为现在系统必须等待所有数据副本被更新
在我们能够响应那个读取请求之前
所以这里只是有点不同
当然当我们想要非常高的准确性时
那么我们更倾向于选择那些强一致性读取
所以当我们有一些关键操作时
这可能是
是的 当需要总是表示最新数据时，这是更好的选择
所以这只是我们可以选择的东西
因此这也影响到读取能力单位
所以我们记得，这里我们有一个每四千比特大小的强一致性读取每秒一个
这意味着我们有一个读取能力单位
但现在一个读取能力单位意味着它允许两个最终一致性读取
所以基本上，最终一致性读取的价格
只是一半的价格，因为我们有双倍的吞吐量能力
因为我们可以更快、更容易地做到
所以这里，一次读取能力单位允许进行两次最终一致性读取
现在 让我们快速做一些计算，使这个概念稍微更易于理解
以实际的方式
因为对于考试你可能也需要这样做
所以假设你有一项需要每秒读取一次的物品
当然，这项物品的大小为8千字节
现在重试将是强一致性的
所以它将一个一致的读取等于一个读取能力单位
所以既然8是4的两倍
我们需要有两个rc的读取这个8千字节的项目
以一个强一致性的读取操作每秒
当然，这总是一个每秒的容量
所以这是一个简单的计算
再次，如果我们有多个项目
它也可能不是像8这样的一个数字
我们向上取整
这将需要我们十次
当然，由于我们这里有最终读取
我们只需要五次
所以这里需要向上取整到最近的四个
所以这里每个项目我们基本上有十次读取
然后，由于我们有每ru
我们有两个最终读取每rcu
我们只能拥有五次
这已经足够用三个千位来覆盖这些十个项目
这就是计算
你也可以这样思考
写下来 这很重要，我们需要在这里理解
这个概念
这就是写入容量单位和读取容量单位
结合那些示例计算
但我们已经谈论了限速
我们需要理解那是什么，以及如何对抗它
让我们详细讨论一些最佳实践
一些机制
可以用来提高性能，避免限速 这就是我们在下一节课要做的
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/096_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p96 110 Read Write Capacity (Hands-on).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈容量单位
所以在实践中写容量单位和读容量单位
所以这取决于我们是否使用按需模式
或者我们是否使用预留模式
所以实际上这些容量单位它们是专门为预留模式设计的
所以当我们创建一个新表
我们会看到如果我们选择自定义设置
然后我们转到预留模式
我们有这个容量计算器
我们可以看到我们需要多少容量单位
如果我们有这个吞吐量
我实际上建议去玩一下这个
并且去猜测或者估计或者计算读取容量单位
尝试自己去找出
以确保你知道这是如何在考试中计算的
例如
我使用 让我们说在这里使用一个值
这意味着我需要
如果我在这里使用强一致性
我需要两个单位
如果我在这里增加值
它会相应增加
所以像这样
你可以去玩一玩并且也做这些计算以确保你能找到正确的值
并且对于按需这些单位不是很必要
但是我们按请求付费
但是再次按需会更贵
如果我们有完全相同的
是的 完全相同的吞吐量
所以让我们快速看一下
所以这里如果我们看一下定价
所以这里有预留容量
并且我们记得在这里我们有这些容量单位
并且我们在之前就定义了他们
所以这就是他们的定义
并且在这种情况下总是每秒
所以这里我们有一个项的读取高达4千字节每秒
然后我们向下看
我们可以看到定价 当然你不需要从你的头脑中记住这个定价
在顶部
但是你可以只有个大概的想法
大致上我们在这里为每个预留容量单位付费
无论我们是否使用它
这与按需模式不同
所以这里我们按请求付费
所以我们没有一个固定的容量
但我们只是取决于我们实际使用了什么
所以我们在多少请求上有确切的数字
所以每写请求不超过1千比特
这以类似的方式定义
但不是以秒为单位
但在这种情况下是按请求计算
在这里我们按每百万请求付费
你可以进行计算
这不必要
但如果你有相同的吞吐量
并且这是一直保持的吞吐量
所以这将是 当然这是提供模式的理想情况
因为这里我们只是提供容量
然后我们为这容量付费
并且在这种理想情况下
这将大约比按需模式便宜7到8倍
但这取决于用例和工作负载的实际情况
所以如果我们有突然的增加或非常不稳定的工作负载
那么使用按需模式仍然可能是更有利的
现在我们看一下
例如 给定的表
让我们看一下
例如
这张表
我们可以看到每当我们探索我们的项目时
我们会执行一些读或写操作
就像我们刚才做的扫描一样
我们可以看到这消耗了
在这种情况下 仅仅0.5个读容量单位
当然 根据我们执行的操作
所以如果我们再运行一次
我们可以看到这是发生了什么 好的 希望这能给你一些背景信息，并在下一节课再见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/097_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p97 111 Hot Partitions & Throttling.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好吧 现在我们来谈谈性能和成本优化
以及我们可能遇到的性能问题
首先，我们要讨论的是称为热点分区的问题
热点分区
是接收读写请求显著高于其他分区的分区
与其他分区相比
我们可能会遇到访问模式或数据分布的不均匀
在某一个分区中
数据是最常被访问的
然后这将是一个热点分区
所以这可能不是一个很好的设计分区键
因为他们负责分配数据
这将导致一些问题
这将导致限速
什么是限速
当请求率超过表的或索引的配置吞吐量时，会发生这种情况
在这种情况下，它将暂时拒绝任何额外的请求
它们无法处理
所以这将是 当然，我们想要避免的事情
因为，是的 这将导致延迟增加或请求失败
这会导致
当然 会影响我们的应用，所以绝对是需要避免的事情
但现在我们如何做好这一点
当然 首先，我们可以通过设计分区键的方式
使得数据均匀分布
当然 或者像调整访问模式
这样在所有的分区中数据被访问得大致相同
这样我们不会有热点，分区运行过热
所以，我们也可以作为额外的策略使用指数后退
所以，我们可以将其实现为重试逻辑
以便更有效地处理那些限速请求
所以，这将只是一个重试逻辑
所以，请求被拒绝后
我们稍作等待
这就是指数级后退
我们基本上每次被拒绝的请求
我们会等更长的时间
这是一个策略
这是一个重试机制，用来对抗这个问题
避免我们因此而出现问题
这是我们可以使用的策略
当然我们应该总是监控
我们可以使用 当然
云监控，只是为了查看指标
然后根据这些调整预留的吞吐量，当这是有必要的时候
并且，我们还可以为读密集型应用
使用堆栈 这样当然会缓存频繁访问的数据
这样我们就可以减少对我们的表的读取
所以这也是一个有用的策略
现在，还有一个机制，也被用来处理这个问题
或者我们有两个机制
所以我们有突发能力
这并不是特别针对热点分区
但这更多是关于总体的限速
这意味着，Dynamo DB提供了突发能力
这样允许表在不限速的情况下，容纳读或写的短峰值吞吐量
这是如何工作的，未使用的容量可以在5分钟内积累
然后我们可以启用短突发，超出我们的预留吞吐量获取一些容量
我们可以基本保存它
然后在这5分钟内使用它
当然我们不能无限期地保存它
比如保存10天，保存所有
然后有巨大的峰值
但我们至少可以保存它，最多5分钟，只是为了适应这些峰值
然后我们还有自适应能力
这会自动重新分配表的预留吞吐量，以适应不均匀的访问模式
当我们有热点分区时
它会重新分配容量到不同的分区
这样帮助在访问模式不平均的情况下，保持性能
这样我们就可以避免来自热点分区的问题
当然，我们还是应该实施并遵循最佳实践
但这里这只是一个机制，帮助一点防止热点分区
所以，当然我们仍然应该实施并遵循最佳实践
但这里这只是一个机制，帮助一点防止热点分区 所以这些是突发能力和自适应能力
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/098_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p98 112 Time To Live.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


好吧 让我们快速讨论一个非常有用的功能
即管理我们数据生命周期的ttl
ttl代表时间到生命
这是一个我们可以启用的功能，以便自动从表中删除项
一旦它们达到一定年龄
所以让我们快速分解这是如何工作的
如前所述，一旦我们启用了这个功能
我们可以为项设置过期时间
然后一旦项的时间戳表明该项现在已过期
超过过期时间
dynamodb将自动安排其进行删除
所以记住，它只是安排了删除
我们有这个48小时的窗口
一旦项被标记为过期
它不会立即被删除
但在48小时内
这给您提供了一个缓冲期，在最终删除发生前
并且由于ttl引起的删除过程
不消耗任何表的预留通过率
这意味着您不会为那些删除支付费用的任何预留容量单位
这也使其成为一种非常有效的清理方法
这也是我们需要考虑的事情
并且更新仍然可以发生
一旦项被ttl安排进行删除
我们可以更新它
例如
当我们删除这个ttl属性时
项将不会被删除
它还可以被更新
这也是我们可以记住的事情
时间戳格式
就是我们需要指定在这里的过期时间
以Unix纪元格式
这是自1970年1月1日零时零秒以来经过的秒数
这是我们需要提供的格式
并且非常重要的是记住
当项被ttl过程删除时
这个操作也可以记录在我们的dynamodb流中
如果我们在我们的dynamodb表中启用了这个
流
然后删除操作将作为删除操作出现在我们的流中
并且可以特别识别为ttl删除
这样我们可以允许下游应用程序或过程，如aws
Lambda函数仍然对ttl删除做出反应
就像对其他删除操作一样
例如 我们可以
将数据存档到某个地方，可能更便宜
或者当我们想要与之进行下游处理时
所以这就是ttl的有用功能
这可以帮助你自动删除过时的数据
这可以帮助你管理存储成本 希望对你有帮助，下次课见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/099_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p99 113 TTL (Hands-On).ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来看看这个特性
在实际操作中
看看如何根据物品的时间或年龄自动删除物品
这是如何工作的
我们可以在我们的数据中为我们的项目添加一个属性
基本上例如
我们可以有一个过期日期的属性
然后我们可以根据这个属性设置ttl
例如这个过期日期
例如
我们有这个作为一项好处，因为这样我们可以自动清理我们的表格
这不仅节省了我们的存储成本
而且它也提高了我们的性能和读取和写入的成本
因为我们的表格中数据较少
这显然也会减少需要扫描的数据量
当我们扫描我们的表格时
因此我们可以提高性能并降低成本
因此这可以自动清理我们的数据非常有益
让我们看看这是如何工作的，在这里
假设我们有这个书籍表格
这就是我们想要使用的属性
为了便于理解，在我们的例子中
它非常描述性
我们使用失效日期这个属性
这非常描述性
现在我们从这个表中获取一个新项目
在这种情况下，我们可以给它一个id
所以这里的id可以是任何数字
我们还想在这里添加一个属性
这将是数字
这就是我们的纪元时间戳
我们需要知道这个
我们有一些转换器
因为我们可能不知道如何转换它
所以我们只想使用当前的纪元时间
这是一个数字
例如我可以使用这个
这是当前时间
这需要在这个格式上对齐
所以我只是复制它
现在我将此作为属性添加
我将使用数字
然后我们有了这个
我们将此称为过期日期
正如我们所提到的
这非常描述性
但这并不重要
名称并不重要 我们称之为过期日期像这样
所以当我们稍后使用ttl时
当我们设置这个时 当我们设置时，我们必须使用相同的名称
我们将在几秒钟内看到这一点
所以现在让我们在这里设置
这个值 这基本上是几秒钟前在这个纪元格式中发生的
所以这是 当然，正如我们在这里看到的，它每秒都在变化
现在，这就是这个物品应该被删除的时间
所以现在让我们看看
我们可以创建这个
所以现在这个物品在那里
我们看到这个物品有这个属性
当然，如果物品中没有这个属性
那么它将永远不会根据我们将要设置的 tdl 删除
这意味着它只会考虑那些有这个属性的物品
所以现在让我们设置这个
因为我们到目前为止还没有设置任何东西，只是设置了属性
在我们的数据中，过期日期
所以现在让我们回到我们的表格
如果我们在这里转到表格详细信息
在附加设置下
我们有这个选项为 ttl
所以我们向下滚动
目前我们看到这个被关闭
所以现在让我们开启它
这就是我们提到的
我们需要指定这个属性
我们存储 tdl 时间戳的属性
根据我们在这个属性中指定的值
例如 在我们的情况下，过期日期
一旦这个值超过了当前时间
它将自动删除这个物品
在这里我们需要指定这个属性名称
在我的情况下，这将是过期日期
我不确定这是否正确
但让我们试试
因为我们这里有这个预览
在这里我们可以简单地模拟时间
然后点击运行预览
然后它将根据这个模拟的时间显示我们
哪些物品将被删除
现在我很惊讶
因为它应该显示一些物品
现在如果我减少时间
它仍然没有显示任何物品
因此我现在将做的事情是
我将检查我是否确实使用了正确的属性名称
所以我将回到我的表格
在这里，让我仔细看看这张桌子的属性
我确实没有抓住正确的名称
所以这不是到期日期
而是实际到期日期
这非常重要
现在如果我运行预览
确实显示了应该删除的项
这就是它如何工作的
现在我们可以测试这是否像我们预期的那样工作
像这样我们现在将其打开
现在会自动删除那些到期日期已经过期的项
所以这是tdl属性
这个属性的值是当这被超过时
会导致该项被删除
基于我们设置好的tdl
在我们这个案例中，让我们快速探索一下项
这就是在这里的一个
所以记住这必须在epoch格式中
因此它必须是一个数字
然后它会自动删除这项
这可能需要长达四八小时
所以这可能需要一点时间
当然，我们的表格越大
这可能也需要更多的时间，所以请记住这一点
而且这不会立即这样做
在我们这个案例中，即使它还在那里
所以即使我们刷新
这应该相对较快
但这可能需要一点时间
这将在四十八小时内完成
而且这也很重要
不会使用我们的权利容量
但它只会使用实际的突发容量
这也是值得知道的事情
现在我们应该已经看到了我们如何成功设置这，并且这是如何工作的 希望对你有帮助，下次讲座见
```

### /content/drive/MyDrive/bilibili/Udemy-CompleteAWSCertifiedDataEngineerAssociate-DEA-C01part1/100_Udemy - Complete AWS Certified Data Engineer Associate  - DEA-C01 part1 p100 115 Redshift Overview.ai-zh.srt

```
【角色设定】
你是一位精通 AWS 认证知识,Snowflake,Airflow的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake和Airflow等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS 等技术课程自动机翻而来，存在以下常见问题：
- AWS 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---

### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS 等专业术语，使其与 AWS 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


现在我们来谈谈亚马逊红移
这在考试中是一个非常重要的话题
因此我们将深入探讨这一点
我们将一步一步来做
首先，亚马逊红移是什么
这是一个完全管理的PB级数据仓库服务
这就是它用于分析目的的数据仓库
所以本质上 当然
像这样 它也是一个与ncc qa兼容的关系型数据库核心
用于数据仓库目的
所以这里是用于分析目的的使用案例
当然，它支持所有命令等等
关键之处在于它提供了非常快的性能
为此，您可以使用基于安全的工具和商业智能应用程序
所以为了加载数据并开始查询它
使用亚马逊红移
您可以使用红移查询编辑器v2或任何其他商业智能工具
因此您可以将红移连接到那些商业智能工具
使用红移作为底层数据仓库
关键在于它被处理
或者它设计用于处理大量数据
具有非常高的性能和可扩展性
因此当数据越来越多
我们可以在红移中处理它
我们可以加载多种格式的数据
我们可以有csv json
也可以列基的parquet
或者也是o c redshift是
正如我所说 设计得非常容易扩展
这是通过使用集群实现的
这样您可以根据数据需求和增长要求调整集群
稍后我们会详细讨论这些集群
稍后会详细介绍
Redshift将数据存储为列格式
所以它不像一个OLTP数据库
在那里我们可能会有大量的写入操作
这可能是数据在生产环境中生成和写入的地方
但我们在这里将其用于分析目的
因此，一切都为了这种分析读取性能进行了优化
其中提到的一点是数据以列形式存储
这优化了查询性能
通过减少I/O操作和压缩比来实现
用简单的话来说
当你在分析方式下想要分析数据库中的数据时
你不想一次性访问所有可能的
让我们说一百列
在一次查询中
但是你通常想要拥有两到三列
或者也展示两到三列
如果以行为基础存储
那么你必须扫描所有不同的列
因为它以行为基础存储
像这样
你可以只说我想要这列或那列
你可以减少表扫描
这可以极大地提高此类读取查询的性能
此外
Also redshift 使用大规模的并行处理
这对我们的读性能也非常有益
所以这里是分发的
或者 它有这种架构，允许将数据和查询分发
到集群中的多个节点
我们稍后会讨论这一点
所以像这样 我们可以实现这些
是的 高性能并行执行
它还与其他服务集成
例如与S3存储桶
DynamoDB AWS数据库
Glue和Lambda
这使得数据导入变得容易
我们还可以转换和分析数据，并且所有内容都集成在一起
然后与红移
当然我们有非常先进的压缩
这也有助于不仅减少存储
但是它也有助于性能提升
所以你可以看到一切都是为了我们的分析目的进行了优化
Redshift 也支持在存储中加密
在传输数据库级别对象访问控制
我们使用iam角色和策略进行精细的访问控制
我们也可以使用vpc安全组
因此也是如此 当然
在数据仓库中，安全性现在是重要的
让我们看看具体的使用案例，我们可以使用这个
我们已经提到过
这包括所有数据分析的使用案例
基本上 因此，这可以将其用作数据仓库
我们可以用它进行商业智能目的
我们可以有不同的BI工具使用红移作为数据源
我们可以做像日志分析这样的事情
物联网数据处理
或者一些实时数据仪表板
是的 可能展示一些非常高的东西
高度时间敏感
所有这些都可以用红移来实现
现在我们有了这个概述
让我们更深入地了解红移的架构 以便我们理解它是如何构建的以及我们如何使用它
```